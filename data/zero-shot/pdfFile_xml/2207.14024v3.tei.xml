<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Letian</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Autonomous driving</term>
					<term>sensor fusion</term>
					<term>transformer</term>
					<term>safety</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns. On the one hand, comprehensive scene understanding is indispensable, a lack of which would result in vulnerability to rare but complex traffic situations, such as the sudden emergence of unknown objects. However, reasoning from a global context requires access to sensors of multiple types and adequate fusion of multi-modal sensor signals, which is difficult to achieve. On the other hand, the lack of interpretability in learning models also hampers the safety with unverifiable failure causes. In this paper, we propose a safety-enhanced autonomous driving framework, named Interpretable Sensor Fusion Transformer (InterFuser), to fully process and fuse information from multimodal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection. Besides, intermediate interpretable features are generated from our framework, which provide more semantics and are exploited to better constrain actions to be within the safe sets. We conducted extensive experiments on CARLA benchmarks, where our model outperforms prior methods, ranking the first on the public CARLA Leaderboard. Our code will be made available at https://github.com/opendilab/InterFuser.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, rapid progress has been witnessed in the field of autonomous driving, while the scalable and practical deployment of autonomous vehicles on public roads is still far from feasible. Their incompetence is mainly observed in high-traffic-density scenes, where a large number of obstacles and dynamic objects are involved in the decision making. In these cases, currently deployed systems could exhibit incorrect or unexpected behaviours leading to catastrophic accidents <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. While many factors contribute to such safety concerns, two of the major challenges are: 1) how to recognize rare adverse events of long-tail distributions, such as the sudden emergence of pedestrians from road sides, and vehicles running a red light, which require a better understanding of the scenes with multi-modal multi-view sensor inputs; 2) how to verify the decision-making process, in other words, identify functioning/malfunctioning conditions of the system and the causes for failures, which requires interpretability of the decision-making system. Safe and reliable driving necessitates comprehensive scene understanding. However, a single sensor generally cannot provide adequate information for perceiving the driving scenes. Single-image approaches can hardly capture the surrounding environment from multiple perspectives and cannot provide 3D measurements of the scene, while single-LiDAR approaches cannot take semantic information such as traffic lights into account. Though there are existing works fusing information from multiple sensors, they either match geometric features between image space and LiDAR projection space by locality assumption <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, or simply concatenate multi-sensor features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The interactions and relationships between multi-modal features are seldomly modeled, such as the interactions between multiple dynamic agents and traffic lights, or features in different views and modalities. To encourage reasoning with a global context, the attention mechanism of Transformer <ref type="bibr" target="#b6">[7]</ref> is utilized. The recent TransFuser <ref type="bibr" target="#b7">[8]</ref> adopts internal feature propagation and aggregation via a multi-stage CNN-transformer architecture to fuse bi-modal inputs, which harms sensor scalability and is limited to fusion between LiDAR and a single-view image. In this paper, we take a onestage architecture to effectively fuse information from multi-modal multi-view sensors and achieve significant improvement. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we consider LiDAR input and multi-view images (left, front, right, and focus) as complementary inputs to achieve comprehensive full-scene understanding. On the other hand, existing end-to-end driving methods barely have a safety ensurance mechanism due to the lack of interpretability of how the control signal is generated. To tackle such a challenge, there are efforts to verify the functioning conditions of neural networks instead of directly understanding the models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Though being helpful for choosing different models for different conditions, these methods still lack feedback from the failure causes for further improvement. Inspired by humans' information collecting process <ref type="bibr" target="#b11">[12]</ref>, in addition to generating actions, our model also outputs intermediate interpretable features, which we call safety mind map. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the safety mind map provides information on surrounding objects and traffic signs. Unveiling the process of perceiving and decision-making, our model is improvable with clear failure conditions and causes. Moreover, by exploiting this intermediate interpretable information as a safety constraint heuristic, we can constrain the actions within a safe action set to further enhance driving safety.</p><p>In this paper, we propose a safety-enhanced driving framework called Interpretable Sensor Fusion Transformer (InterFuser), in which information from multi-modal multi-view sensors is fused, and driving safety is enhanced by providing intermediate interpretable features as safety constraint heuristics. Our contributions are three-fold:</p><p>1. We propose a novel Interpretable Sensor Fusion Transformer (InterFuser) to encourage global contextual perception and reasoning in different modalities and views.</p><p>2. Our framework enhances the safety and interpretability of end-to-end driving by outputting intermediate features of the model and constraining actions within safe sets.</p><p>3. We experimentally validated our method on several CARLA benchmarks with complex and adversarial urban scenarios. Our model outperformed prior methods, ranking the first on the public CARLA Leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>End-to-end autonomous driving in urban scenarios The research of end-to-end auto-driving based on the simulator of urban scenarios has become the mainstream. The topic starts from the development of an urban driving simulator: CARLA <ref type="bibr" target="#b12">[13]</ref>, together with Conditional Imitation Learning (CIL) <ref type="bibr" target="#b2">[3]</ref> proposed. A series of work follow this way, yielding conditional end-to-end driving <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> in urban scenarios. LBC <ref type="bibr" target="#b15">[16]</ref> proposed the mimicking methods aimed at training image-input networks with the supervision of a privileged model or squeezed model. Chitta et al. <ref type="bibr" target="#b16">[17]</ref> presents neural attention fields to enable the reasoning for end-to-end driving models. Imitation learning(IL) approaches lack interpretability and their performance is limited by their handcrafted expert autopilot. Hence, researchers develop Reinforcement Learning (RL) agents to interact with simulated environments. Latent DRL <ref type="bibr" target="#b17">[18]</ref> generates intermediate feature embedding from a top-down view image by training a variational auto-encoder. With the aforementioned mimicking tricks, Roach <ref type="bibr" target="#b18">[19]</ref> trained an RL-based privileged model as the expert agent to provide demonstrations for the IL agent. Toromanoff et al. <ref type="bibr" target="#b5">[6]</ref> proposes to use hidden states which is supervised by semantic information as the input of RL policy.  <ref type="figure">Figure 2</ref>: Overview of our approach. We first use CNN backbones to extract features from multi-modal multiview sensor inputs. The tokens from different sensors are then fused in the transformer encoder. Three types of queries are then fed into the transformer decoder to predict waypoints, object density maps and traffic rules respectively. At last, by recovering the traffic scene from the predicted object density map and utilizing the tracker to get the future predictions of other objects, a safety controller is applied to ensure safe and efficient driving in complex traffic situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer model in vision comprehension</head><p>Transformer was originally established in Natural Language Processing (NLP) <ref type="bibr" target="#b6">[7]</ref>. The attention mechanism demonstrates to be a powerful module in image processing tasks. Vision Transformer (ViT) <ref type="bibr" target="#b19">[20]</ref> computes relationships among pixel blocks with a reasonable computation cost. Later generations move on to generalize Transformer to other computer vision tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">24]</ref> or digging deeper to perform better <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>. The attention mechanism brings a new entry point for modality fusion. TransformerFusion <ref type="bibr" target="#b27">[27]</ref> reconstruct 3D scenes that takes monocular video as input with a transformer architecture. TransFuser <ref type="bibr" target="#b7">[8]</ref> exploits several transformer modules for the fusion of intermediate features of front view and LiDAR. However, such a sensor-pair intense fusion approach hardly scales to more sensors, while information from side views like an emerging pedestrian, and a focus view like the traffic light, are critical for scene understanding and safe driving.</p><p>Safe and interpretable driving Safety has long been studied in the traditional control community. However, in autonomous driving, uncertain behaviors <ref type="bibr" target="#b28">[28]</ref> and diverse driving preferences of drivers <ref type="bibr" target="#b29">[29]</ref> deteriorate the safety concern. Some works take the uncertain and heterogeneous nature of humans into the formulation of control methods for safety assurance <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>. However, these rule-based methods hardly scale to complex environments due to the extensive human labor required. When learning-based methods arise, their lack of interpretability becomes a new puzzle in the way. There are efforts taking a bypass, verifying the functioning conditions of neural network models instead of directly understanding them <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. However, feedback on the failure causes and solutions are still wanted. Recently, some works design auxiliary tasks to output interpretable semantic features <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>, which is showing a great improvement on both the performance and interpretability. In this paper, we output the intermediate interpretable features for more semantics, which effectively enhances the driving safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>As illustrated in <ref type="figure">Fig. 2</ref> Output representations Our model generates two types of outputs: safety-insensitive and safetysensitive outputs. For safety-insensitive outputs, InterFuser predicts a path with L = 10 waypoints for the ego vehicle to steer toward. It guides the future driving route of the ego vehicle. However, driving along the path without a proper speed might be unsafe and violate actual traffic rules. Therefore, InterFuser additionally predicts the safety-sensitive outputs, consisting of an object density map and traffic rule information. The object density map M ? R R?R?7 provides 7 features for detected objects, such as vehicles, pedestrians and bicycles. M i,j indicates an 1m ? 1m grid area indexed by spatial coordinates (i, j) where the ego vehicle is taken as the origin and the y-axis is the forward direction. So the map covers R meters in front of the ego vehicle and R 2 meters on its two sides. The 7 channels of the map are the probability of the existence of an object, 2-dimensional offset from the center of the 1m ? 1m grid, the size of the object bounding box, the object heading, and the object velocity. Besides, InterFuser also outputs the traffic rule information, including the traffic light state, whether there is a stop sign ahead, and whether the ego vehicle is at an intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model architecture</head><p>Backbone For each image input and LiDAR input I ? R 3?H0?W0 , we use a conventional CNN backbone ResNet <ref type="bibr" target="#b36">[36]</ref> to generate a lower-resolution feature map f ? R C?H?W . We set C = 2048 and (H, W ) = ( H0 32 , W0 32 ) in experiments. Transformer encoder For feature map f of each sensor input, we first take a 1?1 convolution to obtain a lower-channel feature map z ? R d?H?W . The spatial dimension of each feature map z is then collapsed into one dimension, resulting in d ? HW tokens. Fixed 2D sinusoidal positional encoding e ? R d?HW is added to each token to retain positional information in each sensor input, and learnable sensor embedding s ? R d?N is added to distinguish tokens from N different sensors:</p><formula xml:id="formula_0">v (x,y) i = z (x,y) i + s i + e (x,y)<label>(1)</label></formula><p>Where z i represents the tokens extracted from i-th view, x, y denotes the token's coordinate index in that sensor. Finally we concatenate the tokens from all sensors, which are then passed through a transformer encoder with K standard transformer layers. Each layer K consists of Multi-Headed Self-Attention <ref type="bibr" target="#b6">[7]</ref>, MLP blocks and layer normalisation <ref type="bibr" target="#b37">[37]</ref>.</p><p>Transformer decoder The decoder follows standard transformer architecture, transforming some query embeddings of size d using K layers of multi-headed self-attention mechanisms. Three types of queries are designed: L waypoints queries, R 2 density map queries and one traffic rule query.</p><p>In each decoder layer, we employ these queries to inquire about the spatial information from the multi-modal multi-view features via the attention mechanism. Since the transformer decoder is also permutation-invariant, the above query embeddings are the same for the decoder and can not produce different results. To this end, we add learnable positional embeddings to these query embeddings.</p><p>The results of these queries are then independently decoded into L waypoints, one density map and the traffic rule by the following prediction headers.</p><p>Prediction headers The transformer decoder is followed by three parallel prediction modules to predict the waypoints, the object density map and the traffic rule respectively. For waypoints prediction, following <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b9">10]</ref>, we take a single layer GRU <ref type="bibr" target="#b39">[39]</ref> to auto-regressively predict a sequence of L future waypoints {w l } L l=1 . The GRU predicts the t-th waypoint by taking in the hidden state from step t ? 1 and the t-th waypoint embedding from the transformer decoder. Note that we first predict each step's differential displacement and then recover the exact position by accumulation. To inform the waypoints predictor of the ego vehicle's goal location, we initialize the initial hidden state of GRU with a 64-dimensional vector embedded by the GPS coordinates of the goal location with a linear projection layer. For the density map prediction, the corresponding R 2 d-dimensional embeddings from the transformer decoder are passed through a 3-layer MLP with a ReLU activation function to get a R 2 ? 7 feature map. We then reshape it into M ? R R?R?7 to obtain the object density map. For traffic rule prediction, the corresponding embedding from the transformer decoder is passed through a single linear layer to predict the state of traffic light ahead, whether there is a stop sign ahead and whether the ego vehicle is at an intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The loss function is designed to encourage predicting the desired waypoints (L pt ), object density map (L map ), and traffic information (L tf ):</p><formula xml:id="formula_1">L = ? pt L pt + ? map L map + ? tf L tf ,<label>(2)</label></formula><p>where ? balances the three loss terms. Readers can refer to Appendix A for detailed description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Safety Controller</head><p>With the waypoints and intermediate interpretable features (object density map and traffic rule) output from the transformer decoder, we are able to constrain the actions into the safe set. Specifically, we use PID controllers to get two low-level actions. The lateral steering action aligns the ego vehicle to the desired heading ? d , which is simply the average heading of the first two waypoints. The longitudinal acceleration action aims to catch the desired speed v d . The determination of v d needs to take surrounding objects into account to ensure safety, for which we resort to the object density map.</p><p>The object in a grid of the object density map M ? R R?R?7 is described by an object existence probability, a 2-dimensional offset from the grid center, a 2-dimensional bounding box and a heading angle. We recognize the existence of an object in a grid once one of the following criteria is met: 1) if the object's existence probability in the grid is higher than threshold 1 . 2) if the object existence probability in the grid is the local maximum in surrounding grids and greater than threshold 2 (threshold 2 &lt; threshold 1 ). While the first rule is intuitive, the second rule is set to recognize the existence of an object with high position uncertainty. In addition to the current state of objects, the safety controller also needs to consider their future trajectory. We first design a tracker to monitor and record their historical dynamics. We then predict their future trajectory by propagating their historical dynamics forward in time with moving average.</p><p>With the recovered surrounding scene and future forecasting of these objects, we then can obtain s t , the maximum safe distance the ego-vehicle can drive at time step t. The desired velocity v d with enhanced safety is then derived by solving a linear programming problem. Note that to avoid attractions of unsafe sets and future safety intractability, we also augment the shape of objects, and consider the actuation limit and ego vehicle's dynamic constraint. For details of desired velocity v d optimization, please refer to Appendix B. Besides the object density map, the predicted traffic rule is also utilized for safe driving. The ego-vehicle will perform an emergency stop if the traffic light is not green or there is a a stop sign ahead.</p><p>Note that while more advanced trajectory prediction methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref> and safety controller <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b43">43]</ref> can be used, we empirically found our dynamics propagation with moving average and linear programming controller sufficient. In case of more complicated driving tasks, those advanced algorithms can be easily integrated into our framework in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We implemented our approach on the open-source CARLA simulator with version 0.9.10.1, including 8 towns and 21 kinds of weather. Please refer to Appendix C for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset collection</head><p>We ran a rule-based expert agent on all 8 kinds of towns and weather with 2 FPS, to collect an expert dataset of 3M frames (410 hours). Note that the expert agent has the access to the privileged information in the CARLA simulator. For the diversity of the dataset, we randomly generated different routes, spawn dynamic objects and adversarial scenarios.</p><p>Benchmark We evaluated our methods on three benchmarks: CARLA public leaderboard <ref type="bibr" target="#b44">[44]</ref>, the Town05 benchmark <ref type="bibr" target="#b7">[8]</ref> and CARLA 42 Routes benchmark <ref type="bibr" target="#b16">[17]</ref>. In these benchmarks, the ego vehicle is required to navigate along predefined routes without collision or violating traffic rules in existence of adversarial events 1 . At each run, the benchmark randomly spawns start/destination points, and generates a sequence of sparse goal locations in GPS coordinates. Our method uses these sparse goal locations to guide the driving without manually setting discrete navigational commands (go straight, lane changing, turning). Please refer to Appendix D for detailed descriptions of the three benchmarks.</p><p>Metrics Three metrics introduced by the CARLA LeaderBoard are used for evaluation: the route completion ratio (RC), infraction score (IS), and the driving score (DS). Route completion is the percentage of the route completed. The infraction score is a performance discount value. When the ego-vehicle commits an infraction or violates a traffic rule, the infractions score will decay by a corresponding percentage. The driving score is the product of the route completion ratio and the infraction score, and thus is a more comprehensive metric to describe both driving progress and safety.  <ref type="bibr" target="#b44">[44]</ref> (accessed Jun 2022). All three metrics are higher the better. Our Interfuser ranks first on the leaderboard, with the highest driving score, the second highest route completion, and the second highest infraction score. <ref type="table">Table 1</ref> compares our method with top 10 state-of-the-art methods on the CARLA leaderboard <ref type="bibr" target="#b44">[44]</ref>. TCP is an anonymous submission without reference. LAV <ref type="bibr" target="#b45">[45]</ref> trains the driving agent with the dataset collected from all the vehicles that it observes. Transfuser <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">46]</ref> is an imitation learning method where the agent uses transformer to fuse information from the front camera image and LiDAR information. The entries "Latent Transfuser" and "Trans-fuser+" are variants of Transfuser. GRIAD <ref type="bibr" target="#b47">[47]</ref> proposes General Reinforced Imitation which combines benefits from exploration and expert data. Rails <ref type="bibr" target="#b49">[49]</ref> uses a tabular dynamicprogramming evaluation to compute actionvalues. IARL <ref type="bibr" target="#b5">[6]</ref> is a method based on a modelfree reinforcement-learning. NEAT <ref type="bibr" target="#b16">[17]</ref> proposes neural attention fields which enables the reasoning for end-to-end imitation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to the state of the art</head><p>Our InterFuser ranks first on the leaderboard, with the highest driving score 76.18, the second highest route completion 88.23, and the second highest infraction score 0.84. We also compared our method with other methods on the Town05 benchmark and CARLA 42 Routes benchmark. As shown in <ref type="table" target="#tab_7">Table 4</ref> and <ref type="table" target="#tab_8">Table 5</ref> of the appendix, our method beats other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>On the Town05 Long benchmark, we investigated the influence of different sensor inputs, sensor/position embedding, sensor fusion approach, and safety controller. In addition to three metrics, we also present infraction details for comprehensive analysis. <ref type="table" target="#tab_4">Table 2</ref>, we evaluated the performance when different combinations of sensor inputs are utilized. In the benchmark, adversarial events such as emerging pedestrians or bikes from occluded regions are very common. Since the baseline F only uses the front RGB image input, it is hard to notice obstacles on the sides of the ego vehicle. Therefore, though the baseline F achieved a high route completion ratio, it significantly downgraded in the driving score and infraction score. When the left and right cameras are added, F + LR achieved a higher driving score and infraction score with effectively reduced collision rate. Since traffic lights are located on the opposite side of the intersection, they are usually too far to be detectable in the original front image. Adding a focusing view for distant sight, F + LR + F c reduced the red light infraction rate by 80% compared to F + LR, resulting in a higher driving score and infraction score. Equipped with another Lidar input for additional 3D context, our Interfuser F + LR + F c + Li further reduced the collision rate and red light infractions, leading to the highest driving score and infraction score .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensor inputs As in</head><p>Sensor Embedding and Positional Encoding In InterFuser, we added the sensor embedding and the positional encoding to help the transformer distinguish tokens in different sensors and positions within one sensor. As in <ref type="table" target="#tab_5">Table 3</ref>, when these operations are removed, the ablation "No sensr/pos embd" had the driving score and the infraction score dropped by 3% and 6% respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensor Fusion Methods</head><p>In <ref type="table" target="#tab_5">Table 3</ref>, we also evaluated the performance when different sensor fusion methods are applied. There are two common methods to fuse multi-view or multi-modal inputs: 1) directly concatenating three RGB views along the channel dimension, as in LBC <ref type="bibr" target="#b4">[5]</ref> and IA <ref type="bibr" target="#b5">[6]</ref>. This ablation "Concatenated input" is assumed to insufficiently fuse information and turned out to have the driving score and infraction score lower by 58% and 63%. 2) multi-stage fusion of images and LiDAR by transformer as in TransFuser <ref type="bibr" target="#b7">[8]</ref>. Though applying the attention mechanism, this method does not consider multi-view fusion and can have problems processing multi-view multi-modal inputs. We implemented this approach by designing a mask to disallow attention between different views in the transformer encoder, resulting in drops of 9% and 15% on the driving score and the infraction score.</p><p>Safety-enhanced controller As in <ref type="table" target="#tab_5">Table 3</ref>, when the safety controller is removed and our model directly outputs waypoints, the driving score and the infraction score dropped significantly by 15%. and 22%. Besides, different driving preferences can be generated by assigning different safety factors in the safety controller. As in <ref type="figure">Fig. 4</ref> of Appendix B, the agents behaves more conservatively with a higher safety factor, leading to higher driving score and infraction score, and lower route completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>In <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>, we visualized two cases to show how our method predicts the waypoints and recovers the surrounding scene. In <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>, we also visualize the attention weights on 20 ? 20 object density map in the transformer decoder, to show how our method aggregates information from multiple views. As in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>, our model extracts features from different views for queries at different grids of the object density map. For example, the queries on the left of the map mostly attend to the features from the left views. More case visualizations can be found in <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 6</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitation</head><p>Though currently ranking the first on public CARLA Leaderboard, our method still has traffic infractions. Visualization of failure cases can be found in <ref type="figure">Fig. 7</ref> in the appendix. Currently, we used two-threshold criteria for detection, and dynamic propagation with moving average for trajectory prediction. In the future, more advanced detection and prediction models can be used. Extending the current deterministic form to a probabilistic form can also help to deal with the uncertainty and multi-modality in detection and prediction. Besides, all this work happened in simulation, where driving scenarios are limited. On actual roads where countless driving situations exist, enhancing the generalizability of our method would be vital for scalable deployment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present InterFuser, a new design for autonomous driving based on an interpretable sensor fusion transformer. The driving agent obtains a perception of global context from multi-view multimodal inputs and generates some interpretable intermediate outputs. Using the interpretable outputs and safety-enhanced controller, InterFuser sets the new performance upper-bound in complex and adversarial urban scenarios on the CARLA. Importing temporal information into the pipeline is our future work. With the historical frames, the model can get a more accurate and stable prediction for the position and future trajectory of the other objects. Given that our method is flexible and generic, it would be interesting to explore it further with different types of interpretable intermediate outputs, or change the controller implemented in another way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Loss Function Design</head><p>The loss function of our method is designed to encourage predicting the desired waypoints (L pt ), object density map (L map ), and traffic information (L tf ):</p><formula xml:id="formula_2">L = ? pt L pt + ? map L map + ? tf L tf ,<label>(3)</label></formula><p>where ? is used to balance the three loss terms. In this section, We will introduce these three loss terms in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Waypoint loss function</head><p>In the waypoints loss (L pt ), we expect to generate waypoints w l as close to the goal waypoint sequence generated by the global planner w p l as possible, by L 1 norm as in <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_3">L pt = L l=1 w l ? w p l 1 ,<label>(4)</label></formula><p>where the l denotes the sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Object density loss function</head><p>The object density map is a R R?R?7 grid map with R rows, R columns, and 7 channels including 1 object probability channel and 6 object meta feature channels. The object density loss (L map ) thus consists of a probability prediction loss L prob and a meta feature prediction loss L meta : L map = L prob + L meta (5) The probability prediction aims at predicting the existence of objects in each map grid. To avoid mostly zero probability predictions due to sparse positive labels, we further construct a balanced loss function by calculating the average loss for positive and negative labels respectively and merging them together:</p><formula xml:id="formula_4">L prob = 1 2 (L 0 prob + L 1 prob ),<label>(6)</label></formula><p>where L 0 prob and L 1 prob denote the loss for negative and positive labels respectively:</p><formula xml:id="formula_5">L 0 prob = 1 C 0 R i R j (1 [Mij0=0] M ij0 ? M ij0 1 ),<label>(7)</label></formula><formula xml:id="formula_6">L 1 prob = 1 C 1 R i R j (1 [Mij0=1] M ij0 ? M ij0 1 ),<label>(8)</label></formula><p>whereM ij0 and M ij0 denote the ground-truth and predicted object probability (channel 0) at the gird of i th row and j th column respectively. 1 [Mij0=0/1] ? {0, 1} denotes the indicator function. C 0 and C 1 denote the counts of positive and negative labels respectively:</p><formula xml:id="formula_7">C 0 = R i R j 1 [Mij0=0]<label>(9)</label></formula><formula xml:id="formula_8">C 1 = R i R j 1 [Mij0=1]<label>(10)</label></formula><p>The other 6 meta feature channels describe 6 meta information: offset x, offset y, heading, velocity x, velocity y, bounding box x, and bounding box y. The goal of meta feature prediction is to minimize the error between predicted and ground-truth meta features. Consequently, the meta feature prediction loss is designed as:</p><formula xml:id="formula_9">L meta = 1 C 1 R i R j 6 k=1 (1 [Mij0=1] M ijk ? M ijk 1 )<label>(11)</label></formula><p>whereM ijk and M ijk denote ground-truth and predicted k th -channel meta feature of the object at the grid of i th row and j th column respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Traffic information loss function</head><p>When predicting the traffic information (L tf ), we expect to recognize the traffic light status (L l ), stop sign (L s ), and whether the vehicle is at junction of roads (L j ):</p><formula xml:id="formula_10">L tf = ? l L l + ? s L s + ? j L j ,<label>(12)</label></formula><p>where ? balances the three loss terms, which are calculated by binary cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Safety controller -desired speed optimization</head><p>The desired velocity is expected to: 1) drive the vehicle to the goal point as soon as possible. 2) ensure collision avoidance in a future horizon. 3) consider the dynamic constraint and actuation limit of the ego vehicle. Toward these goals, we set up a linear programming optimization problem, where we try to maximize the desired velocity while the other requirements are achieved by constraints:</p><formula xml:id="formula_11">max v 1 d v 1 d s.t. (v 0 + v 1 d )T s 1 (v 0 + v 1 d )T + (v 1 d + v 2 d )T s 2 v 1 d ? v 0 T a max v 2 d ? v 1 d T a max 0 v 1 d v max 0 v 2 d v max<label>(13)</label></formula><p>where we consider a horizon of 1 second, and two desired velocities v 1 d and v 2 d are set at 0.5 second and 1 second respectively. v 0 denotes the current velocity of the ego vehicle. T denotes the time step duration (0.5s). s 1 and s 2 denote the maximum safe distance for the ego vehicle to drive at the first step and the second step respectively. v max and a max denotes the constraint on the maximum velocity and acceleration. When determining the maximum safe distance s 1 , we augment the shape of other objects for extra safety:</p><formula xml:id="formula_12">s 1 = max(s 1 ?s, 0) s 2 = max(s 2 ?s, 0)<label>(14)</label></formula><p>wheres denote the augmented distance for extra safety. s 1 and s 2 denote the maximum distance the ego vehicle can drive on the predicted route without collision with other objects. Note that in the optimization problem, we maximize the desired velocity at the first step v 1 d , while we set the desired velocity at the second step v 2 d as a free variable. The constraint on the second step helps the optimization of v 1 d looks into a future horizon, to avoid future safety intractability due to actuation limit and dynamic constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>All cameras have a resolution of 800 ? 600 with a horizontal field of view (FOV) 100 ? . The side cameras are angled at ?60?away from the front. For the front view, we scale the shorter side of the raw front RGB image to 256 and crop its center patch of 224 ? 224 as the front image input I front . For the two side views, the shorter sides of the raw side RGB images are scaled to 160 and a center patch of 128 ? 128 is taken as the side image inputs I left/right . For the focusing-view image input I focus by directly cropping the center patch of the raw front RGB image to 128 ? 128 without scaling.</p><p>For the LiDAR point clouds, we follow previous works <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b7">8]</ref> to convert the LiDAR point cloud data into a 2-bin histogram over a 2-dimensional Bird's Eye View (BEV) grid. The 2-bin of the histogram in each 0.125m 2 grid represents the numbers of points above and below the ground plane respectively. This produces a two-channel LiDAR bird-view projection image input I lidar , covering <ref type="figure">Figure 4</ref>: The driving preference varies when different safety factor is assigned to the safety controller. 100 % safety factor means thats = 2 and vmax = 6.5, and 150 % safety factor means thats = 2 ? 150% and vmax = 6.5/150%. The Town05 Long with adversarial events benchmark is used here.</p><p>the point cloud about 28 meters in front of the ego vehicle and 14 meters to the ego vehicle's two sides.</p><p>The backbone for encoding information from multi-view RGB images is Resnet-50 pretrained on ImageNet <ref type="bibr" target="#b50">[50]</ref>, and the backbone for processing LiDAR BEV representations is ResNet-18 trained from scratch. We take the output of stage 4 in a regular ResNet as the tokens fed into the downstream transformer encoder. The number of layers K in the transformer decoder and the transformer encoder is 6 and the feature dim d is 256. We train our models using the AdamW optimizer <ref type="bibr" target="#b51">[51]</ref> and a cosine learning rate scheduler <ref type="bibr" target="#b52">[52]</ref>. The initial learning rate is set to 5e ?4 ? BatchSize 512 for the transformer encoder &amp; decoder, and 2e ?4 ? BatchSize</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>512</head><p>for the CNN backbones. The weight decay for all models is 0.07. All the models are trained for a maximum of 35 epochs with the first 5 epochs for warm-up <ref type="bibr" target="#b36">[36]</ref>. For data augmentation, we used random scaling from 0.9 to1.1 and color jittering. The parameters used in the object density map and the safety controller is listed in <ref type="table" target="#tab_9">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Benchmark details</head><p>Leaderboard The CARLA Autonomous Driving Leaderboard <ref type="bibr" target="#b44">[44]</ref> is to evaluate the driving proficiency of autonomous agents in realistic traffic situations with a variety of weather conditions. The CARLA leaderboard provides a set of 76 routes as a starting point for training and verifying agents and contains a secret set of 100 routes to evaluate the driving performance of the submitted agents. However, the evaluation on the online CARLA leaderboard usually takes about 150 hours and each team is restricted to using this platform for only 200 hours per month. Therefore, we use the following Town05 benchmark for obtaining detailed statistics and ablation studies.</p><p>Town05 benchmark In the Town05 benchmark, we use Town05 for evaluation and other towns for training. Following <ref type="bibr" target="#b7">[8]</ref>, the benchmark includes two evaluation settings:(1) Town05 Short: 10 short routes of 100-500m, each comprising 3 intersections, (2) Town05 Long: 10 long routes of 1000-2000m, each comprising 10 intersections. Town05 has a wide variety of road types, including multi-lane roads, single-lane roads, bridges, highways and exits. The core challenge of the benchmark is how to handle dynamic agents and adversarial events.</p><p>CARLA 42 routes benchmark The CARLA 42 routes benchmark <ref type="bibr" target="#b16">[17]</ref> considers six towns covering a variety of areas such as US-style intersections, EU-style intersections, freeways, roundabouts, stop signs, urban scenes and residential districts. The traffic density of each town is set to be comparable to busy traffic setting. We use the same benchmark configuration open-sourced by <ref type="bibr" target="#b7">[8]</ref> to evaluate all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left View</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Front View</head><p>Right View Object Density Map Left View Front View Right View Object Density Map <ref type="figure">Figure 5</ref>: Four cases of how our method predicts waypoints and recover the traffic scene. Blue points denote predicted waypoints. Yellow rectangle represents the ego vehicle, and white/grey rectangles denote the current/future positions of detected objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left View Front View</head><p>Right View LiDAR BEV Focus View <ref type="figure">Figure 6</ref>: Visualization of attention weights between the object density map queries and features from different views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left View Front View Right View</head><p>Object Density Map t 0 t 1 t 2 Reasons for the failure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inaccurately predict of the vehicle's heading</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inaccurately predict of the bicycle's speed</head><p>Fail to detect the fallen pedestrian ahead <ref type="figure">Figure 7</ref>: Visualization of failure cases with three RGB views and the predicted object density map. The orange boxes show the objects where the ego-vehicle is about to collide. t0 of object density map denotes the predicted current traffic scenes, t1 and t2 denotes the predicted future traffic scenes after 1 second and 2 seconds. <ref type="table" target="#tab_7">Table 4</ref> and <ref type="table" target="#tab_8">Table 5</ref> additionally compares the driving score, road completion and infraction score of the presented approach (InterFuser) to prior state-of-the-art on the CARLA Town05 benchmark <ref type="bibr" target="#b7">[8]</ref> and CARLA 42 routes <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F The hyper-parameter values</head><p>The hyper-parameter values used in InterFuser are listed in Table6. Cyclists and pedestrians are rendered larger than their actual sizes when we reconstruct the scene from the object density map, this adds some caution when dealing with these road objects.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Safe and efficient driving requires comprehensive scene understanding by fusing information from multiple sensors. Peeking into the intermediate interpretable features of learning models can also unveil the model's decision basis. Such features enable improvable systems with access to failure causes, and can be used as safety heuristic to constrain actions within the safe set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Two cases of how our method predicts waypoints and recovers the traffic scene. Blue points denote predicted waypoints. The yellow rectangle represents the ego vehicle, and white/grey rectangles denote the current/future positions of detected objects. (b) Visualization of attention weights between object density map queries and features from different views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Transformer Decoder 1 Traffic Rule Query R^2 Density Map Query L Waypoints Query Positional Encoding ? ? Probability Velocity Heading Offset BBox Traffic Light State Stop Sign At Junction ? ? Waypoints GRU Sensor Embedding . . . Safety Controller Nominal Actions Safe Actions Object Density Map Traffic Rules Safety Constraint Safety Constraint Value Waypoints Object Density Map Tracker Historical Obversions</head><label></label><figDesc></figDesc><table><row><cell>Left View</cell><cell>Front View</cell><cell cols="2">Right View Focus View</cell><cell>LiDAR</cell><cell>Safety Controller</cell></row><row><cell>CNN</cell><cell>CNN</cell><cell>CNN</cell><cell>CNN</cell><cell>CNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>t 0</cell><cell>Future Predictions</cell></row><row><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>t 1</cell></row><row><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell></cell><cell>Traffic Rules</cell></row><row><cell></cell><cell></cell><cell>Encoder</cell><cell></cell><cell></cell><cell>Traffic Light Stop Sign</cell><cell>t 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Safe Actions</cell><cell>Is Junction</cell><cell>t 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Four image inputs are obtained from the three cameras. In addition to the left, front and right image input {I left , I front , I right }, we also design a focusing-view image input I focus to capture the status of distant traffic lights by cropping the center patch of the raw front RGB image. For the LiDAR point clouds, we follow previous works<ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b7">8]</ref> to convert the LiDAR point cloud data into a 2-bin histogram over a 2-dimensional Bird's Eye View (BEV) grid, resulting in a two-channel LiDAR bird's-eye view projection image input I lidar .</figDesc><table><row><cell>, the proposed framework consists of three main components: 1) a multi-view</cell></row><row><cell>multi-modal fusion transformer encoder that integrates the signals from multiple RGB cameras and</cell></row><row><cell>LiDAR; 2) a transformer decoder providing the low-level actions, and interpretable intermediate</cell></row><row><cell>features including the ego vehicle's future trajectory, the object density map, and the traffic rule</cell></row><row><cell>signals; 3) a safety controller which utilizes the interpretable intermediate features to constrain the</cell></row><row><cell>low-level control within the safe set. This section will introduce the input/output representation,</cell></row><row><cell>model architecture and safety controller.</cell></row><row><cell>3.1 Input and Output Representations</cell></row><row><cell>Input representations Four sensors are utilized, including three RGB cameras (left, front and right)</cell></row><row><cell>and one LiDAR sensor.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? 4.3 40.3 ? 5.2 99.9 ? 0.0 0.09 ? 0.01 0.04 ? 0.01 0.11 ? 0.02 0.09 ? 0.02 0 ? 0 F + Li 47.2 ? 0.9 48.9 ? 2.9 96.6 ? 1.1 0.06 ? 0.01 0.02 ? 0.02 0.09 ? 0.02 0.10 ? 0.02 0 ? 0 F + LR 43.0 ? 5.4 43.2 ? 6.2 98.0 ? 2.7 0.05 ? 0.01 0.01 ? 0 0.09 ? 0.03 0.12 ? 0.03 0 ? 0 F + LR + F c 46.4 ? 3.1 48.5 ? 3.3 97.1 ? 5.0 0.07 ? 0.01 0.01 ? 0.01 0.1 ? 0.02 0.04 ? 0.02 0.02 ? 0.02 F + LR + Li 49.2 ? 0.7 55.3 ? 2.8 89.6 ? 0.7 0.01 ? 0.01 0 ? 0 0.09 ? 0.05 0.10 ? 0.02 0.04 ? 0.02 F + LR + F c + Li 51.6 ? 3.4 58.6 ? 5.2 88.9 ? 2.5 0.01 ? 0.01 0 ? 0 0.09 ? 0.05 0.02 ? 0.02 0.07 ? 0.01</figDesc><table><row><cell></cell><cell>Driving</cell><cell>Infrac.</cell><cell>Route</cell><cell>Collision</cell><cell>Collision</cell><cell>Collision</cell><cell>Red light</cell><cell>Agent</cell></row><row><cell></cell><cell>score</cell><cell>score</cell><cell>compl.</cell><cell>static</cell><cell>pedestrian</cell><cell>vehicle</cell><cell>infraction</cell><cell>blocked</cell></row><row><cell></cell><cell>%, ?</cell><cell>%, ?</cell><cell>%, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell></row><row><cell>F</cell><cell>40.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study for different sensor inputs. F , LR, F c, Li denotes the front view, left and right view, focusing view, LiDAR BEV representations respectively. Our method performs the best when all sensor inputs are leveraged.No sensr/pos embd 48.6 ? 4.7 52.6 ? 6.1 91.7 ? 2.4 0.05 ? 0.04 0.01 ? 0.01 0.11 ? 0.10 0.05 ? 0.01 0.08 ? 0.1 Concatenated input 21.5 ? 7.6 21.5 ? 8.6 97.2 ? 3.9 0.06 ? 0 0.03 ? 0.02 0.26 ? 0.05 0.20 ? 0.02 0 ? 0.01 No attn btw views 46.8 ? 6.6 49.6 ? 9.8 94.1 ? 3.5 0.07 ? 0.01 0.01 ? 0.01 0.1 ? 0.02 0.04 ? 0.02 0.01 ? 0.01 No safety contrl 43.9 ? 4.7 45.6 ? 5.9 96.3 ? 5.2 0.05 ? 0.03 0.02 ? 0.01 0.13 ? 0.10 0.02 ? 0.02 0.08 ? 0.2</figDesc><table><row><cell></cell><cell>Driving</cell><cell>Infrac.</cell><cell>Route</cell><cell>Collision</cell><cell>Collision</cell><cell>Collision</cell><cell>Red light</cell><cell>Agent</cell></row><row><cell></cell><cell>score</cell><cell>score</cell><cell>compl.</cell><cell>static</cell><cell>pedestrian</cell><cell>vehicle</cell><cell>infraction</cell><cell>blocked</cell></row><row><cell></cell><cell>%, ?</cell><cell>%, ?</cell><cell>%, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell><cell>#/Km, ?</cell></row><row><cell>InterFuser (ours)</cell><cell cols="4">51.6 ? 3.4 58.6 ? 5.2 88.9 ? 2.5 0.01 ? 0.01</cell><cell>0 ? 0</cell><cell cols="3">0.09 ? 0.05 0.10 ? 0.02 0.07 ? 0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Ablation study for sensor/position embedding (No sensr/pos embd), sensor fusion approach (Con- catenated input, No attn btw views), and safety controller (No safety-contrl). The results demonstrated the effectiveness of each module.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our InterFuser with six state-of-the-art methods in Town05 benchmark. Metrics: driving score (DS), Road completion (RC). Our method outperformed other strong methods in all metrics and scenarios.</figDesc><table><row><cell>Method</cell><cell cols="3">Driving Score ? Road Completion ? Infraction Score ?</cell></row><row><cell>CILRS [14]</cell><cell>22.97?0.90</cell><cell>35.46?0.41</cell><cell>0.66?0.02</cell></row><row><cell>LBC [5]</cell><cell>29.07?0.67</cell><cell>61.35?2.26</cell><cell>0.57?0.02</cell></row><row><cell>AIM [8]</cell><cell>51.25?0.17</cell><cell>70.04?2.31</cell><cell>0.73?0.03</cell></row><row><cell>TransFuser [8]</cell><cell>53.40?4.54</cell><cell>72.18?4.17</cell><cell>0.74?0.04</cell></row><row><cell>NEAT [17]</cell><cell>65.17?1.75</cell><cell>79.17?3.25</cell><cell>0.82?0.01</cell></row><row><cell>Roach [19]</cell><cell>65.08?0.99</cell><cell>85.16?4.20</cell><cell>0.77?0.02</cell></row><row><cell>WOR [49]</cell><cell>67.64?1.26</cell><cell>90.16?3.81</cell><cell>0.75?0.02</cell></row><row><cell>InterFuser</cell><cell>91.84?2.17</cell><cell>97.12?1.95</cell><cell>0.95?0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our InterFuser with other methods in CARLA 42 routes benchmark. Metrics: Road completion (RC), infraction score (IS), driving score (DS). Our method outperformed other strong methods in all metrics and scenarios.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The parameter used for InterFuser.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The adversarial events include bad road conditions, front vehicle's sudden brake, unexpected entities rushing into the road from occluded regions, vehicles running a red traffic light, etc. Please refer to https://leaderboard.carla.org/scenarios/ for detailed descriptions of adversarial events.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Teslas self-driving system cleared in deadly crash. The New York Times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Boudette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ubers self-driving car saw the pedestrian but didnt swerve-report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Guardian</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end multi-modal sensors fusion system for urban automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdelkarim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elmadawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abdeltawab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Sallab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning by cheating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end model-free reinforcement learning for urban driving using implicit affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7153" to="7162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7077" to="7087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Testing dnn-based autonomous driving systems under critical environmental conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/li21r.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>M. Meila and T. Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can autonomous vehicles identify, recover from, and adapt to distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tigkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lazarus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06758</idno>
		<title level="m">Algorithms for verifying deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Holistic reinforcement learning: the role of structure and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="278" to="292" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the limitations of behavior cloning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9329" to="9338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring data aggregation in policy learning for vision-based urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11763" to="11773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V D</forename><surname>Broeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02973</idno>
		<title level="m">Squeeze-and-mimic networks for conditional visual driving policy learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neat: Neural attention fields for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15793" to="15803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end urban driving by imitating a reinforcement learning coach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15222" to="15232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Blending anti-aliasing into vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/2b3bf3eee2475e03885a110e9acaab61-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5416" to="5429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformerfusion: Monocular rgb scene reconstruction using transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bozic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Palafox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic prediction of vehicle semantic intention and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="307" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Socially-compatible behavior design of autonomous vehicles with verification on real human data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3421" to="3428" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On infusing reachability-based safety assurance within planning frameworks for human-robot vehicle interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1326" to="1345" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Fisac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00109</idno>
		<title level="m">Probabilistically safe robot planning with confidence-based human predictions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00788</idno>
		<title level="m">Hierarchical adaptable and transferable networks (hatn) for driving behavior prediction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03673</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Precog: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Online adaptation of neural network models by modified extended kalman filter for customizable and transferable driving behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06129</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05140</idno>
		<title level="m">Transferable and adaptable driving behavior prediction</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08294</idno>
		<title level="m">Target-driven trajectory prediction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Trajectory prediction with latent belief energybased model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11814" to="11824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Control in a safe set: Addressing safety in human-robot interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamic Systems and Control Conference</title>
		<imprint>
			<publisher>American Society of Mechanical Engineers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">46209</biblScope>
			<biblScope unit="page" from="3" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Carla autonomous driving leaderboard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://leaderboard.carla.org/,2020" />
		<imprint>
			<biblScope unit="page" from="2021" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning from all vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="17222" to="17231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Transfuser: Imitation with transformer-based sensor fusion for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15997</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chekroun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hornauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08575</idno>
		<title level="m">General reinforced imitation and its application to vision-based autonomous driving</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Expert drivers for autonomous driving</title>
		<ptr target="urlhttps://kait0.github.io/files/masterthesisbernhardjaeger.pdf" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to drive from a world on rails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15590" to="15599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
