<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QUARTZNET: DEEP AUTOMATIC SPEECH RECOGNITION WITH 1D TIME-CHANNEL SEPARABLE CONVOLUTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kriman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">? High School of Economics</orgName>
								<orgName type="institution" key="instit1">Univ. of Illinois Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">Univ. of Saint Petersburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Beliaev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Lavrukhin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Leary</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">QUARTZNET: DEEP AUTOMATIC SPEECH RECOGNITION WITH 1D TIME-CHANNEL SEPARABLE CONVOLUTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Automatic speech recognition</term>
					<term>convolu- tional networks</term>
					<term>time-channel separable convolution</term>
					<term>depth- wise separable convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new end-to-end neural acoustic model for automatic speech recognition. The model is composed of multiple blocks with residual connections between them. Each block consists of one or more modules with 1D time-channel separable convolutional layers, batch normalization, and ReLU layers. It is trained with CTC loss. The proposed network achieves near state-of-the-art accuracy on LibriSpeech and Wall Street Journal, while having fewer parameters than all competing models. We also demonstrate that this model can be effectively fine-tuned on new datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the last few years, end-to-end (E2E) neural networks (NN) have achieved new state-of-the-art (SOTA) results on many automatic speech recognition (ASR) tasks. Such models replace the traditional multi-component ASR system with a single, end-to-end trained NN which directly predicts character sequences and therefore greatly simplify training, fine-tuning and inference. The latest E2E models also have very good accuracy, but this often comes at the cost of increasingly large models with high computational and memory requirements.</p><p>The motivation of this work is to build an ASR model that achieves SOTA-level accuracy, while utilizing significantly fewer parameters and less compute power. Smaller models offer multiple advantages: <ref type="bibr" target="#b0">(1)</ref> they are faster to train, (2) they are more feasible to deploy on hardware with limited compute and memory, and (3) they have higher inference throughput.</p><p>We achieve this goal by building a very deep NN with 1D time-channel separable convolutions. This new network reaches near-SOTA word error rate (WER) on LibriSpeech <ref type="bibr" target="#b0">[1]</ref> (see <ref type="table" target="#tab_3">Table 4</ref>) and WSJ <ref type="bibr" target="#b1">[2]</ref> (see <ref type="table" target="#tab_6">Table 7</ref>) datasets with fewer than 20 million parameters, compared to previous end-to-end * Work was conducted while S.Kriman and S.Beliaev were at NVIDIA ASR designs which typically have over 100 million parameters. We have released the source code and pre-trained models in the NeMo toolkit <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>There has been a lot of work done in exploring compact network architectures and on investigating the trade-off between accuracy and size of neural networks, such as SqueezeNet <ref type="bibr" target="#b3">[4]</ref>, ShuffleNet <ref type="bibr" target="#b4">[5]</ref>, and EfficientNet <ref type="bibr" target="#b5">[6]</ref>. Our approach is directly related to MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and Xception <ref type="bibr" target="#b8">[9]</ref>, which uses depthwise separable convolutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Each depthwise separable convolution module is made up of two parts: a depthwise convolutional layer and a pointwise convolutional layer. Depthwise convolutions apply a single filter per input channel (input depth). Pointwise convolutions are 1 ? 1 convolutions, used to create a linear combination of the outputs of the depthwise layer. BatchNorm and ReLU are applied to the outputs of both layers.</p><p>Hannun et al <ref type="bibr" target="#b11">[12]</ref> applied a similar approach to ASR. They introduced an encoder-decoder model with time-depth separable (TDS) convolutions. The TDS model operates on data in time-frequency-channels (T ? w ? c) format, where T is the number of time-steps, w is the input width and c is the number of channels. The basic TDS block is composed of a 2D convolutional block with k ?1 convolutions over (T ?w), and a fully-connected block, consisting of two 1 ? 1 pointwise convolutions operating on (w ? c) channels interleaved with layer-norm layers. In contrast, in our work we operate on data in time-channel format (T ? c) and completely decouple the time and channel-wise parts of convolution. TDS block has k ? c 2 + 2 ? (w ? c) 2 parameters, while QuartzNet model has k ? c + c 2 parameters, which allows for a dramatic reduction in model size while still achieving good WER.</p><p>Another very small ASR model was introduced by Han et al <ref type="bibr" target="#b12">[13]</ref>, which uses multiple parallel streams of self-attention with dilated, factorized, although not separable, 1D convolutions. The parallel streams capture multiple resolutions of speech frames from the input by using different dilation rates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic model</head><p>QuartzNet's design is based on the Jasper <ref type="bibr" target="#b13">[14]</ref> architecture, which is a convolutional model trained with Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b14">[15]</ref>. The main novelty in QuartzNet's architecture is that we replaced the 1D convolutions with 1D time-channel separable convolutions, an implementation of depthwise separable convolutions. 1D timechannel separable convolutions can be separated into a 1D depthwise convolutional layer with kernel length K that operates on each channel individually but across K K K time frames and a pointwise convolutional layer that operates on each time frame independently but across all channels.</p><p>QuartzNet models have the following structure: they start with a 1D convolutional layer C 1 followed by a sequence of blocks. Each block B i is repeated S i times and has residual connections between blocks. Each block B i consists of the same base modules repeated R i times and contains four layers: 1) K-sized depthwise convolutional layer with c out channels, 2) a pointwise convolution, 3) a normalization layer, and 4) ReLU. The last part of the model consists of three addi- <ref type="table">Table 1</ref>. QuartzNet Architecture. The model starts with a conv layer C 1 followed by a sequence of 5 groups of blocks. Blocks in the group are identical, each block B k consists of R time-channel separable K-sized convolutional modules with C output channels. Each block is repeated S times. The model has 3 additional conv layers (C 2 , C 3 , C 4 ) at the end. tional convolutional layers (C 2 , C 3 , C 4 ). The C 1 layer has a stride of 2, and C 4 layer has a dilation of 2. <ref type="table">Table 1</ref> describes the QuartzNet-5x5, 10x5 and 15x5 models. There are five unique blocks across these models:</p><formula xml:id="formula_0">Block R K C S 5x5 10x5</formula><formula xml:id="formula_1">B 1 -B 5 .</formula><p>The different models repeat the blocks a different number of times, represented by S i . QuartzNet-5x5</p><formula xml:id="formula_2">(B 1 ? B 2 ? B 3 ? B 4 ? B 5 ) has each group of blocks repeated 1 time, QuartzNet-10x5 (B 1 ? B 1 ? B 2 ? B 2 ? ... ? B 5 ? B 5 ) -repeated 2 times, and QuartzNet-15x5 (B 1 ? B 1 ? B 1 ? ... ? B 5 ? B 5 ? B 5 ) -repeated 3 times.</formula><p>A regular 1D convolutional layer with kernel size K, c in input channels, and c out output channels has K ? c in ? c out weights. The time-channel separable convolutions use K ? c in + c in ? c out weights split into K ? c in weights for the depthwise layer and c in ? c out for the pointwise layer. The depthwise convolution is applied independently for each channel, so it contributes a relatively small portion of the total number of weights. This allows us to use much wider kernels, roughly 3 times larger than kernels used in wav2letter <ref type="bibr" target="#b15">[16]</ref> or Jasper <ref type="bibr" target="#b13">[14]</ref> models. We experimented with four types of normalization: batch normalization <ref type="bibr" target="#b16">[17]</ref>, layer normalization <ref type="bibr" target="#b17">[18]</ref>, instance normalization <ref type="bibr" target="#b18">[19]</ref>, and group normalization <ref type="bibr" target="#b19">[20]</ref>, and found that models with batch normalization have most stable training and give the best WER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pointwise convolutions with groups</head><p>The total number of weights for a time-channel separable convolution block is K ? c in + c in ? c out weights. Since K is generally several times smaller than c out , most weights are concentrated in the pointwise convolution part. In order to further reduce the number of parameters, we explore using group convolutions for this layer. We also added a group shuffle layer to increase cross-group interchange <ref type="bibr" target="#b4">[5]</ref>. Using groups allows us to significantly reduce the number of weights at the cost of some accuracy. <ref type="table" target="#tab_2">Table 3</ref> shows the trade-off between accuracy and number of parameters for group sizes one, two, and four, evaluated on LibriSpeech.  <ref type="bibr" target="#b20">[21]</ref> can be fine-tuned on a smaller amount of audio data, the WSJ dataset, to achieve better performance than training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">LibriSpeech</head><p>Our best results on the LibriSpeech dataset are achieved with the QuartzNet-15x5 model, consisting of 15 blocks with 5 convolutional modules per block (see <ref type="table">Table 1</ref>). By combining our network with independently trained language models (i.e., n-gram language models and Transformer-XL (T-XL) <ref type="bibr" target="#b21">[22]</ref>) we got WER comparable to the current SOTA. The model with time-channel separable convolutions is much smaller than a model with regular convolutions and is less prone to over-fitting, so we use only data augmentation and weight decay for regularization during training. We experimented with SpecAugment <ref type="bibr" target="#b22">[23]</ref>, SpecCutout, and speed perturbation <ref type="bibr" target="#b23">[24]</ref>. We achieved the best results with 10% speed perturbation combined with Cutout <ref type="bibr" target="#b24">[25]</ref> which randomly cuts small rectangles out of the spectrogram. The models are trained using NovoGrad optimizer <ref type="bibr" target="#b25">[26]</ref> with a cosine annealing learning rate policy. We also found that learning rate warmup helps stabilize early training. The training of the 15x5 model for 400 epochs took ? 5 days on one DGX1 server with 8 Tesla V100 GPUs with a batch size of 32 per GPU. In order to decrease the memory footprint as well as training time, we used mixed-precision training <ref type="bibr" target="#b27">[28]</ref>. We reduced the training time to just over four hours by scaling training to SuperPod with 32 DGX2 nodes with larger number of epochs and with an increased global batch of 16K (see <ref type="table" target="#tab_4">Table 5</ref>). 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Wall Street Journal</head><p>We trained a smaller QuartzNet-5x3 model on the open vocabulary task of the Wall Street Journal dataset <ref type="bibr" target="#b1">[2]</ref>. We used train-si284 set for training, nov93-dev for validation, and nov92-eval for testing. The QuartzNet-5x3 model (see <ref type="table">Table  6</ref>) was trained for 1200 epochs with batch size 32 per GPU, data augmentation (10% speed perturbation, SpecCutout) and dropout of 0.2 using NovoGrad optimizer (? 1 = 0.95, ? 2 = 0.5) with 1000 steps of learning rate warmup, a learning rate of 0.05, and weight decay 0.001. We used 2 external language models during inference: 4gram (beam size=2048, alpha=3.5, beta=1.5) and Transformer-XL (T-XL). Both language models were constructed using only the official LM data of WSJ. <ref type="table">Table 6</ref>. QuartzNet-5x3 for WSJ. The model has the same layers C 1 , C 2 , C 3 , C 4 as QuartzNet-15x5, but the middle part consists of only five blocks, each of which is repeated three times.</p><p>Block  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning</head><p>As our model is smaller than other models, we were interested in how well it could learn to generalize to data from various sources, especially if the amount of target speech is much smaller than the training data. Our setup consists of training QuartzNet 15x5 on a combination of LibriSpeech <ref type="bibr" target="#b0">[1]</ref> and Mozilla's Common Voice <ref type="bibr" target="#b20">[21]</ref> 4 datasets, and then finetuning this trained model on the 80 hour WSJ dataset. <ref type="table" target="#tab_7">Table 8</ref> shows the WER achieved on LibriSpeech prior to fine-tuning and the result on WSJ after fine-tuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE DIRECTIONS</head><p>We introduced a new end-to-end speech recognition model, based on deep neural network with 1D time-channel separable convolutional layers. The model showed close to state-of-the art performance on Wall Street Journal and on LibriSpeech while being significantly smaller than all other end-to-end systems with similar accuracy. The small model footprint opens new possibility for speech recognition on mobile and embedded devices. This work described a CTC-based model, but we are exploring models where the QuartzNet encoder is combined with attention-based decoders.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>QuartzNet BxR architecture per stream, and the results of the individual streams are concatenated into a final embedding. The best model has five streams with dilation rates 1-2-3-4-5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Time-channel separable 1D convolutional module (b) Time-channel separable 1D convolutional module with groups and shuffle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>QuartzNet models with different depth trained on LibriSpeech for 300 epochs, greedy WER (%).</figDesc><table><row><cell cols="4">Model dev-clean dev-other Params, M</cell></row><row><cell>5x5</cell><cell>5.39</cell><cell>15.69</cell><cell>6.7</cell></row><row><cell>10x5</cell><cell>4.14</cell><cell>12.33</cell><cell>12.8</cell></row><row><cell>15x5</cell><cell>3.98</cell><cell>11.58</cell><cell>18.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>QuartzNet-15x5 with grouped convolutions trained on LibriSpeech for 300 epochs, greedy WER (%)</figDesc><table><row><cell cols="4"># Groups dev-clean dev-other Params, M</cell></row><row><cell>1</cell><cell>3.98</cell><cell>11.58</cell><cell>18.9</cell></row><row><cell>2</cell><cell>4.29</cell><cell>12.52</cell><cell>12.1</cell></row><row><cell>4</cell><cell>4.51</cell><cell>13.48</cell><cell>8.70</cell></row><row><cell></cell><cell cols="2">4. EXPERIMENTS</cell><cell></cell></row><row><cell cols="4">We evaluate QuartzNet's performance on LibriSpeech and</cell></row><row><cell cols="4">WSJ datasets. We additionally experiment with a transfer</cell></row><row><cell cols="4">learning showcasing how a QuartzNet model trained with</cell></row><row><cell cols="3">LibriSpeech and Common Voice</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>LibriSpeech results, WER (%)</figDesc><table><row><cell>Model</cell><cell>Augment</cell><cell>LM</cell><cell cols="2">Test clean other</cell><cell>Params,M</cell></row><row><cell>wav2letter++ [27]</cell><cell>speed perturb</cell><cell cols="3">ConvLM 3.26 10.47</cell><cell>208</cell></row><row><cell>LAS [23]</cell><cell>SpecAugment</cell><cell>RNN</cell><cell>2.5</cell><cell>5.8</cell><cell>360</cell></row><row><cell>TDS Conv [12]</cell><cell>dropout+</cell><cell>-</cell><cell cols="2">5.36 15.64</cell><cell>37</cell></row><row><cell></cell><cell>label smooth</cell><cell>4-gram</cell><cell cols="2">4.21 11.87</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ConvLM 3.28</cell><cell>9.84</cell><cell></cell></row><row><cell>MSSA[13]</cell><cell>speed perturb</cell><cell>4-gram</cell><cell>2.93</cell><cell>8.32</cell><cell>23</cell></row><row><cell></cell><cell></cell><cell cols="2">4-LSTM 2.20</cell><cell>5.82</cell><cell></cell></row><row><cell cols="2">JasperDR-10x5[14] SpecAugment+</cell><cell>-</cell><cell cols="2">4.32 11.82</cell><cell>333</cell></row><row><cell></cell><cell>speed perturb</cell><cell>6-gram</cell><cell>3.24</cell><cell>8.76</cell><cell></cell></row><row><cell></cell><cell></cell><cell>T-XL</cell><cell>2.84</cell><cell>7.84</cell><cell></cell></row><row><cell>QuartzNet 15x5</cell><cell>SpecCutout+</cell><cell>-</cell><cell cols="2">3.90 11.28</cell><cell>19</cell></row><row><cell></cell><cell>speed perturb</cell><cell>6-gram</cell><cell>2.96</cell><cell>8.07</cell><cell></cell></row><row><cell></cell><cell></cell><cell>T-XL</cell><cell>2.69</cell><cell>7.25</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>QuartzNet-15x5: large batch training on Lib-riSpeech, time to train (hours) and greedy WER (%).</figDesc><table><row><cell cols="3">Batch Epochs Time, h</cell><cell>Dev clean other clean other Test</cell></row><row><cell>256</cell><cell>400</cell><cell>122</cell><cell>3.83 11.08 3.90 11.28</cell></row><row><cell>16K</cell><cell>1500</cell><cell>4.3</cell><cell>3.71 10.78 4.04 11.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>QuartzNet-5x3, WSJ, WER(%)</figDesc><table><row><cell>Model</cell><cell>LM</cell><cell cols="3">93-test 92-eval Params, M</cell></row><row><cell>RNN-CTC[30]</cell><cell>3-gram</cell><cell>-</cell><cell>8.7</cell><cell>26.5</cell></row><row><cell cols="2">ResCNN-LAS[31] 3-gram</cell><cell>9.7</cell><cell>6.7</cell><cell>6.6</cell></row><row><cell cols="2">Wav2Letter++[29] 4-gram</cell><cell>9.5</cell><cell>5.6</cell><cell>17</cell></row><row><cell></cell><cell>convLM</cell><cell>7.5</cell><cell>4.1</cell><cell></cell></row><row><cell>QuartzNet-5x3</cell><cell>4-gram</cell><cell>8.1</cell><cell>5.8</cell><cell>6.4</cell></row><row><cell></cell><cell>T-XL</cell><cell>7.0</cell><cell>4.5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>QuartzNet15x5 transfer learning. The model was pre-trained of LibriSpeech-train and Mozillas Common Voice datasets, and fine-tuned on the 80-hour WSJ dataset. The model was evaluated on LibriSpeech and WSJ, WER(%).</figDesc><table><row><cell>LM</cell><cell cols="4">LibriSpeech test-clean test-other 93-test 92-eval WSJ</cell></row><row><cell>-</cell><cell>4.19</cell><cell>10.98</cell><cell>8.97</cell><cell>6.37</cell></row><row><cell>4-gram</cell><cell>3.21</cell><cell>8.04</cell><cell>5.57</cell><cell>3.51</cell></row><row><cell>T-XL</cell><cell>2.96</cell><cell>7.53</cell><cell>4.82</cell><cell>2.99</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/NVIDIA/NeMo arXiv:1910.10261v1 [eess.AS] 22 Oct 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Training even longer (3000 epochs) improved greedy WER on test-clean to 3.87% and on test-other to 10.61%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note, that wav2letter++<ref type="bibr" target="#b28">[29]</ref> with trainable front-end and convLM has even better WER: 6.8% for nov93-test, and 3.5% for nov92-dev. Here, we consider only models with a standard mel-filterbanks front-end.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We used the validated set of Common Voice, ver. en 1087h 2019-06-12.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The design for the wall street journal based csr corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language. ACL</title>
		<meeting>the workshop on Speech and Natural Language. ACL</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page">357362</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nemo: a toolkit for building ai applications using neural modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hrinchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beliaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cook</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09577</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;1mb model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1687</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning visual representations at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3785" to="3789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">State-of-theart speech recognition using multi-stream self-attention with dilated 1d convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00716</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Jasper: An end-to-end convolutional neural acoustic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Gadde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03288</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Letter-based speech recognition with gated convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page">319</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Common voice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mozilla</surname></persName>
		</author>
		<ptr target="https://voice.mozilla.org/en" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Stochastic gradient methods with layerwise adaptive moments for training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Castonguay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hrinchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06864</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end speech recognition from the raw waveform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07098</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
