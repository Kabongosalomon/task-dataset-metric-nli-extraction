<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HAA500: Human-Centric Atomic Action Dataset with Curated Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hsin</forename><surname>Wuu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Ru</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HAA500: Human-Centric Atomic Action Dataset with Curated Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We contribute HAA500 1 , a manually annotated humancentric atomic action dataset for action recognition on 500 classes with over 591K labeled frames. To minimize ambiguities in action classification, HAA500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g., "Baseball Pitching" vs "Free Throw in Basketball". Thus HAA500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as "Throw". HAA500 has been carefully curated to capture the precise movement of human figures with little class-irrelevant motions or spatiotemporal label noises.</p><p>The advantages of HAA500 are fourfold: 1) humancentric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20-60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic action classes. Our extensive experiments including crossdata validation using datasets collected in the wild demonstrate the clear benefits of human-centric and atomic characteristics of HAA500, which enable training even a baseline deep learning model to improve prediction by attending to atomic human poses. We detail the HAA500 dataset statistics and collection methodology and compare quantitatively with existing action recognition datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Observe the coarse annotation provided by commonlyused action recognition datasets such as <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b42">42]</ref>, where the same action label was assigned to a given complex video action sequence (e.g., Play Soccer, Play Baseball) typically lasting 10 seconds or 300 frames, thus introducing a lot of ambiguities during training as two or more action categories may contain the same atomic action (e.g., Run is one of the atomic actions for both Play Soccer and Play Baseball). Recently, atomic action datasets <ref type="bibr">[5,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39]</ref> have been introduced in an attempt to resolve the aforementioned issue. Google's AVA actions dataset <ref type="bibr" target="#b17">[17]</ref> provides dense annotations of 80 atomic visual actions in 430 fifteen-minute video clips where actions are localized in space and time. AVA spoken activity dataset <ref type="bibr" target="#b36">[36]</ref> contains temporally labeled face tracks in videos, where each face instance is labeled as speaking or not, and whether the speech is audible. Something-Something dataset <ref type="bibr" target="#b16">[16]</ref> contains clips of humans performing pre-defined basic actions with daily objects.</p><p>However, some of their actions are still coarse which can be further split into atomic classes with significantly different motion gestures. E.g., AVA <ref type="bibr" target="#b17">[17]</ref> and Something-Something <ref type="bibr" target="#b16">[16]</ref> contain Play Musical Instrument and Throw Something as a class, respectively, where the former should be further divided into sub-classes such as Play Piano and Play Cello, and the latter into Soccer Throw In and Pitch Baseball, etc., because each of these atomic actions has significantly different gestures. Encompassing different visual postures into a single class poses a deep neural network almost insurmountable challenge to properly learn the pertinent atomic action, which probably explains the prevailing low performance employing even the most state-of-the-art architecture, ACAR-Net (mAP: 38.30%) <ref type="bibr" target="#b33">[33]</ref>, in AVA <ref type="bibr" target="#b17">[17]</ref>, despite only having 80 classes.</p><p>The other problem with existing action recognition video datasets is that their training examples contain actions irrelevant to the target action. Video datasets typically have fixed clip lengths, allowing unrelated video frames to be easily included during the data collection stage. Kinetics 400 dataset <ref type="bibr" target="#b21">[21]</ref>, with a fixed 10-second clip length, contains a lot of irrelevant actions, e.g., showing the audience before the main violin playing, or a person takes a long run before kicking the ball. Another problem is having too limited or too broad field-of-view, where a video only exhibits a part of a human interacting with an object <ref type="bibr" target="#b16">[16]</ref>, or a single video contains multiple human figures with different actions present <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>Recently, FineGym <ref type="bibr" target="#b39">[39]</ref> has been introduced to solve the aforementioned limitations by proposing fine-grained action annotations, e.g., Balance Beam-Dismount-Salto Forward Tucked. But due to the expensive data collection pro- <ref type="bibr">Figure 1</ref>. HAA500 is a fine-grained atomic action dataset, with fine-level action annotations (e.g., Soccer-Dribble, Soccer-Throw In) compared to the traditional composite action annotations (e.g., Soccer, Baseball). HAA500 is comparable to existing coarse-grained atomic action datasets, where we have distinctions (e.g., Soccer-Throw In, Baseball-Pitch) within an atomic action (e.g., Throw Something) when the action difference is visible. The figure above displays sample videos from three different areas of HAA500. Observe that each video contains one or a few dominant human figures performing the pertinent action. cess, they only contain 4 events with atomic action annotations (Balance Beam, Floor Exercise, Uneven Bars, and Vault-Women), and their clips were extracted from professional gymnasium videos in athletic or competitive events.</p><p>In this paper, we contribute Human-centric Atomic Action dataset (HAA500) which has been constructed with carefully curated videos with a high average of 69.7% detectable joints, where a dominant human figure is present to perform the labeled action. The curated videos have been annotated with fine-grained labels to avoid ambiguity, and with dense per-frame action labeling and no unrelated frames being included in the collection as well as annotation. HAA500 contains a wide variety of atomic actions, ranging from athletic atomic action (Figure Skating -Ina Bauer) to daily atomic action (Eating a Burger). HAA500 is also highly scalable, where adding a class takes only 20-60 minutes. The clips are class-balanced and contain clear visual signals with little occlusion. As opposed to "in-thewild" atomic action datasets, our "cultivated" clean, classbalanced dataset provides an effective alternative to advance research in atomic visual actions recognition and thus video understanding. Our extensive cross-data experiments validate that precise annotation of fine-grained classes leads to preferable properties against datasets with orders of magnitude larger in size. <ref type="figure">Figure 1</ref> shows example atomic actions collected. <ref type="table" target="#tab_0">Table 1</ref> summarizes representative action recognition datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Action Recognition Dataset</head><p>Composite Action Dataset Representative action recognition datasets, such as HMDB51 <ref type="bibr" target="#b25">[25]</ref>, UCF101 <ref type="bibr" target="#b42">[42]</ref>, Hollywood-2 <ref type="bibr" target="#b29">[29]</ref>, ActivityNet <ref type="bibr" target="#b9">[9]</ref>, and Kinetics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">21]</ref> consist of short clips which are manually trimmed to capture a single action. These datasets are ideally suited for training fully supervised, whole-clip video classifiers. A few</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Videos Actions Atomic KTH <ref type="bibr" target="#b37">[37]</ref> 600 6 ? Weizmann <ref type="bibr" target="#b1">[2]</ref> 90 10 ? UCF Sports <ref type="bibr" target="#b34">[34]</ref> 150 10 Hollywood-2 <ref type="bibr" target="#b29">[29]</ref> 1,707 12 HMDB51 <ref type="bibr" target="#b25">[25]</ref> 7,000 51 UCF101 <ref type="bibr" target="#b42">[42]</ref> 13,320 101 DALY <ref type="bibr" target="#b44">[44]</ref> 510 10 AVA <ref type="bibr" target="#b17">[17]</ref> 387,000 80 ? Kinetics 700 <ref type="bibr" target="#b2">[3]</ref> 650,317 700 HACS <ref type="bibr">[</ref> datasets used in action recognition research, such as MSR Actions <ref type="bibr" target="#b47">[47]</ref>, UCF Sports <ref type="bibr" target="#b34">[34]</ref>, and JHMDB <ref type="bibr" target="#b19">[19]</ref>, provide spatio-temporal annotations in each frame for short videos, but they only contain few actions. Aside from the subcategory of shortening the video length, recent extensions such as UCF101 <ref type="bibr" target="#b42">[42]</ref>, DALY <ref type="bibr" target="#b44">[44]</ref>, and Hollywood2Tubes <ref type="bibr" target="#b30">[30]</ref> evaluate spatio-temporal localization in untrimmed videos, resulting in a performance drop due to the more difficult nature of the task. One common issue on these aforementioned datasets is that they are annotated with composite action classes (e.g., Playing Tennis), thus different human action gestures (e.g., Backhand Swing, Forehand Swing) are annotated under a single class. Another issue is that they tend to capture in wide field-of-view and thus include multiple human figures (e.g., tennis player, referee, audience) with different actions in a single frame, which inevitably introduce confusion to action analysis and recognition.</p><p>Atomic Action Dataset To model finer-level events, the AVA dataset <ref type="bibr" target="#b17">[17]</ref> was introduced to provide person-centric spatio-temporal annotations on atomic actions similar to some of the earlier works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b37">37]</ref>. Other special-  <ref type="table">Table 2</ref>. Performance of previous works on Kinetics 400 <ref type="bibr" target="#b21">[21]</ref>, Something-Something <ref type="bibr" target="#b16">[16]</ref>, and NTU-RGB+D <ref type="bibr" target="#b38">[38]</ref> dataset. We evaluate on both cross-subject (X-Sub) and cross-view (X-View) benchmarks for NTU-RGB+D. For a fair comparison, in this paper we use <ref type="bibr" target="#b21">[21]</ref> rather than <ref type="bibr" target="#b2">[3]</ref> as representative action recognition model still use <ref type="bibr" target="#b21">[21]</ref> for pre-training or benchmarking at the time of writing.</p><p>ized datasets such as Moments in Time <ref type="bibr" target="#b32">[32]</ref>, HACS <ref type="bibr" target="#b48">[48]</ref>, Something-Something <ref type="bibr" target="#b16">[16]</ref>, and Charades-Ego <ref type="bibr" target="#b40">[40]</ref> provide classes for atomic actions but none of them is a humancentric atomic action, where some of the video frames are ego-centric which only show part of a human body (e.g., hand), or no human action at all. Existing atomic action datasets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b32">32]</ref> tend to have atomicity under English linguistics, e.g., in Moments in Time <ref type="bibr" target="#b32">[32]</ref> Open is annotated on video clips with a tulip opening, an eye opening, a person opening a door, or a person opening a package, which is fundamentally different actions only sharing the verb open, which gives the possibility of finer division.</p><p>Fine-Grained Action Dataset Fine-grained action datasets try to solve ambiguous temporal annotation problems that were discussed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">31]</ref>. These datasets (e.g., <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39]</ref>) use systematic action labeling to annotate fine-grained labels on a small domain of actions. Breakfast <ref type="bibr" target="#b24">[24]</ref>, MPII Cooking 2 <ref type="bibr" target="#b35">[35]</ref>, and EPIC-KITCHENS <ref type="bibr" target="#b6">[6]</ref> offer fine-grained actions for cooking and preparing dishes, e.g., Twist Milk Bottle Cap <ref type="bibr" target="#b24">[24]</ref>. JIGSAWS <ref type="bibr" target="#b14">[14]</ref>, Diving48 <ref type="bibr" target="#b26">[26]</ref>, and FineGym <ref type="bibr" target="#b39">[39]</ref> offer fine-grained action datasets respectively for surgery, diving, and gymnastics. While existing fine-grained action datasets are well suited for benchmarks, due to their low variety and the narrow domain of the classes, they cannot be extended easily in general-purpose action recognition.</p><p>Our HAA500 dataset differs from all of the aforementioned datasets as we provide a wide variety of 500 finegrained atomic human action classes in various domains, where videos in each class only exhibit the relevant human atomic actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Action Recognition Architectures</head><p>Current action recognition architectures can be categorized into two major approaches: 2D-CNN and 3D-CNN. 2D-CNN <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b49">49]</ref> based models utilize imagebased 2D-CNN models on a single frame where features are aggregated to predict the action. While some methods (e.g., <ref type="bibr" target="#b8">[8]</ref>) use RNN modules for temporal aggregation over visual features, TSN <ref type="bibr" target="#b43">[43]</ref> shows that simple average pooling can be an effective method to cope with temporal aggregation. To incorporate temporal information to 2D-CNN, a two-stream structure <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b41">41]</ref> has been proposed to use RGB-frames and optical flow as separate inputs to convolutional networks. 3D-CNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b20">20]</ref> takes a more natural approach by incorporating spatio-temporal filters into the image frames. Inspired from <ref type="bibr" target="#b41">[41]</ref>, two-streamed inflated 3D-CNN (I3D) <ref type="bibr" target="#b3">[4]</ref> incorporates two-stream structure on 3D-CNN. SlowFast <ref type="bibr" target="#b11">[11]</ref> improves from I3D by showing that the accuracy increases when the 3D kernels are used only in the later layers of the model. A different approach is adopted in TPN <ref type="bibr" target="#b46">[46]</ref> where a high-level structure is designed to adopt a temporal pyramid network which can use either 2D-CNN or 3D-CNN as a backbone. Some models <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b45">45]</ref> use alternative information to predict video action. Specifically, ST-GCN <ref type="bibr" target="#b45">[45]</ref> uses a graph convolutional network to predict video action from pose estimation. However, their pose-based models cannot demonstrate better performance than RGB-frame-based models. <ref type="table">Table 2</ref> tabulates the performance of representative action recognition models on video action datasets, where 2Dskeleton based models <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b45">45]</ref> show considerably low accuracy in Kinetics 400 <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HAA500</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection</head><p>The annotation of HAA500 consists of two stages: vocabulary collection and video clip selection. While the bottom-up approach which annotates action labels on selected long videos was often used in atomic/fine-grained action datasets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b39">39]</ref>, we aim to build a clean and finegrained dataset for atomic action recognition, thus the video clips are collected based on pre-defined atomic actions following a top-down approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Vocabulary Collection</head><p>To make the dataset as clean as possible and useful for recognizing fine-grained atomic actions, we narrowed down the scope of our super-classes into 4 areas; Sport/Athletics, Playing Musical Instruments, Games and Hobbies, and Daily Actions, where future extension beyond the existing classes is feasible. We select action labels where the variations within a class are typically indistinguishable. For example, instead of Hand Whistling, we have Whistling with One Hand and Whistling with Two Hands, as the variation is large and distinguishable. Our vocabulary collection methodology makes the dataset hierarchical where atomic actions may be combined to form a composite action, e.g., Whistling or Playing Soccer. Consequently, HAA500 contains 500 atomic action classes, where 212 are Sport/Athletics, 51 are Playing Musical Instruments, 82 are Games and Hobbies and 155 are Daily Actions.   <ref type="table">Table 3</ref>. Summary of HAA500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Video Clip Selection</head><p>To ensure our dataset is clean and class-balanced, all the video clips are collected from YouTube with the majority having a resolution of at least 720p and each class of atomic action containing 16 training clips. We manually select the clips with apparent human-centric actions where the personof-interest is the only dominant person in the frame at the center with their body clearly visible. To increase diversity among the video clips and avoid unwanted bias, all the clips were collected from different YouTube videos, with different environment settings so that the action recognition task cannot be trivially reduced to identifying the corresponding backgrounds. Clips are properly trimmed in a frameaccurate manner to cover the desired actions while assuring every video clip to have compatible actions within each class (e.g., every video in the class Salute starts on the exact frame where the person is standing still before moving the arm, and the video ends when the hand goes next to the eyebrow). Refer to <ref type="figure">Figure 1</ref> again for examples of the selected videos. <ref type="table">Table 3</ref> summarizes the HAA500 statistics. HAA500 includes 500 atomic action classes where each class contains 20 clips, with an average length of 2.12 seconds. Each clip was annotated with meta-information which contains the following two fields: the number of dominant people in the video and the camera movement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Statistics</head><p>Dataset Clip Length Irr. Actions Camera Cuts UCF101 <ref type="bibr" target="#b42">[42]</ref> Varies HMDB51 <ref type="bibr" target="#b25">[25]</ref> Varies ? AVA <ref type="bibr" target="#b17">[17]</ref> 1 second ? ? HACS <ref type="bibr" target="#b48">[48]</ref> 2 second ? Kinetics <ref type="bibr" target="#b21">[21]</ref> 10 second ? ? M.i.T. <ref type="bibr" target="#b32">[32]</ref> 3 second HAA500</p><p>Just Right <ref type="table">Table 4</ref>. Clip length and irrelevant frames of video action datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Training/Validation/Test Sets</head><p>Since the clips in different classes are mutually exclusive, all clips appear only in one split. The 10,000 clips are split as 16:1:3, resulting in segments of 8,000 training, 500 validation, and 1,500 test clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties and Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Clean Labels for Every Frame</head><p>Most video datasets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b42">42]</ref> show strong label noises, due to the difficulties of collecting clean video action datasets. Some <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b42">42]</ref> often focus on the "scene" of the video clip, neglecting the human "action" thus including irrelevant actions or frames with visible camera cuts in the clip. Also, video action datasets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b48">48]</ref> have fixed-length video clips, so irrelevant frames are inevitable for shorter actions. Our properly trimmed video collection guarantees a clean label for every frame. <ref type="table">Table 4</ref> tabulates clip lengths and label noises of video action datasets. <ref type="figure" target="#fig_1">Figure 2</ref> shows examples of label noises. As HAA500 is constructed with accurate temporal annotation in mind, we are almost free from any adverse effects due to these noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Human-Centric</head><p>One potential problem in action recognition is that the neural network may predict by trivially comparing the background scene in the video, or detecting key elements in a Dataset Detectable Joints Kinetics 400 <ref type="bibr" target="#b21">[21]</ref> 41.0% UCF101 <ref type="bibr" target="#b42">[42]</ref> 37.8% HMDB51 <ref type="bibr" target="#b25">[25]</ref> 41.8% FineGym <ref type="bibr" target="#b39">[39]</ref> 44.7% HAA500 69.7% <ref type="table">Table 5</ref>. Detectable joints of video action datasets. We use Alpha-Pose <ref type="bibr" target="#b10">[10]</ref> to detect the largest person in the frame, and count the number of joints with a score higher than 0.5.</p><p>frame (e.g., a basketball to detect Playing Basketball) rather than recognizing the pertinent human gesture, thus causing the action recognition to have no better performance improvements over scene/object recognition. The other problem stems from the video action datasets where videos captured in wide field-of-view contain multiple people in a single frame <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b48">48]</ref>, while videos captured using narrow field-of-view only exhibit very little body part in interaction with the pertinent object <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>In <ref type="bibr" target="#b17">[17]</ref> attempts were made to overcome this issue through spatial annotation of each individual in a given frame. This introduces another problem of action localization and thus further complicating the difficult recognition task. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates example frames of different video action datasets.</p><p>HAA500 contributes a curated dataset where human joints can be clearly detected over any given frame, thus allowing the model to benefit from learning human movements than just performing scene recognition. As tabulated in <ref type="table">Table 5</ref>, HAA500 has high detectable joints <ref type="bibr" target="#b10">[10]</ref> of 69.7%, well above other representative action datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Atomic</head><p>Existing atomic action datasets such as <ref type="bibr">[5,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b32">32]</ref> are limited by English linguistics, where action verbs (e.g., walk, throw, pull, etc.) are decomposed. Such classification does not fully eliminate the aforementioned problems of composite action datasets. <ref type="figure" target="#fig_3">Figure 4</ref> shows cases of different atomic action datasets where a single action class contains fundamentally different actions.</p><p>On the other hand, our fine-grained atomic actions contain only a single type of action under each class, e.g., Baseball -Pitch, Yoga -Tree, Hopscotch -Spin, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Scalability</head><p>Requiring only 20 video annotations per class, or around 600 frames to characterize a human-centric atomic action curated as described above, our class-balanced dataset is highly scalable compared to other representative datasets requiring annotation of hundreds or even thousands of videos. In practice, our annotation per class takes around 20-60 minutes including searching the Internet for videos with expected quality. The detailed annotation procedure is available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Studies</head><p>We study HAA500 over multiple aspects using widely used action recognition models. Left of <ref type="table">Table 6</ref> shows the performance of the respective models when they are trained with HAA500. For a fair comparison between different models and training datasets, all the experiments have been performed using hyper parameters given by the original authors without ImageNet <ref type="bibr" target="#b7">[7]</ref> pre-training.</p><p>For Pose models except for ST-GCN <ref type="bibr" target="#b45">[45]</ref>, we use threechannel pose joint heatmaps <ref type="bibr" target="#b10">[10]</ref> to train pose models. RGB, Flow <ref type="bibr" target="#b18">[18]</ref> and Pose <ref type="bibr" target="#b10">[10]</ref> all show relatively similar performance in HAA500, where none of them shows superior performance than the others. Given that pose heatmap has far less information than given from RGB frames or optical flow frames, we expect that easily detectable joints of HAA500 benefit the pose-based model performance.  <ref type="table">Table 6</ref>. Left: HAA500 trained over different models. Right: Composite action classification accuracy of different models when they are trained with/without atomic action classification. Numbers are bolded when the difference is larger than 1%. <ref type="figure">Figure 5</ref>. Visualization of HAA500. We extract 1024-vectors from the second last layer of RGB-I3D and plot them using t-SNE.</p><p>Furthermore, we study the benefits of atomic action annotation on video recognition, as well as the importance of human-centric characteristics of HAA500. In this paper, we use I3D-RGB <ref type="bibr" target="#b3">[4]</ref> with 32 frames for all of our experiments unless otherwise specified. We use AlphaPose <ref type="bibr" target="#b10">[10]</ref> for the models that require human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visualization</head><p>To study the atomic action recognition, we train RGB-I3D model on HAA500 and extract embedding vectors from the second last layer and plot them using truncated SVD and t-SNE. From <ref type="figure">Figure 5</ref>, the embedding vectors show clear similarities to the natural hierarchy of human action. On the left of the figure, we see a clear distinction between classes in Playing Sports and classes in Playing Musical Instruments. Specifically, in sports, we see similar super-classes, Snowboarding and Skiing, under close embedding space, while Basketball, Balance Beam (Gymnastics), and <ref type="figure">Figure  Skating</ref> are in their distinctive independent spaces. We observe super-class clustering of composite actions when only the atomic action labeling has been used to train the model. This visualization hints the benefit of fine-grained atomic action labeling for composite action classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Atomic Action</head><p>We have previously discussed that modern action recognition datasets introduce ambiguities where two or more composite actions sharing the same atomic actions, while a single composite action class may contain multiple distinguishable actions (e.g., a composite action Playing Soccer has Soccer-Dribble, Soccer-Throw, etc.). HAA500 addresses this issue by providing fine-grained atomic action labels that distinguish similar atomic action in different composite actions.</p><p>To study the benefits of atomic action labels, specifically, how it helps composite action classification for ambiguous classes, we selected two areas from HAA500, Sports/Athletics and Playing Musical Instruments, in which composite actions contain strong ambiguities with other actions in the area. We compare models trained with two different types of labels: 1) only composite labels and 2) atomic + composite labels, then we evaluate the performance on composite action classification. Results are tabulated on the right of <ref type="table">Table 6</ref>. Accuracy of the models trained with only composite labels are under Inst. and Sport column, and the accuracy of composite action classification trained with atomic action classification is listed on the other columns.</p><p>We can observe improvements in composite action classification when atomic action classification is incorporated. The fine-grained action decomposition in HAA500 enables the models to resolve ambiguities of similar atomic actions and helps the model to learn the subtle differences in the atomic actions across different composite actions. This demonstrates the importance of proper labeling of finegrained atomic action which can increase the performance for composite action classification without changing the model architecture or the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Human-Centric</head><p>HAA500 is designed to contain action clips with a high percentage of detectable human figures. To study the importance of human-pose in fine-grained atomic action recognition, we compare the performance of HAA500 and Fine-Gym when both RGB and pose estimation are given as in- put. For pose estimation, we obtain the 17 joint heatmaps from AlphaPose <ref type="bibr" target="#b10">[10]</ref> and merge them into 3 channels; head, upper-body, and lower-body. <ref type="table">Table 7</ref> tabulates the results. In three out of four areas of HAA500, I3D-RGB shows better performance than I3D-Pose, due to the vast amount of information given to the model. I3D-Pose shows the highest performance on Sports/Athletics with vibrant and distinctive action, while I3D-Pose fails to show comparable performance in Playing Musical Instrument area, where predicting the atomic action from only 17 joints is quite challenging. Nonetheless, our experiments show a performance boost when both pose estimation and RGB frame are fed to the atomic action classification model, implicating the importance of human action in HAA500 action classification. For FineGym -Gym288, due to the rapid athletic movements resulting in blurred frames, the human pose is not easily recognizable which accounts for relatively insignificant improvements when pose has been used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Observations</head><p>We present notable characteristics observed from HAA500 with our cross-dataset experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Fine-Tuning over HAA500</head><p>Here, we test how to exploit the curated HAA500 dataset to detect action in "in-the-wild" action datasets. We pre-train I3D-RGB <ref type="bibr" target="#b3">[4]</ref> using HAA500 or other video action datasets <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref>, and freeze all the layers except for the last three for feature extraction. We then fine-tune the last three layers with "in-the-wild" composite action datasets <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b42">42]</ref>. <ref type="table">Table 8</ref> tabulates the fine-tuning result. Our dataset is carefully curated to have a high variety of backgrounds and  <ref type="table">Table 9</ref>. Accuracy improvements on person-of-interest normalization. Numbers are composite action classification accuracy. people while having consistent actions over each class. Despite being comparably smaller and more "human-centric" than other action recognition datasets, HAA500's cleanness and high variety make it easily transferable to different tasks and datasets.</p><p>Effects of Scale Normalization HAA500 has high diversity in human positions across the video collection. Here, we choose an area of HAA500, Playing Musical Instruments, to investigate the effect of human-figure normalization on detection accuracy. We have manually annotated the bounding box of the person-of-interest in each frame and cropped them for the model to focus on the human action. In <ref type="table">Table 9</ref>, we test models that were trained to detect the composite actions or both composite and atomic actions.</p><p>While HAA500 is highly human-centric with person-ofinterest as the most dominant figure of the frame, action classification on the normalized frames still shows considerable improvement when trained on either atomic action annotations or composite action annotations. This indicates the importance of spatial annotation for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Object Detection</head><p>In most video action datasets, non-human objects exist as a strong bias to the classes (e.g., basketball in Playing Basketball). When highly diverse actions (e.g., Shooting a Basketball, Dribbling a Basketball, etc.) are annotated under a single class, straightforward deep-learning models tend to suffer from the bias and will learn to detect the easiest common factor (basketball) among the video clips, rather than "seeing" the pertinent human action. Poorly designed video action dataset encourages the action classification model to trivially become an object detection model.</p><p>In HAA500, every video clip in the same class contains compatible actions, making the common factor to be the "action", while objects are regarded as "ambiguities" that spread among different classes (e.g., basketball exists in both Shooting a Basketball and Dribbling a Basketball). To test the influence of "object" in HAA500, we design an experiment similar to investigating the effect of human poses, as presented in <ref type="table">Table 7</ref>, where we use object detection heatmap instead. Here we use Fast RCNN <ref type="bibr" target="#b15">[15]</ref> trained with COCO <ref type="bibr" target="#b28">[28]</ref> dataset to generate the object heatmap. Among 80 detectable objects in COCO, we select 42 objects in 5 categories (sports equipment, food, animals, cutleries, and vehicles) to draw a 5-channel heatmap. Similar to Table 7, the heatmap channel is appended to the RGB channel as input.  <ref type="table" target="#tab_0">Table 10</ref>. Accuracy of I3D when trained with object heatmap. HAA-COCO denotes 147 classes of HAA500 expected to have objects that were detected. <ref type="table" target="#tab_0">Table 10</ref> tabulates the negligible effect of objects in atomic action classification of HAA500, including the classes that are expected to use the selected objects (HAA-COCO), while UCF101 shows improvements when object heatmap is used as a visual cue. Given the negligible effect of object heatmaps, we believe that fine-grained annotation of actions can effectively eliminate unwanted ambiguities or bias ("objects") while in UCF101 (composite action dataset), "objects" can still affect action prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Dense Temporal Sampling</head><p>The top of Table 11 tabulates the performance difference of HAA500 and other datasets over the number of frames used during training and testing. The bottom of <ref type="table" target="#tab_0">Table 11</ref> tabulates the performance with varying strides with a window size of 32 frames, except AVA which we test with 16 frames. Top-1 accuracies on action recognition are shown except AVA which shows mIOU due to its multi-labeled nature of the dataset.</p><p>As expected, most datasets show the best performance when 32 frames are fed. AVA shows a drop in performance due to the irrelevant frames (e.g., action changes, camera cuts, etc.) included in the wider window. While all the datasets show comparable accuracy when the model only uses a single frame (i.e., when the problem has been reduced to a "Scene Recognition" problem), both HAA500 and Gym288 show a significant drop compared to their accuracy in 32 frames. While having an identical background contributes to the performance difference for Gym288, from HAA500, we see how temporal action movements are crucial for the detection of atomic actions, and they cannot be trivially detected using a simple scene detecting model.</p><p>We also see that the density of the temporal window is another important factor in atomic action classification. We see that both HAA500 and Gym288, which are fine-grained action datasets, show larger performance drops when the frames have been sampled with strides of 2 or more, reflecting the importance of sampling for short temporal action movements in fine-grained action classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality versus Quantity</head><p>To study the importance of our precise temporal annotation against the size of a dataset, we modify HAA500 by relaxing the temporal annotation requirement, i.e., we take a longer clip than the original annotation. Our relaxed-HAA500 consists of 4400K labeled frames, a significant increase from the original HAA500 with 591K frames. <ref type="table" target="#tab_0">Table 12</ref> tabulates the performance # of frames HAA500 UCF101 <ref type="bibr" target="#b42">[42]</ref> AVA <ref type="bibr" target="#b17">[17]</ref>  comparison between the original and the relaxed version of HAA500 on the original HAA500 test set. We observe the performance drop in all areas, with a significant drop in Playing Sports, where accurate temporal annotation benefits the most. Performance drop in Playing Musical Instruments area is less significant, as start/finish of action is vaguely defined in these classes. We also test the finetuning performance of relaxed-HAA500, where the bottommost row of <ref type="table">Table 8</ref> tabulates the performance drop when the relaxed-HAA500 is used for pre-training. Both of our experiments show the importance of accurate temporal labeling over the size of a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduces HAA500, a new human action dataset with fine-grained atomic action labels and humancentric clip annotations, where the videos are carefully selected such that the relevant human poses are apparent and detectable. With carefully curated action videos, HAA500 does not suffer from irrelevant frames, where videos clips only exhibit the annotated action. With a small number of clips per class, HAA500 is highly scalable to include more action classes. We have demonstrated the efficacy of HAA500 where action recognition can be greatly benefited from our clean, highly diversified, class-balanced finegrained atomic action dataset which is human-centric with a high percentage of detectable poses. On top of HAA500, we have also empirically investigated several important factors that can affect the performance of action recognition. We hope HAA500 and our findings could facilitate new advances in video action recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAA500: Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Video Collection Procedure</head><p>To guarantee a clean dataset with no label noises, we adopt a strict video collecting methodology for every class. We detail the method below.</p><p>1. We assign a single annotator for a single class. This is to assure that the same rule applies to every video in a class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The action class is classified as either continuous ac</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiment Detail</head><p>In this section, we explain some of the experiment details of our paper.</p><p>Variable Length of a Video For model [?, ?, ?, ?], we randomly select 32 adjacent frames of a video during training. If the video is shorter than 32 frames, we replicate the last frame to match the size. During testing, we replicate the last frame to match the size to a multiple of 32, where the video is then divided into smaller mini-clips of size 32. The prediction score of each mini-clip is averaged to get the final prediction. In <ref type="table" target="#tab_0">Table 11</ref>, where we train with fewer frames, we zero-pad on both ends to size 16. On ST-GCN [?] we follow the same procedure of the original paper, where the video is either truncated or replicated to match the length of 300.</p><p>Implementation In all of our experiments, we use Py-Torch for our deep learning framework. We use the official code of the model when they are available. While we use the same hyperparameters which the authors used for their model, for a fair comparison we do not pre-train the model before training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">List of Classes in HAA500</head><p>Here, we list classes of HAA500 in each area.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Composite Classes</head><p>We list how Musical Instrument and Sports/Athletics classes form to become composite actions. We list indices of the classes for each composite action. <ref type="figure">Figure 1</ref> shows the first frame of a video in different classes. <ref type="figure" target="#fig_1">Figure 2</ref> lists diverse videos per class. <ref type="figure" target="#fig_3">Figure 4</ref> shows the hierarchy of action classes in Sports/Athletics area where the actions are grouped together with other actions in the same sports category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sports/Athletics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Hierarchy</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>HAA500 project page: https://www.cse.ust.hk/haa. This work was supported by Kuaishou Technology and the Research Grant Council of the Hong Kong SAR under grant no. 16201818.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Different types of label noise in action recognition datasets. (a): Kinetics 400 has a fixed video length of 10 seconds which cannot accurately annotate quick actions like Shooting Basketball where the irrelevant action of dribbling the ball is included in the clip. (b): A camera cut can be seen, showing unrelated frames (audience) after the main action. (c): By not having a frame-accurate clipping, the clip starts with a person-of-interest in the midair, and quickly disappears after few frames, causing the rest of the video clip not to have any person in action. (d): Our HAA500 accurately annotates the full motion of Uneven Bars -Land without any irrelevant frames. All the videos in the class start with the exact frame an athlete puts the hand off the bar, to the exact frame when he/she finishes the landing pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The video clips in AVA, HACS, and Kinetics 400 contain multiple human figures with different actions in the same frame. Something-Something focuses on the target object and barely shows any human body parts. In contrast, all video clips in HAA500 are carefully curated where each video shows either a single person or the person-of-interest as the most dominant figure in a given frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Coarse-grained atomic action datasets label different actions under a single English action verb. HAA500 (Bottom) has fine-grained classes where the action ambiguities are eliminated as much as possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 .Figure 2 .Figure 3 .</head><label>123</label><figDesc>Video samples of different classes. HAA500 contains diverse videos per action class. Six sample frames of different videos. Each frame has an equal distance from the other, the first and the last sample frame are the first and the last frame of the video. In discrete action classes, (Long Jump -Jump, Push Up, Soccer -Shoot), every video in the class shows a single motion. For action classes where it is hard to define a single motion (i.e., continuous actions, e.g., Play Violin), videos are cut in appropriate length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Hierarchy of action classes in Sports/Athletics area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>48] Moments in Time [32] 1,000,000 1,550,000 FineGym [39] 32,687 HAA500 10,000</cell><cell>200 339 530 500</cell><cell>? ? ? ?</cell></row></table><note>Summary of representative action recognition datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>RGB 66.01% 56.86% 75.82% 77.12% I3D-Flow 73.20% 77.78% 75.16% 74.51% 2-Stream 77.78% 80.39% 83.01% 80.39%</figDesc><table><row><cell>Original</cell><cell>Normalized</cell></row><row><cell cols="2">Composite Both Composite Both</cell></row><row><cell>I3D-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Jihoon Chung 1,2 Cheng-hsin Wuu 1,3 Hsuan-ru Yang 1 Yu-Wing Tai 1,4 Chi-Keung Tang 1 1 HKUST 2 Princeton University 3 Carnegie Mellon University 4 Kuaishou Technology jc5933@princeton.edu cwuu@andrew.cmu.edu hyangap@ust.hk yuwing@gmail.com cktang@cs.ust.hk</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>tion or discrete action. Discrete action is when the action can have a single distinguishable action sequence. (e.g., Baseball-Swing, Yoga-Bridge, etc.). Continuous action otherwise. (Running, Playing Violin, etc.) (a) If it is discrete, make an internal rule to define the action. (e.g., Jumping Jack starts and ends when the person is standing still. The video clip contains only a single jump. Push-up starts and ends when the person is at the highest point. It should only have a single push-up). Every video should follow the internal rule so that every action in the class has compatible motion.</figDesc><table><row><cell>? 20 videos are split into train/val/test set by 16/1/3. The validation set contains the "stan-dard" body action of the class, and 3 videos in the test set should be well diverse.</cell></row><row><cell>4. Two or more reviewers that are not the annotator re-view the video to check for any mistakes.</cell></row><row><cell>(b) For continuous, we take video clips with appro-priate length.</cell></row><row><cell>3. Here are rules that the annotator has to follow.</cell></row><row><cell>? 20 videos should be unique to each other with a varied person, varied backgrounds.</cell></row><row><cell>? The person in action should be the dominant per-son of the frame. If there are people of non-interest, they should not be performing any ac-tion.</cell></row><row><cell>? Camera cuts should not exist.</cell></row><row><cell>? Every video should include a large portion of the human body.</cell></row><row><cell>)</cell></row></table><note>? It is fine to have action variance that doesn't in- fluence the semantics of the action. (e.g., a per- son can sit or stand in Whistling with One Hand as long as the motion of whistling exists.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaver</surname></persName>
		</author>
		<imprint>
			<pubPlace>Rebecca Marvin, Caroline Pantofaru, Nathan Reale, Loretta Guarino Reid, Kevin W</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ava-speech: A densely labeled dataset of speech activity in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjam?n</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>B?jar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Miccai workshop: M2cai</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">STM: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Senjian An, Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">TSM: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Trespassing the boundaries: Labeling temporal bounds for object interactions in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>TPAMI</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Actor-context-actor relation network for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Action MACH a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, and Bernt Schiele. Recognizing fine-grained and composite activities using hand-centric features and script data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ava active speaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Recognizing human actions: A local SVM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sch?ldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">NTU RGB+D: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Finegym: A hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05197</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gangnam Style Dance 449. Grass Skating 450. Guitar Flip 451. Hopscotch Pickup 452. Hopscotch Skip 453. Hopscotch Spin 454. Ice Scuplting 455. Juggling Balls 456. Kick Jianzi 457. Knitting 458. Marble Scuplting 459. Moonwalk 460. Piggyback Ride 461. Play Diabolo 462. Play Kendama 463. Play Yoyo 464. Playing Nunchucks 465. Playing Rubiks Cube 466. Playing Seesaw 467. Playing Swing 468. Rock Balancing 469. Rock Paper Scissors 470. Running On Four 471. Sack Race 472. Sand Scuplting 473. Segway 474. Shoot Dance 475. Shooting Handgun 476. Shooting Shotgun 477. Shuffle Dance 478. Sling 479. Slingshot 480. Snow Angel 481. Speed Stack 482. Spinning Basketball 483. Spinning Book 484. Spinning Plate 485. Stone Skipping 486. Sword Swallowing 487. Taichi Fan 488. Taking Photo Camera 489</title>
	</analytic>
	<monogr>
		<title level="m">Skate Jump Spin 77. Figure Skate Scratch Spin 78. Figure Skate Sit Spin 79. Floor Rotate 80. Floor Spin 81. Football Catch 82. Football Run 83. Football Throw 84. Forward Fold 85. Forward Jump 86. Forward Roll 87. Frisbee Catch 88. Frisbee Throw 89. Golf Part 90. Golf Swing 91. Grass Skiing 92. Gym Lift 93. Gym Lunges 94. Gym Plank 95. Gym Pull 96. Gym Push 97. Gym Ride 98. Gym Run 99. Gym Squat Lifting Hang 203. Weight Lifting Overhead 204. Weight Lifting Stand 205. Windsurfing 206</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Castanet 376. Cello 377. Clarinet 378. Conga Drum 379. Cornett 380. Cymbals Dancing Circulating 442. Fish-Hunting Hold 443. Fish-Hunting Pull 444. Flipping Bottle 445. Floss Dance 446. Flying Kite 447. Ganggangsullae 448</note>
	<note>Using Lawn Mower Riding Type 351. Using Metal Detector 352. Using Scythe 353. Using Spinning Wheel 354. Using String Trimmer 355. Using Typewriter 356. Walking With Crutches 357. Walking With Walker 358. Wall Paint Brush 359. Wall Paint Roller 360. Washing Clothes 361. Washing Dishes 362. Watering Plants 363. Wear Face Mask 364. Wear Helmet 365. Whipping 366. Writing On Blackboard 367. Yawning Musical Instruments 368. Accordian 369. Bagpipes 370. Bangu 371. Banjo 372. Bass Drum 373. Bowsaw 374. Cajon Drum 375. Taking Selfie 490. Tap Dancing 491. Three Legged Race 492. Throw Boomerang 493. Throw Paper-Plane 494. Tight-Rope Walking 495. Trampoline 496. Tug Of War 497. Underarm Turn 498. Walking On Stilts 499. Whistle One Hand 500. Whistle Two Hands</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Here we list the classes in HAA-COCO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haa-Coco</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
