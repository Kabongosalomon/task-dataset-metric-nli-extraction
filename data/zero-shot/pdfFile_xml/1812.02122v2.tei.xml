<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Attraction Field Representation for Robust Line Segment Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
							<email>xuenan@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
							<email>fudong-wang@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
							<email>guisong.xia@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
							<email>tianfuwu@ncsu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">NC State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Attraction Field Representation for Robust Line Segment Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a region-partition based attraction field dual representation for line segment maps, and thus poses the problem of line segment detection (LSD) as the region coloring problem. The latter is then addressed by learning deep convolutional neural networks (ConvNets) for accuracy, robustness and efficiency. For a 2D line segment map, our dual representation consists of three components: (i) A region-partition map in which every pixel is assigned to one and only one line segment; (ii) An attraction field map in which every pixel in a partition region is encoded by its 2D projection vector w.r.t. the associated line segment; and (iii) A squeeze module which squashes the attraction field to a line segment map that almost perfectly recovers the input one. By leveraging the duality, we learn ConvNets to compute the attraction field maps for raw input images, followed by the squeeze module for LSD, in an endto-end manner. Our method rigorously addresses several challenges in LSD such as local ambiguity and class imbalance. Our method also harnesses the best practices developed in ConvNets based semantic segmentation methods such as the encoder-decoder architecture and the a-trous convolution. In experiments, our method is tested on the WireFrame dataset [1] and the YorkUrban dataset [2] with state-of-the-art performance obtained. Especially, we advance the performance by 4.5 percents on the WireFrame dataset. Our method is also fast with 6.6 ? 10.4 FPS, outperforming most of the existing line segment detectors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>lenging low-level task in computer vision. The resulting line segment maps provide compact structural information that facilitate many up-level vision tasks such as 3D reconstruction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, image partition <ref type="bibr" target="#b3">[4]</ref>, stereo matching <ref type="bibr" target="#b4">[5]</ref>, scene parsing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">7]</ref>, camera pose estimation <ref type="bibr" target="#b7">[8]</ref>, and image stitching <ref type="bibr" target="#b8">[9]</ref>. LSD usually consists of two steps: line heat map generation and line segment model fitting. The former can be computed either simply by the gradient magnitude map (mainly used before the recent resurgence of deep learning) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, or by a learned convolutional neural network (ConvNet) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> in state-of-the-art methods <ref type="bibr" target="#b0">[1]</ref>. The latter needs to address the challenging issue of handling unknown multi-scale discretization nuisance factors (e.g., the classic zig-zag artifacts of line segments in digital images) when aligning pixels or linelets to form line segments in the line heat map. Different schema have been proposed, e.g., the -meaningful alignment method proposed in <ref type="bibr" target="#b9">[10]</ref> and the junction <ref type="bibr" target="#b14">[15]</ref> guided alignment method proposed in <ref type="bibr" target="#b0">[1]</ref>. The main drawbacks of existing two-stage methods are in two-fold: lacking elegant solutions to solve the local ambiguity and/or class imbalance in line heat map generation, and requiring extra carefully designed heuristics or supervisedly learned contextual information in inferring line segments in the line heat map.</p><p>In this paper, we focus on learning based LSD framework and propose a single-stage method which rigorously addresses the drawbacks of existing LSD approaches. Our method is motivated by two observations,</p><p>? The duality between region representation and boundary contour representation of objects or surfaces, which is a well-known fact in computer vision.</p><p>? The recent remarkable progresses for image semantic segmentation by deep ConvNet based methods such as U-Net <ref type="bibr" target="#b15">[16]</ref> and DeepLab V3+ <ref type="bibr">[17]</ref>.</p><p>So, the intuitive idea of this paper is that if we can bridge line segment maps and their dual region representations, we will pose the problem of LSD as the problem of region coloring, and thus open the door to leveraging the best practices developed in state-of-the-art deep ConvNet based image semantic segmentation methods to improve performance for LSD. By dual region representations, it means they are capable of recovering the input line segment maps in a nearly perfect way via a simple algorithm. We present an efficient and straightforward method for computing the dual region representation. By re-formulating LSD as the equivalent region coloring problem, we address the aforementioned challenges of handling local ambiguity and class imbalance in a principled way. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the proposed method. Given a 2D line segment map, we represent each line segment by its geometry model using the two end-points 1 . In computing the dual region representation, there are three components (detailed in Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Method Overview</head><p>? A region-partition map. It is computed by assigning every pixel to one and only one line segment based on a proposed point to line segmentation distance function. The pixels associated with one line segment form a region. All regions represent a partition of the image lattice (i.e., mutually exclusive and the union occupies the entire image lattice).</p><p>? An attraction field map. Each pixel in a partition region has one and only one corresponding projection point on the geometry line segment (but the reverse is often a one-to-many mapping). In the attraction field map, every pixel in a partition region is then represented by its attraction/projection vector between the pixel and its projection point on the geometry line segment 2 .</p><p>? A light-weight squeeze module. It follows the attraction field to squash partition regions in an attraction field map to line segments that almost perfectly recovers the input ones, thus bridging the duality between region-partition based attraction field maps and line segment maps.</p><p>The proposed method can also be viewed as an intuitive expansion-and-contraction operation between 1D line segments and 2D regions in a simple projection vector field: The region-partition map generation jointly expands all line segments into partition regions, and the squeeze module degenerates regions into line segments.</p><p>With the duality between a line segment map and the corresponding region-partition based attraction field map, we first convert all line segment maps in the training dataset to their attraction field maps. Then, we learn ConvNets to predict the attraction field maps from raw input images in an end-to-end way. We utilize U-Net <ref type="bibr" target="#b15">[16]</ref> and a modified network based on DeepLab V3+ <ref type="bibr">[17]</ref> in our experiments. After the attraction field map is computed, we use the squeeze module to compute its line segment map.</p><p>In experiments, the proposed method is tested on the WireFrame dataset <ref type="bibr" target="#b0">[1]</ref> and the YorkUrban dataset <ref type="bibr" target="#b1">[2]</ref> with state-of-the-art performance obtained comparing with <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>. In particular, we improve the performance by 4.5% on the WireFrame dataset. Our method is also fast with 6.6 ? 10.4 FPS, outperforming most of line segment detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work and Our Contributions</head><p>The study of line segment detection has a very long history since 1980s <ref type="bibr" target="#b18">[19]</ref>. The early pioneers tried to detect line segments based upon the edge map estimation. Then, the perception grouping approaches based on the Gestalt Theory are proposed. Both of these methods concentrate on the hand-crafted low-level features for the detection, which have become a limitation. Recently, the line segment detection and its related problem edge detection have been studied under the perspective of deep learning, which dramatically improved the detection performance and brings us of great practical importance for real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Detection based on Hand-crafted Features</head><p>In a long range of time, the hand-crafted low-level features (especially for image gradients) are heavily used for line segment detection. These approaches can be divided into edge map based approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18]</ref> and perception grouping approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. The edge map based approaches treat the visual features as a discriminated feature for edge map estimation and subsequently applying the Hough transform <ref type="bibr" target="#b18">[19]</ref> to globally search line configurations and then cutting them by using thresholds. In contrast to the edge map based approaches, the grouping methods directly use the image gradients as local geometry cues to group pixels into line segment candidates and filter out the false positives <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Actually, the features used for line segment detection can only characterize the local response from the image appearance. For the edge detection, only local response without global context cannot avoid false detection. On the other hand, both the magnitude and orientation of image gradients are easily affected by the external imaging condition (e.g. noise and illumination). Therefore, the local nature of these features limits us to extract line segments from images robustly. In this paper, we break the limitation of locally estimated features and turn to learn the deep features that hierarchically represent the information of images from low-level cues to high-level semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Edge and Line Segment Detection</head><p>Recently, HED <ref type="bibr" target="#b12">[13]</ref> opens up a new era for edge perception from images by using ConvNets. The learned multi-scale and multi-level features dramatically addressed the problem of false detection in the edge-like texture regions and approaching human-level performance on the BSDS500 dataset <ref type="bibr" target="#b25">[26]</ref>. Followed by this breakthrough, a tremendous number of deep learning based edge detection approaches are proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">17]</ref>. Under the perspective of binary classification, the edge detection has been solved to some extent. It is natural to upgrade the traditional edge map based line segment detection by alternatively using the edge map estimated by ConvNets. However, the edge maps estimated by ConvNets are usually over-smoothed, which will lead to local ambiguities for accurate localization. Further, the edge maps do not contain enough geometric information for the detection. According to the development of deep learning, it should be more reasonable to propose an end-to-end line segment detector instead of only applying the advances of deep edge detection.</p><p>Most recently, Huang et al. <ref type="bibr" target="#b0">[1]</ref> have taken an important step towards this goal by proposing a large-scale dataset with high quality line segment annotations and approaching the problem of line segment detection as two parallel tasks, i.e., edge map detection and junction detection. As a final step for the detection, the resulted edge map and junctions are fused to produce line segments. To the best of our knowledge, this is the first attempt to develop a deep learning based line segment detector. However, due to the sophisticated relation between edge map and junctions, it still remains a problem unsolved. Benefiting from our proposed formulation, we can directly learn the line segments from the attraction field maps that can be easily obtained from the line segment annotations without the junction cues.</p><p>Our Contributions The proposed method makes the following main contributions to robust line segment detection.</p><p>? A novel dual representation is proposed by bridging line segment maps and region-partition-based attraction field maps. To our knowledge, it is the first work that utilizes this simple yet effective representation in LSD.</p><p>? With the proposed dual representation, the LSD problem is re-formulated as the region coloring problem, thus opening the door to leveraging state-of-the-art semantic segmentation methods in addressing the challenges of local ambiguity and class imbalance in existing LSD approaches in a principled way.</p><p>? The proposed method obtains state-of-the-art performance on two widely used LSD benchmarks, the Wire-Frame dataset (with 4.5% significant improvement) and the YorkUrban dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Attraction Field Representation</head><p>In this section, we present details of the proposed regionpartition representation for LSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Region-Partition Map</head><p>Let ? be an image lattice (e.g., 800 ? 600). A line segment is denote by l i = (x s i , x e i ) with the two end-points being x s i and x e i (non-negative real-valued positions due to sub-pixel precision is used in annotating line segments) respectively. The set of line segments in a 2D line segment map is denoted by L = {l 1 , ? ? ? , l n } . For simplicity, we also denote the line segment map by L. <ref type="figure">Figure 2</ref> illustrates a line segment map with 3 line segments in a 10 ? 10 image lattice.</p><p>Computing the region-partition map for L is assigning every pixel in the lattice to one and only one of the n line segments. To that end, we utilize the point-to-line-segment distance function. Consider a pixel p ? ? and a line segment l i = (x s i , x e i ) ? L, we first project the pixel p to the straight line going through l i in the continuous geometry space. If the projection point is not on the line segment, we use the closest end-point of the line segment as the projection point. Then, we compute the Euclidean distance between the pixel and the projection point. Formally, we define the distance between p and l i by</p><formula xml:id="formula_0">d(p, l i ) = min t?[0,1] ||x s i + t ? (x e i ? x s i ) ? p|| 2 2 , t * p = arg min t d(p, l i ),<label>(1)</label></formula><p>where the projection point is the original point-to-line projection point if t * p ? (0, 1), and the closest end-point if t * p = 0 or 1.</p><p>So, the region in the image lattice for a line segment l i is defined by </p><formula xml:id="formula_1">R i = {p | p ? ?; d(p, l i ) &lt; d(p, l j ), ?j = i, l j ? L}. (2) It is straightforward to see that R i ? R j = ? and ? n i=1 R i = ?, i.e.,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Computing the Attraction Field Map</head><p>Consider the partition region R i associated with a line segment l i , for each pixel p ? R i , its projection point p on l i is defined by</p><formula xml:id="formula_2">p = x s i + t * p ? (x e i ? x s i ),<label>(3)</label></formula><p>We define the 2D attraction or projection vector for a pixel p as,</p><formula xml:id="formula_3">a(p) = p ? p,<label>(4)</label></formula><p>where the attraction vector is perpendicular to the line segment if t * p ? (0, 1) (see <ref type="figure">Figure 2</ref>(b)). <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of the xand y-component of an attraction field map (AFM). Denote by A = {a(p) | p ? ?} the attraction field map for a line segment map L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Squeeze Module</head><p>Given an attraction field map A, we first reverse it by computing the real-valued projection point for each pixel p in the lattice, v(p) = p + a(p),</p><p>and its corresponding discretized point in the image lattice, v ? (p) = v(p) + 0.5 .</p><p>where ? represents the floor operation, and v ? (p) ? ?. Then, we compute a line proposal map in which each pixel q ? ? collects the attraction field vectors whose discretized projection points are q. The candidate set of attraction field vectors collected by a pixel q is then defined by</p><formula xml:id="formula_6">C(q) = {a(p) | p ? ?, v ? (p) = q},<label>(7)</label></formula><p>where C(q)'s are usually non-empty for a sparse set of pixels q's which correspond to points on the line segments. An example of the line proposal map is shown in <ref type="figure">Figure 2(c)</ref>, which project the pixels of the support region for a line segment into pixels near the line segment.</p><p>With the line proposal map, our squeeze module utilizes an iterative and greedy grouping algorithm to fit line segments, similar in spirit to the region growing algorithm used in <ref type="bibr" target="#b9">[10]</ref>.</p><p>? Given the current set of active pixels each of which has a non-empty candidate set of attraction field vectors, we randomly select a pixel q and one of its attraction field vector a(p) ? C(q). The tangent direction of the selected attraction field vector a(p) is used as the initial direction of the line segment passing the pixel q.</p><p>? Then, we search the local observation window centered at q (e.g., a 3 ? 3 window is used in this paper) to find the attraction field vectors which are aligned with a(p) with angular distance less than a threshold ? (e.g., ? = 10 ? used in this paper).</p><p>-If the search fails, we discard a(p) from C(q), and further discard the pixel q if C(q) becomes empty. -Otherwise, we grow q into a set and update its direction by averaging the aligned attraction vectors. The aligned attractiion vectors will be marked as used (and thus inactive for the next round search). For the two end-points of the set, we recursively apply the greedy search algorithm to grow the line segment.</p><p>? Once terminated, we obtain a candidate line segment l q = (x s q , x e q ) with the support set of real-valued projection points. We fit the minimum outer rectangle using the support set. We verify the candidate line segment by checking the aspect ratio between width and length of the approximated rectangle with respect to a predefined threshold to ensure the approximated rectangle is "thin enough". If the checking fails, we mark the pixel q inactive and release the support set to be active again.  <ref type="figure">Figure 3</ref>. Verification of the duality between line segment maps and attraction field maps, and its scale invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Verifying the Duality and its Scale Invariance</head><p>We test the proposed attraction field representation on the WireFrame dataset <ref type="bibr" target="#b0">[1]</ref>. We first compute the attraction field map for each annotated line segment map and then compute the estimated line segment map using the squeeze module. We run the test across multiple scales, ranging from 0.5 to 2.0 with step-size 0.1. We evaluate the estimated line segment maps by measuring the precision and recall following the protocol provided in the dataset. <ref type="figure">Figure 3</ref> shows the precision-recall curves. The average precision and recall rates are above 0.99 and 0.93 respectively, thus verifying the duality between line segment maps and corresponding region-partition based attractive field maps, as well as the scale invariance of the duality.</p><p>So, the problem of LSD can be posed as the region coloring problem almost without hurting the performance. In the region coloring formulation, our goal is to learn ConvNets to infer the attraction field maps for input images. The attraction field representation eliminates local ambiguity in traditional gradient magnitude based line heat map, and the predicting attraction field in learning gets rid of the imbalance problem in line v.s. non-line classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robust Line Segment Detector</head><p>In this section, we present details of learning ConvNets for robust LSD. ConvNets are used to predict AFMs from raw input images under the image-to-image transformation framework, and thus we adopt encoder-decoder network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Processing</head><p>Denote by D = {(I i , L i ); i = 1, ? ? ? , N } the provided training dataset consisting of N pairs of raw images and annotated line segment maps. We first compute the AFMs for each training image. Then, let D = {(I i , a i ); i = 1, ? ? ? , N } be the dual training dataset. To make the AFMs insensitive to the sizes of raw images, we adopt a simple normalization scheme. For an AFM a with the spatial dimensions being W ? H, the size-normalization is done by a x := a x /W, a y := a y /H,</p><p>where a x and a y are the component of a along x and y axes respectively. However, the size-normalization will make the values in a small and thus numerically unstable in training. We apply a point-wise invertible value stretching transformation for the size-normalized AFM z := S(z) = ?sign(z) ? log(|z| + ?),</p><p>where ? = 1e?6 to avoid log(0). The inverse function S ?1 (?) is defined by</p><formula xml:id="formula_9">z := S ?1 (z ) = sign(z )e (?|z |) .<label>(10)</label></formula><p>For notation simplicity, denote by R(?) the composite reverse function, and we still denote by D = {(I i , a i ); i = 1, ? ? ? , N } the final training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inference</head><p>Denote by f ? (?) a ConvNet with the parameters collected by ?. As illustrated in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, for an input image I ? , our robust LSD is defined b?</p><formula xml:id="formula_10">a = f ? (I ? ) (11) L = Squeeze(R(?))<label>(12)</label></formula><p>where? is the predicted AFM for the input image (the size-normalized and value-stretched one), Squeeze(?) the squeeze module andL the inferred line segment map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network Architectures</head><p>We utilize two network architectures to realize f ? (): one is U-Net <ref type="bibr" target="#b15">[16]</ref>, and the other is a modified U-Net, called a-trous Residual U-Net which uses the ASSP module proposed in DeepLab v3+ <ref type="bibr" target="#b30">[31]</ref> and the skip-connection as done in ResNet <ref type="bibr" target="#b31">[32]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the configurations of the two architectures. The network consists of 5 encoder and 4 decoder stages indexed by c1, . . . , c5 and d1, . . . , d4 respectively.</p><p>? For U-Net, the double conv operator that contains two convolution layers is applied and denoted as {?}. The {?} * operator of d i stage upscales the output feature map of its last stage and then concatenate it with the feature map of c i stage together before applying the double conv operator.</p><p>? For the a-trous Residual U-Net, we replace the double conv operator to the Residual block, denoted as [?]. Different from the ResNet, we use the plain convolution layer with 3 ? 3 kernel size and stride 1. Similar to {?} * , the operator [?] * also takes the input from two sources and upscales the feature of first input source.</p><p>The first layer of [?] * contains two parallel convolution operators to reduce the depth of feature maps and then concatenate them together for the subsequent calculations. In the stage d 4 , we apply the 4 ASPP operators with the output channel size 256 and the dilation rate 1, 6, 12, 18 and then concatenate their outputs. The output stage use the convolution operator with 1 ? 1 kernel size and stride 1 without batch normalization <ref type="bibr" target="#b32">[33]</ref> and ReLU <ref type="bibr" target="#b33">[34]</ref> for the attraction field map prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training</head><p>We follow standard deep learning protocol to estimate the parameters ?.</p><p>Loss function. We adopt the l 1 loss function in training. </p><p>Implementation details. We train the two networks (U-Net and a-trous Residual U-Net) from scratch on the training set of Wireframe dataset <ref type="bibr" target="#b0">[1]</ref>. Similar to <ref type="bibr" target="#b0">[1]</ref>, we follow the standard data augmentation strategy to enrich the training samples with image domain operations including mirroring and flipping upside-down. The stochastic gradient descent (SGD) optimizer with momentum 0.9 and initial learning rates 0.01 is applied for network optimization. We train these networks with 200 epochs and the learning rate is decayed with the factor of 0.1 after every 50 epochs. In training phase, we resize the images to 320 ? 320 and then generate the offset maps from resized line segment annotations to form the mini batches. As discussed in Section 3, the rescaling step with reasonable factor will not affect the results. The mini-batch sizes for the two networks are 16 and 4 respectively due to the GPU memory.</p><p>In testing, a test image is also resized to 320 ? 320 as input to the network. Then, we use the squeeze module to convert the attraction field map to line segments. Since the line segments are insensitive to scales, we can directly resize them to original image size without loss of accuracy. The squeeze module is implemented with C++ on CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the proposed line segment detector and make the comparison with existing state-ofthe-art line segment detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>. As shown below, our proposed line segment detector outperforms these existing methods on the WireFrame dataset <ref type="bibr" target="#b0">[1]</ref> and YorkUrban dataset <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metrics</head><p>We follow the evaluation protocol from the deep wireframe parser <ref type="bibr" target="#b0">[1]</ref> to make a comparison. Since we train on the Wreframe dataset <ref type="bibr" target="#b0">[1]</ref>, it is necessary to evaluate our proposed method on its testing dataset, which includes 462 images for man-made environments (especially for indoor scenes). To validate the generalization ability, we also evaluate our proposed approach on the YorkUrban Line Segment Dataset <ref type="bibr" target="#b1">[2]</ref>. Afterthat, we also compared our proposed line segment detector on the images fetched from Internet.</p><p>All methods are evaluated quantitatively by the precision and recall as described in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref>. The precision rate indicates the proportion of positive detection among all of the detected line segments whereas recall reflects the fraction of detected line segments among all in the scene. The detected and ground-truth line segments are digitized to image domain and we define the "positive detection" pixel-wised. The line segment pixels within 0.01 of the image diagonal is regarded as positive. After getting the precision (P) and recall (R), we compare the performance of algorithms with F-measure F = 2 ? P ?R P +R .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons for Line Segment Detection</head><p>We compare our proposed method with Deep Wireframe Parser 3 <ref type="bibr" target="#b0">[1]</ref>, Linelet 4 <ref type="bibr" target="#b11">[12]</ref>, the Markov Chain Marginal Line Segment Detector 5 (MCMLSD) <ref type="bibr" target="#b17">[18]</ref> and the Line Segment Detector (LSD) <ref type="bibr" target="#b5">6</ref>  <ref type="bibr" target="#b9">[10]</ref>. The source codes of compared methods are obtained from the authors provide links. It is noticeable that the authors of Deep Wireframe Parser do not provide the pre-trained model for line segment detection, we reproduced their result by ourselves.</p><p>Threshold Configuration In our proposed method, we finally use the aspect ratio to filter out false detections. Here, we vary the threshold of the aspect ratio in the range (0, 1] with the step size ?? = 0.1. For comparison, the LSD is implemented with the ? log(NFA) in 0.01 ? {1.75 0 , . . . , 1.75 19 } where NFA is the number of false alarm. Besides, Linelet <ref type="bibr" target="#b11">[12]</ref> use the same thresholds as the LSD to filter out false detection. For the MCMLSD <ref type="bibr" target="#b17">[18]</ref>, we use the top K detected line segments for comparison. Due to the architecture of Deep Wireframe Parser <ref type="bibr" target="#b0">[1]</ref>, both the threshold for the junction localization confidence and the orientation confidence of junctions branches are fixed to 0.5. Then, we use the author recommended threshold array <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">50,</ref><ref type="bibr">80,</ref><ref type="bibr">100,</ref><ref type="bibr">150,</ref><ref type="bibr">200,</ref><ref type="bibr">250,</ref><ref type="bibr">255</ref>] to binarize the line heat map and detect line segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision &amp; Recall</head><p>To compare our method with state-ofthe-arts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>, we evaluate the proposed method on the Wireframe dataset <ref type="bibr" target="#b0">[1]</ref> and YorkUrban dataset <ref type="bibr" target="#b1">[2]</ref>. The precision-recall curves and F-measure are reported in <ref type="figure">Figure 4</ref>, <ref type="figure">Figure 5</ref> and <ref type="table" target="#tab_1">Table 2</ref>. Without bells and whistles, our proposed method outperforms all of these approaches on Wireframe and YorkUrban datasets by a significant margin even with a 18-layer network. Deeper network architecture with ASPP module further improves the F-measure performance. Due to the YorkUrban dataset aiming at Manhattan frame estimation, some line segments in the images are not labeled, which causes the F-measure performance of all methods on this dataset decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speed</head><p>We evaluate the computational time consuming for the abovementioned approaches on the Wireframe dataset.</p><p>We run 462 frames with image reading and result writing steps and count the averaged time consuming because the size of testing images are not equal. As reported in <ref type="table" target="#tab_1">Table  2</ref>, our proposed method can detect line segments fast (outperforms all methods except for the LSD) while getting the best performances. All experiments perform on a PC workstation, which is equipped with an Intel Xeon E5-2620 2.10 GHz CPU and 4 NVIDIA Titan X GPU devices. Only one GPU is used and the CPU programs are executed in a single thread.</p><p>Benefiting from the simplicity of original U-Net, our method can detect line segments fast. The deep wireframe parser <ref type="bibr" target="#b0">[1]</ref> spends much time for junction and line map fusion. On the other hand, benefiting from our novel formulation, we can resize the input images into 320 ? 320 and then transform the output line segments to the original scales, which can further reduce the computational cost.</p><p>Visualization Further, we visualize the detected line segments with different methods on Wireframe dataset (see <ref type="figure">Figure 6</ref>), YorkUrban dataset (see <ref type="figure">Figure 7</ref>) and images fetched from Internet (see <ref type="figure">Figure 8</ref>). Since the images LSD MCMLSD Linelet Wireframe Ours GT <ref type="figure">Figure 6</ref>. Some Results of line segment detection on Wireframe <ref type="bibr" target="#b0">[1]</ref> dataset with different approaches LSD <ref type="bibr" target="#b9">[10]</ref>, MCMLSD <ref type="bibr" target="#b17">[18]</ref>, Linelet <ref type="bibr" target="#b11">[12]</ref>, Deep Wireframe Parser <ref type="bibr" target="#b0">[1]</ref> and ours with the a-trous Residual U-Net are shown from left to right. The ground truths are listed in last column as reference.</p><p>fetched from Internet do not have ground truth annotations, we display the input images as reference for comparasion. The threshold configurations for visualization are as follow:</p><p>1. The a-contrario validation of LSD and Linelet are set as ? log = 0.01 ? 1.75 8 ;</p><p>2. The top 90 detected line segments for the MCMLSD are visualized;</p><p>3. The threshold for line heat map is 10 for the deep wireframe parser;</p><p>4. The upper bound of aspect ratio is set as 0.2 for our results.</p><p>By observing these figures, it is easy to find that Deep LSD MCMLSD Linelet Wireframe Ours GT <ref type="figure">Figure 7</ref>. Some Results of line segment detection on YorkUrban <ref type="bibr" target="#b1">[2]</ref> dataset with different approaches LSD <ref type="bibr" target="#b9">[10]</ref>, MCMLSD <ref type="bibr" target="#b17">[18]</ref>, Linelet <ref type="bibr" target="#b11">[12]</ref>, Deep Wireframe Parser <ref type="bibr" target="#b0">[1]</ref> and ours with the a-trous Residual U-Net are shown from left to right. The ground truths are listed in last column as reference.</p><p>Wireframe Parser <ref type="bibr" target="#b0">[1]</ref> can detect more complete line segments compared with the previous methods, however, our proposed approach can get better result in the perspective of completeness. On the other hand, this junction driven approach indeed induces some uncertainty for the detection. The orientation of line segments estimated by junction branches is not accurate, which will affect the orientation of line segments. Meanwhile, some junctions are misconnected to get false detections. In contrast, our proposed method gets rid of junction detection and directly detect the line segments from images.</p><p>Comparing with the rest of approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>, the deep learning based methods (including ours) can utilize the global information to get complete results in the lowcontrast regions while suppressing the false detections in the edge-like texture regions. Due to the limitation of local features, the approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref> cannot handle the results with global information and still get some false detections even with powerful validation approaches. Although the overall F-measure of LSD is slightly better than Linelet, the visualization results of Linelet are cleaner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a method of building the duality between the region-partition based attraction field repre-LSD MCMLSD Linelet Wireframe Ours Input <ref type="figure">Figure 8</ref>. Some Results of line segment detection on images fetched from Internet with different approaches LSD <ref type="bibr" target="#b9">[10]</ref>, MCMLSD <ref type="bibr" target="#b17">[18]</ref>, Linelet <ref type="bibr" target="#b11">[12]</ref>, Deep Wireframe Parser <ref type="bibr" target="#b0">[1]</ref> and ours with the a-trous Residual U-Net are shown from left to right. The input images are listed in last column as reference. sentation and the line segment representation. We then pose the problem of line segment detection (LSD) as the region coloring problem which is addressed by learning convolutional neural networks. The proposed attraction field representation rigorously addresses several challenges in LSD such as local ambiguity and class imbalance. The region coloring formulation of LSD harnesses the best practices developed in ConvNets based semantic segmentation methods such as the encoder-decoder architecture and the a-trous convolution. In experiments, our method is tested on two widely used LSD benchmarks, the WireFrame dataset <ref type="bibr" target="#b0">[1]</ref> and the YorkUrban dataset <ref type="bibr" target="#b1">[2]</ref>, with state-of-the-art performance obtained and 6.6 ? 10.4 FPS speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Motivation and ObjectiveLine segment detection (LSD) is an important yet chal-Our approach for line segment detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the proposed method. (a) The proposed attraction field dual representation for line segment maps. A line segment map can be almost perfectly recovered from its attraction filed map (AFM), by using a simple squeeze algorithm. (b) The proposed formulation of posing the LSD problem as the region coloring problem. The latter is addressed by learning ConvNets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Support regions (b) Attraction vectors(c) Squeeze moduleFigure 2. A toy example illustrating a line segment map with 3 line segments, its dual region-partition map, selected vectors of the attraction field map and the squeeze module for obtaining line segments from the attraction field map. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>all R i 's form a partition of the image lattice. Figure 2(a) illustrates the partition region generation for a line segment in the toy example (Figure 2). Denote by R = {R 1 , ? ? ? , R n } the region-partition map for a line segment map L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>y) ??(x, y) 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>773] Ours (a-trous) [F=.752] Ours (U-Net) [F=.728] Wireframe [F=.647] LSD [F=.644] Linelet [F=.566] MCMLSD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>The PR curves of different line segment detection methods on the WireFrame [1] dataset. 646] Ours (a-trous) [F=.639] Ours (U-Net) [F=.627] Wireframe [F=.591] LSD [F=.585] Linelet [F=.564] MCMLSD The PR curves of different line segment detection methods on the YorkUrban datasets [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Network architectures we investigated for the attraction field learning. {} and [] represent the double conv in U-Net and the residual block. Inside the brackets are the shape of convolution kernels. The suffix * represent the bilinear upsampling operator with the scaling factor 2. The number outside the brackets is the number of stacked blocks on a stage.</figDesc><table><row><cell>stage</cell><cell>U-Net</cell><cell></cell><cell></cell><cell cols="3">a-trous Residual U-Net</cell></row><row><cell>c1</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell></cell><cell></cell><cell cols="3">3 ? 3, 64, stride 1</cell></row><row><cell></cell><cell cols="2">2 ? 2 max pool, stride 2</cell><cell></cell><cell cols="3">3 ? 3 max pool, stride 2</cell></row><row><cell>c2</cell><cell>3 ? 3, 128 3 ? 3, 128</cell><cell></cell><cell></cell><cell>? ?</cell><cell>1 ? 1, 64 1 ? 1, 256 3 ? 3, 64</cell><cell>? ? ? 3</cell></row><row><cell></cell><cell cols="2">2 ? 2 max pool, stride 2</cell><cell></cell><cell>?</cell><cell>1 ? 1, 128</cell><cell>?</cell></row><row><cell>c3</cell><cell>3 ? 3, 256</cell><cell></cell><cell></cell><cell>?</cell><cell>3 ? 3, 128</cell><cell>? ? 4</cell></row><row><cell></cell><cell>3 ? 3, 256</cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 512</cell></row><row><cell></cell><cell cols="2">2 ? 2 max pool, stride 2</cell><cell></cell><cell>?</cell><cell>1 ? 1, 256</cell><cell>?</cell></row><row><cell>c4</cell><cell>3 ? 3, 512</cell><cell></cell><cell></cell><cell>?</cell><cell>3 ? 3, 256</cell><cell>? ? 6</cell></row><row><cell></cell><cell>3 ? 3, 512</cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 1024</cell></row><row><cell></cell><cell cols="2">2 ? 2 max pool, stride 2</cell><cell></cell><cell>?</cell><cell>1 ? 1, 512</cell><cell>?</cell></row><row><cell>c5</cell><cell>3 ? 3, 512</cell><cell></cell><cell></cell><cell>?</cell><cell>3 ? 3, 512</cell><cell>? ? 3</cell></row><row><cell></cell><cell>3 ? 3, 512</cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 2048</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASPP</cell></row><row><cell>d4</cell><cell>3 ? 3, 256 3 ? 3, 256</cell><cell>*</cell><cell>? ?</cell><cell cols="3">1 ? 1, 256; 1 ? 1, 256 3 ? 3, 512</cell><cell>? ?  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1, 512</cell></row><row><cell>d3</cell><cell>3 ? 3, 128 3 ? 3, 128</cell><cell>*</cell><cell>? ?</cell><cell cols="3">1 ? 1, 128; 1 ? 1, 128 1 ? 1, 256 3 ? 3, 256</cell><cell>? ?  *</cell></row><row><cell>d2</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>*</cell><cell>? ?</cell><cell cols="3">1 ? 1, 64; 1 ? 1, 64 1 ? 1, 128 3 ? 3, 128</cell><cell>? ?  *</cell></row><row><cell>d1</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>*</cell><cell>? ?</cell><cell cols="3">1 ? 1, 32; 1 ? 1, 32 1 ? 1, 64 3 ? 3, 64</cell><cell>? ?  *</cell></row><row><cell>output</cell><cell cols="5">1 ? 1, stride 1, w.o. BN and ReLU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>F-measure evaluation with state-of-the-art approaches on the WireFrame dataset and York Urban dataset. The last column reports the averaged speed of different methods in frames per second (FPS) on the WireFrame dataset.</figDesc><table><row><cell>Methods</cell><cell>Wireframe dataset</cell><cell>York Urban dataset</cell><cell>FPS</cell></row><row><cell>LSD [10]</cell><cell>0.647</cell><cell>0.591</cell><cell>19.6</cell></row><row><cell>MCMLSD [18]</cell><cell>0.566</cell><cell>0.564</cell><cell>0.2</cell></row><row><cell>Linelet [12]</cell><cell>0.644</cell><cell>0.585</cell><cell>0.14</cell></row><row><cell>Wireframe parser [1]</cell><cell>0.728</cell><cell>0.627</cell><cell>2.24</cell></row><row><cell>Ours (U-Net)</cell><cell>0.752</cell><cell>0.639</cell><cell>10.3</cell></row><row><cell>Ours (a-trous)</cell><cell>0.773</cell><cell>0.646</cell><cell>6.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will have discrepancy for some intermediate points of a line segment between their annotated pixel locations and the geometric locations when the line segment is not strictly horizontal or vertical.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">They are the same point when the pixel is on the geometry line segment, and thus we will have a zero vector. We observed that the total number of those points are negligible in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/huangkuns/wireframe 4 https://github.com/NamgyuCho/ Linelet-code-and-YorkUrban-LineSegment-DB 5 http://www.elderlab.yorku.ca/resources/ 6 http://www.ipol.im/pub/art/2012/gjmr-lsd/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to Parse Wireframes in Images of Man-Made Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient Edge-Based Methods for Estimating Manhattan Frames in Urban Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Depth and Motion Analysis Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2&amp;3</biblScope>
			<biblScope unit="page" from="353" to="385" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image Partitioning Into Convex Polygons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lafarge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Line Assisted Light Field Triangulation and Stereo Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Layout-Net: Reconstructing the 3D Room Layout from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>abs/1803.08999</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scene Parsing by Integrating Function, Geometry and Appearance Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3119" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pose Estimation from Line Correspondences: A Complete Analysis and a Series of Solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1209" to="1222" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image stitching by line-guided local warping with global similarity constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="481" to="497" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LSD: A Fast Line Segment Detector with a False Detection Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anisotropic-Scale Junction Detection and Matching for Indoor Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="91" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Novel Linelet-Based Representation for Line Segment Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Holistically-Nested Edge Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Oriented Boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate Junction Detection and Characterization in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="56" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICCA</title>
		<meeting>of MICCA</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Three Birds One Stone: A Unified Framework for Salient Object Segmentation, Edge Detection and Skeleton Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MCMLSD: A Dynamic Programming Approach to Line Segment Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalizing the Hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction by analyzing distribution around peaks in hough space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shinagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complete description of multiple line segments using the hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kamat-Sadekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="597" to="613" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction using minimum entropy with hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Closed form linesegment extraction using the hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="4012" to="4023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A statistical method for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="73" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting straight lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pushing the Boundaries of Boundary Detection Using Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Richer Convolutional Features for Edge Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">UberNet: Training a &apos;Universal&apos; Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
