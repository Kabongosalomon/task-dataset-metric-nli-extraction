<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Deep Learning on Point Clouds by Reconstructing Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Sauder</surname></persName>
							<email>jonathan.sauder@student.hpi.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute Potsdam</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarne</forename><surname>Sievers</surname></persName>
							<email>bjarne.sievers@student.hpi.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Hasso Plattner Institute</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Deep Learning on Point Clouds by Reconstructing Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point clouds provide a flexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervised learning tasks such as object classification and semantic segmentation. While massive point cloud datasets can be captured using modern scanning technology, manually labelling such large 3D point clouds for supervised learning tasks is a cumbersome process. This necessitates methods that can learn from unlabelled data to significantly reduce the number of annotated samples needed in supervised learning. We propose a self-supervised learning task for deep learning on raw point cloud data in which a neural network is trained to reconstruct point clouds whose parts have been randomly rearranged. While solving this task, representations that capture semantic properties of the point cloud are learned. Our method is agnostic of network architecture and outperforms current unsupervised learning approaches in downstream object classification tasks. We show experimentally, that pre-training with our method before supervised training improves the performance of state-of-the-art models and significantly improves sample efficiency.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Point clouds provide a natural and flexible representation of objects in metric spaces. They can also be easily captured by modern scanning devices and techniques. Algorithms that can recognize objects in point clouds are crucial to countless applications such as robotics and self-driving cars. Traditionally, systems for such tasks have relied on the approximate computation of geometric features such as faces, edges or corners <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11]</ref> and hand-crafted features encoding statistical properties <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>. However, these approaches are often tailored to specific tasks, thus not providing the necessary flexibility for modern applications. Recently, Convolutional Neural Networks (CNNs) which are domain-independent have shown promising performance on point clouds in supervised learning tasks such as object classification and semantic segmentation, outperforming conventional approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>The advent of scalable 3D point cloud scanning technologies such as LiDAR scanners and stereo cameras gives rise to massive point cloud datasets, possibly spanning large entities such as entire cities or regions. However, manually annotating such massive amounts of data for supervised learning tasks such as semantic segmentation poses problems due to typical real-world point clouds reaching billions of points and petabytes of data, opposing the innate limitations of user-interfaces for 3D data labelling (e.g. drawing bounding boxes) on 2D screens. Therefore, it is of large interest to develop methods which can reduce the number of annotated samples required for strong performance on supervised learning tasks. Unsupervised or self-supervised learning approaches for deep learning have shown to be effective in this scenario in various domains <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref>. On point clouds, self-supervised approaches have been largely focused on applying either Autoencoders <ref type="bibr" target="#b12">[13]</ref> or Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[10]</ref>. While GAN-based approaches have not been successfully applied to raw point cloud data due to the non-triviality of sampling unordered sets with neural networks, Autoencoders for point clouds rely on possibly problematic similarity metrics <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this work we address these limitations and present a self-supervised learning method for neural networks operating on raw point cloud data in which a neural network is trained to correctly reconstruct point clouds whose parts have been randomly displaced. An example of the proposed self-supervised task given in <ref type="figure" target="#fig_0">Figure 1</ref>. The proposed method is agnostic of the specific network architecture and can be flexibly used to pre-train any deep learning model operating on raw point clouds for other tasks. In a series of experiments, we show that powerful representations of point clouds are obtained from self-supervised training with our method. Our method outperforms previous unsupervised methods in a downstream object classification task in a transfer learning setting. We also explore per-point features and show pre-training with our method improves the performance and sample efficiency in supervised tasks. To highlight our main contributions:</p><p>? We present an architecture-agnostic self-supervised learning method operating on raw point clouds in which a neural network is trained to reconstruct a point cloud whose parts have been randomly displaced. Our method avoids computationally expensive and possibly flawed reconstruction losses or similarity metrics on point clouds.</p><p>? We demonstrate the effectiveness of the learned representations: our method outperforms state-of-the-art unsupervised methods in a downstream object classification task. Pre-training with our method improves results in all evaluated supervised tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning on Point Clouds</head><p>Deep neural networks have shown impressive performance on regularly structured data representations such as images and time series. However point clouds are unordered sets of vectors, therefore exemplifying a class of problems posing challenges for deep learning for which the term geometric deep learning <ref type="bibr" target="#b3">[4]</ref> has been coined. Although deep learning methods for unordered sets <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref> have been proposed and also applied to point clouds <ref type="bibr" target="#b24">[25]</ref>, these approaches do not leverage spatial structure.</p><p>To address this problem, popular point cloud representations suitable for deep learning include volumetric approaches, in which the containing space is voxelized to be suitable for 3D CNNs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>, and multi-view approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>, in which 3D point clouds are rendered into 2D images fed into 2D CNNs. However, voxelized representations can be difficult to use when the point cloud density varies, and as such are constrained by the resolution and limited by the computational cost of 3D convolutions. Despite multi-view approaches having shown strong performance in classification of standalone objects, it is unclear how to extend them to work reliably in larger scenes (e.g. with covered objects) and on per-point tasks such as part segmentation <ref type="bibr" target="#b22">[23]</ref>.</p><p>A more recent approach, pioneered by PointNet <ref type="bibr" target="#b22">[23]</ref>, is feeding raw point cloud data into neural networks. As point clouds are unordered sets, these networks have to be permutation invariant -PointNet achieves this by using the max-pooling operation to form a single feature vector representing the global context from a variable amount of points. PointNet++ <ref type="bibr" target="#b23">[24]</ref> proposes an extension that introduces local context by stacking multiple PointNet layers. Further improvements were made by introducing Dynamic Graph CNNs (DGCNNs) <ref type="bibr" target="#b32">[33]</ref>, in which a graph convolution is applied to edges of the k-nearest neighbor graph of the point clouds, which is dynamically recomputed in feature space after each layer. Similar performance was achieved by PointCNN <ref type="bibr" target="#b15">[16]</ref>, which uses a hierarchical convolution that is trained to learn permutation invariance. All neural networks operating on raw point cloud data naturally provide per-point embeddings, making them particularly useful for point segmentation tasks. Our proposed method can leverage these methods as it is flexible with regards to the use of specific neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised and Self-Supervised Deep Learning</head><p>Deep learning algorithms have demonstrated the ability to learn powerful internal hierarchical embeddings through unsupervised learning tasks, in which no supervision is given at all, or selfsupervised tasks, where the labels are generated from the data itself <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. These representations can be directly used in downstream tasks or as strong initializers for supervised tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8]</ref>. In cases where large amounts of data are available but annotated samples are scarce, unsupervised or self-supervised learning can significantly reduce the number of annotated training samples required for strong performance in various tasks <ref type="bibr" target="#b36">[37]</ref>, making such methods particularly desirable for point clouds.</p><p>Following the impressive results that have been achieved with GANs <ref type="bibr" target="#b9">[10]</ref> and Autoencoders <ref type="bibr" target="#b12">[13]</ref> in the image domain, previous efforts for unsupervised learning on point clouds have been adaptations of these approaches. However, GANs for point clouds have been limited to either work on voxelized representations <ref type="bibr" target="#b33">[34]</ref>, on 2D-rendered images of point clouds <ref type="bibr" target="#b11">[12]</ref>, or through adversarial learning on the learned embedding space from an external Autoencoder <ref type="bibr" target="#b0">[1]</ref> as sampling unordered but intradependent sets of points with neural networks is non-trivial. Autoencoders on the other hand work by learning to encode inputs into a latent space before reconstructing them, therefore requiring similarity or reconstruction metrics. Besides Autoencoders on voxelized representations <ref type="bibr" target="#b28">[29]</ref> in which conventional loss functions can be applied per-voxel, Autoencoders have also been applied on raw point clouds <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15]</ref>. When operating on raw point clouds, Autoencoder-based methods for point clouds rely on similarity metrics such as the Chamfer (pseudo) distance, which acts as a differentiable approximation to the computationally infeasible Earth Mover's Distance <ref type="bibr" target="#b25">[26]</ref>.</p><p>Computing the Chamfer distance can be limited by memory requirements in large point clouds, but more importantly, the authors <ref type="bibr" target="#b0">[1]</ref> observe that specific pathological cases are handled incorrectly. This motivates self-supervised methods such as ours which avoid potentially problematic similarity functions.</p><p>A completely different approach to self-supervised learning in the image domain was taken by <ref type="bibr" target="#b6">[7]</ref>, in which a neural network is trained to predict the spatial relation between two randomly chosen image patches. The authors demonstrate the effectiveness of the learned features in a range of experiments and argue that such a classification task tackles the problem of the extremely large variety of pixels that can arise from the same semantic object in images. This holds even more true when moving from images to point clouds, i.e. from regular grids in 2D space to unordered sets in 3D space. These ideas were extended in <ref type="bibr" target="#b20">[21]</ref>, where a neural network with a limited receptive field was trained to correctly place randomly displaced image patches to their original position. The authors of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> identify the challenge of trivial solutions for such self-supervised tasks in the image domain, such as chromatic aberration or the matching of low-level feature such as the position of lines in image segments. They take extensive precautions to alleviate this problem, one of which is limiting the receptive field of the neural network, which prevents the same neural network used for pre-training from being used without any changes in further supervised training. Another approach for self-supervised learning was taken by <ref type="bibr" target="#b8">[9]</ref>, in which a neural network learns to identify the correct rotation on an image. However, this approach is limited to domains in which a clear height-axis is defined. We build on the concepts of <ref type="bibr" target="#b20">[21]</ref> and adapt the idea of reordering patches to point clouds, which have certain characteristics that make them particularly well-suited for such a task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Algorithm 1: Generation of Self-Supervised Labels</p><formula xml:id="formula_0">1: function GET_SELF_SUPERVISED_LABEL(X ? R 3 , k ? R) 2: X 1 ? scale_to_unit_cube(X) 3: X 1 , y ? voxelize(X 1 , k) get corresponding voxel ID for each point in X 4: ? ? random_permutation(0..k 3 ) 5: for i in 0..k 3 do 6: new_position ? move_to_voxel(X 1 [i], ?[y[i]])) 7: X 1 [i] ? augment(new_position) 8:</formula><p>return X 1 , y</p><p>In this paper we propose a self-supervised method that learns powerful representations from raw point cloud data. Our method works by training a neural network to reassemble point clouds whose parts have been randomly displaced. The key assumption of the proposed method is that learning to reassemble displaced point cloud segments is only possible by learning holistic representations that capture the high-level semantics of the objects in the point cloud.</p><p>We phrase the self-supervised learning task as a point segmentation task, in which the label for each point is generated from the point cloud itself with the following procedure: the input point cloud is scaled to unit cube before each axis is split into k equal lengths, forming k 3 voxels. We use these to assign each point its voxel ID as a label. Subsequently all voxels are randomly swapped with other voxels and a neural network is trained to predict the original voxel ID of each point. The points in each voxel can also be augmented (e.g. randomly shifted by a small amount) to improve generalization. Pseudo-code for this entire procedure is provided in Algorithm 1. Note that using the voxel ID as per-point label admits a unique solution even for almost all axis-symmetric point clouds, as long as the individual voxels are not all randomly rotated, i.e. as long as a general sense of the orientation of the input point cloud is maintained. While k may be varied across domains, depending on the amount of detail in the input point clouds, we list all results with k = 3. Additional details are discussed in Section 5.</p><p>The proposed method is agnostic of the specific neural network architecture at hand -any neural network capable of point segmentation tasks, such as PointNet <ref type="bibr" target="#b22">[23]</ref>, PointNet++ <ref type="bibr" target="#b23">[24]</ref>, DGCNN <ref type="bibr" target="#b32">[33]</ref>, or PointCNN <ref type="bibr" target="#b15">[16]</ref> can be used out-of-the-box. These network architectures can be pre-trained in a self-supervised manner with our method and used as-is for further supervised training. Furthermore, as point clouds do not suffer from the same trivial solutions as identified in the image domain by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>, no limitation is needed on the receptive field size. Phrasing the self-supervised task as a point segmentation task brings many advantages: there is no reliance on possibly flawed similarity metrics as with Autoencoders, it is not necessary to sample unordered sets of points from a neural network as with GANs, and the method can work on raw point cloud data and does not require voxelized VConv-DAE <ref type="bibr" target="#b28">[29]</ref> 75.50% 80.50% 3D-GAN <ref type="bibr" target="#b33">[34]</ref> 83.30% 91.00% Latent-GAN <ref type="bibr" target="#b0">[1]</ref> 85.70% 95.30% FoldingNet <ref type="bibr" target="#b36">[37]</ref> 88.40% 94.40% VIP-GAN <ref type="bibr" target="#b11">[12]</ref> 90 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Object Classification</head><p>In this section, we show that the embeddings learned with our method outperform state-of-the-art unsupervised methods in a downstream object classification task and demonstrate the benefits of pre-training with our method before fully supervised training. In line with previous approaches, we evaluate our performance on the object classification problem using the ModelNet dataset <ref type="bibr" target="#b34">[35]</ref>, which contains CAD models from different categories of man-made objects. For this we use the standard train/test split, with the same uniform point sample as defined in <ref type="bibr" target="#b22">[23]</ref> with ModelNet40 on 40 classes containing 9843 train and 2468 test models and ModelNet10 on ten classes containing 3991 and 909 models respectively.</p><p>In the first experiment, we follow the same procedure as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b11">12]</ref>. We train a model in a selfsupervised manner on the ShapeNet dataset <ref type="bibr" target="#b4">[5]</ref>, which consists of 57448 models from 55 categories. After that, we train a linear Support Vector Machine (SVM) <ref type="bibr" target="#b5">[6]</ref> on the obtained embeddings of the ModelNet40 train split and evaluate it on the test split. We do this with a PointNet and a DGCNN with the exact same setup as proposed by the authors for object classification <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b22">23]</ref>, the object embeddings are obtained after the last max-pooling layer. This experiment evaluates the learned embeddings in a transfer learning task, demonstrating their generalizability. From every model in ShapeNet we use the same random sample of 2048 points on the model surface as provided by <ref type="bibr" target="#b36">[37]</ref>. The results are displayed in <ref type="table" target="#tab_0">Table 1</ref>. Our method outperforms all previous approaches on ModelNet40, and all except Latent-GAN on ModelNet10. However, as noted by <ref type="bibr" target="#b36">[37]</ref>, the point cloud       format and sampling procedure from Latent-GAN is not publicly available, making a comparison on ModelNet10 accuracy inconclusive. <ref type="figure" target="#fig_2">Figure 2a</ref> shows that a decrease in self-supervised training loss on ShapeNet gives a better downstream classification accuracy on ModelNet40, which suggests that correctly reconstructing the point cloud parts results requires learning representations that capture the semantics of the objects at hand. The obtained embeddings from a DGCNN with out method for the ModelNet10 test data are visualized using t-SNE <ref type="bibr" target="#b16">[17]</ref> in <ref type="figure" target="#fig_4">Figure 2b</ref>. One can see that clear, separable clusters are formed for each class except for the classes dresser (violet) vs nightstand (pink), which are almost visually indiscernible when scaled to unit cube, as done in the ShapeNet dataset.</p><p>In a second experiment, we show that a very small number of labelled samples can suffice to achieve strong performance in a downstream task, which is one of the main motivations of self-supervised learning. We evaluate our method in such a scenario by limiting the number of training samples available in the ModelNet object classification task. We sample according to the following procedure: first we randomly sample one object per class, and then sample the remaining objects uniformly out of the entire training set. We compare the performance of a linear SVM trained on the embeddings obtained from training a DGCNN on ShapeNet with our method to those obtained with FoldingNet <ref type="bibr" target="#b36">[37]</ref> in <ref type="figure" target="#fig_2">Figure 3a</ref>. The embeddings obtained from our method lead to higher accuracy than those obtained with FoldingNet with any amount of training labels. Using only 1 % of training data, equivalent to three or less samples per class, our model is able to achieve 65.2 % accuracy on the test set. When using 10 % of available training samples, this accuracy rises up to 84.4 %.</p><p>Finally, we demonstrate the benefit of pre-training with our method, by pre-training a DGCNN in a self-supervised manner on the ShapeNet dataset with 1024 points chosen randomly from each model for 100 epochs before fully supervised training on the ModelNet40 dataset. As seen in 3b, self-supervised pre-training acts as a strong initializer, reducing the number of supervised epochs needed for strong performance and even improving the final object classification accuracy with DGCNN ( <ref type="table">Table 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Part Segmentation</head><p>In this section we explore the per-point embeddings obtained through unsupervised training in a part segmentation task. Again, we train our model in a self-supervised fashion on the ShapeNet <ref type="table">Table 2</ref>: Comparison to state-of-the-art supervised methods in ModelNet40 classification accuracy. All models are trained and evaluated on 1024 points. Self-supervised pre-training is performed on the ShapeNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy</head><p>PointNet <ref type="bibr" target="#b22">[23]</ref> 89.2% PointNet++ <ref type="bibr" target="#b23">[24]</ref> 90.7% PointCNN <ref type="bibr" target="#b15">[16]</ref> 92.2% DGCNN + Random Init <ref type="bibr" target="#b32">[33]</ref> 92.2% DGCNN + Pre-Training (Ours) 92.4% dataset. The supervised task is then to correctly classify each point of an object into the correct object part on the ShapeNet Part dataset <ref type="bibr" target="#b37">[38]</ref>, which is a subset of the full ShapeNet containing 16881 3D objects from 16 categories, annotated with 50 parts in total. We use the official train / validation / test splits <ref type="bibr" target="#b37">[38]</ref>. Following the same procedure as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>, the one-hot encoded object class label of the object is given as an input during supervised training. During the 200 epochs of pre-training, a random class label is given to each object. Part segmentation is evaluated on the mean Intersection-over-Union (mIoU) metric, calculated by averaging IoUs for each part in an object before averaging the obtained values for each object class. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. A DGCNN pre-trained with our method slightly outperforms a randomly initialized DGCNN, the differences in accuracy being particularly notable on the classes with few samples.</p><p>In <ref type="figure">Figure 4</ref> we show a visualization of the features learned for objects after self-supervised training but before any fully supervised training. The visualizations are obtained by selecting a random point and visualizing the distance to the two (sequentially chosen) furthest points in the learned feature space using a color scale. The visualizations show that the features learned in a self-supervised manner can capture high-level semantics such as object parts without ever having seen part IDs. In <ref type="figure" target="#fig_7">Figure 5</ref> a visualization of the features for each point from ten airplanes and ten chairs is shown. The features are projected into two dimensions using UMAP <ref type="bibr" target="#b18">[19]</ref>. One can clearly see that the two object classes form clear, separable clusters in the feature spaces and that clear, discernible clusters are formed for the individual object parts. Individual objects from the classes are not identifiable, showing that the learned features generalize over reoccurring structures. This highlights the semantics of the high-level features learned with our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Segmentation</head><p>In this semantic segmentation task we evaluate the effectiveness on our method on data that goes beyond simple, free-standing objects. The task is evaluated on the Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset <ref type="bibr" target="#b1">[2]</ref>. The dataset consists of 3D point cloud scans from 6 indoor areas totalling 272 rooms. The points are classified into 13 semantic classes such as board, chair, ceiling, beam, and clutter. Each room is split into blocks of 1m ? 1m area and each point is given as a 6D vector containing XYZ coordinates and RGB color values. In this setup we evaluate the case in which there is large amounts of unlabelled data and only few annotated samples are available. For this the largest area (area 5) is chosen as the test set, and the other areas form distinct training sets. We compare two <ref type="figure">Figure 4</ref>: A visualization of the features learned through self-supervised training with our method for individual objects. A color scale shows the distance in feature space between a randomly sampled point and its two (mutually) furthest neighbors in feature space.  DGCNNs with the architecture proposed for semantic segmentation by the authors for each training area, one that has been pre-trained on all areas except area 5, and one that is not pre-trained. The task is evaluated in mIoU% per object class and total per-point classification accuracy. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Pre-training improves the mIoU and classification accuracy in all cases except two, in which the two methods are tied. As expected, the difference is the largest for area 3, where the number of training samples for fully supervised learning is the smallest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Throughout all experiments, our proposed method learns representations that prove to be effective. This leads us to believe that trivial solutions to the task of reconstructing the inputs, as discussed for the image domain by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> are not a significant problem for point clouds. Point clouds do not suffer from chromatic aberration and point cloud parts can be shifted and rotated freely in the coordinates, alleviating the issue of simply matching lines and edges. In this paper we performed all experiments with a three-by-three voxel grid during self-supervised pre-training, which we observed to outperform both k = 2 and k = 4. We found that randomly rotating 15% of the individual voxels and randomly replacing one voxel in each input point cloud with a random voxel from a randomly drawn input point cloud from the same dataset leads to a slightly higher quality of the embeddings in the object classification task (consistently around 0.2% SVM accuracy in the downstream object classification task), therefore we kept this setup throughout all experiments. An extensive evaluation on how to fine-tune the self-supervised task to a specific dataset or domain is not the focus of this paper, instead we show that our simple approach works reliably in all evaluated cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we propose a self-supervised method for learning representations from unlabelled raw point cloud data. In this easy-to-implement method, a neural network learns to reconstruct input point clouds whose parts have been randomly displaced. While solving this task, high-level representations of the underlying input point clouds are learned. We demonstrate the effectiveness of the learned representations in downstream tasks and show our method can improve the sample efficiency and the accuracy of state-of-the-art models when used to pre-train with large amounts of data before fully supervised training. As our method is independent of the specific neural network architecture, we expect to see further benefits of using our results as more effective neural networks for processing raw point cloud data are developed in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A visual example of the proposed self-supervised learning task. (a) The original object is split into voxels along the axes, each point is assigned a voxel label. (b) The voxels are randomly rearranged. (c) A neural network predicts the voxel labels, here visualized with the original point positions. (d) Points with correctly predicted voxel labels (blue) and misclassifications (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>The self-supervised training loss on the ShapeNet dataset and the linear SVM accuracy trained on obtained embeddings for the ModelNet dataset. Performing better on the unsupervised tasks results in stronger embeddings for downstream object classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2</head><label>2</label><figDesc>Figure 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure showinghow the linear SVM classification accuracy for ModelNet40 behaves when few annotated training samples are available. The training curves on the ModelNet40 object classification task of a DGCNN pre-trained with our self-supervised method (blue) on the ShapeNet dataset and a randomly initialized DGCNN (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3</head><label>3</label><figDesc>Figure 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the per-point features of 10 airplanes and 10 chairs from the ShapeNet Part dataset. UMAP is used for dimensionality reduction for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our method against previous unsupervised methods in downstream object classification on the ModelNet40 and ModelNet10 dataset in terms of accuracy. A linear SVM is trained on the representations learned in an unsupervised manner on the ShapeNet dataset.</figDesc><table><row><cell>Model</cell><cell>MN40</cell><cell>MN10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The effect of pre-training on ShapeNet Part Segmentation. Metric is mean IoU% of parts per object class. Aero Bag Cap Car Chair Earphone Guitar Knife Lamp Laptop Motor Mug Pistol Rocket Skateboard Table</figDesc><table><row><cell cols="2">Mean # Shapes</cell><cell>2690 76 55 898 3758</cell><cell>69</cell><cell>787 392 1547 451</cell><cell>202 184 283</cell><cell>66</cell><cell>152</cell><cell>5271</cell></row><row><cell>PointNet</cell><cell cols="2">83.7 83.4 78.7 82.5 74.9 89.6</cell><cell>73.0</cell><cell cols="3">91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9</cell><cell>72.8</cell><cell>80.6</cell></row><row><cell cols="3">PointNet++ 85.1 82.4 79.0 87.7 77.3 90.8</cell><cell>71.8</cell><cell cols="3">91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7</cell><cell>76.4</cell><cell>82.6</cell></row><row><cell>DGCNN</cell><cell cols="2">85.1 84.2 83.7 84.4 77.1 90.9</cell><cell>78.5</cell><cell cols="3">91.5 87.3 82.9 96.0 67.8 93.3 82.6 59.7</cell><cell>75.5</cell><cell>82.0</cell></row><row><cell>Ours</cell><cell cols="2">85.3 84.1 84.0 85.8 77.0 90.9</cell><cell>80.0</cell><cell cols="3">91.5 87.0 83.2 95.8 71.6 94.0 82.6 60.0</cell><cell>77.9</cell><cell>81.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of semantic segmentation on the S3DIS dataset. Results are evaluated on area 6.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Random Init</cell><cell cols="2">Pre-Training (ours)</cell></row><row><cell cols="5">Supervised Train Area # Samples mIoU% Acc % mIoU%</cell><cell>Acc %</cell></row><row><cell>Area 1</cell><cell>3687</cell><cell>43.6%</cell><cell>82.9%</cell><cell>44.7%</cell><cell>83.5%</cell></row><row><cell>Area 2</cell><cell>4440</cell><cell>34.6%</cell><cell>81.2%</cell><cell>34.9%</cell><cell>81.2%</cell></row><row><cell>Area 3</cell><cell>1650</cell><cell>39.9%</cell><cell>82.8%</cell><cell>42.4%</cell><cell>84.0%</cell></row><row><cell>Area 4</cell><cell>3662</cell><cell>39.4%</cell><cell>82.8%</cell><cell>39.9%</cell><cell>82.9%</cell></row><row><cell>Area 6</cell><cell>3294</cell><cell>43.9%</cell><cell>83.1%</cell><cell>43.9%</cell><cell>83.3%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning and adversarial generation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1707.02392</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The wave kernel signature: A quantum mechanical approach to shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Schlickewei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1626" to="1633" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Shapenet: An information-rich 3d model repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d object recognition in cluttered scenes with local surface features: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2270" to="2287" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">View inter-prediction gan: Unsupervised representation learning for 3d shapes by learning global shape memories to support local view predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<idno>abs/1811.02744</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reducing the dimensionality of data with neural networks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for realtime object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning with sets and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoltan Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
	<note>Intelligent Robots and Systems</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale 3d shape retrieval from shapenet core55: Shrec&apos;17 track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiko</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutarou</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on 3D Object Retrieval</title>
		<meeting>the Workshop on 3D Object Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vconv-dae: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Hamarneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1681" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno>abs/1801.07829</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Xiaoou Tang, and Jianxiong Xiao. 3d shapenets for 2.5d object recognition and next-best-view prediction. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5670</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
