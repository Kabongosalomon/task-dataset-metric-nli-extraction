<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COST VOLUME PYRAMID NETWORK WITH MULTI-STRATEGIES RANGE SEARCHING FOR MULTI-VIEW STEREO A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-26">July 26, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Gao</surname></persName>
							<email>gaoshiyu@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COST VOLUME PYRAMID NETWORK WITH MULTI-STRATEGIES RANGE SEARCHING FOR MULTI-VIEW STEREO A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-26">July 26, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multi-view stereo ? 3D reconstruction ? Cost volume ? Coarse-to-fine</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-view stereo is an important research task in computer vision while still keeping challenging. In recent years, deep learning-based methods have shown superior performance on this task. Cost volume pyramid network-based methods which progressively refine depth map in coarse-to-fine manner, have yielded promising results while consuming less memory. However, these methods fail to take fully consideration of the characteristics of the cost volumes in each stage, leading to adopt similar range search strategies for each cost volume stage. In this work, we present a novel cost volume pyramid based network with different searching strategies for multi-view stereo. By choosing different depth range sampling strategies and applying adaptive unimodal filtering, we are able to obtain more accurate depth estimation in low resolution stages and iteratively upsample depth map to arbitrary resolution. We conducted extensive experiments on both DTU and BlendedMVS datasets, and results show that our method outperforms most state-of-the-art methods. Code is available at: https://github.com/SibylGao/MSCVP-MVSNet.git</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-view stereo is one of the fundamental computer vision tasks which is widely used in augmented reality, 3D modeling and autonomous driving. In deep learning era, deep CNNs used for cost regularization and extracting representative image features have achieved promising performance. <ref type="bibr" target="#b0">Yao et al. Yao et al. [2018]</ref> first proposed an end-to-end MVS pipeline that constructs cost volume based on plane sweeping algorithm and aggregates different views by minimizing differential variance. However, this method consumes huge memory because that 3D CNN used for regularization is cubically proportional to image resolution. As a result, subsequent methods like <ref type="bibr" target="#b0">Yao et al. [2018</ref><ref type="bibr" target="#b1">Yao et al. [ , 2019</ref> downsample high resolution images to regularize cost volume in a smaller resolution. To this end, methods designed in coarse-to-fine manner , <ref type="bibr" target="#b3">Yu and Gao [2020]</ref>, <ref type="bibr" target="#b4">Yang et al. [2020]</ref>, <ref type="bibr" target="#b5">Gu et al. [2020]</ref> are put forward, which iteratively refine depth map based on cost volume pyramid and consume less memory.</p><p>However, current coarse-to-fine methods suffer from two limitations. First, the accuracy of the predicted depth map is highly dependent on the initial low-resolution depth map, since it is difficult to correct the depth of ill-posed and occluded pixels in the following narrow range. Second, current coarse-to-fine methods use same searching strategies in refinement stages after gaining initial depth map, which, however, not fully considered the characteristics of the cost volumes in each stage.</p><p>In this work, we propose a multi-strategies cost volume pyramid multi-view stereo network (MSCVP-MVSNet). Instead of single depth range searching strategy, we utilize multi-dimensional information to calculate depth searching range for each layer. To further utilize the information contained in the cost volume, we introduce unimodal distribution as a training label at second stage during the training process.</p><p>Our main contributions can be summarized as follows:</p><p>We present multiple depth range searching methods in different stages of pyramid structure, leveraging multi-dimension information. On the second stage, variance-based strategy is applied to exploit previous predicted probabilities for each pixel. For the succeeding refinement stage, we employ parameter-free method to propagate neighboring information to an arbitrary resolution during upsampling.</p><p>To further exploit information in cost volume of deep MVS and obtain more accurate predictions in low-resolution stage before refinement, we propose unimodal assumption as a training label in second stage.</p><p>Quantitative results show that our method obtains SOTA results on DTU dataset and satisfactory qualitative results on BlendedMVS.</p><p>2 Related Work 2.1 Coarse-to-fine MVS methods. <ref type="bibr" target="#b0">Yao et al. [2018</ref><ref type="bibr" target="#b1">Yao et al. [ , 2019</ref> based on pipeline of MVSNet <ref type="bibr" target="#b0">Yao et al. [2018]</ref> build cost volume at the resolution of output images, which usually occupy large memory dealing with high resolution dataset such as DTU <ref type="bibr" target="#b6">Aanaes et al. [2016]</ref> or Tanks and Temples <ref type="bibr" target="#b7">Knapitsch et al. [2017]</ref>. In order to save memory and computation consumption, coarse-to-fine methods <ref type="bibr" target="#b3">Yu and Gao [2020]</ref>, <ref type="bibr" target="#b4">Yang et al. [2020]</ref>, <ref type="bibr" target="#b5">Gu et al. [2020]</ref>are put forward. For example, CVP-MVSNet <ref type="bibr" target="#b4">Yang et al. [2020]</ref> and Cascade-MVSNet <ref type="bibr" target="#b5">Gu et al. [2020]</ref> build cost volume across the entire depth range in the coarsest resolution, after that, a narrowed sampling range is calculated based on previous depth predictions. Based on these works <ref type="bibr" target="#b4">Yang et al. [2020]</ref>, <ref type="bibr" target="#b5">Gu et al. [2020]</ref>, <ref type="bibr" target="#b8">Yu et al Yu et al. [2021]</ref> propose AACVP-MVSNet, which introduces attention mechanism to CVP-MVSNet <ref type="bibr" target="#b4">Yang et al. [2020]</ref> framework. <ref type="bibr" target="#b9">Zhang et al. Zhang et al. [2020a]</ref> took into account the visibility between different views based on Cascade-MVSNet <ref type="bibr" target="#b5">Gu et al. [2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep MVS methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Depth sampling range.</head><p>Coarse-to-fine pyramid networks uniformly sample the entire depth range in the first stage. In the following stage, they iteratively narrow depth searching range by various strategies. CVP-MVSNet <ref type="bibr" target="#b4">Yang et al. [2020]</ref> determines the local sampling range around the current depth by back projecting the corresponding pixels along epipolar line in source views. Cas-MVSNet <ref type="bibr" target="#b5">Gu et al. [2020]</ref> narrows sampling range of each stage by hand-crafted range with specific decay ratio. For the first time, Cheng et al. <ref type="bibr" target="#b10">Cheng et al. [2020]</ref> utilized variance of probability distribution to describe the uncertainty of depth estimation.</p><p>All these methods mentioned above employ identical sampling range searching strategies in each stage of threeor four-layer pyramid. In order to leverage both variance and neighbouring contextual information without adding complicated neural network modules, we apply different sampling range calculation strategies in different stage of coarse-to-fine MVS framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cost volume.</head><p>Recently, cost volume is widely used in MVS and stereo matching methods. MVSNet <ref type="bibr" target="#b0">Yao et al. [2018]</ref> first introduces cost volume for end-to-end MVS pipeline by calculating photometric matching cost of each pixel in different frontoparallel planes hypothesis. A standard cost volume has a resolution of H ? W ? D ? F , where H, W , D, F are height, width, number of plane hypothesis and feature channels, respectively. While cost volume indicates matching cost of each depth hypothesis of each pixel intuitively, it is regularized by 3D UNet to generate an estimated probability value and indirectly supervised as an intermediate layer. In order to integrate multi-scale information of cost volume, Shen et al. <ref type="bibr" target="#b11">Shen et al. [2021]</ref> proposed cost volume fusion module to obtain better initial disparity map. Like CFNet <ref type="bibr" target="#b11">Shen et al. [2021]</ref>, we further utilize cost volume to obtain better initial depth map before refinement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In this section,we introduce our multi-strategies cost volume pyramid network for high-resolution MVS reconstruction in details. The overview of the network is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We assume the input reference image denoted by I 0 ? R H?W , and source images represented by {I i } N ?1 i=1 . To build a pyramidal structure, we downsample input images L times to obtain images pyramid</p><formula xml:id="formula_0">{I j i } L j=1 , where i ? {0, 1, ? ? ? , N }. Feature pyramid {F j i } L j=1</formula><p>are build by weights-shared feature extraction module.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, three different sampling strategies and two separated UNets are employed in our framework. <ref type="bibr" target="#b12">Guo et al. [2019]</ref>, we build cost volume by group-wise correlation instead of calculating feature volume variance over all views proposed by Yao et al. <ref type="bibr" target="#b0">Yao et al. [2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inspired by GwcNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Depth sampling range estimation</head><p>As introduced in related work, previous methods <ref type="bibr" target="#b4">(Yang et al. [2020]</ref>, <ref type="bibr" target="#b11">Shen et al. [2021]</ref>, <ref type="bibr" target="#b8">Yu et al. [2021]</ref>) employ single strategy in each stage to calculate depth range, which either ignore statistical properties of each pixel or neighbouring information. To solve this, we fuse multi-dimensional information by simply combine different uncertainty estimation strategies in different stage without any additional neural network modules and achieve satisfactory results.</p><p>In this section, we present our depth hypothesis sampling strategies in details. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the number of pyramid layers in our framework is flexible, we train 3 different layers while evaluate with arbitrary number of stages.</p><p>In the first stage, we uniformly sampled depth hypothesis over the entire range to obtain a coarsest initial depth map. Due to the large sampling range, we sampled more depth hypothesis (D 1 = 48) in this stage. For second stage, we take advantage of probability distributions to calculate specific depth sampling range of each pixel. Previous methods <ref type="bibr" target="#b13">(Zhang et al. [2020b]</ref>, <ref type="bibr" target="#b11">Shen et al. [2021]</ref>) indicate that texture-less and occluded pixels tend to have multiple or wrong matches, as a result, the expectation of the per-pixel distributions can not depict the properties of multimodality and dispersion. To solve this issue, we leverage the variance of the probability distribution as well as adaptive unimodal constraints (Sec. 3.3) to estimate per-pixel uncertainty and reduce local maxima of probabilities. We set the number of depth hypothesis, D 2 = 32 in this stage. For stage l, the variance at pixel i is defined as:</p><formula xml:id="formula_1">V l i = D l j=1 P l i,j (d l i,j ?d l i ) 2 ,<label>(1)</label></formula><p>where P l i,j is the probability of pixel i at sampled depth j,d l i,j is the depth of sampled plane j,d l i is the estimated depth of pixel i at current stage. Different from UCSNet <ref type="bibr" target="#b10">Cheng et al. [2020]</ref>, we adopt the idea of CFNet[18] that originally proposed in stereo matching task, which use learned instead of hand-crafted scale parameters to determine confidence interval:</p><formula xml:id="formula_2">d l+1 max (i) =d l i + ? l V l i + ? l , d l+1 min (i) =d l i ? ? l V l i ? ? l ,<label>(2)</label></formula><p>where ? l and ? l are learned parameters in stage l. Same as CFNet <ref type="bibr" target="#b11">Shen et al. [2021]</ref>, we initial ? l and ? l as 0 at the beginning of training. In texture-less regions with multi-modal distributions, the variances tend to be large, and adaptive uncertainty range estimation algorithm adjust depth hypothesis to a larger range so as not to miss the truth depth value before small-range refinement. Depth searching ranges in <ref type="figure">Fig.3</ref> show the effectiveness of our variance-based method in 2 nd stage.</p><p>Our first two layers have yielded fair results at the low resolution stage, and the depth values of high-resolution depth maps are obtained via upsampling operation. Specifically, we apply parameters-free method to determine sampling range, which takes advantage of contextual information provided by neighboring pixels along epipolar line <ref type="bibr" target="#b4">Yang et al. [2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Supervise on Cost Volume</head><p>As shown in <ref type="figure">Fig.3</ref>, it is hard to correct misestimated depth in refinement stages. To attain better predictions before neighboring-based refinement, we further utilize the information in cost volume at 2 nd stage.</p><p>Cost volume is defined to reflect the similarity between different views, where the true depth value should have the lowest cost, which means the probability distribution should be unimodal and peaked at the true depth hypothesis under ideal circumstances. Based on this assumption, we construct unimodal distributions as reference distributions which directly constraint on the cost volume to reduce errors introduced by multi-modal distributions. Following Zhang et al.</p><p>[2020b], we defined reference unimodal distribution as:</p><formula xml:id="formula_3">P l (i) = sof tmax(? |d l (i) ? d l gt (i)| ? i ),<label>(3)</label></formula><p>where ? i is variance of reference distribution for pixel i, which controls the sharpness of peak and it is defined as:</p><formula xml:id="formula_4">? l i = ? l c (1 ? f l i ) + ? l c ,<label>(4)</label></formula><p>where f l i is confidence value of pixel i in stage l. We estimate confidence value for each pixel by a 2D confidence estimation network. ? l c and ? l c are scale factor and lower bound, respectively. Different from <ref type="bibr" target="#b13">Zhang et al. [2020b]</ref>, we use learned neural network parameters instead of hand-crafted factors to adapt different properties of probability distributions for different datasets. Large ? indicates low confidence of pixel, which usually caused by mismatch in textureless regions.</p><p>We leverage stereo focal loss proposed by AcfNet <ref type="bibr" target="#b13">Zhang et al. [2020b]</ref> to guide network to generate unimodal distributions for each pixel. The stereo focal loss is defined as:</p><formula xml:id="formula_5">L SF = 1 |P| i?P ( D?1 d=0 (1 ? P i (d)) ?? ? (?P i (d) ? logP i (d))),<label>(5)</label></formula><p>where P i (d) is probability value of reference unimodal distribution at depth d of pixel i, andP i (d) is estimated probability of pixel i at depth d given by our UNet. Instead of simple cross entropy loss, we set ? ? 0 to force unimodal guidance to focus on high-confidence regions.</p><p>After adaptive unimodal filtering (AUF), some local maximas are eliminated, and the errors in stage 2 are decreased. <ref type="figure">Fig.3</ref>(left) presents depth searching range of our method with and without AUF, respectively. The depth sampling range in 2 nd stage indicates that AUF narrows down the sampling range and contributes to a more accurate initial depth map before refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>Our total loss consists of three parts: regression loss in each stage, stereo focal loss and confidence loss, which is denoted as:</p><formula xml:id="formula_6">L =? SF L SF + ? C L C + L l=1 ? l L l regression<label>(6)</label></formula><p>Where ? SF and ? C are two factors to balance stereo focal loss and confidence loss on second stage. The confidence loss L C is defined as:</p><formula xml:id="formula_7">L C = 1 |P| i?P ?logf i<label>(7)</label></formula><p>We apply negative log-likelihood function as confidence loss to encourage confidence estimation network to predict high confidence values for each pixel.</p><p>Regression loss L l regression is defined to reflect the difference between the predicted depth map and ground-truth at stage l. We use hand-crafted weight ? at each stage. For stage l, the L1 norm is defined as: Following the official training and validation list given by the released dataset files, we divided 106 scenes for training and the other 7 for validation in low-resolution BlendedMVS. We train our model on low-resolution BlendedMVS and evaluate on both low-resolution and high-resolution.  </p><formula xml:id="formula_8">L l regression = i?P d l i ?d l i 1<label>(8</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Training. We train and evaluate our model on DTU dataset and low-resolution BlendedMVS. For first stage, we uniformly sample the whole depth range <ref type="bibr">[425,</ref><ref type="bibr">1065]</ref> with D 1 = 48, while for 2nd and 3rd stage, we choose D 2 = 32 and D 3 = 8, respectively. As the training process with high-resolution inputs is memory and time consuming, we downsample the training set into a size of 320 ? 256, and the coarsest resolution is 40 ? 32 in the first stage. We set hyperparameters ? SF = 10, ? C = 80 in equation <ref type="formula" target="#formula_6">(6)</ref> and choose ? 1 = 0.5, ? 2 = 1, ? 3 = 2 to balance L1 loss in each stage. As for the reference unimodal distribution, the scale factors are initialized as ? 2 c = 13 and ? 2 c = 9, respectively, based on empirical evidence from <ref type="bibr" target="#b13">Zhang et al. [2020b]</ref>. We use 3 different views as inputs and Adam Kingma and Ba <ref type="bibr">[2014]</ref> as optimizer in the training stage of the proposed network. We set batch size as 16 and train our model on 2 Nvidia GeForce RTX 3090 for 40 epoches with initial learning rate 0.001 multiplied by 0.5 at 10th, 12th, 14th, 20th epoch.</p><p>Evaluation. For DTU dataset,we crop the original images to 1600 ? 1184 for evaluation. We set L = 5 for image feature pyramid to maintain a similar size with training stage at the coarsest stage (50 ? 37). Similar to <ref type="bibr" target="#b0">Yao et al. [2018</ref><ref type="bibr" target="#b1">Yao et al. [ , 2019</ref>, <ref type="bibr" target="#b4">Yang et al. [2020]</ref>, we choose 5 views in evaluation for fair comparison. The depth sampling numbers D in each stage the same as training process. As for BlendedMVS, we evaluate our proposed method on both low-resolution and high-resolution dataset.</p><p>Post processing and Metrics. After estimating the depth map, we fuse all views into a dense point cloud model for each scene. For fair comparison, we follow the common post processing method used by <ref type="bibr" target="#b0">Yao et al. [2018</ref><ref type="bibr" target="#b1">Yao et al. [ , 2019</ref>, <ref type="bibr" target="#b4">Yang et al. [2020]</ref>, which is a fusion method provided by <ref type="bibr" target="#b19">Galliani et al. Galliani et al. [2015]</ref>. We run the official evaluation code provided by DTU dataset <ref type="bibr" target="#b6">Aanaes et al. [2016]</ref> to obtain quantitative results of mean accuracy (acc.), mean completeness (com.) and overall score (overall). The evaluation results are listed in Tab.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on DTU dataset</head><p>We train and evaluate our method on DTU dataset to conduct quantitative results in comparison with other learning based methods. As shown in Tab.1, our method achieves state-of-the-art results in overall score, which is comparable to <ref type="bibr">PVSNet Xu and Tao [2020]</ref>. Especially, our method outperforms all methods in Tab. 1 in terms of completeness. As shown in <ref type="figure">Fig. 4</ref>, We visualize several reconstructed 3D models constructed by CVP-MVSNet <ref type="bibr" target="#b4">Yang et al. [2020]</ref>, AACVP-MVSNet <ref type="bibr" target="#b8">Yu et al. [2021]</ref> and our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on BlendedMVS</head><p>As BlendedMVS dataset does not provide any reference point clouds for quantitative evaluation, we conduct the visual comparison with CVP-MVSNet <ref type="bibr" target="#b4">Yang et al. [2020]</ref>. L = 3 in training process, while for evaluation, we set L = 5 and L = 6 for low and high resolution evaluation sets, respectively. In the same way, we compare our method with CVP-MVSNet <ref type="bibr" target="#b4">Yang et al. [2020]</ref> and the results of low-and high-resoluion dataset are shown in <ref type="figure" target="#fig_3">Fig.5</ref>. On high-resolution data sets, the superiority of our method in terms of completeness is even more evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effectiveness of Multi-strategies</head><p>As shown in Tab.2, we compare our proposed multi-strategies with other combinations of strategies. For strategy "DHS1", we apply DHS1 and uniformly sample at each stage with handcrafted searching range <ref type="bibr">[40,</ref><ref type="bibr">20,</ref><ref type="bibr">10,</ref><ref type="bibr">5]</ref> from 2 nd to 5 th stage (range 1 corresponds to stage 2). "DHS1+DHS2" and "DHS1+DHS3" perform results of single strategy DHS2 and DHS3 from 2 nd to 5 th stage, respectively.</p><p>As shown in <ref type="figure">Fig.3</ref>, CVPMVSNet <ref type="bibr" target="#b4">Yang et al. [2020]</ref> applies DHS3 in each stage and fails to locate an interval which contains true depth value from 2 nd to the last stage. Its single and inflexible range searching strategy makes it hard to jump out of the pattern and rectify mismatch in previous stage. We believe that DHS2 which is based on the variance of previous prediction is more accurate and effective to locate true depth value (see (b) range 1 in <ref type="figure">Fig.3</ref>), but proper scale factors are needed in each specific stage. Our proposed multi-strategies method combines both DHS2 and DHS3, in the second stage, DHS2 gives a reasonable searching range based on previous predicted probabilities, while for the rest stages, DHS3 which is parameter-free provides an effective way to propagate depth of neighboring pixels along epipolar line to an arbitrary resolution during upsampling refinement. Noteworthy, multi-strategies method works better when it combines with two separated UNets (see CVP-MS and CVP-MS-U 2 Net in Tab.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Variance  ? Non-parameter-sharing UNet. 3D UNet is designed for cost volume regularisation and explore cost volume information in three dimensions. Quantitative results on DTU dataset show that our two parameter-separating UNets gain better results (0.328 vs.0.360) than parameter-sharing UNet. The huge gap indicates that former stages which search in a wider range have different characteristics with refinement stages in the cost volume regularization process.</p><p>? Supervise on cost volume. While multi-strategies with two non-parameter-sharing UNet framework has achieve promising results (see CVP-MS-U 2 Net in Tab.3), we obtain even better results when further adding adaptive unimodal filtering (AUF) on 2 nd stage. As shown in <ref type="figure">Fig.3</ref>, the depth sampling range in 2 nd stage is narrowed after adding AUF module. Interestingly, quantitative results of CVP-MS-Auf and CVP-MS in Tab.3 show that adaptive unimodal filtering gives a greater boost when parameter-sharing UNet is adopted.  ? Image resolution during training and evaluation. Tab. 4 shows that the performance of the model trained with higher resolution input is better than that with lower resolution input. To discover the relationship between pyramid levels and quality of output depth map, we also evaluate our method with different pyramid levels on DTU dataset. As shown in Tab. 4, coarse-to-fine network with 5 pyramid stages achieves the best overall score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present an efficient deep-learning based cost volume pyramid network for MVS. By combining different sampling range estimation strategies for each stage, we integrate multi-dimensional information without additional neural network modules. Then, we apply adaptive unimodal filters to further improve the low-resolution depth map before refinement. Results on different datasets show the effectiveness and generalisability of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our method during training and evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The network structure of MSCVP-MVSNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Before and after AUF in stage 2.(b) Depth searching range in each stageFigure 3: Depth searching range visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Reconstruction results on BlendedMVS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We use the same splited training andFigure 4: Reconstruction results on DTU dataset.evaluation sets with<ref type="bibr" target="#b14">Yao et al. [2020]</ref>,<ref type="bibr" target="#b4">Yang et al. [2020]</ref>. While the original size of evaluation image is 1600 ? 1200, we crop it to 1600 ? 1184 to fit the upsample process.BlendedMVS. BlendedMVS<ref type="bibr" target="#b14">Yao et al. [2020]</ref> is a collection of images captured from different views of 113 various</figDesc><table><row><cell>)</cell></row><row><cell>4 Experiment</cell></row><row><cell>4.1 Dataset</cell></row></table><note>DTU Dataset. We train and evaluate our network on DTU dataset Aanaes et al. [2016] to obtain quantitative results. DTU dataset Aanaes et al. [2016] consists of 124 large scale of scenes in 49 or 64 different views and 7 different light conditions, with the evaluation reference obtained by a structured light scanner.scenarios. It contains 17K training samples in low-resolution (768 ? 576) as well as high-resolution (2048 ? 1536).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on DTU</figDesc><table><row><cell>Methods</cell><cell>acc.</cell><cell>comp.</cell><cell>overall</cell></row><row><cell>MVSNetYao et al. [2018]</cell><cell>0.396</cell><cell>0.527</cell><cell>0.462</cell></row><row><cell>R-MVSNetYao et al. [2019]</cell><cell>0.383</cell><cell>0.452</cell><cell>0.418</cell></row><row><cell>MVSCRFXue et al. [2019]</cell><cell>0.371</cell><cell>0.426</cell><cell>0.398</cell></row><row><cell>PointMVSNetChen et al. [2019]</cell><cell>0.361</cell><cell>0.421</cell><cell>0.391</cell></row><row><cell>CVP-MVSNetYang et al. [2020]</cell><cell>0.296</cell><cell>0.406</cell><cell>0.351</cell></row><row><cell>AACVP-MVSNetYu et al. [2021]</cell><cell>0.357</cell><cell>0.326</cell><cell>0.341</cell></row><row><cell>Vis-MVSNetZhang et al. [2020a]</cell><cell>0.369</cell><cell>0.361</cell><cell>0.365</cell></row><row><cell>USCNetMao et al. [2021]</cell><cell>0.338</cell><cell>0.349</cell><cell>0.344</cell></row><row><cell>PVSNetXu and Tao [2020]</cell><cell>0.337</cell><cell>0.315</cell><cell>0.326</cell></row><row><cell>Ours</cell><cell>0.379</cell><cell>0.278</cell><cell>0.328</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Different strategies</figDesc><table><row><cell>Strategies</cell><cell>acc.</cell><cell>comp.</cell><cell>overall</cell></row><row><cell>DHS1</cell><cell>0.444</cell><cell>0.361</cell><cell>0.402</cell></row><row><cell>DHS1+DHS2</cell><cell>0.338</cell><cell>0.349</cell><cell>0.344</cell></row><row><cell>DHS1+DHS3</cell><cell>0.404</cell><cell>0.321</cell><cell>0.362</cell></row><row><cell>DHS1+DHS2+DHS3</cell><cell>0.389</cell><cell>0.279</cell><cell>0.334</cell></row><row><cell cols="4">Note: DHS1 denotes uniformly sampling the whole</cell></row><row><cell cols="4">range, DHS2 denotes variance-based method, and</cell></row><row><cell cols="4">DHS3 back-projects pixels along epipolar line to cal-</cell></row><row><cell cols="2">culate depth searching range.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on DTU dataset4.6 Ablation studyIn this section, we perform ablation experiments on DTU dataset to validate the effectiveness of each component of our proposed network. Results are shown in Tab.3. Below we analyse each component in details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Coarsest Res T Coarsest Res E Levels E mem.(M) runtime(s) acc. comp. overall</figDesc><table><row><cell>40 ? 32 20 ? 16</cell><cell>25 ? 18 25 ? 18</cell><cell>6 6</cell><cell>6809</cell><cell>2.543</cell><cell>0.372 0.292 0.332 0.382 0.324 0.353</cell></row><row><cell>40 ? 32 20 ? 16</cell><cell>50 ? 37 50 ? 37</cell><cell>5 5</cell><cell>7863</cell><cell>2.550</cell><cell>0.379 0.278 0.328 0.371 0.328 0.349</cell></row><row><cell>40 ? 32 20 ? 16</cell><cell>100 ? 74 100 ? 74</cell><cell>4 4</cell><cell>6935</cell><cell>2.483</cell><cell>0.360 0.311 0.335 0.349 0.478 0.413</cell></row><row><cell>40 ? 32 20 ? 16</cell><cell>200 ? 148 200 ? 148</cell><cell>3 3</cell><cell>7861</cell><cell>2.366</cell><cell>0.375 0.530 0.452 0.531 1.959 1.245</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results on DTU dataset with different training and evaluation resolution.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent mvsnet for high-resolution multi-view stereo depth inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5525" to="5534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1538" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gaussnewton refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1949" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cost volume pyramid based depth inference for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4877" to="4886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2495" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsb?l Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders Bjorholm</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention aware cost volume pyramid based multi-view stereo network for 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anzhu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchuan</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="448" to="460" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07928</idno>
		<title level="m">Visibility-aware multi-view stereo network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep stereo using adaptive thin volume representation with uncertainty awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2524" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cfnet: Cascade and fused cost volume for robust stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhelun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13906" to="13915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive unimodal cost volume filtering for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suihanjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12926" to="12934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blendedmvs: A large-scale dataset for generalized multi-view stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1790" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mvscrf: Learning multi-view stereo with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youze</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4312" to="4321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uasnet: Uncertainty adaptive sampling network for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Tae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6311" to="6319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pvsnet: Pixelwise visibility-aware multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07714</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
