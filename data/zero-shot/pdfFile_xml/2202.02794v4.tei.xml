<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Learning on a Budget: Opposite Strategies Suit High and Low Budgets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Hacohen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avihu</forename><surname>Dekel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
						</author>
						<title level="a" type="main">Active Learning on a Budget: Opposite Strategies Suit High and Low Budgets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Investigating active learning, we focus on the relation between the number of labeled examples (budget size), and suitable querying strategies. Our theoretical analysis shows a behavior reminiscent of phase transition: typical examples are best queried when the budget is low, while unrepresentative examples are best queried when the budget is large. Combined evidence shows that a similar phenomenon occurs in common classification models. Accordingly, we propose TypiClust -a deep active learning strategy suited for low budgets. In a comparative empirical investigation of supervised learning, using a variety of architectures and image datasets, TypiClust outperforms all other active learning strategies in the low-budget regime. Using TypiClust in the semisupervised framework, performance gets an even more significant boost. In particular, state-of-theart semi-supervised methods trained on CIFAR-10 with 10 labeled examples selected by Typi-Clust, reach 93.2% accuracy -an improvement of 39.4% over random selection. Code is available at https://github.com/avihu111/TypiClust.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the emergence of deep learning as the dominant force in advancing machine learning and its applications. But this development is data-hungry -to a large extent, deep learning owes its success to the growing availability of annotated data. This is problematic even in our era of Big Data, as the annotation of data remains costly.</p><p>Active Learning (AL) aims to alleviate this problem (see <ref type="bibr">*</ref>   surveys in <ref type="bibr" target="#b25">Settles, 2009;</ref><ref type="bibr" target="#b23">Schr?der &amp; Niekler, 2020)</ref>. Given a large pool of unlabeled data, and possibly a small set of labeled examples, learning is done iteratively: the learner employs the labeled examples (if any), then queries an oracle by submitting examples to be annotated. This may be done repeatedly, often until a fixed budget is exhausted.</p><p>Many traditional active learning approaches are based on uncertainty sampling (e.g., <ref type="bibr">Lewis &amp; Gale, 1994;</ref><ref type="bibr" target="#b22">Ranganathan et al., 2017;</ref><ref type="bibr" target="#b16">Gissin &amp; Shalev-Shwartz, 2019;</ref><ref type="bibr" target="#b30">Sinha et al., 2019)</ref>. In uncertainty sampling, the learner queries examples about which it is least certain, presumably because such labels contain the most information about the problem. Another principle guiding deep AL approaches is diversity sampling (e.g., <ref type="bibr">Hu et al., 2010;</ref><ref type="bibr" target="#b11">Elhamifar et al., 2013;</ref><ref type="bibr" target="#b24">Sener &amp; Savarese, 2018;</ref><ref type="bibr" target="#b27">Shui et al., 2020)</ref>. Here, queries are chosen to minimize the correlation between samples, in order to avoid redundancy in the annotations. Sensibly, diversity sampling is often combined with uncertainty sampling (see App. A for further discussion of related work).</p><p>At present, effective deep active learning strategies are known to require a large initial set of labeled examples to work properly <ref type="bibr">(Yuan et al., 2020;</ref><ref type="bibr">Pourahmadi et al., 2021)</ref>. We call this the high budget regime. In the low budget regime, where the initial labeled set is small or absent, it has been shown that random selection outperforms most deep AL strategies (see <ref type="bibr" target="#b1">Attenberg &amp; Provost, 2010;</ref><ref type="bibr">Mittal et al.,</ref> arXiv:2202.02794v4 <ref type="bibr">[cs.</ref>LG] 16 Jun 2022 <ref type="bibr">Zhu et al., 2020;</ref><ref type="bibr" target="#b28">Sim?oni et al., 2021;</ref><ref type="bibr" target="#b6">Chandra et al., 2021)</ref>. This "cold start" phenomenon is often explained by the poor ability of neural models to capture uncertainty, which is more severe with a small budget of labels <ref type="bibr">(Nguyen et al., 2015;</ref><ref type="bibr" target="#b12">Gal &amp; Ghahramani, 2016)</ref>. The low-budget scenario is relevant in many applications, especially those requiring an expert tagger whose time is expensive. If we want to expand deep learning to these domains, overcoming the cold start problem becomes an important challenge.</p><p>In this paper, we suggest that the low and high budgets regimes are qualitatively different, and require opposite querying strategies. Furthermore, we claim that the uncertainty principle is only suited for the high-budget regime, while the opposite strategy -the selection of the least ambivalent points -is suitable for the low-budget regime.</p><p>We begin, in Section 2, by establishing the theoretical foundations for this claim. We analyze a mixture model where two general learners, each limited to a distinct region of the input space, are independently learned. In this framework, we see a phase-transition-like phenomenon: in the lowbudget regime, over-sampling the "easier" region, which can be learned from fewer examples, improves the outcome of learning. In the high-budget regime, over-sampling the alternative region is more beneficial. In other words, opposing querying strategies are suitable for the low-budget and high-budget regimes. This is illustrated in <ref type="figure" target="#fig_0">Fig. 1a</ref>.</p><p>We continue by identifying a set of sufficient conditions, which guarantee that two independent learners display this phase-transition-like phenomenon. We then give a formal argument, showing that linear classifiers satisfy these conditions. We further provide empirical evidence to the effect that neural models may also satisfy these conditions.</p><p>Previous art established that in the high-budget regime, it is beneficial to preferentially sample uncertain examples. The phase transition result predicts that in the low-budget The data is first clustered into 30 clusters, and the densest region within every cluster is sampled. We show t-SNE dimensionality reduction of the feature space, colored by cluster assignment, where selected examples are marked by ?. (b) The selected images, organized column-wise by class. Note that the ensuing labeled set is approximately class-balanced, even though the queries are chosen without access to class labels.</p><p>regime, a strategy that preferentially samples the most certain examples is beneficial. However, estimating prediction certainty is difficult, and cannot be reliably accomplished in the low-budget regime with access to very few labeled examples. We therefore adopt an alternative approach, replacing the notion of certainty with the notion of typicality (loosely defined): a point is typical if it lies in a high-density region of the input space, irrespective of labels.</p><p>In Section 3, guided by these observations, we propose Typical Clustering (TypiClust) -a strategy for active learning in the low-budget regime. TypiClust aims to pick a diverse set of typical examples, which are likely to be representative of the entire dataset. To this end, TypiClust employs selfsupervised representation learning and then estimates each point's density in this representation. Diversity is obtained by clustering the dataset and sampling the densest point from each cluster (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>In Section 4, we compare TypiClust to various AL strategies in the low-budget regime. TypiClust consistently improves generalization by a large margin, across different datasets and architectures, reaching state-of-the-art (SOTA) results in many problems. In agreement with Zhu et al. <ref type="formula" target="#formula_4">(2020)</ref>, we also observe that the alternative AL strategies are not effective in this domain, and are even detrimental.</p><p>TypiClust is especially beneficial for semi-supervised learning. Although in the AL framework the learner has access to a big pool of unlabeled data by construction, most AL strategies do not exploit the unlabeled data for learning, beyond query selection. Recent studies report that the benefit of AL is marginal when incorporated into semi-supervised learning <ref type="bibr" target="#b5">(Chan et al., 2021;</ref><ref type="bibr" target="#b2">Bengar et al., 2021)</ref>, with little added value over the random selection of labels. Re-examining this observation, we note that semi-supervised learning is most beneficial in the low-budget regime, wherein the explored AL strategies are inherently not suitable. When incorporating TypiClust, which is designed for the low-budget regime, into semi-supervised learning, performance is indeed improved by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Contribution</head><p>i. A novel theoretical model analyzing Active Learning (AL) as biased sampling strategies in a mixture model.</p><p>ii. Prediction of the cold start phenomenon in AL.</p><p>iii. Prediction that opposite strategies suit AL in the lowbudget and high-budget regimes.</p><p>iv. Empirical support of these theoretical principles.</p><p>v. TypiClust, a novel strategy that significantly improves active learning in the low-budget regime.</p><p>vi. Large performance boost to SOTA semi-supervised methods by TypiClust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theoretical Analysis</head><p>Given a large pool of U unlabeled examples and a possibly small (or even empty) set of L labeled examples, an Active Learning (AL) method selects a small subset of B examples from U to submit as label queries to an oracle. We call the number of labeled examples known to the learner budget, whose size is m = B + L. In this section, we aim to study the optimal query selection strategy as it depends on m.</p><p>To this end, we analyze a mixture model of two general learners. In ?2.1, the model is defined by first splitting the support of the data distribution into two distinct regions, R 1 and R 2 , further assuming that each region is independently learned by its own general learner. R 1 and R 2 are distinguished by the property that if they are learned independently, R 1 is easier to learn than R 2 (as formalized in Def. 2 below). We then define the error score E D (m) of this model, which measures the expected error of the model over all training samples of size m, as a function of m.</p><p>Within this framework, in ?2.2 we derive a threshold test on the budget size m and E D (m), which determines whether an optimal AL strategy should oversample R 1 or R 2 . In ?2.3 we obtain sufficient conditions on E D (m), which guarantee phase transition as illustrated in <ref type="figure" target="#fig_0">Fig. 1a</ref>. In accordance, an optimal AL strategy will oversample R 1 if m is smaller than some threshold m 0 , and oversample R 2 otherwise. We let the term low budget regime denote budgets with m ? m 0 , and high budget regime denote budgets with m &gt; m 0 . We may now conclude that for any learner model whose error score meets our sufficient conditions, opposite strategies suit the low and high budget regimes.</p><p>In ?2.4 we analytically prove that the error score of a mixture of two linear classifiers satisfies the required conditions, while in App. D.2 we empirically show that this is the case also with deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mixture Model Definition</head><p>We analyze a mixture of two learners model, where each learner is independently trained on a distinct part of the domain of the data distribution. Formally, let X = [x, y] denote a single data point, where x ? R d denotes an input point and y ? Y its corresponding label or target value. For example, Y is R in regression problems, and [k] in classification problems. Each point X is drawn from a distribution D with density f D (X). We denote an i.i.d. sample of m points from D as</p><formula xml:id="formula_0">X m = {X 1 , ..., X m } ? D m . Let R 1 , R 2 ? R d ?Y denote a partition of the domain of f D , where R 1 ? R 2 = R d ? Y and R 1 ? R 2 = ?.</formula><p>Let D 1 , D 2 denote the conditional distributions obtained when restricting D to regions R 1 , R 2 respectively. Note that D can now be viewed as a mixture distribution, where points are sampled from D 1 with probability p = X?R1 f D (X)dX, and D 2 with probability (1 ? p). Let m i , i ? [2] denote the number of points in R i when sampling m points from D, and X mi denote the restriction of sample X m to R i . We denote the hypothesis of a learner when trained independently on X mi as h(X mi ).</p><p>Next, we define the error score of a learner, which is a function of m -the training set size. It measures the expected generalization error of the learner over all such training sets.</p><p>Definition 1 (Error score). Assume training sample X m ? D m , with the corresponding learned hypothesis h(X m )a random variable whose distribution is denoted D h . Let Er (h (X m )) denote the expected generalization error of this hypothesis. The expected error of the learner, over all training sets of size m, is given by</p><formula xml:id="formula_1">E D (m) = E X m ?D m E h(X m )?D h [Er (h (X m ))] .</formula><p>We adopt two common assumptions regarding E D (m). During training, we assume a mixture of independent learners in R 1 , R 2 , and a training set composed of m 1 , m 2 examples from each region respectively. The error score of the mixture learner on D for m = m 2 + m 1 is:</p><formula xml:id="formula_2">E D (m) = p ? E D1 (m 1 ) + (1 ? p) ? E D2 (m 2 ). (1)</formula><p>As an important ingredient of the mixture model, we assume that one region requires fewer examples to be adequately learned, and call this region R 1 . Essentially, we expect the error score to decrease faster at R 1 , with E D1 (m) &lt; E D2 (m) ?m. Other than this difference, we expect the error score to be similar in R 1 and R 2 .</p><p>Making this notion more precise, we define an order relation on partition R 1 , R 2 as follows:</p><p>Definition 2 (Order R 1 ? R 2 ). Let R 1 , R 2 denote a partition of the domain of f D . Assume that the error score in R 1 and R 2 can be written as E D1 (m) = E(m) and E D2 (m) = E(?m) for a single function E(m) and ? &gt; 0. We say that R 1 is easier to learn than R 2 and denote</p><formula xml:id="formula_3">R 1 ? R 2 if ? &lt; p 1?p , where p is the probability of R 1 .</formula><p>Note that ? &lt; 1 if p = 0.5. We now assume that R 1 ? R 2 , and rewrite (1) as follows:</p><formula xml:id="formula_4">E D (m) = p ? E(m 1 ) + (1 ? p) ? E(?m 2 ).<label>(2)</label></formula><p>Finally, we extend E(m) to domain R ?0 with the continuation of E(m) denoted E(x) : R ?0 ? R ?0 , which is in C ? and positive. Efficiency is extended to imply E (x) &lt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deriving the Optimal Sampling Strategy</head><p>Considering the extended error score E(x) : R ?0 ? R ?0 , we define a biased sampling strategy as follows:</p><formula xml:id="formula_5">m 1 = p ? m + ?, m 2 = (1 ? p) ? m ? ?.</formula><p>? = 0 is essentially equivalent to random sampling from D. ? &gt; 0 implies that more training points are sampled from R 1 than R 2 , and vice versa.</p><p>In Thm. 1 we show that to minimize the expected generalization error while considering a mixture model of two independent learners as defined above, choosing between the different sampling strategies can be done using a simple threshold test. Theorem 1. Given partition R 1 ? R 2 and error score E(x), let p = P rob(R 1 ) and 0 &lt; ? &lt; p 1?p . The following threshold test decreases the error score for sample size m:</p><formula xml:id="formula_6">E (pm) E (? (1 ? p) m) ? ? ? ? ? ? ? &gt; ?(1?p) p =? over sample region R 1 &lt; ?(1?p) p =? over sample region R 2</formula><p>Proof. Starting from <ref type="formula" target="#formula_4">(2)</ref>, we obtain the test whereby the error score E D (m) decreases when ? &gt; 0</p><formula xml:id="formula_7">p ? E (p ? m + ?) + (1 ? p) ? E (? ((1 ? p) m ? ?)) &lt; p ? E (p ? m) + (1 ? p) ? E (? (1 ? p) m) =? (1 ? p) [E (? (1 ? p) m) ? E (? ((1 ? p) m ? ?))] &gt; p [E (p ? m + ?) ? E (p ? m)] =? ? (1 ? p) E (? (1 ? p) m) ? E (? (1 ? p) m ? ??) ?? &gt; p E (p ? m + ?) ? E (p ? m) ? .</formula><p>Since E(x) is differentiable and strictly monotonically decreasing with E &lt; 0, in the limit of infinitesimal ?</p><formula xml:id="formula_8">E (pm) E (? (1 ? p) m) &gt; ? (1 ? p) p .</formula><p>The proof for ? &lt; 0 is similar.</p><p>Example: exponentially decreasing function. Assume E (m) = e ?m , and a mixture model with p = 0.8, ? = 0.1. We simulate the error in (2) when biasing the train sample with ? = ?0.01. <ref type="figure" target="#fig_0">Fig. 1a</ref> shows the differences between the error score of biased sampling (in favor of either R 1 in blue or R 2 in orange) and random sampling, as a function of the number of examples m. For small m it is beneficial to favorably bias region R 1 , while for large m it is beneficial to favorably bias R 2 . This is the behavior often seen in our empirical investigation, see <ref type="figure" target="#fig_0">Fig. 1b</ref> and Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Error Scores Analyzed</head><p>We now report sufficient conditions on E(m), which guarantee the phase-transition-like behavior illustrated in <ref type="figure" target="#fig_0">Fig. 1a</ref>, starting with some formal definitions.</p><p>Given partition R 1 ? R 2 and Error score E(x), we say that E(x) is undulating if it displays the following behavior: in the beginning, when the number of training examples is small, the generalization error decreases faster when oversampling region R 1 . In the end, after seeing sufficiently many training examples, the generalization error decreases faster when over-sampling region R 2 . Formally:</p><formula xml:id="formula_9">Definition 3 (Undulating). An error score E(m) is undu- lating if there exist z 1 , z 2 ? R such that E (pm) E (?(1?p)m) &gt; ?(1?p) p ?m &lt; z 1 , and E (pm) E (?(1?p)m) &lt; ?(1?p) p ?m &gt; z 2 .</formula><p>For undulating error scores, there could potentially be any number of transitions between the two conditions, switching the preference of R 1 to R 2 , and vice versa. We extend the above definition to capture a case of particular interest, where this transition occurs only once, as follows:</p><p>Definition 4 (SP-undulating). An error score E(m) is Single-Phase undulating if it is undulating, and z 1 = z 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">UNDULATING ERROR SCORES</head><p>As motivated in Section 2.1, we define a proper error score as follows:</p><p>Definition 5 (Proper error score). E(x) : R ?0 ? R ?0 is a proper error score if it is a positive twice differentiable function, which is strictly monotonically decreasing (E &gt; 0,</p><formula xml:id="formula_10">E &lt; 0), E(0) = c 0 ? R &gt;0 , and where lim x?? E(x) = 0.</formula><p>Not all proper error scores will exhibit the phase undulating behavior. In Thm. 2 we state sufficient conditions that ensure this behavior (see proof in App. B.1). Theorem 2 (Undulating error score: sufficient conditions). Given partition R 1 ? R 2 and Error score E(x), let p = P rob(R 1 ) and 0 &lt; ? &lt; p 1?p . E(x) is undulating if the following assumptions hold:</p><formula xml:id="formula_11">(i) E(x) is a proper error score (see Def. 5). (ii) lim x?? E (x) E(x) , lim x?? E(x) E(ax) , lim x?? E (x) E (ax) exist ?a ? (0, 1). (iii) ? log(E(x)) ? ? (log(x)).</formula><p>Corollary 1 (Exponential error as a bound). Error score E(x) is undulating if it satisfies assumptions (i) and (ii) of Thm. 2, and is bounded from above as follows</p><formula xml:id="formula_12">E(x) ? ke ??x ?x ? R ?0 ,<label>(3)</label></formula><p>for some constants ?, k ? R &gt;0 .</p><p>Proof. It can be readily verified that assumption (iii) of Thm. 2 follows from (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">SP-UNDULATING ERROR SCORES</head><p>Thm. 3 extends the results of the previous section, by stating a set of sufficient conditions that ensure an SP-undulating error score. The proof can be found in App. B.2.</p><p>Theorem 3 (SP-undulating: sufficient conditions). Given partition R 1 ? R 2 and Error score E(x), let p = P rob(R 1 ) and 0 &lt; ? &lt; p 1?p . E(x) is SP-undulating if the following assumptions hold:</p><p>1. E(x) is an undulating error score. 2. At least one of the following conditions holds:</p><formula xml:id="formula_13">(a) ?E (x)?x E (x)</formula><p>is monotonically increasing with x.</p><formula xml:id="formula_14">(b) ?E (x)</formula><p>is strictly monotonically decreasing and log-concave.</p><p>Corollary 2 (Exponential error is SP-undulating). Consider error scores of the form E(x) = ke ??x for constants ?, k ? R &gt;0 . Such functions are SP-undulating.</p><p>Cor. 2 shows that classifiers with an exponentially decreasing error score are SP-undulating, with a single transition from favoring R 1 to favoring R 2 .</p><p>In practice, we cannot assume that the error score of commonly used learners is exponentially decreasing. However, frequently we can bound the error score from above by an exponentially decreasing function, as we demonstrate theoretically (see Section 2.4.1) and empirically (see <ref type="figure" target="#fig_0">Fig. 11</ref> in App. D.2). In such cases, it follows from Cor. 1 that these functions are undulating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Simple Classification Models</head><p>To make the analysis above more concrete, we analyze a mixture model of two linear classifiers in Section 2.4.1, as an example of an actual undulating model in common use. Additionally, we analyze the nearest neighbors classification model in Section 2.4.2, to shed light on the rationale behind the partition of the support to regions R 1 and R 2 . This case analysis demonstrates specific circumstances that make it possible to learn from fewer examples in certain regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">MIXTURE OF TWO LINEAR CLASSIFIERS</head><p>Consider a binary classification problem and assume a learner that delivers a mixture of two linear classifiers in R d . The two classifiers are obtained by independently minimizing the L 2 loss on points in R 1 and R 2 respectively.</p><p>(i) Bounding the error of each mixture component. We first derive a bound on the error separately for R 1 and R 2 , as it depends on the sample size m j for j ? [2]. Let X ? R d?mj denote the matrix whose columns are the training vectors that lie in region R j . Let y ? {?1, 1} mj denote a row labels vector, where 1 marks positive examples and ?1 negative examples. The learner seeks a separating row</p><formula xml:id="formula_15">vector? ? R d , wher? w = arg min w?R d wX ? y 2 =?? = yX (XX ) ?1 .</formula><p>In Thm. 4, we bound the error of a linear model by some exponential function of the number of training examples m j . The proof and further details can be found in App. C.1.1.</p><p>Theorem 4 (Error bound on a linear classifier). Assume:</p><formula xml:id="formula_16">(i) a bounded sample x i ? ?,</formula><p>where XX is sufficiently far from singular so that its smallest eigenvalue is bounded from below by 1 ? ; (ii) a realizable binary problem where the classes are separable by margin ?; (iii) full rank data covariance, where 1 ? denotes its smallest singular value. Then there exist some positive constants k, ? &gt; 0, such that ?m j ? N and every sample X mj , the expected error of? obtained using X mj is bounded by:</p><formula xml:id="formula_17">E X?Dj [0 ? 1 loss of?] ? ke ??mj .</formula><p>(ii) A mixture classifier. Assume a mixture of two linear classifiers, and let E(m) = p?E D1 (m 1 )+(1?p)?E D2 (m 2 ) denote its error score. The following theorem characterizes this function (the proof can be found in App. C.1.2): Theorem 5 (Undulating error). Retain the assumptions of Thm. 4, and assume that ?a ? (0, 1) the following limits exist lim <ref type="bibr">am)</ref> . Then the error score of a mixture of two linear classifiers is undulating.</p><formula xml:id="formula_18">m?? E (m) E(m) , lim m?? E(m) E(am) , lim m?? E (m) E<label>(</label></formula><p>In practice, the error score in this case is also SP-undulating, as demonstrated in <ref type="figure" target="#fig_0">Fig. 10</ref> in App. D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">KNN CLASSIFIER AND HIGH-DENSITY REGIONS</head><p>Our analysis in Section 2.3 shows that given some partition of the data into R 1 and R 2 , where R 1 ? R 2 (see Def. 2), then oversampling from R 1 is preferable in the low budget regime, while oversampling from R 2 is preferable in the high budget regime. To shed light on the nature of the assumed partition, we analyze below the discrete one-Nearest-Neighbor (1-NN) classification framework. Specifically, we show that selecting R 1 as the set of the most probable points in the dataset has the property that R 1 ? {? \ R 1 }.</p><p>We further show in App. C.2 that in this framework, the selection of an initial pool of size m will benefit from the following heuristic:</p><p>? Max density: when selecting a point X i , maximize its density f D (X i ). ? Diversity: select points that are far apart, so that their corresponding sets of nearest neighbors do not overlap.</p><p>While 1-NN is a rather simplistic model, we propose to use the derived heuristics to guide deep active learning. In the rest of this paper, we show how these guiding principles benefit deep active learning in the low-budget regime.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method: Low Budget Active Learning</head><p>In the low-budget regime, our theoretical analysis shows that it may be beneficial to bias the training sample in favor of certain regions in the data domain. It also establishes a connection between such regions and the principles of max density (or typicality) and diversity. Here, we incorporate these principles into a simple new active learning strategy called TypiClust, designed for the low-budget regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework and Definitions</head><p>Let L 0 denote an initial labeled set of examples, and U 0 denote an initial unlabeled pool. Active learning is done iteratively: at each iteration i, a set of B unlabeled examples is picked according to some strategy. These examples are annotated by an oracle, added to L i?1 , and removed from U i?1 . This process is repeated until the labels budget is exhausted, or some predefined termination conditions are satisfied. In the low-budget regime, the total number of labeled examples |L i?1 | + B is assumed to be small. The case where L 0 = ? is called "initial pool selection".</p><p>To capture the principle of max density, we define the Typicality of an example by its density in some semantically meaningful feature space. Formally, we measure an example's Typicality by the inverse of the average Euclidean distance to its K nearest neighbors 1 , namely:  <ref type="figure" target="#fig_4">Fig. 3</ref>.</p><formula xml:id="formula_19">T ypicality(x) = 1 K xi?K-NN(x) ||x ? x i || 2 ?1 .<label>(4</label></formula><p>To overcome these obstacles we propose a novel method, <ref type="bibr">1</ref> We use K = 20, but other choices yield similar results.</p><p>called TypiClust, which attempts to select typical examples while probing different regions of the data distribution. In our method, self-supervised representation learning is used to overcome (a), while clustering is used to overcome (b).</p><p>TypiClust is therefore composed of three steps:</p><p>Step 1: Representation learning. Utilize the large unlabeled pool U 0 to learn a semantically meaningful feature space: first train a deep self-supervised task on U 0 ? L 0 , then use the penultimate layer of the resulting model as feature space. Such methods are commonly used for semantic feature extraction <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b17">Grill et al., 2020)</ref>.</p><p>Step 2: Clustering for diversity. As typicality in <ref type="formula" target="#formula_18">(4)</ref> is evaluated by measuring distances to neighboring points, the most typical examples are usually close to each other, often resembling the same image (see <ref type="figure" target="#fig_4">Fig. 3c</ref>). To enforce diversity and thus better represent the underlying data distribution, we employ clustering. Specifically, at each AL iteration i, we partition the data into |L i?1 | + B clusters. This choice guarantees that there are at least B clusters that do not intersect with the existing labeled examples. We refer to such clusters as uncovered clusters.</p><p>Step 3: Querying typical examples. We select the most typical examples from the B largest uncovered clusters. Selecting from uncovered clusters enforces diversity (also w.r.t L i?1 ), while selecting the most typical example in each cluster favors the selection of representative examples.</p><p>As the steps above do not depend on any specific representation or clustering method, different variants of the TypiClust strategy can be constructed. Below we evaluate two variants, both of which outperform by a large margin the uncertaintybased strategies in the low-budget regime:</p><p>1. T P C DC : Using a deep clustering algorithm both for the self-supervised and clustering tasks. In our experiments, we used SCAN (Van Gansbeke et al., 2020). 2. T P C RP : Using representation learning followed by a clustering algorithm. We used DINO <ref type="bibr" target="#b4">(Caron et al., 2021)</ref> for ImageNet, and SimCLR <ref type="bibr" target="#b7">(Chen et al., 2020)</ref> for all other datasets, followed by K-means.</p><p>The pseudo-code of TypiClust for initial pool selection is given in Alg. 1 (see more details in App. F.1). Note that, unlike traditional active learning strategies, TypiClust relies on self-supervised representation learning, and therefore can be used for initial pool selection.</p><p>Algorithm <ref type="formula">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Study</head><p>We now report our empirical results. Section 4.1 describes the evaluation protocol, datasets, and baseline methods. Section 4.2 describes the actual experiments and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Methodology</head><p>We evaluate active learning separately in the following three frameworks. (i) Fully supervised: training a deep network solely on the labeled set, obtained by active queries. In (i) and (ii), we adopt the AL evaluation framework created by <ref type="bibr">Munjal et al. (2020)</ref>, which implements several AL methods including all baselines used here except BADGE. In (iii) we adopt the code and hyper-parameters provided by FlexMatch. As FlexMatch is computationally intensive, it was not evaluated on ImageNet, confining the study to datasets it was reported to handle. In all evaluated cases, TypiClust achieves large improvements (see App. F.2 for implementation details).</p><p>We compare TypiClust to the following baseline strategies for the selection of B points from U : (1) Randomuniformly.</p><p>(2) Uncertainty -lowest max softmax output.</p><p>(3) Margin -lowest margin between the two highest softmax outputs. (4) Entropy -highest entropy of softmax outputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results: Low Budget Regime</head><p>The amount of labeled data that makes a budget "low" will vary between tasks. In the following experiments, unlike most earlier work, we focus on scenarios where on average,  We see that in the low budget regime, both TypiClust variants outperform the baselines by a large margin. Specifically, all other baseline AL methods perform on par with random selection or worse, in accordance with <ref type="bibr">Pourahmadi et al. (2021)</ref>. In contrast, the typicality-based strategy achieves large accuracy gains. Noting that most of the baselines are possibly hampered by their use of random initial pool selection when L 0 = ?, our ablation study in Section 4.3.1 demonstrates that this is not a decisive factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">FULLY SUPERVISED WITH SELF-SUPERVISED EMBEDDING</head><p>As self-supervised embeddings can be semantically meaningful, they are often used as features for a linear classifier.</p><p>Accordingly, in this framework, we use the extracted features from the representation learning step and train a linear classifier on the queried labeled set L i . Unlike the fully supervised framework, here we use the unlabeled data while training the classifier, albeit in a basic manner. This framework outperforms the fully supervised framework, but still lags behind the semi-supervised framework. Once again, TypiClust outperforms all baselines by a large margin, as shown in <ref type="figure">Fig. 5</ref> (see App. G.2 for additional datasets).</p><p>(a) CIFAR-10 (b) CIFAR-100 (c) ImageNet-100 <ref type="figure">Figure 5</ref>. Similar to <ref type="figure" target="#fig_8">Fig. 4</ref> in the "fully supervised with selfsupervised embedding" framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">SEMI-SUPERVISED FRAMEWORK</head><p>In this framework, we evaluate TypiClust and different AL strategies by examining the performance of FlexMatch when trained on their respective queried examples. As semisupervised methods often achieve competitive performance with only a few labeled examples, we focus on the extreme low-budget regime, where only 0.02% ? 1% of the data is labeled. Note that semi-supervised algorithms typically assume a class-balanced labeled set, which is not feasible in active learning. To compare with this scenario which dominates the literature, we add a class-balanced random baseline for reference.</p><p>In <ref type="figure">Fig. 6</ref>, we compare the final performance of FlexMatch using the labeled sets provided by different AL strategies. We show results for a budget of 10 examples in CIFAR-10 ( <ref type="figure" target="#fig_3">Fig. 6a)</ref>, 300 examples in CIFAR-100 ( <ref type="figure">Fig. 6b</ref>), and 1000 examples in TinyImageNet <ref type="figure">(Fig. 6c</ref>). We see that both TypiClust variants outperform random sampling, whether balanced or not, by a large margin. In contrast, other AL baselines do not improve the results of random sampling. Similar results using additional budgets, baselines, datasets, and semi-supervised algorithms, can be found in App. G.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We now report the results of a set of ablation studies, checking the added value of each step in our suggested strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">RANDOM INITIAL POOL SELECTION</head><p>As TypiClust is based on self-supervised learning, both its variants are well suited for the case L 0 = ?, and can actively query the initial selection of labeled examples. By contrast, the other AL baselines use random initial pool selection when L 0 = ?. To isolate the effect of this difference, we conducted the same experiment as reported in <ref type="figure" target="#fig_3">Fig. 4a</ref>, giving TypiClust a random initial pool selection just like the other baselines. Results are reported in <ref type="figure" target="#fig_3">Fig. 7a</ref>, showing that TypiClust still outperforms all baselines. Importantly, this comparison reveals that non-random initial pool selection yields further generalization gains when combined with active learning. Additional results can be seen in <ref type="figure" target="#fig_0">Fig. 16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">COMPARING CLASS DISTRIBUTION</head><p>With an extremely low budget, covering the support of the distribution comprehensively is challenging. To compare the success of the different AL strategies in this task, we measure the Total Variation (TV) distance between the labeled set class distribution and the ground truth class distribution for each strategy. <ref type="figure">Fig. 7b</ref> shows that the TypiClust variants achieve a significantly better (lower) score than the alternatives, resulting in queries with better class balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">THE IMPORTANCE OF DENSITY AND DIVERSITY</head><p>TypiClust clusters the dataset and selects the most typical examples from every cluster. To assess the added value of clustering and typicality selection, we consider the fol-lowing alternative selection criteria: (a) Select a random example from each cluster (TPC Rand ). (b) Select the most atypical example in every cluster (TPC Inv ). (c) Select typical samples greedily, without clustering (TPC N oClust ).</p><p>The results in <ref type="figure">Fig. 7c</ref> show that both clustering and highdensity sampling are crucial for the success of TypiClust.</p><p>The low performance of TPC Rand shows that representation learning and clustering alone cannot account for all the performance gain, while the low performance of TPC N oClust shows that typicality without diversity is not sufficient (see a visualization of these variants in <ref type="figure" target="#fig_4">Fig. 3</ref> To test this hypothesis we first train an "oracle" network (see <ref type="bibr">Lowell et al. (2018)</ref> and App. F.3) on the entire CIFAR-10 dataset and use its softmax margin to estimate uncertainty. This "oracle margin" is then used to choose the query examples. Subsequently, another network is trained similarly to the setup of <ref type="figure" target="#fig_3">Fig. 4a</ref>, adding in each iteration the examples with either the highest or lowest softmax response margin according to the oracle. <ref type="figure">Figure 8</ref>. Certainty, as estimated by the margin of an oracle that knows all the labels, is used for AL. We plot the mean test accuracy of 100 models trained on CIFAR-10, |L0| = 10, B = 10. STE is very small, as shown.</p><p>The results are shown in <ref type="figure">Fig. 8</ref>. We see that even a reliable measure of uncertainty leads to poor performance in the low-budget regime, even worse than the baseline uncertainty-based methods. This may be because these methods compute the uncertainty in an unreliable way, and thus behave more like the random selection strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5.">IMBALANCED DATA</head><p>Unsupervised representation learning methods often assume class-balanced datasets. As TypiClust is based on representation learning, it could potentially fail in imbalanced settings. We repeated our experiments on the class-imbalanced subset of CIFAR-10 proposed by Munjal et al. <ref type="bibr">(2020)</ref>. As before, we show that TypiClust outperforms other methods in the low-budget regime, and under-performs in the high-budget regime (see low budget results in <ref type="figure" target="#fig_0">Fig. 17</ref> of App. G.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Discussion</head><p>We show, theoretically and empirically, that strategies for active learning in the high and low-budget regimes should be based on opposite principles. Initially, in the low-budget regime, the most typical examples, which the learner can learn most easily, are the most helpful to the learner. When reaching the high-budget regime, the best examples to query are those that the learner finds most confusing. This is the case both in the fully supervised and semi-supervised settings: we show that semi-supervised algorithms get a significant boost from seeing the labels of typical examples. The point of transition -what makes the budget "small" or "large", depends on the task and corresponding data distribution. In complex real-life problems, the low-budget regime may still contain a large number of examples, increasing the practicality of our method. Determining the range of training sizes with "low budget" characteristics is a challenging problem, which we leave for future work. Deep AL strategies often enforce diversity on the queried batch. The motivation is to avoid redundancy in the annotations, and to represent all parts of the training distribution. <ref type="bibr" target="#b24">Sener &amp; Savarese (2018)</ref>   <ref type="bibr">(2020)</ref>. Notably, as deep active learning is practical only in batch settings, the importance of diversity is amplified <ref type="bibr" target="#b15">(Geifman &amp; El-Yaniv, 2017;</ref><ref type="bibr" target="#b24">Sener &amp; Savarese, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE</head><p>Diversity sampling is orthogonal to uncertainty sampling, and can be added to almost any strategy. As opposed to previous works, our strategy aims to query a diverse set of characteristic examples, while other strategies aim to achieve diverse sets of uncharacteristic examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. AL Strategies for Low Budgets</head><p>Recently, AL in the low-budget regime received increased attention. Strategies designed to address this regime usually employ self-supervised or semi-supervised methods using the unlabeled pool <ref type="bibr" target="#b14">(Gao et al., 2020;</ref><ref type="bibr">Hong et al., 2020;</ref><ref type="bibr">Mahmood et al., 2021;</ref><ref type="bibr">Yehuda et al., 2022)</ref>. The embedding of such methods is often utilized by methods that estimate uncertainty, as it gives an informative distance measure <ref type="bibr">(Zhang et al., 2018)</ref>.</p><p>In particular, <ref type="bibr">Yuan et al. (2020)</ref> aims to solve the cold-start problem by using the embedding of a pre-trained model on some unsupervised task. Their experiments use pre-trained language models, with a strategy that decreases the dependency on high budgets but is still faithful to uncertainty sampling. Mahmood et al. <ref type="formula" target="#formula_4">(2021)</ref>  </p><formula xml:id="formula_20">F (ax) , lim x?? f (x) f (ax) exist ?a ? (0, 1). Denote g(x) = ? ln (F (x)). If g (x) ? ? 1 x ,</formula><p>then:</p><formula xml:id="formula_21">lim x?? f (x) f (ax) = 0 ?a ? (0, 1).</formula><p>Proof. We can write F (x) = e ?g(x) . It follows from the mean value theorem that ?t ax &lt; t &lt; x such that</p><formula xml:id="formula_22">F (x) F (ax) = e ?(g(x)?g(ax)) = e ?(g (t)x(1?a))</formula><p>= e ?(g (t)t? x t ?(1?a)) .</p><p>Since g (x) ? ? 1 x we get lim t?? t ? g (t) = ?.</p><formula xml:id="formula_23">As (1 ? a) &lt; x t (1 ? a) &lt; (1?a) a , it follows that lim x?? F (x) F (ax) = 0.</formula><p>From the assumption that the limits exist, and since lim x?? F (x) = lim x?? F (ax) = 0, we can use L'H?pital's rule and get</p><formula xml:id="formula_24">lim x?? F (x) F (ax) = lim x?? f (x) af (ax) = 1 a lim x?? f (x) f (ax) = 0.</formula><p>Lemma 3. Let F : R ?0 ? R &gt;0 denote a positive differentiable function (F &gt; 0). Denote g(x) = ? ln(F (x)). Assume that lim x?? g (x) exists, and</p><formula xml:id="formula_25">g(x) ? ? (log(x)) , then g (x) ? ? 1 x . Proof. g(x) ? ?(log(x)) implies that lim x?? g(x) ln(x) = ?, lim x?? g(x) = ?.</formula><p>We can now use L'H?pital's rule and get</p><formula xml:id="formula_26">? = lim x?? g(x) ln(x) = lim x?? g (x) 1 x = lim x?? xg (x).</formula><p>Theorem 2. Given partition R 1 ? R 2 and Error score E(x), let p = P rob(R 1 ) and 0 &lt; ? &lt; p 1?p . E(x) is undulating if the following assumptions hold:</p><formula xml:id="formula_27">(i) E(x) is a proper error score (see Def. 5). (ii) lim x?? E (x) E(x) , lim x?? E(x) E(ax) , lim x?? E (x) E (ax) exist ?a ? (0, 1). (iii) ? log(E(x)) ? ? (log(x)). Proof. We define f (x) = ?E (px), a = ?(1?p) p &lt; 1.</formula><p>From assumption (i) and using Lemma 1, we get</p><formula xml:id="formula_28">lim x?0 + E (px) E (?(1 ? p)x) = 1.</formula><p>Therefore there exists some z 1 ? R ?0 such that ?x &lt; z 1</p><formula xml:id="formula_29">E (px) E (?(1 ? p)x) &gt; ?(1 ? p) p .</formula><p>From assumptions (i)-(iii) and using Lemmas 2-3, we get</p><formula xml:id="formula_30">lim x?? E (x) E (ax) = lim x?? E (px) E (?(1 ? p)x) = 0,</formula><p>and therefore there is some z 2 ? R ?0 such that ?x &gt; z 2</p><formula xml:id="formula_31">E (px) E (?(1 ? p)x) &lt; ?(1 ? p) p .</formula><p>From Def. 3 we get that E(x) is undulating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. SP-undulating Error Score: Sufficient Conditions</head><p>We provide next the proof for Thm. 3, which is stated in Section 2.3.2, and which lists sufficient conditions for error scores to be SP-undulating (see Def. 3), extending Thm. 2. Once again, we start with a few lemmas. Lemma 4. Let f : R ?0 ? R &gt;0 denote a positive differentiable function (f &gt; 0). Let 0 &lt; a &lt; 1 denote some constant. If</p><formula xml:id="formula_32">h (x) = ?f (x) x f (x)</formula><p>is strictly monotonically increasing, then</p><formula xml:id="formula_33">g (x) = f (x) f (ax)</formula><p>is strictly monotonically decreasing.</p><p>Proof. g (x) is monotonically decreasing iff g (x) &lt; 0, where</p><formula xml:id="formula_34">g (x) = f (x) f (ax) ? af (x) f (ax) f (ax) 2 .</formula><p>This condition translates to</p><formula xml:id="formula_35">f (x) f (ax) ? af (x) f (ax) &lt; 0.</formula><p>As by assumption h (x) is monotonically increasing and h (ax) &lt; h (x), we get that ?x &gt; 0</p><formula xml:id="formula_36">?f (ax) ax f (ax) &lt; ?f (x) x f (x) =? ?f (ax) a f (ax) &lt; ?f (x) f (x) =? ? f (ax) f (x) a &lt; ?f (x) f (ax) .</formula><p>Lemma 5. Let f : R ?0 ? R &gt;0 denote a positive differentiable log-concave function which is strictly monotonically decreasing (f &gt; 0, f &lt; 0, (log(f )) ? 0). Then the following function is strictly monotonically increasing</p><formula xml:id="formula_37">h(x) = ?f (x) x f (x) .</formula><p>Proof. h(x) is strictly monotonically increasing iff h (x) &gt; 0, which holds iff</p><formula xml:id="formula_38">h (x) = xf (x) 2 ? xf (x)f (x) ? f (x)f (x) f (x) 2 &gt; 0 =? x f (x) 2 ? f (x)f (x) ? f (x)f (x) &gt; 0.</formula><p>Recall that x &gt; 0 and ?f (x)f (x) &gt; 0 ?x ? R ?0 . Since f is log-concave, we also have that f (x) 2 ? f (x)f (x) ? 0, which concludes the proof.</p><p>Theorem 3. Given partition R 1 ? R 2 and Error score E(x), let p = P rob(R 1 ) and 0 &lt; ? &lt; p 1?p . E(x) is SP-undulating if the following assumptions hold:</p><p>1. E(x) is an undulating proper error score. 2. At least one of the following conditions holds:</p><formula xml:id="formula_39">(a) ?E (x)?x E (x)</formula><p>is monotonically increasing with x. (b) ?E (x) is strictly monotonic decreasing and logconcave.</p><p>Proof. Define the following positive continuous function</p><formula xml:id="formula_40">H (x) = E (px) E (? (1 ? p) x)</formula><p>.</p><p>Let f (x) = ?E (x), a = a(1?p) p &lt; 1. Note that assumption 2a follows from assumption 2b and Lemma 5. Assumption 2 therefore implies that ?f (x)x f (x) is strictly monotonically increasing, and by Lemma 4 we can conclude that H(x) is monotonically decreasing. Together with assumption 1, H (x) = ?(1?p) p at a single point, and we may therefore conclude that E(x) is SP-undulating.</p><p>Corollary 3. If p -the probability of region R 1 -is sufficiently small so that p &lt; ? 1+? , then the conclusions are reversed: it is beneficial to initially over-sample R 2 , and vice versa. . Let x i denote the i-th data point and i-th column of X. Let ? 1 and ? 2 denote the respective means of the two classes, and ? = ? 1 ? ? 2 denote the vector difference between the means.</p><p>Assuming that the data is normalized to 0 mean, the maximum likelihood estimators for the covariance matrix of the distribution ? and class means, denoted? and? 1 ,? 2 respectively, are the followin?</p><formula xml:id="formula_41">? = 1 m m i=1 x i x i = 1 m XX (5) ? j = 1 m j xi?Cj x i =? yX = m 1?1 ? m 2?2 ,<label>(6)</label></formula><p>where C j denotes the set of points in class j ? [2]. Thus, the ML linear separator can be written a?</p><formula xml:id="formula_42">w = yX (XX ) ?1 = [m 1?1 ? m 2?2 ] (m?) ?1 =?? ?1 , where? = 1 m yX . Note that? is the sample mean of vec- tors {y i x i } m i=1 , and? is the sample covariance of {x i } m i=1 .</formula><p>When d &gt; m (fewer training points than the input space dimension),? is rank deficient and therefore? ?1 is not defined. Moreover, the solution is not unique. Nevertheless, it can be shown that the minimal norm solution is the Penrose pseudo-inverse? + , wher?</p><formula xml:id="formula_43">? = U DU =?? + = U D + U ,</formula><p>and using the notations <ref type="figure" target="#fig_0">(d 1 , . . . , d m , 0, .</ref> . . , 0) <ref type="figure">, 0, . . . , 0)</ref>.</p><formula xml:id="formula_44">D = diag</formula><formula xml:id="formula_45">D + = diag(d ?1 1 , . . . , d ?1 m</formula><p>Ignoring the question of uniqueness, estimating w is therefore reduced to evaluating the estimators in <ref type="formula" target="#formula_18">(5)</ref> and <ref type="formula" target="#formula_18">(6)</ref>. These ML estimators have the following known upper bounds on their error:</p><p>1. Bounding?: from known results on covariance estimation <ref type="bibr" target="#b31">(Tropp, 2015)</ref>, using Bernstein matrix inequality</p><formula xml:id="formula_46">P ( ? ? ? op ? t) ? 2de ??mt 2 .<label>(7)</label></formula><p>Constant ? does not depend on m; it is determined by the assumed bound on the L 2 norm of vectors x i , and the norm of the true covariance matrix ?. 2. Bounding?: starting from Hoeffding's inequality in one dimension, we have that</p><formula xml:id="formula_47">P ( ? k ? ? k ? t) ? 2e ?2mt 2 4? 2 ?k ? [d]</formula><p>, where we assume a bounded distribution x ? ?. Thus</p><formula xml:id="formula_48">P ( ? ? ? ? t) = P ( ? ? ? 2 ? t 2 ) = P ( d k=1 ? k ? ? k 2 ? t 2 ) ? d k=1 P ( ? k ? ? k 2 ? t 2 d ) = d k=1 P ( ? k ? ? k ? t ? d ) ? 2de ?2m t 2 4? 2 d<label>(8)</label></formula><p>The first inequality follows from the union-bound inequality.</p><p>Lemma 6.</p><formula xml:id="formula_49">? [? ?1 ?? + ]x =? [? + (? ? ?)? ?1 ]x.<label>(9)</label></formula><p>Proof. Because? ? R d?d is of rank m, Q =? +? = U diag(1, . . . , 1, 0, . . . , 0)U is a projection matrix of rank m, projecting vectors to the subspace spanned by the training set {x i } m i=1 . Thus Q? =?. Additionally, by definition, ? +??+ =? + and Q is symmetric. It follows that</p><formula xml:id="formula_50">? (? ?1 ?? + )x =? (Q? ?1 ?? + )x, while? + (? ? ?)? ?1 = Q? ?1 ?? + .</formula><p>Together, we get (9).</p><p>Theorem 4. Assume: (i) a bounded sample x i ? ?, where XX is sufficiently far from singular so that its smallest eigenvalue is bounded from below by 1 ? ; (ii) a realizable binary problem where the classes are separable by margin ?; (iii) full rank data covariance, where 1 ? denotes its smallest singular value. Then there exist some positive constants k, ? &gt; 0, such that ?m j ? N and every sample X mj , the expected error of? obtained using X mj is bounded by:</p><formula xml:id="formula_51">F (m j ) = E X?Dj [0 ? 1 loss of?] ? ke ??mj</formula><p>Proof. It follows from our assumptions that ? + op ? ?. An error will occur at x ? C 1 if w opt x ? ? and?x &lt; 0, and vice versa for x ? C 2 . In either case, the difference between the predictions of w opt and? deviate by more than ?. Thus</p><formula xml:id="formula_52">F (m) = E X?D [0 ? 1 loss] = P (error) ? P ? ? ?1 x ?? ? + x &gt; ? .</formula><p>Invoking the triangular inequality</p><formula xml:id="formula_53">? = ? ? ?1 x ?? ? + x = (? ??) ? ?1 x +? (? ?1 ?? + )x ? (? ??) ? ?1 x + ? (? ?1 ?? + )x .<label>(10)</label></formula><p>We now use Lemma 6 in order to shift (? ?1 ?? + ) to (? ??). Specifically, we insert (9) into (10) to get</p><formula xml:id="formula_54">? ? ? ?? ? ?1 op x + ? ? + op ? ? ? op ? ?1 op x ? ? ?? ?? + ?? ? ? ? op ??.</formula><p>It follows that</p><formula xml:id="formula_55">P ? ? ?1 x ?? ? + x &gt; ? ? P ( ? ?? ?? + ? ? ? op ??? 2 &gt; ?) ? P ( ? ?? ?? &gt; ? 2 ) + P ( ? ? ? op ??? 2 &gt; ? 2 ) ? 2de ?2m( ? 2?? ) 2 1 4? 2 d + 2de ?am ? 2??? 2 2 ? 4de ?km? 2 .</formula><p>The second transition follows from the union-bound inequality, and the third from (7)-(8) (where constant ? is defined). For the last transition we define</p><formula xml:id="formula_56">k = min 2 4? 2 d 1 2?? 2 , ? 1 2??? 2 2 . C.1.2. A MIXTURE CLASSIFIER</formula><p>Assume a mixture of two linear classifiers, and let E(m) = p ? E D1 (m 1 ) + (1 ? p) ? E D2 (m 2 ) denote its error score.</p><p>Theorem 5.</p><p>Keep the assumptions stated in Thm. 4, and assume in addition that ?a ? (0, 1), <ref type="bibr">am)</ref> . Then the error score of a mixture of two linear classifiers is undulating.</p><formula xml:id="formula_57">? lim m?? E (m) E(m) , lim m?? E(m) E(am) lim m?? E (m) E (</formula><p>Proof. In each region j of the mixture, Thm. 4 implies that its corresponding error score as defined in Def. 1,</p><formula xml:id="formula_58">E Dj (m j ) = E X m j ?D m j j [F (m j )]</formula><p>is bounded by an exponentially decreasing function of m j . Since E Dj (m j ) measures the expected error over all samples of size m j , it can also be shown that E Dj (m j ) is monotonically decreasing with m j . From the separability assumption, lim mj ?? E Dj (m j ) = 0. Finally, since E(m) is a linear combination of two such functions, it also has these properties. We conclude from Cor. 1 that the error score of a mixture of two linear classifiers is undulating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. 1-NN Classifier</head><p>If the training sample size m is small, our analysis shows that under certain circumstances, it is beneficial to prefer sampling from a region R where E D R (m) &lt; E D ?\R (m). We now show that the set of densest points has this property.</p><p>To this end, we adopt the one-Nearest-Neighbor (1-NN) classification framework. This is a natural framework to address the aforementioned question for two reasons: (i) It involves a general classifier with desirable asymptotic properties. (ii) The computation of both class density and 1-NN is governed by local distances in the input feature space.</p><p>To begin with, assume a classification problem with k classes that are separated by at least ?. More specifically, assume that ?x, x ? R d , if x ? x ? ? then y = y. Let B v (x i , r) denote a ball centered at x i ? R d , with radius smaller than r and volume v. For X = (x, y), let f D (X) denote the density function from which data is drawn when sampling is random (and specifically at test time).</p><p>Assume a 1-NN classifier whose training sample is T = {x i , y i } m i=1 , and whose prediction at X = (x, y) is</p><formula xml:id="formula_59">y = ? ? ? y ? , ? = arg min i?[m] x ? x i x ? B v (x i , ?) y ? U (1, k) otherwise</formula><p>The error probability of this classifier at x is</p><formula xml:id="formula_60">P (x) = 0 ? i such that x ? B v (x i , ?) k?1 k otherwise</formula><p>The 0 ? 1 loss of this classifier is</p><formula xml:id="formula_61">E X?D [P (x)] = k ? 1 k P rob x / ? m i=1 B v (x i , ?) ,<label>(11)</label></formula><p>where B v (x i , r) denotes a ball centered at x i ? R d , with radius smaller than r and volume v.</p><p>The next theorem states properties of set T which are beneficial to the minimization of this loss:</p><p>Theorem 6. Let A i denote the event {x ? B v (x i , r)}, and assume that these events are independent. Then we have</p><formula xml:id="formula_62">L(T ) = k ? 1 k 1 ? m i=1 f D (X i )v + O(v 2 ) .</formula><p>Proof. Using the independence assumption and (11), and assuming that v is sufficiently small</p><formula xml:id="formula_63">L(T ) = k ? 1 k 1 ? P m i=1 A i = k ? 1 k 1 ? m i=1 P (A i ) = k ? 1 k 1 ? m i=1 X 1 x?Bv(xi,r) f D (X)dX = k ? 1 k 1 ? m i=1 f D (X i )v + O(v 2 ) .</formula><p>In Thm. 6, we show that if v is sufficiently small, the 0-1 loss is minimized when choosing a set of independent points</p><formula xml:id="formula_64">{X i } m i=1 that maximizes m i=1 f D (X i )</formula><p>. This suggests that the selection of an initial pool of size m will benefit from the following heuristic:</p><p>? Max density: when selecting a point X i , maximize its density f D (X i ). We now empirically analyze the error score function of the mixture of two linear classifiers, defined in Section 2.4.1. Each linear classifier is trained on a different area of the support. The data is 100 dimensional, linearly separable in each region. The margin is used to determine the ? of the data. The data is chosen such that p = 0.9, ? = 0.2. The results are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. The differences in accuracy when over-sampling from either R1 and R2 over a random sampling from the data distribution. Although the error score is only proven to be undulating, we can see that in practice it is also SP-undulating.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Error Scores of Deep Neural Networks</head><p>Next, we plot the error scores of deep neural networks on image classification tasks. In all the datasets we evaluated, the error of deep networks as a function of the number of examples drops much faster than an exponential function and therefore can be shown to be undulating. In practice, such error functions are bounded from above by an exponent, and hence are also SP-undulating. To see some examples of error functions of neural networks trained on super-classes of CIFAR-100, refer to <ref type="figure" target="#fig_0">Fig. 11</ref>. Recall that TypiClust first clusters the dataset to 30 clusters -using SCAN clustering algorithm. We plot the tSNE dimensionality reduction of the model's feature space, colored in various ways: <ref type="figure" target="#fig_0">Fig. 12a</ref> shows the tSNE embedding colored by the GT labels. <ref type="figure" target="#fig_0">Fig. 12b</ref> shows the tSNE embedding colored by the cluster assignment. <ref type="figure" target="#fig_0">Fig. 12c</ref> shows the tSNE embedding colored by the log density (for better visualization). Examples marked with ? are selected for labeling. <ref type="figure" target="#fig_0">Fig. 12d</ref> shows the selected images. <ref type="figure" target="#fig_0">Fig. 13</ref> shows 100 examples selected by TypiClust from ImageNet-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization of Query Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Method Implementation Details</head><p>Step 1: Representation learning -CIFAR and TinyIm-ageNet. We trained SimCLR using the code provided by <ref type="bibr">Van Gansbeke et al. (2020)</ref> for CIFAR-10, CIFAR-100 and TinyImageNet. Specifically, we used ResNet18 with an MLP projection layer to a 128 vector, trained for 500 epochs. All the training hyper-parameters were identical to those used by SCAN. After training, we used the 512 dimensional penultimate layer as the representation space. As in SCAN, we used an SGD optimizer with 0.9 momentum, and an initial learning rate of 0.4 with a cosine scheduler. The batch size was 512 and weight decay of 0.0001. The augmentations were random resized crops, random horizontal flips, color jittering, and random grayscaling. We refer to <ref type="bibr">Van Gansbeke et al. (2020)</ref> for additional details. We used the L2 normalized penultimate layer as embedding.</p><p>Step 1: Representation learning -ImageNet. We extracted embedding from the official (ViT-S/16) DINO weights pre-trained on ImageNet. We used the L2 normalized penultimate layer as embedding.</p><p>Step 2: Clustering for diversity. We limited the number of clusters when partitioning the data to max_clusters (a hyperparameter). This parameter was arbitrarily picked as 500 for CIFAR-10 and CIFAR-100 and 1000 for TinyIma-geNet and ImageNet subsets (other values resulted in similar behavior). This was done for two reasons: (a) prevent over clustering (ending up with clusters that are too small); (b) stabilize the clustering algorithms. The number of clusters chosen is K = min(|L i?1 | + B, max_clusters).</p><p>K-Means. We used scikit-learn KMeans when K ? 50 and MiniBatchKMeans otherwise. This was done to reduce runtime when the number of clusters is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCAN.</head><p>We used the code provided by SCAN and modified the number of clusters to K. We only trained the first step of SCAN (we did not perform the pseudo labeling step, since it degraded clustering performance).</p><p>Step 3: Clustering for diversity. Since we introduced max_cluster, we are no longer guaranteed to have B clusters that don't intersect the labeled set. Moreover, to estimate typicality, we require &gt; 20 samples in every cluster. To solve this, we used min{20, cluster_size} nearest neighbors. To avoid inaccurate estimation of the typicality, we dropped clusters with less than 5 samples 2 .</p><p>After all is done, the method adds points iteratively until the budget is exhausted, in the following way: (1) Out of the clusters with the fewest labeled points and of size larger than 5, select the largest cluster.</p><p>(2) Compute the Typicality of every point in the selected cluster, using min{20, cluster_size} neighbors.</p><p>(3) Add to the query the point with the highest typicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Evaluation and Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.1. FULLY SUPERVISED EVALUATION</head><p>We used the active learning comparison framework by <ref type="bibr">Munjal et al. (2020)</ref>. Specifically, we trained a ResNet18 on the labeled set, optimizing using SGD with 0.9 momentum and Nesterov momentum. The initial learning rate is 0.025 and was modified using a cosine scheduler. The augmentations used are random crops and horizontal flips. Our changes to this framework are listed below.</p><p>Re-Initialize weights between iterations When training with extremely low budgets, networks tend to produce overconfident predictions. As a result, when querying samples and fine-tuning from the existing model, the loss tends to "spike", which leads to optimization issues. Therefore, we re-initialized the weights between iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TinyImageNet modifications</head><p>As training did not converge in the original implementation over Tiny-ImageNet, we increased the number of epochs from 100 to 200 and changed the minimal crop side from 0.08 to 0.5. This ensured stable convergence and a more reliable evaluation in AL experiments.</p><p>ImageNet modifications ImageNet hyper-parameters were identical to TinyImageNet except for the number of epochs, which was set to 100 due to high computational cost, and the batch size, which was set to 50 to fit into a standard GPU virtual memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.2. LINEAR EVALUATION ON SELF-SUPERVISED EMBEDDING</head><p>In these experiments, we also used the framework by <ref type="bibr">Munjal et al. (2020)</ref>. We extracted an embedding as described in Appendix F.1, and trained a single linear layer of size d ? C where d is the feature dimensions, and C is the number of classes. To optimize this single layer, we increased the initial learning rate by a factor of 100 to 2.5, and as the training time is much shorter, we multiplied the number of epochs by 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.3. SEMI-SUPERVISED EVALUATION</head><p>When training FlexMatch, we used the semi-supervised framework by <ref type="bibr">Zhang et al. (2021)</ref>. All experiments were repeated 3 times. We used the following hyper-parameters when training each experiment:</p><p>CIFAR-10. We trained WideResNet-28, for 400k iterations. We used SGD optimizer, with 0.03 learning rate, 64 batch size, 0.9 momentum, 0.0005 weight decay, 2 widen factor, 0.1 leaky slope and without dropout. The augmentations are similar to those used in FlexMatch. The weak augmen-tations include random crops and horizontal flips, while the strong augmentations are according to RandAugment <ref type="bibr" target="#b9">(Cubuk et al., 2020)</ref>.</p><p>CIFAR-100. We trained WideResNet-28, for 400k iterations. We used an SGD optimizer, with 0.03 learning rate, 64 batch size, 0.9 momentum, 0.0001 weight decay, 8 widen factor, 0.1 leaky slope, and without dropout. The augmentations are similar to those used in CIFAR-10.</p><p>TinyImageNet. We trained ResNet-50, for 1.1m iterations. We used an SGD optimizer, with a 0.03 learning rate, 32 batch size, 0.9 momentum, 0.0003 weight decay, 0.1 leaky slope, and without dropout. The augmentations are similar to those used in FlexMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Ablation studies</head><p>Margin by an "oracle" network. In Section 4.3.4, we compute an "oracle" uncertainty measure. When training an oracle, we use a VGG-19 <ref type="bibr" target="#b29">(Simonyan &amp; Zisserman, 2014)</ref> trained on CIFAR-10, using the hyper-parameters of the original paper. We calculate the margin of each example according to this network. We note that an AL strategy based on this margin works well in the high-budget regime for both the oracle and the "student" network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Empirical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Supervised Framework</head><p>In the main paper, we presented results on 1 and 5 samples per class on average. <ref type="figure" target="#fig_0">Fig. 14</ref> shows similar results using additional budget sizes. In <ref type="figure" target="#fig_0">Fig. 15</ref> we present results on additional datasets, which in-clude ImageNet-50, ImageNet-100 and TinyImageNet. Typ-iClust outperforms all competing methods on these datasets as well. In Section 4.3.1, we show that even when the initial pool is sampled randomly, TypiClust still improves over random selection. <ref type="figure" target="#fig_0">Fig. 16</ref> provides additional evidence for this phenomenon, on several datasets and budget sizes.  G.2. Supervised Using Self-Supervised Embeddings</p><p>In <ref type="figure" target="#fig_0">Fig. 18</ref>, we show results using additional datasets of a linear classifier trained over a self-supervised pre-trained embedding. We see that the initial pool selection provides a very large boost in performance -especially with ImageNet-50 and ImageNet-200. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Semi-Supervised Framework</head><p>Finally, below we describe additional experiments to those plotted in <ref type="figure">Fig. 6</ref>, within the semi-supervised framework.</p><p>To test the dependency of our deep clustering variant of Typi-Clust on SCAN, we evaluated another variant based on RUC <ref type="bibr">(Park et al., 2021)</ref>, which is henceforth denoted T P C RU C . We plot its performance on CIFAR-10 and CIFAR-100 in <ref type="figure" target="#fig_0">Fig. 19</ref>. As RUC is computationally demanding, we fix the number of clusters to the number of classes in the corresponding dataset, and then (when needed) further subcluster the data using K-means to the desired number of clusters. In all the tested settings, T P C RU C surpassed the performance of the random baseline by a large margin, suggesting that using SCAN is not crucial for TypiClust, and may be replaced by any competitive clustering algorithm.</p><p>Additionally, we performed experiments with other budgets. In <ref type="figure" target="#fig_0">Fig. 19a</ref>, we plot the same experiment as <ref type="figure" target="#fig_3">Fig. 6a</ref>, with a budget of 40 examples on CIFAR-10, seeing similar results.</p><p>To verify that the observed performance boost is not unique to FlexMatch, we repeat the same experiments with another competitive semi-supervised learning method, <ref type="bibr">Semi-MMDC (Lerner et al., 2020)</ref>. Using the code provided by <ref type="bibr">Lerner et al. (2020)</ref>, and following the exact training proto- We note that in all the experiments we performed within the low budget regime, TypiClust always surpassed the random baseline by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of phase transition in deep active learning, as revealed by plotting the difference in accuracy between different AL strategies to a random baseline as a function of budget size (the number of labeled examples). We see similar behavior both theoretically and empirically: when the budget is low, oversampling typical examples is more beneficial, whereas when the budget is high, oversampling atypical examples is more beneficial. (a) The behavior of an idealized model (see Section 2). (b) The behavior of TypiClust, contrasted with 2 basic uncertainty-based strategies, as seen in deep neural models trained on CIFAR-10 (see Section 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Visualizing the selection of 30 examples from CIFAR-10 by TypiClust. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(i) Efficiency: E D (m) is strictly monotonically decreasing, namely, on average the learner benefits from additional examples. (ii) Realizability: lim m?? E D (m) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>TypiClust -typical and diverse (b) A-typical and diverse (c) Typical and not diverse</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative visualization of diversity and typicality in the low budget regime on CIFAR-10. (a) Diverse typical images chosen by TypiClust. (b) Picking the least typical example in each cluster. (c) Picking the most typical examples, without enforcing diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(ii) Fully supervised with self-supervised embeddings: training a linear classifier on the embedding obtained from a pre-trained self-supervised model. (iii) Semi-supervised: training a deep network on the labeled and unlabeled sets, using the competitive method FlexMatch (Zhang et al., 2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(5) DBAL<ref type="bibr" target="#b13">(Gal et al., 2017)</ref>. (6) CoreSet<ref type="bibr" target="#b24">(Sener &amp; Savarese, 2018</ref>).(7) BALD (Kirsch et al., 2019). (8) BADGE (Ash et al., 2020). All strategies are evaluated on the following image classification tasks: CIFAR-10/100 (Krizhevsky et al., 2009), TinyImageNet (Le &amp; Yang, 2015) and ImageNet-50/100/200. The latter group includes subsets of ImageNet (Deng et al., 2009) containing 50/100/200 classes respectively, following Van Gansbeke et al. (2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>only 1-10 examples per class are labeled each round. 4.2.1. FULLY SUPERVISED FRAMEWORK Fig. 4 shows accuracy results for CIFAR-10/100 and ImageNet-100, using the labeled examples queried by different AL strategies. Denoting the number of classes by M , we show results with Budget B = M or B = 5M labeled examples and L 0 = ? (see App. G.1 for additional budgets). (a) CIFAR-10 (b) CIFAR-100 (c) ImageNet-100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>"Fully supervised" framework: comparing TypiClust with baseline AL strategies on CIFAR10, CIFAR100, and ImageNet-100 for 5 active learning iterations in the low budget regime. The budget B is equal to (top) the number of classes, or (bottom) 5 times the number of classes. The final average test accuracy in each iteration is reported, using 10 (CIFAR) and 3 (ImageNet) repetitions. The shaded area reflects standard error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Comparison of AL strategies in a semi-supervised task. Each bar shows the mean test accuracy after 3 repetitions of Flex-Match trained on: (a) 10 examples from CIFAR-10, (b) 300 examples from CIFAR-100, (c) 1000 examples from TinyImageNet. Error bars show the standard error. (a-b) Same experiment as in Fig. 4a top, but where: (a) TypiClust uses random initial set selection; (b) the Total Variation (TV) distance between the labeled set distribution and the ground truth class distribution is shown. (c) To isolate the added value of clustering for diversity and typical sample selection, we evaluate 3 additional selection heuristics on CIFAR-10 (see Section 4.3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>TypiClust achieves major accuracy gains as compared to the random selection baseline in the fully-supervised (3 repetitions on ImageNet, 10 otherwise), semi-supervised (3 reps) and selfsupervised embedding (5 reps) frameworks. We use 10, 300, 1000, 50, 100, 200 examples in CIFAR-10, CIFAR-100, TinyImageNet, ImageNet-50, ImageNet-100 and ImageNet-200 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9</head><label>9</label><figDesc>summarizes all our empirical results.Our results are closely related to curriculum learning<ref type="bibr" target="#b3">(Bengio et al., 2009;</ref><ref type="bibr" target="#b18">Hacohen &amp; Weinshall, 2019;</ref> Weinshall  &amp; Amir, 2020), hard data mining, and self-paced learning(Kumar et al., 2010), all of which reflect the added value of typical ("easy") examples when there is little information about the task, as against atypical ("hard") examples which are more beneficial later on. Our results are also closely related to the study of the learning order of neural networks, which also characterize "easy" and "hard" examples Gissin&amp; Shalev-Shwartz (2019); Hacohen et al. (2020); Shah et al. (2020); Hacohen &amp; Weinshall (2021); Choshen et al. (2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>introduced the coreset approach, querying examples that cover the training distribution in a greedy manner. Many other AL algorithms incorporate diversity as part of their sampling strategy, including: Hu et al. (2010); Elhamifar et al. (2013); Yang et al. (2015); Wang et al. (2016); Yin et al. (2017); Zhdanov (2019); He et al. (2019); Kirsch et al. (2019); Ash et al. (2020); Shui et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>C</head><label></label><figDesc>. Error Function of Simple Mixture Models C.1. Mixture of Two Linear Classifiers We next prove Thm. 4 and Thm. 5, which are stated in Section 2.4.1. Thm. 4 provides a bound on the error score of a single linear classifier, showing that under mild conditions, this score is bounded by an exponentially decreasing function in the number of training examples m. Thm. 5 states conditions under which the error score of a mixture of two linear models E(m) is undulating, and presents the phase-transition behavior. C.1.1. BOUNDING THE ERROR OF EACH MIXTURE COMPONENT Henceforth we use the notations of Section 2.4.1, where for clarity, m j is replaced by m while we are discussing the bound on a single component j ? [2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>? Diversity: select varied points, for which the events {x ? B v (x i , ?)} are approximately independent. D. Theoretical Analysis: Visualization D.1. Mixture of Two Linear Models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(a) Error score (b) Phase transitionFigure 10. (a) The error score E(m) as a function of the number of examples, averaged over 10k repetitions. While the error score is not exponential, it could be upper bounded by an exponential function, as analytically shown in Section 2.4.1. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 .</head><label>11</label><figDesc>Log-error scores of image classification datasets as a function of the number of training examples. Each score is calculated as 1 ? accuracy, averaged on 100 VGG-16 networks trained on super-classes of CIFAR-100. Each line represents the error of a different super-class. As the log of the error is plotted, it can be seen that in all cases the error scores are monotonic decreasing and can be bounded above by some exponential function, suggesting that often the assumptions in Thm. 3 hold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Color by labels (b) Colored by cluster assignment (c) Color by log-density (d) The 30 samples are selected after clustering by SCAN to 30 clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 .</head><label>12</label><figDesc>(a)-(c) Visualizing the selection of 30 examples using the SCAN clustering algorithm -examples marked with ? are selected for labeling. (d) The selected images, each column represents a different label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 .</head><label>13</label><figDesc>100 ImageNet-100 examples selected by TypiClust.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 12</head><label>12</label><figDesc>demonstrates the selection of 30 examples from CIFAR-10 using TypiClust in greater detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 14 .</head><label>14</label><figDesc>Additional results in the supervised framework, including an average of 2 and 10 examples per class on CIFAR10 and CIFAR100, similarly to Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 15 .</head><label>15</label><figDesc>Similar to Fig. 4, we report the results on TinyImageNet, ImageNet-50 and ImageNet-200. G.1.1. RANDOM INITIAL POOL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 16 .</head><label>16</label><figDesc>Similar to Fig. 7a, we also report results on CIFAR-100 and ImageNet-100 with an additional budget. G.1.2. IMBALANCED CIFAR-10 Fig. 17 presents results on an imbalanced subset of CIFAR-10, where the number of samples per class decreases exponentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 17 .</head><label>17</label><figDesc>Imbalanced CIFAR-10 results on 3 different budgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 18 .</head><label>18</label><figDesc>Similar to Fig. 5, we report the linear evaluation results on TinyImageNet, ImageNet-50 and ImageNet-200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 19 .Figure 20 .</head><label>1920</label><figDesc>Similar to Fig. 6, using the T P CRUC variant of Typi-Clust. (a) 40 labels on CIFAR-10, (b) 10 labels on CIFAR-10, (c) 300 labels on CIFAR-100 (a) CIFAR-10 20 labels Similar to Fig. 6, using Semi-MMDC instead of Flex-Match. We train 20 labels on CIFAR-10, observing a large performance gain when using TypiClust to perform the initial selection of the labeled data. col, we train Semi-MMDC using 20 labels on CIFAR-10. Similarly to the results on FlexMatch, we report a significant increase in performance when training on examples chosen by TypiClust, see Fig. 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Equal contribution 1 School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel 2 Edmond &amp; Lily Safra Center for Brain Sciences, The Hebrew University of Jerusalem, Jerusalem, Israel. Correspondence to: Guy Hacohen &lt;guy.hacohen@mail.huji.ac.il&gt;, Avihu Dekel &lt;avihu.dekel@mail.huji.ac.il&gt;, Daphna Weinshall &lt;daphna@cs.huji.ac.il&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>TypiClust initial pooling algorithm</figDesc><table><row><cell>Input: Unlabeled pool U , Budget B</cell></row><row><cell>Output: B typical and diverse examples to query</cell></row><row><cell>Embedding ? Representation_Learning(U )</cell></row><row><cell>Clust ? Clustering_algorithm(Embedding, B)</cell></row><row><cell>Queries ? ?</cell></row><row><cell>for all i = 1, ..., B do</cell></row><row><cell>Add arg max x?Clust[i] {T ypicality(x)} to Queries</cell></row><row><cell>end for</cell></row><row><cell>return Queries</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>International Conference on Multimedia and Expo (ICME), pp. 1360-1365. IEEE, 2019. Hong, S., Ha, H., Kim, J., and Choi, M.-K. Deep active learning with augmentation-based consistency estimation. arXiv preprint arXiv:2011.02666, 2020. Park, S., Han, S., Kim, S., Kim, D., Park, S., Hong, S., and Cha, M. Improving unsupervised image clustering with robust learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12278-12287, 2021. Pourahmadi, K., Nooralinejad, P., and Pirsiavash, H. A simple baseline for low-budget active learning. arXiv preprint arXiv:2110.12033, 2021. images without labels. In European Conference on Computer Vision, pp. 268-285. Springer, 2020. Wang, Z., Du, B., Zhang, L., and Zhang, L. A batch-mode active learning framework by querying discriminative and representative samples for hyperspectral image classification. Neurocomputing, 179:88-100, 2016.</figDesc><table><row><cell>Appendix</cell><cell>.</cell></row><row><cell>A. Related Work</cell><cell></cell></row><row><cell>A.1. Diversity Sampling</cell><cell></cell></row><row><cell>Hu, R., Mac Namee, B., and Delany, S. J. Off to a good Weinshall, D. and Amir, D. Theory of curriculum learning, start: Using clustering to select the initial training set in with convex loss functions. Journal of Machine Learning active learning. In Twenty-Third International FLAIRS Conference, 2010. Research, 21(222):1-19, 2020.</cell><cell></cell></row><row><cell>Yang, Y., Ma, Z., Nie, F., Chang, X., and Hauptmann, A. G. Kirsch, A., Van Amersfoort, J., and Gal, Y. Batchbald: Ef-Multi-class active learning by uncertainty sampling with ficient and diverse batch acquisition for deep bayesian diversity maximization. International Journal of Com-active learning. Advances in neural information process-ing systems, 32:7026-7037, 2019. puter Vision, 113(2):113-127, 2015.</cell><cell></cell></row><row><cell>Yehuda, O., Dekel, A., Hacohen, G., and Weinshall, D.</cell><cell></cell></row><row><cell>Krizhevsky, A., Hinton, G., et al. Learning multiple layers Active learning through a covering lens. arXiv preprint</cell><cell></cell></row><row><cell>of features from tiny images. Online, 2009. arXiv:2205.11320, 2022.</cell><cell></cell></row><row><cell>Kumar, M. P., Packer, B., and Koller, D. Self-paced learning Yin, C., Qian, B., Cao, S., Li, X., Wei, J., Zheng, Q., and</cell><cell></cell></row><row><cell>for latent variable models. In NIPS, volume 1, pp. 2, Davidson, I. Deep similarity-based batch mode active</cell><cell></cell></row><row><cell>2010. learning with exploration-exploitation. In 2017 IEEE</cell><cell></cell></row><row><cell>International Conference on Data Mining (ICDM), pp.</cell><cell></cell></row><row><cell>Le, Y. and Yang, X. Tiny imagenet visual recognition chal-575-584. IEEE, 2017.</cell><cell></cell></row><row><cell>lenge. CS 231N, 7(7):3, 2015.</cell><cell></cell></row><row><cell>Yuan, M., Lin, H., and Boyd-Graber, J. L. Cold-start active</cell><cell></cell></row><row><cell>Lerner, B., Shiran, G., and Weinshall, D. Boosting the per-learning through self-supervised language modeling. In</cell><cell></cell></row><row><cell>formance of semi-supervised learning with unsupervised Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Pro-</cell><cell></cell></row><row><cell>clustering. arXiv preprint arXiv:2012.00504, 2020. ceedings of the 2020 Conference on Empirical Methods</cell><cell></cell></row><row><cell>in Natural Language Processing, EMNLP 2020, Online,</cell><cell></cell></row><row><cell>Lewis, D. D. and Gale, W. A. A sequential algorithm for November 16-20, 2020, pp. 7935-7948. Association for</cell><cell></cell></row><row><cell>training text classifiers. In SIGIR'94, pp. 3-12. Springer, Computational Linguistics, 2020.</cell><cell></cell></row><row><cell>1994. Zhang, B., Wang, Y., Hou, W., Wu, H., Wang, J., Oku-</cell><cell></cell></row><row><cell>Lowell, D., Lipton, Z. C., and Wallace, B. C. Practical mura, M., and Shinozaki, T. Flexmatch: Boosting semi-</cell><cell></cell></row><row><cell>obstacles to deploying active learning. arXiv preprint supervised learning with curriculum pseudo labeling.</cell><cell></cell></row><row><cell>arXiv:1807.04801, 2018. CoRR, abs/2110.08263, 2021.</cell><cell></cell></row><row><cell>Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, Mahmood, R., Fidler, S., and Law, M. T. Low budget active O. The unreasonable effectiveness of deep features as a learning via wasserstein distance: An integer program-perceptual metric. In Proceedings of the IEEE conference ming approach. arXiv preprint arXiv:2106.02968, 2021. on computer vision and pattern recognition, pp. 586-595,</cell><cell></cell></row><row><cell>Mittal, S., Tatarchenko, M., ?i?ek, ?., and Brox, T. Parting 2018.</cell><cell></cell></row><row><cell>with illusions about deep active learning. arXiv preprint Zhdanov, F. Diverse mini-batch active learning. arXiv arXiv:1912.05361, 2019. preprint arXiv:1901.05954, 2019.</cell><cell></cell></row><row><cell>Munjal, P., Hayat, N., Hayat, M., Sourati, J., and Khan, S. Zhu, Y., Lin, J., He, S., Wang, B., Guan, Z., Liu, H., and Cai,</cell><cell></cell></row><row><cell>Towards robust and reproducible active learning using D. Addressing the item cold-start problem by attribute-</cell><cell></cell></row><row><cell>neural networks. ArXiv, abs/2002.09564, 2020. driven active learning. IEEE Trans. Knowl. Data Eng., 32</cell><cell></cell></row><row><cell>(4):631-644, 2020. doi: 10.1109/TKDE.2019.2891530.</cell><cell></cell></row></table><note>Nguyen, A., Yosinski, J., and Clune, J. Deep neural net- works are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pp. 427-436, 2015.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>We start with a few lemmas that will be used in this proof. Lemma 1. Let f : R ?0 ? R denote a differentiable function with f (0) = 0. Then Let F : R ?0 ? R &gt;0 and f = F denote a positive differentiable strictly monotonically decreasing function (F &gt; 0, f &lt; 0). Assume that lim</figDesc><table><row><cell>lim x?0 +</cell><cell cols="2">f (x) f (ax)</cell><cell>= 1 ?a ? (0, 1).</cell></row><row><cell>Proof. Omitted.</cell><cell></cell><cell></cell></row><row><cell cols="4">Lemma 2. x??</cell><cell>F (x) = 0, and</cell></row><row><cell cols="2">x?? that the limits lim</cell><cell>F (x)</cell></row><row><cell>suggested querying a diverse</cell><cell></cell><cell></cell></row><row><cell>set of examples with minimal Wasserstein distance from the</cell><cell></cell><cell></cell></row><row><cell>unlabeled pool. They report a significant performance boost</cell><cell></cell><cell></cell></row><row><cell>in the low-budget regime. Unlike our work, they conduct</cell><cell></cell><cell></cell></row><row><cell>experiments only in a fully supervised with self-supervised</cell><cell></cell><cell></cell></row><row><cell>embedding settings, related to, but somewhat different from,</cell><cell></cell><cell></cell></row><row><cell>the one described in Section 4.2.2.</cell><cell></cell><cell></cell></row><row><cell>B. Mixture Model Lemmas and Proofs</cell><cell></cell><cell></cell></row></table><note>B.1. Undulating Error Score: Sufficient Conditions Below we provide the proof for Thm. 2, which is stated in Section 2.3, and which lists sufficient conditions for error scores to be undulating (see Def. 3).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This limiting case was rarely encountered, as clusters are usually balanced.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Israeli Ministry of Science and Technology, and by the Gatsby Charitable Foundations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep batch active learning by diverse, uncertain gradient lower bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa</title>
		<meeting><address><addrLine>Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Why label when you can search? alternatives to active learning for applying human resources to build classification models under extreme class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Attenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing label effort: Self-supervised meets active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Bengar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1631" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the marginal benefit of active learning: Does self-supervision eat its cake?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3455" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On initial pools for deep active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devaguptapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020 Workshop on Pre-registration in Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The grammar-learning trajectories of neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abend</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06096</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convex optimization framework for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sasrty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised active learning: Towards minimizing labeling cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">?</forename><surname>Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep active learning over the long tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gissin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06347</idno>
		<title level="m">S. Discriminative active learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">?</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the power of curriculum learning in training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2535" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05553</idno>
		<title level="m">Principal components bias in deep neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Let&apos;s agree to agree: Neural networks share classification order on real datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3950" to="3960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Towards better uncertainty sampling: Active learning with multiple views for deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>In</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3934" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A survey of active learning for text classification using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schr?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niekler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07267</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<idno>1648</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Computer Sciences Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The pitfalls of simplicity bias in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tamuly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9573" to="9585" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep active learning: Unified and principled method for query and training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gagn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1308" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking deep active learning: Using unlabeled data at model training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1220" to="1227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variational adversarial active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5972" to="5981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An introduction to matrix concentration inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000048</idno>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="230" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to classify</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
