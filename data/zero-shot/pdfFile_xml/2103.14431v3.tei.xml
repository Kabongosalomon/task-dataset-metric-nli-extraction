<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Knowledge Expansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihui</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sucheng</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Institute</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">T</forename><surname>Austin</surname></persName>
						</author>
						<title level="a" type="main">Multimodal Knowledge Expansion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The popularity of multimodal sensors and the accessibility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modality gap between a unimodal network and unlabeled multimodal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task with extra unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework to effectively utilize multimodal data without requiring labels. Opposite to traditional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently rectifies pseudo labels and generalizes better than its teacher. Extensive experiments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the expansion capability of a multimodal student.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks and supervised learning have made outstanding achievements in fields like computer vision <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref> and computer audition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47]</ref>. With the popularity of multimodal data collection devices (e.g., RGB-D cameras and video cameras) and the accessibility of the Internet, a large amount of unlabeled multimodal data has become available. A couple of examples are shown in <ref type="figure" target="#fig_0">Figure 1</ref>: (a) A unimodal dataset has been previously annotated for the data collected by an old robot; after a hardware upgrade with an additional sensor, the roboticist has access to some new unlabeled multimodal data. (b) Internet videos are abundant and easily accessible. While there are existing unimodal datasets and models for tasks such as image recognition, we further want to perform the same task on unlabeled videos. A natural question arises: how to transfer a unimodal network to the unlabeled multimodal data?</p><p>One naive solution is to directly apply the unimodal network for inference using the corresponding modality of unlabeled data. However, it overlooks information described by the other modalities. While learning with multimodal data has the advantage of facilitating information fusion and inducing more robust models compared with only using one modality, developing a multimodal network with supervised learning requires tremendous human labeling efforts.</p><p>In this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework, to make the best use of unlabeled multimodal data. MKE enables a multimodal network to learn on the unlabeled data with minimum human labor (i.e., no annotation of the mul- In knowledge distillation, a cumbersome teacher network is considered as the upper bound of a lightweight student network. Contradictory to that, we introduce a unimodal teacher and a multimodal student. The multimodal student achieves knowledge expansion from the unimodal teacher. timodal data is required). As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, a unimodal network pre-trained on the labeled dataset plays the role of a teacher and distills information to a multimodal network, termed as a student. We observe an interesting phenomenon: our multimodal student, trained only on pseudo labels provided by the unimodal teacher, consistently outperforms the teacher under our training framework. We term this observation as knowledge expansion. Namely, a multimodal student is capable of refining pseudo labels. We conduct experimental results on various tasks and different modalities to verify this observation. We further offer empirical and theoretical explanations to understand the expansion capability of a multimodal student.</p><p>A closely related setting to ours is semi-supervised learning (SSL), whose goal is to improve a model's performance by leveraging unlabeled data of the same source, including modality. Different from SSL, we aim to develop an additional multimodal network on an unlabeled dataset. Despite the differences in modalities, MKE bears some similarity to SSL in terms of the mechanism. We provide a new perspective in addressing confirmation bias, a traditionally bothering problem in SSL. This bias stems from using incorrect predictions on unlabeled data for training and results in marginal performance gain over the original teacher network <ref type="bibr" target="#b2">[3]</ref>. In SSL, various methods, i.e., data augmentation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43]</ref>, injecting noise <ref type="bibr" target="#b43">[44]</ref>, meta-learning <ref type="bibr" target="#b28">[29]</ref> have been proposed to address it. This work provides a novel angle orthogonal to these techniques in alleviating confirmation bias, by resorting to multimodal information. We demonstrate that multimodal inputs serve as a strong regularization, which helps correct inaccurate pseudo labels and overcome the limitation of unimodal networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-supervised Learning</head><p>Pseudo labeling, also known as self-training, is a simple and powerful technique in SSL, leading to great improvements on tasks such as image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29]</ref>, semantic segmentation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b9">10]</ref> and domain adaptation <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22]</ref>. One important limitation of pseudo labeling is confirmation bias <ref type="bibr" target="#b2">[3]</ref>. Since pseudo labels are inaccurate, the student network may potentially learn these mistakes. Various works have been proposed to alleviate this bias <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29]</ref> while their discussion is limited to unimodality. Consistency regularization is another important brand of SSL. Based on model smoothness assumption, model predictions are constrained to be invariant to small perturbations of either inputs or model hidden states. A series of works have been proposed on producing random perturbations, such as using an exponential moving average of model parameters <ref type="bibr" target="#b35">[36]</ref>, data augmentation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b33">34]</ref>, dropout <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref> or adversarial perturbations <ref type="bibr" target="#b26">[27]</ref>. Recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref> combine consistency regularization with pseudo labeling together and demonstrate great benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-modal Distillation</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b17">[18]</ref> is an effective technique in transferring information from one network to another. KD has been broadly applied to model compression, where a lightweight student network learns from a cumbersome teacher network <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. Another important application of KD is cross-modal distillation, where a teacher network transfers knowledge from one modality to a student learning from another modality. Various works have been proposed along this direction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49]</ref>. Gupta et al. <ref type="bibr" target="#b14">[15]</ref> proposes a framework that transfers supervision from labeled RGB images to unlabeled depth and optical flow images. SoundNet <ref type="bibr" target="#b3">[4]</ref> learns sound representations from well established visual recognition models using unlabeled videos. Zhao et al. <ref type="bibr" target="#b49">[50]</ref> introduces an approach that estimates human poses using radio signals with crossmodal supervision signals provided by vision models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multimodal Learning</head><p>Models fusing data from multiple modalities has shown superior performance over unimodal models in various applications, for instance, sentiment analysis <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b25">26]</ref>, emotion recognition <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref>, semantic segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref> and event classification <ref type="bibr" target="#b0">[1]</ref>. One recent work <ref type="bibr" target="#b19">[20]</ref> provides theoretical justifications on the advantages of multimodal learning over learning with single modality.</p><p>We compare our problem setting with prior works in Table 1. SSL adopts data from the same modality. Crossmodal distillation has the same training data assumption as us while they focus on testing with unimodal data only. Su-  pervised multimodal learning does not take unlabeled data into consideration. Contrary to them, this work discusses a novel and practical scenario where only labeled unimodal and unlabeled multimodal data are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multimodal Knowledge Expansion</head><p>Problem formulation. Without loss of generality, we limit our discussion to two modalities, denoted as ? and ?, respectively. We assume that a collection of labeled uni-</p><formula xml:id="formula_0">modal data D l = {(x ? i , y i )} N i=1 is given. Each sample input x ?</formula><p>i has been assigned a one-hot label vector y i = {0, 1} K ? R K , where K is the number of classes. Besides the labeled dataset, an unlabeled multimodal dataset</p><formula xml:id="formula_1">D u = {(x ? i , x ? i )} M i=1 is available.</formula><p>Our goal is to train a network parameterized by ? (i.e., f (x; ?)) that could accurately predict the label y when its feature x = (x ? , x ? ) is given.</p><p>To transfer the knowledge of a labeled unimodal dataset D l to an unlabeled multimodal dataset D u , we present a simple and efficient model-agnostic framework named multimodal knowledge expansion (MKE) in Algorithm 1. We first train a unimodal teacher network ? ? t on the labeled dataset D l . Next, the obtained teacher is employed to generate pseudo labels for the multimodal dataset D u , yielding D u . Finally, we train a multimodal student ? ? s based on the pseudo-labeledD u with the loss term described in Equation (3)- <ref type="bibr" target="#b4">(5)</ref>.</p><p>In order to prevent the student from confirming to teacher's predictions (i.e., confirmation bias <ref type="bibr" target="#b2">[3]</ref>), the loss term in Equation (3)-(5) has been carefully designed. It combines the standard pseudo label loss (i.e., Equation (4)) and a regularization loss (i.e., Equation <ref type="formula" target="#formula_14">(5)</ref>). Intuitively speaking, pseudo label loss aims to minimize the difference between a multimodal student and the unimodal teacher, while regularization loss enforces the student to be invariant to small perturbations of input or hidden states. In the context of multimodal learning, the regularization term encourages the multimodal student to learn from the information brought by the extra modality ?, and meanwhile, ensures that the student does not overfit to teacher's predictions based solely on modality ?. Note that in our implementation, to avoid introducing and tuning one extra hyperparameter ? and save computation time, we train the student network with ? ? s = argmin ?s</p><formula xml:id="formula_2">1 M M i=1 l cls (? i , T (f s (x ? i , x ? i ; ? s ))</formula><p>, which is equivalent to Equation (3). The detailed proof is provided in the supplementary material.</p><p>An illustrative example. We consider a variant of the 2D-TwoMoon <ref type="bibr" target="#b2">[3]</ref> problem shown in <ref type="figure" target="#fig_2">Figure 3a</ref>. The data located at the upper moon and lower moon have true labels 0 and 1, and are colored by red and blue, respectively. The deeply blue-or red-colored large dots compose the labeled unimodal dataset D l , and only their X coordinates are known. On the other hand, D u consists of all lightlycolored small dots, with both X and Y coordinates available. Namely, modality ? and ? are interpreted as observing from the X-axis and Y-axis, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 multimodal knowledge expansion (MKE)</head><p>(1) Train a unimodal teacher ? ? t with the labeled dataset</p><formula xml:id="formula_3">D l = {(x ? i , y i )} N i=1 : ? ? t = argmin ?t 1 N N i=1 l cls (y i , f t (x ? i ; ? t )) (1) (2) Generate pseudo labels for D u = {(x ? i , x ? i )} M i=1 by using the teacher model ? ? t , yielding the pseudo-labeled datasetD u = {(x ? i , x ? i ,? i )} M i=1 : y i = f t (x ? i ; ? ? t ), ? (x ? i , x ? i ) ? D u<label>(2)</label></formula><p>(3) Train a multimodal student ? ? s withD u :</p><formula xml:id="formula_4">? ? s = argmin ?s (L pl + ?L reg )<label>(3)</label></formula><formula xml:id="formula_5">L pl = 1 M M i=1 l cls (? i , f s (x ? i , x ? i ; ? s ))<label>(4)</label></formula><formula xml:id="formula_6">L reg = M i=1 l reg [f s (x ? i , x ? i ; ? s ), T (f s (x ? i , x ? i ; ? s ))] (5)</formula><p>l cls : cross entropy loss for hard? i and KL divergence loss for soft? i . l reg : a distance metric (e.g., L2 norm). ?: a constant balancing the weight of L pl and L reg .</p><p>T : a transformation defined on the student model, realized via input or model perturbations (i.e., augmentations, dropout).</p><p>We first train a teacher with the labeled unimodal dataset D l . The learned classification boundary is demonstrated in <ref type="figure" target="#fig_2">Figure 3b</ref>. Next, we adopt the learned teacher to generate pseudo labels for D u . As indicated in <ref type="figure" target="#fig_2">Figure 3c</ref>, pseudo labels may be inaccurate and disagree with ground truth: in our toy example, the unimodal teacher only yields 68% accuracy. As shown in <ref type="figure" target="#fig_2">Figure 3f</ref>, provided with these not-soaccurate pseudo labels, the student could still outperform the teacher by a large margin (i.e., about 13% more accurate). It presents a key finding in our work: Despite no (a) Deeply-colored large dots observed from the X-axis compose D l and lightly-colored small dots observed from the XY-plane compose Du.</p><p>(b) Train a unimodal classifier on labeled data points. Since only X coordinates of the data in D l are known, it is natural that the boundary is vertical.</p><p>(c) The teacher network generates pseudo labels for unlabeled data (d) Naive pseudo labeling overfits to inaccurate pseudo labels (e) Consistency regularization slightly improves a unimodal student (f) MKE greatly improves a multimodal student access to ground truth, a multimodal student is capable of correcting inaccurate labels and outperforms the teacher network. Knowledge expansion is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Rectifying Pseudo Labels</head><p>The somewhat surprising finding about knowledge expansion further motivates our thinking: where does the expansion capability of a multimodal student come from? In this section, we will answer this question with the TwoMoon example.</p><p>To start with, we consider directly adopting unimodal SSL for this problem. Namely, given a teacher network ? ? t trained with labeled data D l and an unlabeled multi-modal dataset D u , the student network takes x ? i ? D u as input. Naive pseudo labeling <ref type="bibr" target="#b22">[23]</ref> uses the following loss to minimize the disagreement between the fixed teacher ? ? t and a student network ? s :</p><formula xml:id="formula_7">L ? pl = E x ? i ?Du {l cls [f t (x ? i ; ? ? t ), f s (x ? i ; ? s )]}<label>(6)</label></formula><p>However, due to confirmation bias <ref type="bibr" target="#b2">[3]</ref>, the student network is likely to overfit to incorrect pseudo labels pro-vided by the teacher network, yielding f s (x; ? ? s ) similar to f t (x; ? ? t ), if not identical. In the TwoMoon example, we observe that the unimodal student trained with Equation <ref type="formula" target="#formula_7">(6)</ref> achieves similar performance as its teacher. This is demonstrated in <ref type="figure" target="#fig_2">Figure 3d</ref>.</p><p>To address this bias, we follow the thought of consistency training methods in SSL <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b33">34]</ref> and introduce one general regularization loss term to enforce model smoothness:</p><formula xml:id="formula_8">L ? reg = E x ? i ?Du {l reg [f s (x ? i ; ? s ), T ? (f s (x ? i ; ? s ))]} (7)</formula><p>Namely, L ? reg encourages the model to output similar predictions for small perturbations of the input or the model. T ? (f s (x ? i ; ? s )) denotes transformation applied to unimodal inputs or model hidden states, which can be realized via input augmentation, noise, dropout, etc. As shown in <ref type="figure" target="#fig_2">Figure 3e</ref>, the unimodal student trained with a combined loss of Equation <ref type="formula" target="#formula_7">(6)-(7)</ref> achieves about 69.50% prediction accuracy. While it indeed outperforms the teacher of 68.00% accuracy shown in <ref type="figure" target="#fig_2">Figure 3b</ref>, the unimodal student under consistency regularization fails to utilize unlabeled data effectively and only brings marginal improvement. Although confirmation bias is slightly reduced by the regularization term in Equation <ref type="formula" target="#formula_14">(7)</ref>, it still heavily constrains performance of unimodal SSL methods.</p><p>Therefore, we turn to multimodality as a solution and resort to the information brought by modality ?. Utilizing both modalities in D u , we substitute unimodal inputs shown in Equation <ref type="formula" target="#formula_7">(6)</ref>- <ref type="bibr" target="#b6">(7)</ref> with multimodal ones and derive the loss terms for training a multimodal student:</p><formula xml:id="formula_9">L pl = E{l cls [f t (x ? i ; ? ? t ), f s (x ? i , x ? i ; ? s )]}<label>(8)</label></formula><formula xml:id="formula_10">L reg = E{l reg [f s (x ? i , x ? i ; ? s ), T (f s (x ? i , x ? i ; ? s ))]} (9)</formula><p>where both expectations are performed with respect to (x ? i , x ? i ) ? D u . In fact, Equation (8)-(9) reduces to Equation (4)-(5) when D u is a finite set containing M multimodal samples. As shown in <ref type="figure" target="#fig_2">Figure 3f</ref>, we observe substantial improvement of a multimodal student (i.e., 81.00% accuracy) over the teacher (i.e., 68.00% accuracy). It implies that a multimodal student effectively alleviates confirmation bias and leads to superior performance over the teacher.</p><p>To understand the principles behind this phenomenon, we train one unimodal student with Equation (6)- <ref type="bibr" target="#b6">(7)</ref> and one multimodal student with Equation (8)-(9) on the TwoMoon data. Transformation T is defined on model inputs and implemented as additive Gaussian noise. <ref type="figure" target="#fig_3">Figure 4</ref> visualizes the transformation space of one data sample A with both pseudo label and true label being "red". Data B is one point that the teacher predicts "blue" while its true label is "red". The pseudo label and true label of data C are "blue". When training a unimodal student, we only know the X coordinates of data points, and the transformation space defined by T ? is given by the 1-D red line on X-axis. Under this circumstance, minimizing L ? reg in Equation <ref type="formula" target="#formula_14">(7)</ref> encourages the unimodal student to predict label "red" for the data point located in the red line. This is the case for B, but it will also flip the teacher's prediction for C and make it wrong! The intrinsic reason is that restricted by unimodal inputs, the student network can not distinguish along the Yaxis and mistakenly assumes that C locates near A.</p><p>On the contrary, the extra modality ? helps us see the real distances among A, B, and C. Transformation space of data A in the case of a multimodal student is given by the red circle in <ref type="figure" target="#fig_3">Figure 4</ref>. A multimodal student is guided to predict "red" for data falling inside the circle. This time B locates in the transformation space, while C doesn't. Therefore, the multimodal student can correct the wrong pseudo label of data B due to the regularization constraint in Equation <ref type="bibr" target="#b8">(9)</ref>, and its decision boundary is pushed closer to the ground truth. This example demonstrates that multimodality serves as a strong regularization and enables the student to "see" something beyond the scope of its teacher, resulting in knowledge expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Theoretical Analysis</head><p>In this section, we provide a theoretical analysis of MKE. Building upon unimodal self-training <ref type="bibr" target="#b41">[42]</ref>, we prove that our multimodal student improves over pseudo labels given by the teacher.</p><p>Consider a K-way classification problem, and assume that we have a teacher network pre-trained on a collection of labeled data D l . We further assume a set of unlabeled mul-</p><formula xml:id="formula_11">timodal data D u = {x i = (x ? i , x ? i ) ? X } M i=1 is available, where X = X ? ? X ? . Let f ? (x; ? ? ), f t (x; ? t ), f s (x; ? s )</formula><p>denote the ground truth classifier, a teacher classifier, and a student classifier, respectively. Error of an arbitrary classifier f (x; ?) is defined as:</p><formula xml:id="formula_12">Err(f (x; ?)) = E x [f (x; ?) ? = f ? (x; ? ? )].</formula><p>Let P refer to a distribution of unlabeled samples over input space X . P i denotes the class-conditional distribution of x conditioned on f ? (x; ? ? ) = i. We use M(? t ) ? D u to denote the set of multimodal data that the teacher gives wrong predictions on, i.e., M(? t ) =</p><formula xml:id="formula_13">{(x ? , x ? )|f t (x ? ; ? t ) ? = f ? (x ? ; ? ? ), (x ? , x ? ) ? D u }. Let a = max i {P i (M(? t )}</formula><p>refer to the maximum fraction of data misclassified by the teacher network in any class.</p><p>We first require data distribution P to satisfy the following expansion assumption, which states that data distribution has good continuity in input spaces. Assumption 1 P satisfies (?, c 1 ) and (?, c 2 ) expansion <ref type="bibr" target="#b41">[42]</ref> on X ? and X ? , respectively, with 1 &lt; min(c 1 , c 2 ) ? max(c 1 , c 2 ) ? 1 a and c 1 c 2 &gt; 5. <ref type="bibr" target="#b10">11)</ref> where N (V ) denotes the neighborhood of a set V , following the same definition as in <ref type="bibr" target="#b41">[42]</ref>. Furthermore, we assume conditional independence of multimodal data in Assumption 2, which is widely adopted in the literature of multimodal learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35]</ref>.</p><formula xml:id="formula_14">P i (N (V ? )) ? min{c 1 P i (V ? ), 1}, ? i ? [K], ? V ? ? X ? with P i (V ? ) ?? (10) P i (N (V ? )) ? min{c 2 P i (V ? ), 1}, ? i ? [K], ? V ? ? X ? with P i (V ? ) ??<label>(</label></formula><p>Assumption 2 Conditioning on ground truth labels, X ? and X ? are independent.</p><formula xml:id="formula_15">P i (V ? , V ? ) = P i (V ? ) ? P i (V ? ), ? i ? [K], ? V ? ? X ? , ? V ? ? X ?<label>(12)</label></formula><p>Lemma 1 Data distribution P on X satisfies (?, c 1 c 2 ) expansion.</p><p>Proof of Lemma 1 is provided in the supplementary material. We state below that the error of a multimodal student classifier is upper-bounded by the error of its teacher. We follow the proof in <ref type="bibr" target="#b41">[42]</ref> to prove Theorem 1. </p><formula xml:id="formula_16">Err(f s (x ? , x ? ; ? s )) ? 4 ? Err(f t (x ? ; ? t )) c 1 c 2 ? 1 + 4? (13)</formula><p>where ? appears in Assumption 3.3 of <ref type="bibr" target="#b41">[42]</ref> and is expected to be small or negligible. Theorem 1 helps explain the empirical finding about knowledge expansion. Training a multimodal student f (x ? , x ? ; ? s ) on pseudo labels given by a pre-trained teacher network f (x ? ; ? t ) refines pseudo labels. In addition, the error bound of a unimodal student f s (x ? ; ? s ) that only takes inputs from modality ? and pseudo labels is given by:</p><formula xml:id="formula_17">Err(f s (x ? ; ? s )) ? 4 ? Err(f t (x ? ; ? t )) c 1 ? 1 + 4?<label>(14)</label></formula><p>By comparing Equation <ref type="formula" target="#formula_4">(13)</ref> and <ref type="formula" target="#formula_5">(14)</ref>, we observe that the role of multimodality is to increase the expansion factor from c 1 to c 1 c 2 and to improve the accuracy bound. This observation further confirms our empirical finding and unveils the role of MKE in refining pseudo labels from a theoretical perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To verify the efficiency and generalizability of the proposed method, we perform a thorough test of MKE on various tasks: (i) binary classification on the synthetic TwoMoon dataset, (ii) emotion recognition on RAVDESS <ref type="bibr" target="#b24">[25]</ref> dataset, (iii) semantic segmentation on NYU Depth V2 <ref type="bibr" target="#b31">[32]</ref> dataset, and (iv) event classification on AudioSet <ref type="bibr" target="#b13">[14]</ref> and VGGsound <ref type="bibr" target="#b8">[9]</ref> dataset. We emphasize that the above four tasks cover a broad combination of modalities. For instance, modalities ? and ? represent images and audios in (ii), where images are considered as a "weak" modality in classifying emotions than images. In (iii), modality ? and ? refer to RGB and depth images, respectively, where RGB images play a central role in semantic segmentation and depth images provide useful cues.</p><p>Baselines. Our multimodal student (termed as MM student) trained with MKE is compared with the following baselines:</p><p>? UM teacher: a unimodal teacher network trained on (x ? , y i ) ? D l .</p><p>? UM student: a unimodal student network trained on (x ? ,? i ) ?D u (i.e., uni-modal inputs and pseudo labels given by the UM teacher).</p><p>? NOISY student <ref type="bibr" target="#b43">[44]</ref>: a unimodal student network trained on (x ? , y i ) ? D l ? (x ? ,? i ) ?D u with noise injected during training.</p><p>? MM student (no reg): a multimodal student network trained with no regularization (i.e., Equation <ref type="formula" target="#formula_14">(5)</ref> is not applied during training).</p><p>? MM student (sup): a multimodal student trained on D u with true labels provided. This supervised version can be regarded as the upper bound of our multimodal student.</p><p>Since iterative training <ref type="bibr" target="#b43">[44]</ref> can be applied to other baselines and our MM student as well, the number of iterations of a NOISY student is set as one to ensure a fair comparison. We employ different regularization techniques as T in Equation <ref type="bibr" target="#b4">(5)</ref> for the four tasks to demonstrate the generalizability of our proposed methods. Regularization is applied to all baselines identically except for MM student (no reg).</p><p>Furthermore, we present an ablation study of various components of MKE, i.e., unlabeled data size, teacher model, hard vs. soft labels, along with dataset and implementation details in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">TwoMoon Experiment</head><p>We first provide results on synthetic TwoMoon data. We generate 500 samples making two interleaving half circles, each circle corresponding to one class. The dataset is randomly split as 30 labeled samples, 270 unlabeled samples and 200 test samples. X and Y coordinates of data are interpreted as modality ? and ?, respectively.</p><p>Baselines &amp; Implementation. We implement both the UM teacher and the UM student networks as 3-layer MLPs with 32 hidden units, while the MM student has 16 hidden units. We design three kinds of transformations T = {T 1 , T 2 , T 3 } used in Equation <ref type="formula" target="#formula_14">(5)</ref>: (i) T 1 : adding zero-mean Gaussian noise to the input with variance v 0 , (ii) T 2 : adding zeromean Gaussian noise to outputs of the first hidden layer with variance v 1 , and (iii) T 3 : adding a dropout layer with dropout rate equal to r 0 . By adjusting the values of v 0 , v 1 and r 0 , we could test all methods under no / weak / strong regularization. Specifically, higher values indicate stronger regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Test Accuracy (%) UM teacher 68.00  Results. <ref type="table" target="#tab_4">Table 2</ref> demonstrates that a MM student under consistency regularization outperforms its unimodal counterpart in all cases of T . Specifically, a MM student under strong regularization achieves closes results with MM student (sup), as shown in the last column. The small gap between a MM student (trained on pseudo labels) and its upper bound (trained on true labels) indicates the great expansion capability of MKE. In addition, we observe better performance of both UM and MM student with increasing regularization strength, demonstrating that consistency regularization is essential in alleviating confirmation bias.</p><formula xml:id="formula_18">T 1 v 0 = 0 v 0 = 1 v 0 = 2 UM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Emotion Recognition</head><p>We evaluate MKE on RAVDESS <ref type="bibr" target="#b24">[25]</ref> dataset for emotion recognition. The dataset is randomly split as 2:8 for D l and D u and 8:1:1 as train / validation / test for D u . Images and audios are considered as modality ? and ?, respectively.</p><p>Baselines &amp; Implementation. For the MM student, we adopt two 3-layer CNNs to extract image and audio features, respectively. The two visual and audio features are concatenated into a vector and then passed through a 3-layer MLP. The UM teacher, UM student and NOISY student are identical to the image branch of a MM student network, also followed by a 3-layer MLP. T in Equation <ref type="formula" target="#formula_14">(5)</ref> is implemented as one dropout layer of rate 0.5. <ref type="table">Table 3</ref>, with the assistance of labeled data and consistency regularization, NOISY student generalizes better than the UM teacher and UM student, achieving 83.09% accuracy over 80.33% and 77.79%. Still, the improvement is trivial. In contrast, our MM student network improves substantially over the original teacher network despite no access to ground truth and leads to 91.38% test accuracy. The great performance gain can be attributed to additional information brought by audio modality. It demonstrates that MKE can be plugged into existing SSL methods like NOISY student for boosting performance when multimodal data are available. Furthermore, regularization helps our MM student yield better performance than the MM student (no reg). More results are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Train  <ref type="table">Table 3</ref>: Results of emotion recognition on RAVDESS. mod, i and a denote modality, images and audios, respectively. Data used for training each method is listed. ? means that the MM student (sup) is trained on true labels instead of pseudo labels inDu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic Segmentation</head><p>We evaluate our method on NYU Depth V2 <ref type="bibr" target="#b31">[32]</ref>. It contains 1449 RGB-D images with 40-class labels, where 795 RGB images are adopted as D l for training the UM teacher and the rest 654 RGB-D images are for testing. Besides labeled data, NYU Depth V2 also provides unannotated video sequences, where we randomly extract 1.5K frames of RGB-D images as D u for training the student. Modality ? and ? represents RGB images and depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train   <ref type="bibr" target="#b48">[49]</ref>. Due to different problem settings, we slightly modified cross-modal distillation methods to make them comparable. Since RGB-D images from D u are unannotated, we are unable to train a supervised version of the MM student (i.e., MM student (sup)) in this task. We adopt ResNet-101 <ref type="bibr" target="#b15">[16]</ref> as backbone and DeepLab V3+ <ref type="bibr" target="#b10">[11]</ref> as decoder for the UM teacher. In terms of training a MM student, depth images are first converted to HHA images and then passed to a fusion network architecture proposed in <ref type="bibr" target="#b11">[12]</ref> along with RGB images. We design the UM student architecture as the RGB branch of a MM student network. For the regularization term, we employ input augmentation for RGB images, i.e., random horizontal flipping and scaling with scales [0.5,1.75].</p><p>Results. <ref type="table" target="#tab_7">Table 4</ref> reports mean Intersection-over-Union (mIoU) of each method. We observe that a MM student greatly improves over the UM teacher, i.e., achieves a mIoU of 48.88 % while it is trained on pseudo labels of approximately 44.15% mIoU. Furthermore, provided with no ground truth, our MM student outperforms all baselines with a considerable performance gain. This demonstrates the effectiveness of MKE. We also arrive at the same conclusion that regularization helps improve the MM student since our MM student yields higher accuracy than a MM student (no reg). It indicates that MKE and current SSL methods that focus on designing augmentations to emphasize consistency regularization can be combined together to boost performance.</p><p>Visualization results in <ref type="figure" target="#fig_4">Figure 5</ref> demonstrate that our MM student refines pseudo labels and achieves knowledge expansion. Although it receives noisy predictions given by the UM teacher, our MM student does a good job in handling details and maintaining intra-class consistency. As shown in the third and fourth row, the MM student is robust to illumination changes while the UM teacher and NOISY student easily get confused. Depth modality helps our MM student better distinguish objects and correct wrong predictions it receives. More qualitative examples are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Event Classification</head><p>We present experimental results on a real-world application, event classification. 3.7K audios from AudioSet <ref type="bibr" target="#b13">[14]</ref> and 3.7K audio-video pairs from VGGSound <ref type="bibr" target="#b8">[9]</ref> are taken as the labeled unimodal dataset D l and unlabeled multimodal dataset D u , respectively. In this task, modality ? and ? correspond to audios and videos.</p><p>Baslines &amp; Implementation. For the UM teacher, we take ResNet-18 as the backbone and a linear layer as classification layer. For the MM student, the audio backbone is identical to that of the UM teacher, and the video backbone is a ResNet-18 with 3D convolution layers. Features from the audio and video backbone are concatenated together before feeding into one classification layer. Following the same regularization term of <ref type="bibr" target="#b8">[9]</ref>, we randomly sample audio clips of 5 seconds and apply short-time Fourier Transformation for 257 ? 500 spectrograms during training.</p><p>Results. <ref type="table">Table 5</ref> reports mean Average Precision (mAP) of each method. The baseline model is the UM teacher trained on D l , which achieves a 0.345 mAP. Benefiting from the video modality, our MM student achieves best performance with a mAP of 0.427, outperforming NOSIY student <ref type="bibr" target="#b43">[44]</ref> and cross-modal distillation methods <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b48">[49]</ref>. Notably, the difference between our MM student and its upper bound (i.e., MM student (sup)) is small, showing great potentials of MKE in correcting pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train data Test mAP mod D lDu UM teacher a ? 0.345 UM student a ? 0.406 NOISY student <ref type="bibr" target="#b43">[44]</ref> a ? ? 0.411 Owens et al. <ref type="bibr" target="#b27">[28]</ref> a, v ? 0.371 CMKD <ref type="bibr" target="#b48">[49]</ref> a, v ? 0.372 MM student (no reg) a, v ? 0.421 MM student (ours) a, v ? 0.427 MM student (sup) a, v ? 0.434 <ref type="table">Table 5</ref>: Results of event classification on AudioSet and VG-GSound. a and v indicate audios and videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Motivated by recent progress on multimodal data collection, we propose a multimodal knowledge expansion framework to effectively utilize abundant unlabeled multimodal data. We provide theoretical analysis and conduct extensive experiments, demonstrating that a multimodal student corrects inaccurate predictions and achieves knowledge expansion from the unimodal teacher. In addition, compared with current semi-supervised learning methods, MKE offers a novel angle in addressing confirmation bias.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The popularity of multimodal data collection devices and the Internet engenders a large amount of unlabeled multimodal data. We show two examples above: (a) after a hardware upgrade, lots of unannotated multimodal data are collected by the new sensor suite; (b) large-scale unlabeled videos can be easily obtained from the Internet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework of MKE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a)-(c) problem description and illustration of MKE using the TwoMoon example; (d)-(f) comparison of naive pseudo labeling, consistency training methods, and the proposed MKE. Values in the bottom right corner denotes test accuracy (%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the transformation space of one data sample A. The 1-D red line on X-axis corresponds to the transformation space of a unimodal student while the 2-D red circle corresponds to that of a multimodal student.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a) RGB images (b) depth images (c) ground truth (d) MM student (ours) (e) UM teacher (f) NOISY student Qualitative segmentation results on NYU Depth V2 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our data assumption with prior works. UM and MM denotes unimodality and multimodality respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Theorem 1</head><label>1</label><figDesc>Suppose Assumption 3.3 of [42] holds, a student classifier f s (x ? , x ? ; ? s ) that minimizes loss in Equation (3) (in the form of Equation 4.1 of [42]) satisfies:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of TwoMoon experiment. A MM student significantly outperforms a UM student and teacher under consistency regularization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results of semantic segmentation on NYU Depth V2. rgb and d denote RGB images and depth images.</figDesc><table /><note>Baselines &amp; Implementation. We compare MKE against SSL methods [10] [44] and cross-modal distillation [15]</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal categorization of crisis events in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengli</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="14679" to="14689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vggsound: A large-scale audio-visual dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-andaggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Haase-Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudius</forename><surname>Glaeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-modal adaptation for rgb-d detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5032" to="5039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What makes multimodal learning better than single (provably)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhuang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longbo</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04538</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding self-training for gradual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Naive (bayes) at forty: The independence assumption in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="4" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
	<note>Alexander Gelbukh, Erik Cambria, and Soujanya Poria</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="801" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Meta pseudo labels. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition using deep learning architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Hiranmayi Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tcgm: An information-theoretic framework for semi-supervised multimodality learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="171" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-toend multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Selfsupervised model adaptation for multimodal semantic segmentation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Theoretical analysis of self-training with deep networks on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kendrick</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03622</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Training deep neural networks in generations: A more tolerant teacher educates better students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5628" to="5635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Speech Recogni-Tion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Knowledge as priors: Cross-modal knowledge generalization for datasets without superior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Through-wall human pose estimation using radio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Abu Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7356" to="7365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking pretraining and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
