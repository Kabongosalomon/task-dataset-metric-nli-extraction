<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Label Graph Matching for Unsupervised Video Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-27">27 Sep 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
							<email>mangye@comp.hkbu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
							<email>andyjhma@comp.hkbu.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<email>liangzheng06@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
							<email>jwli@comp.hkbu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
							<email>pcyuen@comp.hkbu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Label Graph Matching for Unsupervised Video Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-27">27 Sep 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant crosscamera variations, this paper propose a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (re-ID), a retrieval problem in its essence <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>, aims to search for the queried person from a gallery of disjoint cameras. In recent years, impressive progress has been reported in video based re-ID <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>, because video sequences provide rich visual and temporal information and can be trivially obtained by tracking algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> in practical video surveillance applications. Nevertheless, the annotation difficulty limits the scalability of supervised methods in large-scale camera networks, which motivates us to investigate an unsupervised solution for video re-ID.</p><p>The difference between unsupervised learning and supervised learning consists in the availability of labels. Considering the good performance of supervised methods, an <ref type="bibr" target="#b0">1</ref>   <ref type="figure">Figure 1</ref>. Pipeline Illustration. Graph matching is conducted after constructing a graph for samples in each camera to obtain the intermediate labels. Instead of using the labels directly, label reweighting is introduced to handle the noisy intermediate labels.</p><p>Iteratively, the graph is updated, labels are estimated, and distance metrics are learnt.</p><p>intuitive idea for unsupervised learning is to estimate re-ID labels as accurately as possible. In previous works, part from directly using hand-crafted descriptors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16]</ref>, some other unsupervised re-ID methods focus on finding shared invariant information (saliency <ref type="bibr" target="#b35">[36]</ref> or dictionary <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>) among cameras. Deviating from the idea of estimating labels, these methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22]</ref> might be less competitive compared with the supervised counterparts. Meanwhile, these methods also suffer from large cross-camera variations. For example, salient features are not stable due to occlusions or viewpoint variations. Different from the existing unsupervised person re-ID methods, this paper is based on a more customized solution, i.e., cross-camera label estimation. In other words, we aim to mine the labels (matched or unmatched video pairs) across cameras. With the estimated labels, the remaining steps are exactly the same with supervised learning.</p><p>To mine labels across cameras, we leverage the graph matching technique (e.g., <ref type="bibr" target="#b27">[28]</ref>) by constructing a graph for samples in each camera for label estimation. Instead of estimating labels independently, the graph matching approach has shown good property in finding correspondences by minimize the globally matching cost with intra-graph relationship. Meanwhile, label estimation problem for re-ID task is to link the same person across different cameras, which perfectly matches the graph matching problem by treating each person as a graph node. However, labels directly estimated by existing graph matching are very likely to be inaccurate and noisy due to the significant appearance changes across cameras. So a fixed graph constructed in the original feature space usually does not produce satisfying results. Moreover, the assumption that the assignment cost or affinity matrix is fixed in most graph matching methods may be unsuitable for re-ID due to large cross-camera variations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>In light of the above discussions, this paper proposes a dynamic graph matching (DGM) method to improve the label estimation performance for unsupervised video re-ID (the main idea is shown in <ref type="figure">Fig. 1</ref>). Specifically, our pipeline is an iterative process. In each iteration, a bipartite graph is established, labels are then estimated, and then a discriminative metric is learnt. Throughout this procedure, labels gradually become more accurate, and the learnt metric more discriminative. Additionally, our method includes a label re-weighting strategy which provides soft labels instead of hard labels, a beneficial step against the noisy intermediate label estimation output from graph matching.</p><p>The main contributions are summarized as follows:</p><p>? We propose a dynamic graph matching (DGM) method to estimate cross-camera labels for unsupervised re-ID, which is robust to distractors and noisy initial training data. The estimated labels can be used for further discriminative re-ID models learning.</p><p>? Our experiment confirms that DGM is only slightly inferior to its supervised baselines and yields competitive re-ID accuracy compared with existing unsupervised re-ID methods on three video benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Re-ID. Since unsupervised methods could alleviate the reliance on large-scale supervised data, a number of unsupervised methods have been developed. Some transfer learning based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref> are proposed. Andy et al. <ref type="bibr" target="#b17">[18]</ref> present a multi-task learning method by aligning the positive mean on the target dataset to learn the re-ID models for the target dataset. Peng et al. <ref type="bibr" target="#b21">[22]</ref> try to adopt the pre-trained models on the source datasets to estimate the labels on the target datasets. Besides that, Zhao et al. <ref type="bibr" target="#b35">[36]</ref> present a patch based matching method with inconsistent salience for re-ID. An unsupervised cross dataset transfer learning method with graph Laplacian regularization terms is introduced in <ref type="bibr" target="#b21">[22]</ref>, and a similar constraint with graph Laplacian regularization term for dictionary learning is proposed in <ref type="bibr" target="#b8">[9]</ref> to address the unsupervised re-ID problem. Khan et al. <ref type="bibr" target="#b7">[8]</ref> select multiple frames in a video sequence as positive samples for unsupervised metric learning, which has limited extendability to the cross-camera settings.</p><p>Two main differences between the proposed method and previous unsupervised re-ID methods are summarized.</p><p>Firstly, this paper estimates labels with graph matching to address the cross-camera variation problem instead of directly learning an invariant representation. Secondly, output estimated labels of dynamic graph matching can be easily expanded with other advanced supervised learning methods, which provides much flexibility for practical applications in large-scale camera network.</p><p>Two contemporary methods exists <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3]</ref> which also employ the idea of label estimation for unsupervised re-ID. Liu et al. <ref type="bibr" target="#b16">[17]</ref> use a retrieval method for labeling, while Fan et al. <ref type="bibr" target="#b2">[3]</ref> employ k-means for label clustering.</p><p>Graph Matching for Re-ID. Graph matching has been widely studied in many computer vision tasks, such as object recognition and shape matching <ref type="bibr" target="#b27">[28]</ref>. It has shown superiority in finding consistent correspondences in two sets of features in an unsupervised manner. The relationships between nodes and edges are usually represented by assignment cost matrix <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref> or affinity matrix <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>. Currently graph matching mainly focuses on optimizing the matching procedure with two fixed graphs. That is to say, the affinity matrix is fixed first, and then graph matching is formulated as linear integer programs <ref type="bibr" target="#b3">[4]</ref> or quadratic integer programs <ref type="bibr" target="#b12">[13]</ref>. Different from the literature, the graph constructed based on the original feature space is sub-optimal for re-ID task, since we need to model the camera variations besides the intra-graph deformations. Therefore, we design a dynamic graph strategy to optimize matching. Specifically, partial reliable matched results are utilized to learn discriminative metrics for accurate graph matching in each iteration.</p><p>Graph matching has been introduced in previous re-ID works which fall into two main categories. (1) Constructing a graph for each person by representing each node with body parts <ref type="bibr" target="#b26">[27]</ref> or local regions <ref type="bibr" target="#b34">[35]</ref>, and then a graph matching procedure is conducted to do re-identification. (2) Establishing a graph for each camera view, Hamid et al. <ref type="bibr" target="#b4">[5]</ref> introduces a joint graph matching to refine final matching results. They assume that all the query and gallery persons are available for testing, and then the matching results can be optimized by considering their joint distribution. However, it is hard to list a practical application for this method, since only the query person is available during testing stage in most scenarios. Motivated by <ref type="bibr" target="#b4">[5]</ref>, we construct a graph for each camera by considering each person as a node during the training procedure. Subsequently, we could mine the positive video pairs in two cameras with graph matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph Matching for Video Re-ID</head><p>Suppose that unlabelled graph G A contains m persons, which is represented by [A] = {x i a |i = 1, 2, ? ? ? , m} for camera A, and another graph G B consists of n persons denoted by [B] 0 = {x j b |j = 0, 1, 2, ? ? ? , n} for camera B. Note that [B] 0 contains another 0 element besides the n per-sons. The main purpose is to model the situation that more than one person in G A cannot find its correspondences in G B , i.e. allowing person-to-dummy assignments. To mine the label information across cameras, we follow <ref type="bibr" target="#b3">[4]</ref> to formulate it as a binary linear programming with linear constraints:</p><formula xml:id="formula_0">G(y) = arg min Y C T y s.t. ?i ? [A], ?j ? [B] 0 : y j i ? {0, 1}, ?j ? [B] 0 : i?[A] y j i ? 1, ?i ? [A] : j?[B]0 y j i = 1,<label>(1)</label></formula><p>where y = {y j i } ? R m(n+1)?1 is an assignment indicator of node i and j, representing whether i and j are the same person (y j i = 1) or not (y j i = 0). C = {C(i, j)} is the assignment cost matrix with each element illustrating the distance of node i to node j. The assignment cost is usually defined by node distance like C(i, j) = Dist(x i a , x j b ), as done in <ref type="bibr" target="#b4">[5]</ref>. Additionally, some geometry information is added in many feature point matching models <ref type="bibr" target="#b12">[13]</ref>.</p><p>For video re-ID, each node (person) is represented by a set of frames. Therefore, Sequence Cost (C S ) and Neighborhood Cost (C N ) are designed as the assignment cost in the graph matching model for video re-ID under a certain metric. The former cost penalizes matchings with mean set-to-set distance, while the latter one constrains the graph matching with within-graph data structure. The assignment cost between person i and j is then formulated as a combination of two costs with a weighting parameter ? in a loglogistic form:</p><formula xml:id="formula_1">C = log(1 + e (CS+?CN ) ).<label>(2)</label></formula><p>Sequence Cost. The sequence cost C S penalizes the matched sequences with the sequence difference. Under a discriminative metric M learnt from frame-level features, the average set distance between video sequences {x i a } and {x j b } is defined as the sequence cost, i.e.,</p><formula xml:id="formula_2">C S (i, j) = 1 |{x i a }||{x j b }| D M (x im a , x jn b ). (3)</formula><p>Neighborhood Cost. The neighborhood cost C N models the within camera data structure with neighborhood similarity constraints. Specifically, the correctly matched person pair's neighborhood under two cameras should be similar <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. A primarily experiment on PRID2011 dataset with features in <ref type="bibr" target="#b15">[16]</ref> is conducted to justify this point. Results shown in <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates that the percentages of the same person having common neighbors are much larger than that of different persons. It means that the same person under two different cameras should share similar neighborhood <ref type="bibr" target="#b24">[25]</ref>. Moreover, compared with image-based re-ID, the neighborhood similarity constraints for video-based re-ID are much more effective. It verifies our idea to integrate the neighborhood constraints for graph matching in video re-ID, which could help to address the camera camera variations. The neighborhood cost C N penalizes the neighborhood difference between all matched sequences, which is formulated by,</p><formula xml:id="formula_3">C N (i, j) = 1 |N i a ||N j b | x i ? a ?N i a x j ? b ?N j b D M (x i ? a ,x j ? b ) s.t. N i a (i, k) = x i ? a |D M (x i a ,x i ? a ) &lt; k , N j b (j, k) = x j ? b |D M (x j b ,x j ? b ) &lt; k ,<label>(4)</label></formula><p>where N i a and N j b denote the neighborhood of person i in camera A and person j in camera B, k is the neighborhood parameter. For simplicity, a general kNN method is adopted in our paper, and k is set as 5 for all experiments. Meanwhile, a theoretical analysis of the neighborhood constraints is presented. Letx p a be a neighbor of person i in camera A andx q b be its neighbor in camera B. From the geometry perspective, we have</p><formula xml:id="formula_4">D M (x p a ,x q b ) ? D M (x p a ,x i a ) + D M (x i b ,x q b ) + D M (x i a ,x i b ).<label>(5)</label></formula><p>Sincex p a andx q b are the neighbors ofx i</p><formula xml:id="formula_5">a andx i b , re- spectively, D M (x p a ,x i a ) and D M (x i b ,x q b ) are small positive numbers. On the other hand, D M (x i a ,x i b )</formula><p>is also a small positive under a discriminative metric D M . Thus, the distance between two neighborsx p a andx q b is small enough, i.e.,  </p><formula xml:id="formula_6">D M (x p a ,x q b ) ? ?.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dynamic Graph Matching</head><p>A number of effective graph matching optimization methods could be adopted to solve the matching problem. After that, an intuitive idea to solve unsupervised video re-ID is learning a re-identification model based on the output of graph matching. However, there still remains two obvious shortcomings:</p><p>? Since existing graphs are usually constructed in the original feature space with fixed assignment cost, it is not good enough for re-ID problem due to the large cross camera variations. Therefore, we need to learn a discriminative feature space to optimize the graph matching results.</p><p>? The estimated labels output by graph matching may bring in many false positives and negatives to the training process. Moreover, the imbalanced positive and negative video pairs would worsen this situation further. Therefore, it is reasonable to re-encode the weights of labels for overall learning, especially for the uncertain estimated positive video pairs.</p><p>To address above two shortcomings, a dynamic graph matching method is proposed. It iteratively learns a discriminative metric with intermediate estimated labels to update the graph construction, and then the graph matching is improved. Specifically, a re-weighting scheme is introduced for the estimated positive and negative video pairs. Then, a discriminative metric learning method is introduced to update the graph matching. The block diagram of the proposed method is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Label Re-weighting</head><p>This part introduces the designed label re-weighting scheme. Note that the following re-weighting scheme is based on the output (y) of optimization problem Eq. 1. y j i ? {0, 1} is a binary indicator representing whether i and j are the same person (y j i = 1) or not (y j i = 0).  Positive Re-weighting. All y j i = 1 estimated by graph matching are positive video pairs. Since the labels are uncertain, it means that considering all y j i = 1 equally is unreasonable. Therefore, we design a soft label l + (i, j) encoded with a Gaussian kernel for y j i = 1,</p><formula xml:id="formula_7">l + (i, j) = e ?C(i,j) , if C(i, j) &lt; ? + 0, others<label>(7)</label></formula><p>where ? + is the pre-defined threshold. C means the assignment cost computed in Eq. 2 in current iteration. In this manner, the positive labels (y = 1) are converted into soft labels, with smaller distance assigned larger weights while larger distance with smaller weights. Meanwhile, the filtering strategy could reduce the impact of false positives. Negative Re-weighting. Since abundant negative video pairs exist in video re-ID task compared with positive video pairs, some hard negative are selected for efficient training, l ? (i, j) for all y j i = 0 is defined as</p><formula xml:id="formula_8">l ? (i, j) = ?1, if C(i, j) &lt; ? ? 0, others,<label>(8)</label></formula><p>where ? ? is the pre-defined threshold. Considering both Eq. 7 and Eq. 8, we define ? + = ? ? = c m based on the observation shown in <ref type="figure" target="#fig_3">Fig 4.</ref> c m denotes the mean of C, which would be quite efficient. Thus, the label re-weighting scheme is refined by</p><formula xml:id="formula_9">l(i, j) = ? ? ? e ?C(i,j) * y j i , if 0 &lt; y j i C(i, j) &lt; c m 0, if C(i, j) &gt; c m ?1,</formula><p>others.</p><p>The label re-weighting scheme has the following advantages: (1) for positive video pairs, it could filter some false positives and then assign different positive sample pairs different weights; (2) for negative video pairs, a number of easy negatives would be filtered. The re-weighing scheme is simple but effective as shown in the experiments. Label Re-weighting l t with Eq. 9;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update M t with Eq. 11 as done in <ref type="bibr" target="#b14">[15]</ref>; <ref type="bibr">6:</ref> Update cost matrix C t with Eq. 2; 7:</p><p>Solve Eq. 1 to get y t ; <ref type="bibr">8:</ref> if G t ? G t?1 then 9:</p><p>y t = y t?1 ;</p><p>10:</p><p>end if <ref type="bibr">11:</ref> if converge then <ref type="bibr">12:</ref> break; <ref type="bibr">13:</ref> end if 14: end for Output: Estimated labels y, learnt metric M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metric Learning with Re-weighted Labels</head><p>With the label re-weighting scheme, we could learn a discriminative metric similar to many previous supervised metric learning works. We define the loss function by loglogistic metric learning as done in <ref type="bibr" target="#b14">[15]</ref>, i.e.,</p><formula xml:id="formula_11">f * M (x i a ,x j b ) = log(1 + e l(i,j)(D M (x i a ,x j b )?c 0 ) ),<label>(10)</label></formula><p>where c 0 is a positive constant bias to ensure D M has a lower bound. It is usually defined by the average distance between two cameras. The function D M denotes the distance ofx i a andx j b under the distance metric M , which is defined by</p><formula xml:id="formula_12">D M (x i a ,x j b ) = (x i a ?x j b ) T M (x i a ?x j b ).</formula><p>We choose the first-order statisticsx i a andx j b to represent each person as done in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>By summing up all of sequence pairs, we obtain the probabilistic metric learning problem under an estimated y formulated by,</p><formula xml:id="formula_13">F (M ; y) = m i=1 n j=1 ? ij f * M (x i a ,x j b ),<label>(11)</label></formula><p>where ? ij is a weighting parameter to deal with the imbalanced positive and negative pairs. The weights ? ij are caculated by ? ij = 1 |{l(i,j)|l(i,j)&gt;0}| if l(i, j) &gt; 0, and</p><formula xml:id="formula_14">? ij = 1 |{l(i,j)|l(i,j)=?1}| if l(i, j) = ?1,</formula><p>where | ? | denotes the number of candidates in the set. Note that some uncertain pairs are assigned with label l(i, j) = 0 without affecting the overall metric learning. The discriminative metric can be optimized by minimizing Eq. 11 using existing accelerated proximal gradient algorithms (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Iterative Updating</head><p>With the label information estimated by graph matching, we could learn an improved metric by selecting highconfident labeled video pairs. By utilizing the learnt metric, the assignment cost of Eq. 3 and Eq. 4 could be dynamically updated for better graph matching in a new iteration. After that, better graph matching could provide more reliable matching results, so as to improve the previous learnt metric. Iteratively, a stable graph matching result is finally achieved by a discriminative metric. The matched result could provide label data for further supervised learning methods. Meanwhile, a distance metric learnt in an unsupervised way could also be directly adopted for re-ID. The proposed approach is summarized in Algorithm 1.</p><p>Convergence Analysis. Note that we have two objective functions F and G optimizing y and M in each iteration. To ensure the overall convergence of the proposed dynamic graph matching, we design a similar strategy as discussed in <ref type="bibr" target="#b22">[23]</ref>. Specifically, M can be easily optimized by choosing a suitable working step size ? ? L, where L is the Lipschitz constant of the gradient function ?F (M, y). Thus, it could ensure F (M t ; y t?1 ) ? F (M t?1 ; y t?1 ), a detailed proof is shown in <ref type="bibr" target="#b0">[1]</ref>. For y t at iteration t, we constrain the updating procedure by keep on updating the assignment cost matrix C t until getting a better y which satisfies G(M t ; y t ) ? G(M t ; y t?1 ), similar proof can be derived from <ref type="bibr" target="#b22">[23]</ref>. By constrain the updating procedure, it could satisfy the crite-</p><formula xml:id="formula_15">ria G t (y; M ) + F t (M ; y) ? G t?1 (y; M ) + F t?1 (M ; y)</formula><p>. This is validated in our experiments as discussed in Section 5.2. Particularly, the proposed method converges steadily.</p><p>Complexity Analysis. In the proposed method, most computational costs focus on the iterative procedure, since we need to conduct the graph matching with Hungarian algorithm at each iteration. We need to compute the sequence cost O(n 2 ) and neighborhood cost O(kn + n 2 ) for each camera, and then graph matching time complexity is O(n 3 ). Updating M with accelerated proximal gradient is extremely fast as illustrated in <ref type="bibr" target="#b0">[1]</ref>. However, the proposed method is conducted offline to estimate labels, which is suitable for practical applications. During the online testing procedure, we only need to compute the distance between the query person p and the gallery persons with the learnt reidentification model. The distance computation complexity is O(n) and ranking complexity is O(n log n), which is the same as existing methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>Datasets. Three publicly available video re-ID datasets are used for evaluation: PRID-2011 <ref type="bibr" target="#b5">[6]</ref>, iLIDS-VID <ref type="bibr" target="#b23">[24]</ref> and MARS <ref type="bibr" target="#b36">[37]</ref> dataset. The PRID-2011 dataset is collected from two disjoint surveillance cameras with significant color inconsistency. It contains 385 person video tracks in camera A and 749 person tracks in camera B. Among all persons, 200 persons are recorded in both camera views. Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37]</ref>, 178 person video pairs with no less than 27 frames are employed for evaluation. iLIDS-VID dataset is captured by two non-overlapping cameras located in an airport arrival hall, 300 person videos tracks are sampled in each camera, each person track contains 23 to 192 frames. MARS dataset is a large scale dataset, it contains 1,261 different persons whom are captured by at least 2 cameras, totally 20,715 image sequences achieved by DPM detector and GMCCP tracker automatically.</p><p>Feature Extraction. The hand-craft feature LOMO <ref type="bibr" target="#b13">[14]</ref> is selected as the frame feature on all three datasets. LOMO extracts the feature representation with the Local Maximal Occurrence rule. All the image frames are normalized to 128 ? 64. The original 26960-dim features for each frame are then reduced to a 600-dim feature vector by a PCA method for efficiency considerations on all three datasets. Meanwhile, we conduct a max-pooling for every 10 frames to get more robust video feature representations.</p><p>Settings. All the experiments are conducted following the evaluation protocol in existing works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b33">34]</ref>. PRID-2011 and iLIDS-VID datasets are randomly split by half, one for training and the other for testing. In testing procedure, the regularized minimum set distance <ref type="bibr" target="#b28">[29]</ref> of two persons is adopted. Standard cumulated matching characteristics (CMC) curve is adopted as our evaluation metric. The procedure are repeated for 10 trials to achieve statistically reliable results, the training/testing splits are originated from <ref type="bibr" target="#b33">[34]</ref>. Since MARS dataset contains 6 cameras with imbalanced tracklets in different cameras, we initialize the tracklets in camera 1 as the base graph, the same number of tracklets from other five cameras are randomly selected to construct a graph for matching. The evaluation protocol on MARS dataset is the same as <ref type="bibr" target="#b36">[37]</ref>, CMC curve and mAP (mean average precision) value are both reported.</p><p>Implementation. Both the graph matching and metric learning optimization problems can be solved separately using existing methods. We adopt Hungarian algorithm to solve the graph matching problem for efficiency considerations, and metric learning method (MLAPG) in <ref type="bibr" target="#b14">[15]</ref> as the baseline methods. Some advanced graph matching and metric learning methods may be adopted as alternatives to produce even better results as shown in Section 5.3. We report the results at 10th iteration, with ? = 0.5 for all three datasets if without specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Self Evaluation</head><p>Evaluation of iterative updating. To demonstrate the effectiveness of the iterative updating strategy, the rank-1 matching rates of training and testing at each iteration on three datasets are reported in <ref type="figure" target="#fig_4">Fig. 5</ref>. Specifically, the rank-1 accuracy for testing is achieved with the learnt metric at each iteration, which could directly reflect the improvements for re-ID task. Meanwhile, the overall objective values on three datasets are reported.  <ref type="table">Table 1</ref>. Rank-1 matching rates with (/without) label re-weighting on three datasets. <ref type="figure" target="#fig_4">Fig. 5(a)</ref> shows that the performance is improved with iterative updating procedure. We could achieve 81.57% accuracy for PRID-2011, 49.33% for iLIDS-VID and 59.64% for MARS dataset. Compare with iteration 1, the improvement at each iteration is significant. After about 5 iterations, the testing performance fluctuates mildly. This fluctuation may be caused by the data difference of the training data and testing data. It should be pointed out that there is a huge gap on the MARS dataset, this is caused by the abundant distractors during the testing procedure, while there is no distractors for training <ref type="bibr" target="#b36">[37]</ref>. Experimental results on the three datasets show that the proposed iterative updating algorithm improves the performance remarkably. Although without theoretical proof, it is shown in <ref type="figure" target="#fig_4">Fig. 5(b)</ref> that DGM converges to steady and satisfactory performance.</p><p>Evaluation of label re-weighting. We also compare the performance without label re-weighting strategy. The intermediate labels output by graph matching are simply transformed to 1 for matched and ?1 for unmatched pairs. The rank-1 matching rates on three datasets are shown <ref type="table">Table 1</ref>. Consistent improvements on three datasets illustrate that the proposed label-re-weighting scheme could improve the re-ID model learning.</p><p>Evaluation of label estimation. To illustrate the label estimation performance, we adopt the general precision, recall and F-score as the evaluation criteria. The results on three datasets are shown in <ref type="table">Table 2</ref>. Since graph matching usually constrains full matching, the precision score is quite close to the recall on the PRID-2011 and iLIDS-VID datasets. Note that the precision score is slightly higher than recall is due to the proposed positive re-weighting strategy.  <ref type="figure">Figure 6</ref>. Estimated labels for other supervised learning methods. "DGM" represents the re-identification performance with our estimated labels. "GT" provides upper bounds with fully supervised learning. Rank-1 matching rates (%) are reported for three datasets. Running time. The running times on three datasets with the settings described in Section 5.1 are evaluated. It is implemented with Matlab and executed on a desktop PC with i7-4790K @4.0 GHz CPU and 16GB RAM. The training and testing time are reported by the average running time in 10 trials. For training, since we adopt an efficient graph matching algorithm and accelerated metric learning <ref type="bibr" target="#b14">[15]</ref>, the training time is acceptable. The training time for the PRID2011 dataset is about 13s, about 15s for iLIDS-VID dataset, about 2.5 hours for the MARS dataset due to the large amount of tracklets. For testing, the running time is fast for our method, since standard 1-vs-N matching scheme is employed. The testing times are less than 0.001s on PRID2011 and iLIDS-VID datasets for each query process, and around 0.01s on MARS with 636 gallery persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Estimated Labels for Supervised Learning</head><p>This subsection evaluates the effectiveness of the output estimated labels for other supervised learning methods. Compared with the re-identification performances with groundtruth labels (GT), they provide upper bounds as references to illustrate the effectiveness of DGM. Specifically, two metric learning methods MLAPG <ref type="bibr" target="#b14">[15]</ref> and XQDA <ref type="bibr" target="#b13">[14]</ref>, and an ID-discriminative Embedding (IDE) deep model <ref type="bibr" target="#b36">[37]</ref> are selected for evaluation as shown in <ref type="figure">Fig. 6</ref>.</p><p>Configured with MLAPG and XQDA, the performances outperform the baseline l 2 -norm on all three datasets, usually by a large margin. The results show that the estimated labels also match well with other supervised methods. Compared with the upper bounds provided by supervised metric learning methods with groundtruth labels, the results on PRID-2011 and MARS datasets are quite close to the upper bounds. Although the results on iLIDS-VID dataset are not that competitive, the main reason can be at-tributed to its complex environment with many background clutters, such as luggage, passengers and so on, which cannot be effectively solved by a global descriptor (LOMO) <ref type="bibr" target="#b13">[14]</ref>.</p><p>Another experiment with IDE deep model on the three datasets shows the expendability of the proposed method to deep learning methods. Specifically, about 441k out of 518k image frames are labelled for 625 identities on the large scale MARS dataset, while others are left with Eq. 9. The labelled images are then resized to 227 ? 227 pixels as done in <ref type="bibr" target="#b36">[37]</ref>, square regions 224 ? 224 are randomly cropped from the resized images. Three fully convolutional layers with 1,024, 1,024 and N blobs are defined by using AlexNet <ref type="bibr" target="#b9">[10]</ref>, where N denotes the labelled identities on three datasets. The FC-7 layer features (1,024-dim) are extracted from testing frames, maxpooling strategy is adopted for each sequence <ref type="bibr" target="#b36">[37]</ref>. Our IDE model is implemented with MxNet. <ref type="figure">Fig. 6</ref> shows that the performance is improved with a huge gap to hand-craft features with deep learning technique on the large scale MARS dataset. Comparably, it does not perform well on two small scale datasets (PRID-2011 and iLIDS-VID dataset) compared to hand-craft features due to the limited training data. Meanwhile, the gap between the estimated labels to fully supervised deep learning methods is consistent to that of metric learning methods. Note that since one person may appear in more than one cameras on the MARS dataset, the rank-1 matching rates may be even higher than label estimation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with Unsupervised re-ID</head><p>This section compares the performances to existing unsupervised re-ID methods. Specifically, two image-based re-ID methods, Salience <ref type="bibr" target="#b35">[36]</ref> results originated from <ref type="bibr" target="#b23">[24]</ref>, and GRDL <ref type="bibr" target="#b8">[9]</ref> is implemented by averaging multiple frame features in a video sequence to a single feature vector. Four state-of-the-art unsupervised video re-ID methods are included, including DVDL <ref type="bibr" target="#b6">[7]</ref>, FV3D <ref type="bibr" target="#b15">[16]</ref>, STFV3D <ref type="bibr" target="#b15">[16]</ref> and UnKISS <ref type="bibr" target="#b7">[8]</ref>. Meanwhile, our unsupervised estimated labels are configured with three supervised baselines MLAPG <ref type="bibr" target="#b14">[15]</ref>, XQDA <ref type="bibr" target="#b13">[14]</ref> and IDE <ref type="bibr" target="#b36">[37]</ref> to learn the re-identification models as shown in <ref type="table" target="#tab_4">Table 3</ref>. It is shown in <ref type="table" target="#tab_4">Table 3</ref> that the proposed method outperforms other unsupervised re-ID methods on PRID-2011 and MARS dataset often by a large margin. Meanwhile, a comparable performance with other state-of-the-art performances is obtained on iLIDS-VID dataset even with a poor baseline input. In most cases, our re-ID performance could achieve the best performances on all three datasets with the learnt metric directly. We assume that the proposed method may yield better results by adopting better baseline descriptors, other advanced supervised learning methods would also boost the performance further. The advantages can be attributed to two folds: (1) unsupervised estimating cross cameras labels provides a good solution for unsupervised re-ID, since it is quite hard to learn invariant feature representations without cross-camera label information; (2) dynamic graph matching is a good solution to select matched video pairs with the intra-graph relationship to address the cross camera variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Robustness in the Wild</head><p>This subsection mainly discusses whether the proposed method still works under practical conditions.</p><p>Distractors. In real applications, some persons may not appear in both cameras. To simulate this situation for training, we use the additional 158 person sequences in camera A and 549 persons in camera B of PRID-2011 dataset to conduct the experiments. d% * N distractor persons are randomly selected from these additional person sequences for each camera. They are added to the training set as distractors. N is the size of training set. We use these distractors to model the practical application, in which many persons cannot find their correspondences in another camera.</p><p>Trajectory segments. One person may have multiple sequences in each camera due to tracking errors or reappear in the camera views. Therefore, multiple sequences of the same person may be unavoidable to be false treated as different persons. To test the performance, p% * N person sequences are randomly selected to be divided into two  <ref type="table">Table 4</ref>. Matching rates (%) on the PRID-2011 dataset achieved by the learnt metric without one-to-one matching assumption. halves in each camera on PRID-2011 dataset. In this manner, about p% persons would be false matched since the p% are both randomly selected for two cameras. <ref type="table">Table 4</ref> shows that the performance without one-to-one matching assumption is still stable, with only a little degradation in both situations, this is because: (1) Without oneto-one assumption, it will increase the number of negative matching pairs, but due to the abundant negatives pairs in re-ID task, the influence is not that much. (2) The label re-weighting strategy would reduce the effects of lowconfidence matched positive pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes a dynamic graph matching method to estimate labels for unsupervised video re-ID. The graph is dynamically updated by learning a discriminative metric. Benefit from the two layer cost designed for graph matching, a discriminative metric and an accurate label graph are updated iteratively. The estimated labels match well with other advanced supervised learning methods, and superior performances are obtained in extensive experiments. The dynamic graph matching framework provides a good solution for unsupervised re-ID.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the neighborhood similarity. With various values of k, we record the percentages of having intersection of same (different) person's kNN under two different cameras. The Same Person (Video-based) refers to video re-ID task in which one person have multiple person images. Same Person (Image-based) denotes the image based re-ID task in which each person only have single image per camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Block diagram of the proposed approach. The estimated labels and learnt metric are updated in an iterative manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration about the choice of ?+ in Eq. 7 and ?? in Eq. 8 on the PRID-2011 dataset. It is shown that most positive pair costs are smaller than the mean cost, while cost larger than mean cost is likely to be negative sample pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) Rank-1 accuracy of training and testing at each iteration on three datasets. (b) Overall objective values at each iteration on three datasets. For better view, the objective values are normalized. Datasets PRID-2011 iLIDS-VID MARS w/o re-weighting 72</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Code is available at www.comp.hkbu.edu.hk/%7emangye/</figDesc><table><row><cell>Data Graphs</cell><cell>Matching</cell><cell>Labels</cell><cell></cell><cell>Learn Re ID</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Re-weighting</cell><cell></cell></row><row><cell>Data Graphs</cell><cell>Matching</cell><cell>Labels</cell><cell>f</cell><cell>Learn Re-ID</cell></row><row><cell></cell><cell>Update</cell><cell>Metric</cell><cell>Learn</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 Dynamic Graph Matching (DGM)Input: Unlabelled features X a , X b , M 0 = I.1: Compute C 0 with Eq. 2; 2: Solve Eq. 1 to get y 0 and G 0 ; 3: for t = 1 to maxIter do</figDesc><table /><note>4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art unsupervised methods including image and video based methods on three datasets. Red indicates the best performance while Blue for second best.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell cols="2">PRID-2011</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID</cell><cell></cell><cell></cell><cell></cell><cell>MARS</cell><cell></cell><cell></cell></row><row><cell>Rank at r</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>mAP</cell></row><row><cell>L2</cell><cell>40.6</cell><cell>66.7</cell><cell>79.4</cell><cell>92.3</cell><cell>9.2</cell><cell>20.0</cell><cell>27.9</cell><cell>46.9</cell><cell>14.9</cell><cell>27.4</cell><cell>33.7</cell><cell>40.8</cell><cell>5.5</cell></row><row><cell>FV3D [16]</cell><cell>38.7</cell><cell>71.0</cell><cell>80.6</cell><cell>90.3</cell><cell>25.3</cell><cell>54.0</cell><cell>68.3</cell><cell>87.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STFV3D  *  [16]</cell><cell>27.0</cell><cell>54.0</cell><cell>66.3</cell><cell>80.9</cell><cell>19.1</cell><cell>38.8</cell><cell>51.7</cell><cell>70.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Salience [36]</cell><cell>25.8</cell><cell>43.6</cell><cell>52.6</cell><cell>62.0</cell><cell>10.2</cell><cell>24.8</cell><cell>35.5</cell><cell>52.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DVDL [7]</cell><cell>40.6</cell><cell>69.7</cell><cell>77.8</cell><cell>85.6</cell><cell>25.9</cell><cell>48.2</cell><cell>57.3</cell><cell>68.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GRDL [9]</cell><cell>41.6</cell><cell>76.4</cell><cell>84.6</cell><cell>89.9</cell><cell>25.7</cell><cell>49.9</cell><cell>63.2</cell><cell>77.6</cell><cell>19.3</cell><cell>33.2</cell><cell>41.6</cell><cell>46.5</cell><cell>9.56</cell></row><row><cell>UnKISS [8]</cell><cell>58.1</cell><cell>81.9</cell><cell>89.6</cell><cell>96.0</cell><cell>35.9</cell><cell>63.3</cell><cell>74.9</cell><cell>83.4</cell><cell>22.3</cell><cell>37.4</cell><cell>47.2</cell><cell>53.6</cell><cell>10.6</cell></row><row><cell>DGM + MLAPG [15]</cell><cell>73.1</cell><cell>92.5</cell><cell>96.7</cell><cell>99.0</cell><cell>37.1</cell><cell>61.3</cell><cell>72.2</cell><cell>82.0</cell><cell>24.6</cell><cell>42.6</cell><cell>50.4</cell><cell>57.2</cell><cell>11.8</cell></row><row><cell>DGM + XQDA [14]</cell><cell>82.4</cell><cell>95.4</cell><cell>98.3</cell><cell>99.8</cell><cell>31.3</cell><cell>55.3</cell><cell>70.7</cell><cell>83.4</cell><cell>23.6</cell><cell>38.2</cell><cell>47.9</cell><cell>54.7</cell><cell>11.2</cell></row><row><cell>DGM + IDE [37]</cell><cell>56.4</cell><cell>81.3</cell><cell>88.0</cell><cell>96.4</cell><cell>36.2</cell><cell>62.8</cell><cell>73.6</cell><cell>82.7</cell><cell>36.8</cell><cell>54.0</cell><cell>61.6</cell><cell>68.5</cell><cell>21.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work is partially supported by Hong Kong RGC General Research Fund HKBU (12202514), NSFC (61562048). Thanks Guangcan Mai for the IDE implementation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on imaging sciences</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reweighted random walks for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised person reidentification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint probabilistic data association revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint probabilistic matching using m-best solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In Image analysis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification with discriminatively trained viewpoint invariant dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised data association for metric learning in the context of multi-shot person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person reidentification by unsupervised l1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint sparse representation and robust feature-level fusion for multi-cue visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust mil-based feature template learning for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A spatiotemporal appearance representation for viceo-based pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A labeling-by-search approach for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised adaptive re-identification in open world dynamic camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised crossdataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Et</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the convergence of graph matching: Graduated assignment revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person reidentification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Zero-shot person reidentification via cross-view consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TMM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical inference of gaussian-laplace distribution for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human reidentification by matching compositional template with cluster sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multigraph matching via affinity optimization with graduated consistency regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face recognition based on regularized nearest points between image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled-view based ranking optimization for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ranking optimization for person re-identification via similarity and dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person re-identification via ranking aggregation of similarity pulling and dissimilarity pushing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TMM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Top-push videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prism: Person re-identification via structured matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TCSVT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">SIFT meets CNN: A decade survey of instance retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video-based person re-identification by simultaneously learning intra-video and inter-video distance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
