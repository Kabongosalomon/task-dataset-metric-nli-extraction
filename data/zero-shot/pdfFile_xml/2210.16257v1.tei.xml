<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving Math Word Problem via Cooperative Reasoning induced Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
							<email>zhanglin@idea.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Gan</surname></persName>
							<email>ganruyi@idea.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
							<email>zhangjiaxing@idea.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Solving Math Word Problem via Cooperative Reasoning induced Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale pre-trained language models (PLMs) bring new opportunities to challenge problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.8% increase over best baselines. * Equal contribution. ? Corresponding Author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Addressing math problems is a hallmark of human intelligence, reasoning, and adapting from limited data. We want Artificial Intelligence (AI) to be able to do the same, however, quick and flexible reasoning is challenging to AI as it must possess a certain level of prior experience from a limited amount of new data while avoiding overfitting. The rapid growth of large-scale Pre-trained Language Models (PLMs) offers unprecedented potential for this issue, often relying on well-designed trigger prompts <ref type="bibr" target="#b24">(Wei et al., 2022b;</ref><ref type="bibr" target="#b11">Li et al., 2022;</ref><ref type="bibr" target="#b0">Brown</ref>   <ref type="bibr">et al., 2020)</ref>. Although appealing in terms of efficiency, its success relies on memorizing patterns with a sufficiently large number of parameters (? 100B) <ref type="bibr">(Wei et al., 2022a)</ref>, differentiating it from the fast adaptivity in the human reasoning process and resulting in poor generalization.</p><p>Active disciplines like neuroscience and cognitive science attempt to uncover the mechanism of human reasoning, and agree that our learning process is governed by an interaction mechanism, often referred to as System 1 and System 2 <ref type="bibr" target="#b5">(Evans, 2003;</ref><ref type="bibr" target="#b4">Daniel, 2017)</ref>. In particular, System 1 offers fast responses like human instinct, and System 2 performs deliberate reasoning. Interactions between them are important for adapting to a continuously changing environment. PLMs behave more like System 1, according to the above theory, and thus lack the generalization ability in reasoning <ref type="bibr" target="#b15">(Nye et al., 2021b)</ref>.</p><p>In this work, we explore a new line of zero-shot math problem reasoning, using a human reasoning-alike framework with feedback in the solution generation loop as opposed to pure PLM-based methods, called Cooperative Reasoning (CoRe). Intuitively, System 1 and System 2 are embodied as generators and verifiers, respectively, and they are defined as follows: generators for generating reasoning paths, and verifiers for supervising the paths' evaluation. More specifically, we train an LM beyond the question-answer paradigm by integrating in-the-loop reasoning, i.e., we let the LM output both the answer and the corresponding reasoning process for a given question. Meanwhile, we introduce two types of verifiers, including token-level and sentence-level, allowing us to provide feedback in the whole solution generation lifecycle. Notice that the solution path is generated by selecting candidate tokens with some probability so that it is tree-alike and much coincides with the tree search process of Monte Carlo Tree Search (MCTS) <ref type="bibr" target="#b7">(Kocsis and Szepesv?ri, 2006)</ref>. With this in mind, the verifiers can score tokens along the solution generation process from start to end when using the MCTS. Therefore, we can use the score to evaluate the quality of the generation process during inferring before finalizing the solution, making timely feedback available for supervising the generation process. With this, the evaluation goes beyond the quality of the final result at the granularity of each reasoning step, extending the supervision from the solution level to the path level. We combine the solution score and the perplexity of its corresponding reasoning path to encourage the overall training towards high-quality solutions while aligning with the reliable reasoning process, aiming to improve generalization ability.</p><p>Our experiments evaluate CoRe on multiple mathematical reasoning datasets in both zero-shot and fine-tuned settings. CoRe consistently achieves better performance than competing baselines. Notably, CoRe has up to 9.8% improvements on Mul-tiArith over SoTA baselines, which are dozens of times larger than our model.</p><p>In summary, our contributions are as follows.</p><p>? We propose a novel reasoning method for mathematical problem solving, called Cooperative Reasoning (CoRe), that introduces feedback in the loop during solution generation as opposed to the sequential learning process in the previous one, resulting in the first method for this task that builds on top of the learning mechanism in the human brain. ? We develop a self-thinking strategy for further boosting reasoning ability with generated data from the cooperation between System 1 and System 2. ? We demonstrate the superiority of CoRe comparing to other zero-shot and fine-tune methods, which has 9.8% improvements on Multi-Arith over SoTA baselines.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dual Process System</head><p>Dual-process theory <ref type="bibr" target="#b5">(Evans, 2003;</ref><ref type="bibr" target="#b4">Daniel, 2017)</ref> argues there are two cognitive systems underpinning human reasoning: System 1 and System 2. The purpose of clarifying these systems is that they have the potential to help us construct artificial intelligence systems that benefit from human flexibility and methodical generalisation. Dual process system model guidance is not new. <ref type="bibr" target="#b15">Nye et al. (2021b)</ref> simulated Systems 1 and 2 to improve neural sequence model consistency and coherence. Similar to <ref type="bibr" target="#b2">Cobbe et al. (2021)</ref> and <ref type="bibr" target="#b11">Li et al. (2022)</ref>, in addition to System 1 (Generator), which is used for generation, we develop a distinct model as System 2, called Verifier. Verifier checks the feasibility and correctness of Generator's content and collaboratively solves the reasoning task with Generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-step Reasoning</head><p>There are many works on exploiting the multi-step reasoning ability of language models. <ref type="bibr" target="#b2">Cobbe et al. (2021)</ref> released a grade school math word problems dataset GSM8K, experiments on GSM8K showed that training a verifier to score the solutions generated by a fine-tuned GPT-3 could significantly improve the performance compared to solely finetuning a GPT-3. <ref type="bibr" target="#b14">Nye et al. (2021a)</ref> discovered that letting language model write down the intermediate process could achieve better results on various nlp tasks. Likewise, Chain-of-Thought(CoT) prompt <ref type="bibr" target="#b24">(Wei et al., 2022b)</ref> prepended exemplars with intermediate reasoning steps as prompt and achieved state-of-the-art on several reasoning benchmarks using sufficiently large language models.  further boosted CoT's performance by sampling a bunch of possible solutions and then obtained the final answer by majority voting. DIVERSE <ref type="bibr" target="#b11">(Li et al., 2022)</ref> proved more diverse CoT prompt and an extra verifier were both helpful for PLMs to solve reasoning problems. <ref type="bibr" target="#b8">Kojima et al. (2022)</ref> discovered that by simply adding "Let's think step by step" after the question, PLMs could successfully step by step solve the problems, they called the method Zero-shot-CoT.</p><p>These works all heavily rely on extremely large language models and therefore are very computationally intensive and time consuming. What's more, according to <ref type="bibr" target="#b24">Wei et al. (2022b)</ref> and <ref type="bibr" target="#b8">Kojima et al. (2022)</ref> neither CoT nor Zero-shot-CoT are helpful to smaller models. While our method doesn't necessarily need extremely large language models and can be applied to models with different parameter scales, thus reducing computational cost and inference time. Our approach has competitive zero-shot performance thanks to the efficient and collaborative application of dual process system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cooperative Reasoning</head><p>In this section, we will present the proposed cooperative reasoning framework, CoRe, that enforces System 1 and System 2 mutually cooperating, which includes 3 sequential steps: cooperative training, cooperative inference, and self-thinking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preparation</head><p>As discussed in Sec. 1, we expect a PLM (G) to fast generate multiple reasoning paths like System 1. Then, considering that System 2 is responsible for deliberate evaluations of the reasoning paths, we employ two modules: a step verifier (V step ) for reasoning steps, a path verifier (V path ) for reasoning paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cooperative Training</head><p>Before applying System 1&amp;2 to inference, a critical issue for them is learn how to generate reasoning paths and how to evaluate reasoning steps/paths. Inspired by a widely-used training strategy for reasoners <ref type="bibr" target="#b2">(Cobbe et al., 2021)</ref>, we present cooperative training method as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> Step 1. Moreover, we discuss hyper-parameter configurations and extra training details in Appendix B.1 and Appendix B.2.</p><p>Step 1.1:</p><formula xml:id="formula_0">We first fine-tune G on a dataset D = {(q i , p i , gt i )} N i=1 consisting of N samples.</formula><p>Each sample x is composed of a question q, a reasoning path p and a ground truth answer gt. We fine-tuen G with standard language modeling objective L LM as Eq. <ref type="formula">(1)</ref>.</p><formula xml:id="formula_1">LLM = ? |p|+|gt| i=1 log P (xi | x&lt;i)</formula><p>(1)</p><p>Step 1.2: Once G has learned how to generate solutions, we employ it on questions q from D.</p><p>As a result, we obtain a new dataset</p><formula xml:id="formula_2">D + = q i , rp i,j , a i,j i=1,...,N j=1,.</formula><p>..,M with M generated reasoning paths (rp) and answers (a) for each q.</p><p>Step 1.3: Different from the popular methods, we train two verifiers to model human reasoning problems with deliberate analysis for each step and the whole path. For evaluating several reasoning steps in a path, we desire a token-level scorer, which is named step verifier V step . Therefore we fine-tune a PLM with a joint objective L V S where it predicts the generated tokens as correct or incorrect , in addition to the language modeling objective L LM . In detail, L V S is computed with Mean Squared Error (MSE) as follows:</p><formula xml:id="formula_3">LV S = |rp|+|a| i=1 (scorei ? I(a == gt)) 2 ,<label>(2)</label></formula><p>where, (rp, a) from D + and gt with same q from D.</p><p>On the other hand, we need a path-level scorer for reasoning paths. Different from step verifier, we simply extract an overall presentation of reasoning path for prediction. Specifically, we employ a BERT-like model and take the [CLS] token to calculate MSE loss L V P similar to L V S .</p><p>In summary, the overall training objective for verifiers is given by:</p><formula xml:id="formula_4">LV = LV S + LLM + LV P .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cooperative Inference</head><p>After obtaining a generator (System 1, generating) and verifiers (System 2, evaluating), we propose cooperative inference to generate solutions for unseen questions. Instead of only treating verifiers as voters, we argue that verifiers should offer appropriate guidance and feedback during generation. Therefore, we integrate a cooperative search algorithm. In particular, we adopt the popular Monte Carlo Tree Search (MCTS) <ref type="bibr" target="#b7">(Kocsis and Szepesv?ri, 2006)</ref> to enable controlled reasoning. The cooperative inference starts from the root node, which includes question tokens. We detail the cooperative inference process as follows.</p><p>Selection. If current node has children, with 50% probability we select a node from it children as System 1 (Generator)</p><p>Step 1: Cooperative Training System 1&amp;2</p><p>Step 1.1 Fine-tuning ({Q, P, GT})</p><p>Step <ref type="formula">1</ref>  Step 3: Self-Thinking current node with modified PUCT <ref type="bibr" target="#b3">(Czech et al., 2020)</ref> as Eq. <ref type="formula">(4)</ref>,</p><formula xml:id="formula_5">n * = arg max n?C (R(n)+cpuct?(n|s) b?C N (s, b) 1 + N (s, n)</formula><p>), <ref type="formula">(4)</ref> where the state s represents the sequence consisting of all tokens in the current search path. And, N (s, n) means the times node n has been selected in state s. Reward R(n) records all the score received from backup. Otherwise, we perform once expansion and choose the new node as current node.</p><p>Expansion. During expansion, the generator is required to generate a sequence of tokens based on current state. A new node is created to store the generated tokens and added to current node's children. Then, V step evaluates the current reasoning path and update R(n) of nodes by backup. Roll-Out. After selection and expansion, we start from current node and let generator complete the reasoning path until it meets [EOS] token or reaches max token length limit. Next, V path evaluates the whole reasoning path and produce a score score path . Remember that V step also provides a score score step during expansion. Therefore to leverage two scores, we introduce a hyperparameter ? to weigh their contribution to node's reward,</p><formula xml:id="formula_6">s = score path + ? ? scorestep<label>(5)</label></formula><p>where s is the final score that each node receives by backup. Backup. We update the rewards back from current node to root. The scores produced by verifiers are added to R(n) and visited time N (s, n) is increased by 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Self-Thinking</head><p>It is a challenging to fine-tune models on their generated data, which indicates they have to trust themselves. High risk high gain. A proper selftraining method can enhance the robustness of the whole system and allow deep data mining. Therefore, we introduce self-thinking as described in <ref type="figure" target="#fig_2">Fig. 2</ref> Step 3 and Algorithm 1. Considering the noise contained in generated data, we build a filter by using scores from verifiers and perplexity (PPL) from generator. In detail, we select high-quality reasoning paths by setting a score threshold. Moreover, we only keep the reasoning paths with no higher PPL than ground truth solutions. After filtering, we merge D new with D and send it to Step 1. Once the several iterations completed, we obtain a powerful System 1&amp;2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Zero-shot Inference</head><p>We simply perform cooperative inference as <ref type="figure" target="#fig_2">Fig. 2</ref> Step 2 with trained System 1&amp;2 on unseen datasets. After obtaining several reasoning paths with scores, we arrive at the final answer by weighted voting based on scores following <ref type="bibr" target="#b11">(Li et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We evaluate our proposal on several widely-used math word problem benchmarks: ASDiv-A (Miao et al., 2020), SingleOp , SinlgeEq <ref type="bibr" target="#b9">(Koncel-Kedziorski et al., 2015)</ref> and MultiArith . Following the general setting <ref type="bibr" target="#b8">(Kojima et al., 2022;</ref><ref type="bibr" target="#b24">Wei et al., 2022b)</ref>, we apply accuracy as evaluation metric in all datasets. Implementation Details. Since cooperative training requires a high-quality dataset with reasoning paths, we treat GSM8K <ref type="bibr" target="#b2">(Cobbe et al., 2021)</ref> as the seed dataset D in Sec. 3.2. Unless otherwise, we employ GPT-J <ref type="bibr" target="#b20">(Wang and Komatsuzaki, 2021)</ref> as the generator and the step verifier, DeBERTalarge <ref type="bibr" target="#b6">(He et al., 2021)</ref> as the path verifier. Since the default setting consists of two GPT-J (6B) and a DeBERTa-large (0.4B), we note our backbone as "GPT-J 12B", which implies around 12.4B parameters in total. During generation, we apply calculator as assistant following <ref type="bibr" target="#b2">Cobbe et al. (2021)</ref>. Our zeroshot setting is similar to the transferring setting in T0 <ref type="bibr">(Sanh et al., 2021)</ref> and FLAN <ref type="bibr" target="#b22">(Wei et al., 2021)</ref>. Baselines. For comparison under zero-shot setting, the results of Instruct GPT3 and PaLM with their various methods are from <ref type="bibr" target="#b8">Kojima et al. (2022)</ref>. The zero-shot * and zero-shot-CoT * imply not the standard prompt (see details in Appendix B.4). We also provide our generator as a baseline when comparing to previous fine-tune methods. Regarding to sampling multiple solutions, we search 40 paths with the same setting as Self-Consistency .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Zero-shot Results</head><p>As shown in <ref type="table">Table 1</ref>, We compare the zero-shot methods on two mathematical reasoning datasets. CoRe achieves superior performance on both datasets, demonstrating its capability of mathematical reasoning on unseen datasets. Note that the baselines are several dozen times larger than ours. Therefore, the results present the effectiveness of cooperative working with System 1&amp;2 and selfthinking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Zero-shot v.s. Fine-tuning</head><p>We compare CoRe with previous fine-tuned SoTA baselines on four datasets and results are presented in <ref type="table" target="#tab_0">Table 2</ref>. To show the importance of cooperative reasoning, we apply our generator as a baseline. <ref type="table" target="#tab_0">Table 2</ref> demonstrates that without any guidance generator underperforms previous methods on most datasets. Despite the gain from self-consistency, it still lags behind other fine-tuned SoTAs. While CoRe can greatly boosts PLMs' reasoning performance and is better than previous fine-tuned SoTAs on all datasets. <ref type="table" target="#tab_6">Table 3</ref> summarizes the performance on zero-shot datasets with different setting of System 2. Dual to computing resource limitations, the experiments are conducted using models without self-thinking, which means only Step 1 and Step 2 in Sec. 3 are involved. For "w/o System 2", the solutions are predicted by System 1 only and applied with "Self-Consistency". We demonstrate that the step-level guidance from V step helps System 1. As described in Eq. (5), increasing the weight of reward from V step , the CoRe obtains a higher scores on both SingleOp and MultiArith. Due to the assistance and feedback from System 2, System 1 tends to reason on paths with higher scores. As a result, CoRe increases the accuracy on SingleOP from 59.6% to 80.2% and MultiArith from 92.3% to 96.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">How does System 2 help Sytstem 1?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">How to choose the answer aggregation strategy?</head><p>Since there are multiple reasoning paths with scores, we utilize the score and consider two popular methods: choosing best and weighted voting. Before the searching iteration increased to 40 paths,   <ref type="bibr" target="#b10">(Lan et al., 2022)</ref>, b: LogicForm <ref type="bibr" target="#b12">(Liang et al., 2016)</ref>, c: UNITDEP <ref type="bibr" target="#b17">(Roy and Roth, 2017)</ref>, d: Relevance and LCA operation classifier .   <ref type="table">Table 4</ref>: Zero-shot results with different answer aggregation strategies. "best" is to choose the solution with highest score. "vote" indicates weighted voting base on scores.</p><p>weighted voting and choosing best perform similarly, which shows the robustness of verifier. Furthermore, more reasoning paths always outcome a better performance, which indicates that the answer would be more reliable with more solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we follow the human reasoning mechanism to develop an effective reasoning method for solving the math word problem, mimicking the dual system of human cognition. The proposed method involves two parts: a generator as System 1 and a verifier as System 2, and overall reasoning is generated based on their mutual reinforcement. From the robustness and generalization aspects, CoRe obtains superior reasoning ability over other language models, and thus outperforms PLMs with several tens of times more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hyper-parameters</head><p>For generator and step verifier, we train them for two epochs. Batch size is set to 16. The learning rate (lr) is set to 1e ? 5 at the first epoch and 1e ? 6 the second epoch for generator, while for step verifier we apply the warmup then linearly decaying scheduler, lr is set to 1e?6 and warmup ratio is 0.1. For path verifier, we train it for three epochs with batch size set to 128 and lr set to 1e ? 5. Same lr scheduler as step verifier is applied for path verifier. We set gradient clip norm to 1.0 and sampling temperature to 0.7. Random seed is set to 19990303 through all the training process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Details of Training Verifiers</head><p>Before two verifiers are fine-tuned, we utilize the generator to sample 100 solutions for each question following <ref type="bibr" target="#b2">Cobbe et al. (2021)</ref>. Then we train the two verifiers on the generated data as described in Sec. 3.2 Step 1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Details of Self-Thinking</head><p>During self-thinking, since we use cooperative inference rather than random sampling, solutions are expected more high-quality, thus the number of generating solutions M mentioned in Sec. 3.2 is set to 50 for saving computational cost and time consuming, which is different from Appendix B.2 that samples 100 solutions to train verifiers at the beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Baseline Settings</head><p>As shown in <ref type="table">Table 1</ref>, the Instruct GPT3 is based on text-davinci-002 version. Moreover, since <ref type="bibr" target="#b8">Kojima et al. (2022)</ref> provides difference prompt setting, we list them in <ref type="table" target="#tab_10">Table 7</ref>. For few-shot scenarios with chain of thought prompts, we follow the original paper <ref type="bibr" target="#b24">(Wei et al., 2022b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments</head><p>Since we treat GSM8K as our seed dataset for finetuning our models, we also provide the results of our method on it in <ref type="table" target="#tab_8">Table 6</ref>. Comparing to previous fine-tune SoTA <ref type="bibr" target="#b2">(Cobbe et al., 2021)</ref>, which applies a GPT3-175B as generator and a GPT3-175B as verifier, CoRe outperforms it with much less parameters, computation and inference time (It samples 100 solutions while we only search 40 paths  underperforms PaLM-540B that strenthened by self-consistency, and is better than the others, further proving the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparing our CoRe with popular methods in mathematical logic reasoning tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Cooperative reasoning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Step 2 :</head><label>2</label><figDesc>Cooperative inference by Trained System 1&amp;2</figDesc><table><row><cell>D: {Q}</cell><cell></cell><cell>RP with scores</cell></row><row><cell>Generating</cell><cell>System 1 (Generator)</cell><cell>System2 (V path )</cell></row><row><cell>Reasoning Steps ({Q})</cell><cell></cell><cell>Scoring</cell></row><row><cell></cell><cell></cell><cell>Reasoning Paths</cell></row><row><cell></cell><cell>System2</cell><cell></cell></row><row><cell></cell><cell>(V step )</cell><cell>Scoring</cell></row><row><cell></cell><cell></cell><cell>Reasoning Steps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot results v.s. previous fine-tuned SoTA results on math reasoning tasks. The previous SoTA baselines are obtained from:a:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">: Zero-shot results with different levels of guid-</cell></row><row><cell cols="2">ance from System 2.</cell><cell></cell><cell></cell></row><row><cell># of path</cell><cell>Strategy</cell><cell>SingleOp</cell><cell>MultiArith</cell></row><row><cell>5</cell><cell cols="2">best / vote 20.8 / 21.0</cell><cell>5.2 / 5.3</cell></row><row><cell>10</cell><cell cols="3">best / vote 46.3 / 45.4 27.0 / 27.2</cell></row><row><cell>20</cell><cell cols="3">best / vote 73.8 / 73.5 80.0 / 80.0</cell></row><row><cell>40</cell><cell cols="3">best / vote 80.1 / 82.9 95.5 / 96.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: CoRe v.s. CoT results on GSM8K. Results ob-</cell></row><row><cell>tained are from Cobbe et al. (2021); Wei et al. (2022b);</cell></row><row><cell>Wang et al. (2022).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>). With regard to few-shot methods applied on large PLMs with ? 100B parameters, CoRe only Backbone Method Reasoning Extraction Prompts Answer Extraction Prompts Instruct GPT3 175B zero-shot Let's think step by step. The answer (arabic numerals) is zero-shot * Let's think step by step. The answer is zero-shot-CoT Let's think step by step. The answer (arabic numerals) is zero-shot-CoT * Let's think step by step.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>The answer is</cell></row><row><cell>PaLM 540B</cell><cell>zero-shot</cell><cell>Let's think step by step.</cell><cell>The answer (arabic numerals) is</cell></row><row><cell></cell><cell>zero-shot-CoT</cell><cell>Let's think step by step.</cell><cell>The answer (arabic numerals) is</cell></row><row><cell></cell><cell>+ Self-Consistency</cell><cell>Let's think step by step.</cell><cell>The answer (arabic numerals) is</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Prompt setting for baselines inTable 1.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Dataset Details</head><p>The mathematical reasoning datasets with details are follows.</p><p>The dataset in Cooperative Training: GSM8K <ref type="bibr" target="#b2">(Cobbe et al., 2021</ref>) is a high-quality datasets with reasoning paths. It consists of 8.8K grade school math problems created by human writers, which is divided to a train set (7.5K) and a test set (1.3K). The reasoning paths includes 2 to 8 steps with considering basic arithmetic operations. Furthermore, we conduct cooperative training on its train set.</p><p>The datasets (Detailed descriptions in <ref type="table">Table 5</ref>) in zero-shot inference: ASDiv-A (Miao et al., 2020) includes diverse math word problems, which is required to answer a number for each question. SingleOP  is proposed with elementary math problems of single operation. SingleEq <ref type="bibr" target="#b9">(Koncel-Kedziorski et al., 2015)</ref> is construed with both single-step and multi-step math problems from mixed sources. MultiArith  includes elementary math problems with multiple steps.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<editor>Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/2110.14168</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Monte-carlo graph search for alphazero. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Czech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Korus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<idno>abs/2012.11045</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kahneman</forename><surname>Daniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In two minds: dual-process accounts of reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">B T</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2003.08.012</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="454" to="459" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deberta: decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bandit based monte-carlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levente</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesv?ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">4212</biblScope>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iwasawa</surname></persName>
		</author>
		<idno>abs/2205.11916</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena</forename><surname>Dumas Ang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00160</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mwptoolkit: An open-source framework for deep learningbased math word problem solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuai</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="13188" to="13190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the advance of making language models better reasoners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.02336</idno>
		<idno>abs/2206.02336</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A tag-based statistical english math word problem solver with understanding, reasoning and explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Yi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Tsung</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen-Yu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>IJCAI/AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4254" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A diverse corpus for evaluating and developing English math word problem solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Shen-Yun Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.92</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="975" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Show your work: Scratchpads for intermediate computation with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">I</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><forename type="middle">Johan</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>abs/2112.00114</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving coherence and consistency in neural sequence models with dualsystem, neuro-symbolic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">I</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Henry</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="25192" to="25204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1743" to="1752" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unit dependency graph and its application to arithmetic word problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3082" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasoning about quantities in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00118</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanya</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Zheng Xin Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trishala Neeraj</title>
		<editor>Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F?vry, Jason Alan Fries, Ryan Teehan</editor>
		<imprint/>
	</monogr>
	<note>and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization. CoRR, abs/2110.08207</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<title level="m">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/2203.11171</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/2109.01652</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.07682</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<publisher>Percy Liang</publisher>
		</imprint>
	</monogr>
	<note>and William Fedus. 2022a. Emergent abilities of large language models. CoRR, abs/2206.07682</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/2201.11903</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dataset # of samples Avg # of words in questions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Table 5: Dataset details. We take their test sets in our experiments</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
