<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Many Hands Make Light Work: Using Essay Traits to Automatically Score Essays</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">IIT Patna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Mathias</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
							<email>sriparna@iitp.ac.in&amp;sam</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT Patna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Many Hands Make Light Work: Using Essay Traits to Automatically Score Essays</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most research in the area of automatic essay grading (AEG) is geared towards scoring the essay holistically while there has also been some work done on scoring individual essay traits. In this paper, we describe a way to score essays holistically using a multi-task learning (MTL) approach, where scoring the essay holistically is the primary task, and scoring the essay traits is the auxiliary task. We compare our results with a single-task learning (STL) approach, using both LSTMs and BiLSTMs. We also compare our results of the auxiliary task with such tasks done in other AEG systems. To find out which traits work best for different types of essays, we conduct ablation tests for each of the essay traits. We also report the runtime and number of training parameters for each system. We find that MTL-based BiLSTM system gives the best results for scoring the essay holistically, as well as performing well on scoring the essay traits. The MTL systems also give a speed-up of between 2.30 to 3.70 times the speed of the STL system, when it comes to scoring the essay and all the traits.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An essay is a piece of text that is written in response to a topic, called a prompt <ref type="bibr" target="#b16">(Mathias and Bhattacharyya 2020)</ref>. Qualitative evaluation of the essay consumes a lot of time and resources. Hence, in 1966, Page proposed a method of automatically scoring essays using computers <ref type="bibr" target="#b19">(Page 1966)</ref>, giving rise to the domain of Automatic Essay Grading.</p><p>Essay traits are different aspects of the essay that can aid in explaining the score assigned to the essay. Examples of essay traits include content (how much information is present in the essay) <ref type="bibr" target="#b19">(Page 1966)</ref>, organization (how well the essay is structured) <ref type="bibr" target="#b21">(Persing, Davis, and Ng 2010)</ref>, style (how well written the essay is) <ref type="bibr" target="#b19">(Page 1966)</ref>, prompt adherence (how much the essay stays on topic for the essay prompt) <ref type="bibr" target="#b23">(Persing and Ng 2014)</ref>, etc.</p><p>Most of the research work done in the field of AEG is geared toward scoring the essay holistically, rather than studying the importance of essay traits in the overall essay score. In this paper, we ask the question:</p><p>"Can we use information learnt from scoring essay traits to score an essay holistically?"</p><p>Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>In our paper, not only do we score essays holistically, but we also describe how to score essay traits simultaneously in a multi-task learning framework. Scoring essay traits is essential as it could help in explaining why the essay was scored the way it was, as well as providing valuable insight to the writer about what aspects of the essay were wellwritten and what the writer needs to improve.</p><p>Multi-task learning is a machine learning technique where we use information from multiple auxiliary tasks to perform a primary task <ref type="bibr" target="#b1">(Caruana 1997)</ref>. In our experiments, scoring the individual essay traits is the auxiliary task, and scoring the essay holistically is the primary task.</p><p>Contributions. In this paper, we describe a way to simultaneously score essay traits and the essay itself using multitask learning. We evaluate our system against different types of essays and essay traits. We also share our code and the data for reproducibility and further research.</p><p>Organization of the Paper. The rest of the paper is organized as follows. The motivation for our work is described in Section 2. We describe related work in Section 3. We describe our system's architecture and dataset in Sections 4 and 5 respectively. We describe our experiment setup in Section 6. We report our results and analyze them in Section 7. Finally, we conclude our paper and describe future work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Most of the work done in the area of automatic essay grading is in the area of holistic AEG -where we provide a single score for the entire essay based on its quality. However, for writers of an essay, a holistic score alone would not be enough. Providing trait-specific scores will tell the writer which aspects of the essay need improvement.</p><p>In our dataset, we observe that writers of good essays usually have a lot of content, appropriate word choice, very few errors, etc. Essays that are poorly written often lack one or more of these qualities (i.e. they are either too short, have lots of errors, etc.). We, therefore, observe a high correlation between individual trait scores and the overall essay score (Pearson correlation trait scores and overall essay score &gt; 0.7 across all essay sets in our dataset). Hence, we believe that using essay trait scores will benefit in scoring the essay holistically, as their scores will provide more relevant information to the AEG system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>In this section, we describe related work in the area of automatic essay grading and multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Holistic Essay Grading</head><p>Holistic essay grading involves assigning an overall score for an essay <ref type="bibr" target="#b16">(Mathias and Bhattacharyya 2020)</ref>. The first AEG system was designed by <ref type="bibr" target="#b19">Page (1966)</ref>. In the decade of the 2000s there were a lot of AEG systems which were developed commercially (see <ref type="bibr" target="#b27">Shermis and Burstein (2013)</ref> for more details).</p><p>After the release of Kaggle's Automatic Student Assessment Prize's (ASAP) Automatic Essay Grading (AEG) dataset in 2012 1 , there has been a lot of research on holistic essay grading. Initial approaches, such as those of <ref type="bibr" target="#b26">Phandi, Chai, and Ng (2015)</ref> and <ref type="bibr" target="#b34">Zesch, Heilman, and Cahill (2015)</ref> used machine learning techniques in scoring the essays. More recent papers look at using a number of deep learning approaches, such as LSTMs <ref type="bibr" target="#b32">(Taghipour and Ng 2016;</ref><ref type="bibr" target="#b33">Tay et al.)</ref> and CNNs <ref type="bibr" target="#b8">(Dong and Zhang 2016)</ref> or both <ref type="bibr" target="#b9">(Dong, Zhang, and Yang 2017;</ref><ref type="bibr">Litman 2018, 2020)</ref>. <ref type="bibr" target="#b36">Zhang and Litman (2020)</ref> describe a way to extract important information, called topical components, from a sourcedependent response 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Trait-specific Essay Grading</head><p>In the last decade or so, there has been some work done in scoring essay traits such as sentence fluency <ref type="bibr" target="#b2">(Chae and Nenkova 2009</ref>), organization <ref type="bibr" target="#b21">(Persing, Davis, and Ng 2010;</ref><ref type="bibr" target="#b31">Taghipour 2017;</ref><ref type="bibr" target="#b17">Mathias et al. 2018;</ref><ref type="bibr" target="#b30">Song et al. 2020)</ref>, thesis clarity <ref type="bibr" target="#b22">(Persing and Ng 2013;</ref><ref type="bibr" target="#b12">Ke et al. 2019</ref>) coherence <ref type="bibr" target="#b28">(Somasundaran, Burstein, and Chodorow 2014;</ref><ref type="bibr" target="#b17">Mathias et al. 2018)</ref>, prompt adherence <ref type="bibr" target="#b23">(Persing and Ng 2014)</ref>, argument strength <ref type="bibr" target="#b24">(Persing and Ng 2015;</ref><ref type="bibr" target="#b31">Taghipour 2017)</ref>, stance <ref type="bibr" target="#b25">(Persing and Ng 2016)</ref>, style <ref type="bibr" target="#b15">(Mathias and Bhattacharyya 2018b</ref>) and narrative quality <ref type="bibr" target="#b29">(Somasundaran et al. 2018)</ref>. None of the above work, however, uses trait information to score the essay holistically.</p><p>There has also been work on scoring multiple essay traits <ref type="bibr" target="#b31">(Taghipour 2017;</ref><ref type="bibr">Bhattacharyya 2018a, 2020)</ref>. <ref type="bibr" target="#b16">Mathias and Bhattacharyya (2020)</ref> describes work on the use of neural networks for scoring essay traits. Our work combines the scores of essay traits for holistic essay grading. We focus on using trait-specific essay grading to improve the performance of an automatic essay grading system. We also show how using multi-task learning-simultaneously scoring both the essay and its traits-we are able to speed up the training of our system without too much of a loss in scoring the essay traits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-task Learning</head><p>Multitask Learning was proposed by <ref type="bibr" target="#b1">Caruana (1997)</ref> where the argument was that training signals from related tasks could help in a better generalization of the model. <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> successfully demonstrated how tasks like Partof-Speech tagging, chunking and Named Entity Recognition can help each other when trained jointly using deep neural networks. <ref type="bibr" target="#b30">Song et al. (2020)</ref> described a multi-task learning approach to score organization in essays, where the auxiliary tasks were classifying the sentences and paragraphs, and the primary task was scoring the essay's organization. <ref type="bibr" target="#b0">Cao et al. (2020)</ref> also use a domain adaptive MTL approach to grade essays, where their auxiliary tasks are sentence reordering, noise identification, as well as domain adversarial training. However, they also use all the other essay sets as part of their training, whereas we use only the essays present in the respective essay set for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System Architecture</head><p>In this section, we describe the architecture of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">STL Essay Grading Stack</head><p>For scoring the essays, we use essay grading stacks. Each stack is used for scoring a single essay trait. The architecture of the stack is based on the architecture of the holistic essay grading system proposed by <ref type="bibr" target="#b9">Dong, Zhang, and Yang (2017)</ref>. The essay grading stack takes the essay as input (split into tokens and sentences) and returns the score of the essay / essay trait as the output. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture for the essay grading stack.</p><p>For each essay, we first split the essay into tokens and sentences. This is given as an input to the essay grading stack. In the word embedding layer, we look up the word embeddings of each token. Just like <ref type="bibr" target="#b32">Taghipour and Ng (2016)</ref>, <ref type="bibr" target="#b9">Dong, Zhang, and Yang (2017)</ref>, <ref type="bibr">Tay et al., and Mathias and Bhattacharyya (2020)</ref>, we use the most frequent 4000 words as the vocabulary with all other words mapping to a special unknown token. This sequence of word embeddings is then sent to the next layer -the 1 dimension CNN layer -to get local information from nearby words. The output of the CNN layer is aggregated using attention pooling to get the sentence representation of the sentence. This is done for every sentence in the essay.</p><p>Each of the sentence representations are then sent through a recurrent layer. We experiment on two different types of recurrent layers -a unidirectional LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber 1997)</ref> and bidirectional LSTM (BiLSTM) -as the type of recurrent layer. The outputs of the recurrent layer are pooled using attention pooling to get the representation for the essay. This essay representation is then sent through a fully-connected Dense layer with a sigmoid activation function to score the essay either holistically or a particular essay trait. For our experiments, we minimize the mean squared error loss.</p><p>Prior to input, we scale the scores to the range of [0, 1] using min-max normalization. The output of the sigmoid function is a scalar in the range of [0, 1] which is rescaled back up to a score in the original score range and rounded off to get the score for the essay. This essay stack is used for the scoring of the single-task learning (STL) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MTL Model</head><p>The architecture of our MTL model for an essay of M traits is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Here, the word embedding layer is shared across all the tasks. In the multi-task learning framework, each stack is used to learn an essay representation for each essay trait. In a similar manner, the essay representation for the overall score is learnt and it is concatenated with the predicted trait scores before being sent to a Dense layer with a sigmoid activation function to score the essay holistically. For calculating each score -both overall and trait scores -we use the mean squared error loss function. We experimented with multiple weights for the loss function for the essay trait scoring task, but settled on uniform weights for all the traits and the overall scoring task 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset Used</head><p>For our experiments, we use the Automated Student's Assessment Prize (ASAP) Automatic Essay Grading (AEG) dataset. The dataset has a total of 8 essay sets -where each essay set has a number of essays written in response to the same essay prompt. In total, there are nearly 13,000 essays in the dataset. Each of the essays was written by high school students belonging to classes 7 to 10. <ref type="table" target="#tab_0">Table 1</ref> gives the properties of each of the essay sets in our dataset. It reports the overall essay scoring range, traits</p><p>We use the overall scores directly from the ASAP AEG dataset. Since the original dataset only provided traitspecific scores for Prompts 7 &amp; 8, we use the trait-specific scores provided by <ref type="bibr" target="#b14">Mathias and Bhattacharyya (2018a)</ref>.</p><p>Depending on the type of prompt for the essay set, each essay set has a different set of traits. Argumentative / Persuasive essays are essays which the writer is prompted to take a stand on a topic and argue for their stance. These essay sets have traits like content, organization, word choice, sentence fluency, and conventions. Source-dependent responses <ref type="bibr" target="#b35">(Zhang and Litman 2018)</ref> are essays where the writer reads a piece of text and answers a question based on the text that they just read 4 . These essay sets have traits like content, prompt adherence <ref type="bibr" target="#b23">(Persing and Ng 2014)</ref>, language and narrativity <ref type="bibr" target="#b29">(Somasundaran et al. 2018)</ref>. Narrative / Descriptive essays are essays where the writer has to narrate a story or incident or anecdote. They have traits like content, organization, style, conventions, voice, word choice, and sentence fluency 5 . <ref type="table" target="#tab_2">Table 2</ref> lists the different essay traits for each essay set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we describe our methodology and evaluation metric, as well as experiment configurations and network hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Metric</head><p>We use Cohen's Kappa with quadratic weights <ref type="bibr" target="#b3">(Cohen 1968</ref>) (QWK) as the evaluation metric. This is done for the following reasons. Firstly, the final scores predicted by the system are distinct numbers/grades, rather than continuous values; so we cannot use the Pearson Correlation Coefficient or Mean Squared Error. Secondly, evaluation metrics like F-Score and accuracy do not take into account chance agreements. For example, if we are to grade every essay with the mean score or most frequent score, we would get F-Score and accuracy as high as 60% or more, whereas the Kappa score will be 0! Thirdly, the fact that the scores given are ordered (i.e. 0 &lt; 1 &lt; 2 &lt; 3...) means that we need to use weighted Kappa to capture the distance between the actual and predicted scores. Between linear weighted Kappa and QWK, we choose QWK because it rewards matches and punishes mismatches more distinctly than linear weighted Kappa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Method</head><p>We evaluate our experiments using five-fold cross validation, where, for each essay set, we use 60% of the data as training data, 20% as development/validation data, and the remaining 20% as testing data. We use the same data splits as used by <ref type="bibr" target="#b32">Taghipour and Ng (2016)</ref>. To avoid overfitting, for each fold, we choose the model which gives the best result on the validation set for evaluating on the test set. <ref type="table" target="#tab_3">Table 3</ref> gives the different hyperparameters used in our systems. For the sake of uniformity, we use these hyperparameters irrespective of the network configuration (STL vs MTL, or LSTM vs BiLSTM).   We use the GloVe (Pennington, Socher, and Manning 2014) pre-trained word-embeddings, trained on the Wikpedia 2014 + Gigaword 5 corpus (6 billion tokens, 400K vocabulary, uncased 50 dimensions) 6 . For the Word-level CNN layer, we use a window size of 5, with a filter of size 100 and for the Sentence-level recurrent layer (LSTM / BiL-STM), we have 100 hidden units. We run our experiments for 100 epochs over a batch size of 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Network Hyperparameters</head><p>We use the RMSProp Optimizer (Dauphin, De Vries, and Bengio 2015) with an initial learning rate of 0.001 and a momentum of 0.9 and a dropout rate of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Configurations</head><p>To evaluate the performance of our systems in scoring the essay overall, we use 4 different configurations-STL-LSTM, STL-BiLSTM, MTL-LSTM, and MTL-BiLSTM. In addition to the above systems, we also compare our approach with a state-of-the-art string kernel system designed by <ref type="bibr" target="#b5">Cozma, Butnaru, and Ionescu (2018)</ref>, using the same splits for training, testing, and validation 7 .</p><p>In the STL configurations, we train our system to predict <ref type="bibr">6</ref> We used GloVe instead of BERT <ref type="bibr" target="#b7">(Devlin et al. 2019)</ref> due to the fact that BERT's training cost was far too large with little or no increase in performance for AEG <ref type="bibr" target="#b18">(Mayfield and Black 2020)</ref>. <ref type="bibr">7</ref>  <ref type="bibr" target="#b5">Cozma, Butnaru, and Ionescu (2018)</ref> do not provide their folds, so we run their system on our training/validation/test split, as given by <ref type="bibr" target="#b32">Taghipour and Ng (2016).</ref> a single score at a time-either the overall essay score or the score for any of the essay traits. In the MTL configurations, our system learns to score the essay and all its traits simultaneously. The LSTM configurations use only a forward direction LSTM, while the BiLSTM configurations use a bidirectional (i.e. forward and reverse) LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Analysis</head><p>In this section, we report our results and analyze them for different experiments. <ref type="table">Table 4</ref> gives the QWK scores of each of our systems as they score each essay set holistically. The different systems used are the Single Task Learning (STL) (only scoring the essay overall) and Multitask Learning (MTL) (scoring the essay and the traits simultaneously). The first column lists out the different essay sets (Prompts 1 to 8). The next three columns report results for STL using both LSTM and BiL-STM, as well as results using the string kernel-based approach of <ref type="bibr" target="#b5">Cozma, Butnaru, and Ionescu (2018)</ref>. The next two columns report results for the MTL systems using both LSTM and BiLSTM. The last column shows the results reported in a state-of-the-art system using neural networks by <ref type="bibr">Tay et al..</ref> From the table, we see that the MTL-BiLSTM performs the best of all the systems (as good as the results of <ref type="bibr">Tay et al.)</ref>. In order to see if the improvements observed are statistically significant, we run the Paired T-Test for each of the essay sets and compare the results using a p-value of p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Performance on Holistic Essay Scoring</head><p>We test each of the essay sets in both the MTL systems -MTL-LSTM and MTL-BiLSTM -against the bestperforming STL system which we implemented, the STL-LSTM system 8 . We mark results which are statistically significant with a * next to the QWK value in <ref type="table">Table 4</ref>. Similarly, we also test whether or not the improvements we achieved using the BiLSTM layer instead of the LSTM layer are statistically significant. We mark improvements which are statistically significant in the MTL-BiLSTM column, over the MTL-LSTM column with a .  <ref type="table">Table 4</ref>: Results of our experiments for scoring the essays holistically. The first column is the different essay sets. The next three columns represent the baseline results for holistic essay grading using single-task learning. Next two columns are results using multi-task learning. The final column is the results as reported from another state-of-the-art system by <ref type="bibr">Tay et al.. Figures</ref> in boldface represent the best results per essay set. * represents a statistically significant improvement using the MTL systems over the STL-LSTM system. represents a statistically significant improvement of using the MTL-BiLSTM system over the MTL-LSTM system. <ref type="figure">Figure 3</ref>: Results of the performance of scoring traits for different systems. NOTE that the y-axis starts from a QWK of 0.4. This was done to highlight the difference in the performance of each system for each trait.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Performance on Scoring Essay Traits</head><p>We also look at how our system performs in the auxiliary tasks -namely scoring the different essay traits. <ref type="figure">Figure  3</ref> gives the results of our experiments in scoring the essay traits, using a Random Forest (Mathias and Bhattacharyya 2018a), the String Kernel (HIST + SVR) <ref type="bibr" target="#b5">(Cozma, Butnaru, and Ionescu 2018)</ref>, CNN-LSTM (STL) <ref type="bibr" target="#b9">(Dong, Zhang, and Yang 2017)</ref>, and Our System (CNN-BiLSTM (MTL)). We use the same evaluation method, which we used for scoring essay traits, with the same data splits. For the STL systems, we train them for every essay trait individually.</p><p>We compare the results with that of our MTL-BiLSTM system, which was trained to score the essay traits as auxiliary tasks. <ref type="figure">Figure 3</ref> gives the results of our experiments. From the figure, we see that, while the STL-LSTM system is able to outperform our MTL-BiLSTM system, for most of the traits, the MTL-BiLSTM system performs about 97% as good as the STL-LSTM system. The main reason for this difference in performance is that the STL-LSTM system tries to optimize for scoring the trait alone, while our MTL system tries to optimize for scoring the essay holistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Ablation Tests</head><p>In order to know which trait is most important for each essay set, we run a series of ablation tests. For each essay set, we ablate one essay trait at a time before scoring the essay. <ref type="table" target="#tab_6">Table 5</ref> reports the results of the ablation test. The values in the table correspond to the drop in performance in scoring the essay holistically. We find that the Content is the most important essay trait for 3 of the essay sets. Prompt Adherence and Word Choice are the most important traits for 2 of the essay sets where they are scored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Error Analysis</head><p>As we have seen, the MTL model generally helps over the STL model when it comes to holistic essay scoring, especially if there is no well-defined rule (Example: Holistic Score = Sum of trait scores) for scoring the essay holistically.</p><p>A possible scenario where STL could help over MTL is if the holistic score is a well-defined function of the trait scores AND the STL system can predict the trait scores with a good deal of accuracy. The essay sets corresponding to Prompts 7 &amp; 8 are two such essay sets, where the overall score is a function of the individual trait scores. To verify this, we ran the experiments in a pipelined manner -first scoring the essay traits, then calculating the holistic score using the predicted trait scores and comparing it with the gold standard holistic scores. We found no difference in QWK for Prompt 7 (a QWK of 0.796 vs. 0.795), but a much lesser performance with Prompt 8 (a QWK of 0.684 vs. 0.699) as compared to our MTL-based system. One of the main reasons for this is due to the poor performance in predicting the trait scores as single tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Runtime Analysis</head><p>We also ran experiments to see how much resources and time our approaches will take. <ref type="table" target="#tab_7">Table 6</ref> gives the total training    time (in hours). The total training time is the total time taken to train our system to score the essay holistically as well as all the traits in that essay set for all 100 epochs. We also report the speed-up when using the MTL approach as compared to the STL approach. From our results, we observe a 2.30 to 3.70 speed-up in using the MTL models as compared to using the STL models. We also report the average number of training parameters per system in <ref type="table" target="#tab_8">Table 7</ref>. For the STL systems, the number of trainable parameters is the same irrespective of essay set. For the MTL systems, the models, the number of training parameters varies based on the number of essay traits in the essay set. Prompts 3 to 7, which have only 4 traits, have about 1.38 million training parameters. On the other hand, Prompt 8, which has 6 essay traits, has over 1.85 million training parameters.</p><formula xml:id="formula_0">0.0062 - - Style - - - - - - 0.0030 - Voice - - - - - - - 0.0094</formula><p>All our experiments were run on an nVidia GeForce GTX 1080 Ti Graphics Card, using Python version 3.5.2, Keras version 2.2.4 and Tensorflow version 1.14. The essays are tokenized and sentence split using NLTK version 3.4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper, we described an approach to use multi-task learning to automatically score essays and their traits. We achieve this by concatenating a representation of the essay with the trait scores -predicted as an auxiliary task. We compared our results with single-task learning models as well. We found out that the MTL system with the Bi-Directional LSTM outperforms the STL-based systems and has results comparable with the state-of-the-art system of Tay et al.. Next, we evaluated our system's performance in scoring individual essay traits and found that its performance is close to that of the STL systems. We then ran an ablation test and found out which essay trait was important for the corresponding essay sets. We also report our system's performance, which shows a 2.30 to 3.70 speed-up of using the multi-task learning system, compared to using a single task learning system.</p><p>An exciting avenue of future work is using trait scoring to aid in providing text feedback to the writer, like showing where the low score for the trait originates, similar to <ref type="bibr" target="#b10">Hellman et al. (2020)</ref> (for content scoring), rather than a trait-specific score only. We also plan to explore how we can use this approach in the area of cross-domain AEG, where we train our system using essays written in response to one prompt, and test it on essays written in another prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This submission protects the privacy of the writers of the essays. All named entities, dates, numbers, etc. which could identify the students who wrote the almost 13,000 essays as part of the data set are anonymized (as per the original ASAP AEG dataset). No attempt has been made to solicit their identities.</p><p>The trait-specific data, collected by Mathias and Bhattacharyya (2018a) was also collected in an ethical manner. Annotators for that task were adequately compensated INR 5 per essay for annotation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Essay stack architecture. This is the architecture for the Single-Task Learning systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of our MTL system showing an input essay with M traits being scored, with the overall score and each trait's essay grading stack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Essay Set Score Range Trait Sc. Range Word Count No. of Traits No. of Essays</figDesc><table><row><cell>Essay Type</cell></row></table><note>: Properties of the different essay sets in the ASAP AEG dataset we used in our experiments. Average word count numbers are rounded up to the nearest multiple of 25.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Traits that are present in each essay set in our dataset. The trait scores are taken from the original ASAP dataset, as well as from ASAP++<ref type="bibr" target="#b14">Mathias and Bhattacharyya (2018a)</ref>.</figDesc><table><row><cell>Layer</cell><cell>Param. Name</cell><cell>Param. Value</cell></row><row><cell>Embedding</cell><cell>Embedding Dim. Embeddings</cell><cell>50 GloVe</cell></row><row><cell>Word CNN</cell><cell>Window Size Filters</cell><cell>5 100</cell></row><row><cell cols="2">Sentence LSTM Hidden Units</cell><cell>100</cell></row><row><cell></cell><cell>Epochs</cell><cell>100</cell></row><row><cell></cell><cell>Batch Size</cell><cell>100</cell></row><row><cell></cell><cell>Dropout Rate</cell><cell>0.5</cell></row><row><cell></cell><cell>Initial Learning Rate</cell><cell>0.001</cell></row><row><cell></cell><cell>Momentum</cell><cell>0.9</cell></row><row><cell></cell><cell>Optimizer</cell><cell>RMSProp</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Neural network hyper-parameters for each layer, showing the hyper-parameter name and its corresponding value.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results of the ablation tests. The numbers show the drop in performance when we ablate each of the essay traits from each of the essay sets(Prompt 1 to 8). The most important features in each essay set are written in boldface and underlined. Cells with a -in them mean that the essay trait was not present in that essay set.</figDesc><table><row><cell>System</cell><cell cols="2">STL Time MTL Time Speed-Up</cell></row><row><cell>LSTM</cell><cell>24.62 hours 10.45 hours</cell><cell>2.30</cell></row><row><cell cols="2">BiLSTM 40.98 hours 11.32 hours</cell><cell>3.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Total training time for each system for all prompts, traits and folds, using our neural network systems.</figDesc><table><row><cell>System</cell><cell>Average</cell><cell>Range</cell></row><row><cell>STL-LSTM</cell><cell>326K</cell><cell>326K</cell></row><row><cell>STL-BiLSTM</cell><cell>436K</cell><cell>436K</cell></row><row><cell>MTL-LSTM</cell><cell>891K</cell><cell>829K -1.08M</cell></row><row><cell>MTL-BiLSTM</cell><cell>1.5M</cell><cell>1.38M -1.85M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Average and range of training parameters per essay set for each system.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.kaggle.com/c/asap-aes2  We define what a source-dependent response is in the Dataset Section (i.e. Section 5).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This is done because we want to get accurate predictions of the traits scores which are used for predicting the overall score. scoring, average word count, number of traits, number of essays and essay type.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A sample prompt is "Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt." It involves the writer reading the excerpt from The Empire State Building by Marcia Amidon Lusted before writing the essay.5  Neither the original ASAP dataset, nor Mathias and Bhattacharyya (2018a) have scored narrativity for the narrative essays.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We do not compare it with Tay et al.'s system since they have not released their code.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Domain-Adaptive Neural Automated Essay Scoring. 1011-1020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>: Association for Computing Machinery. ISBN 9781450380164</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting the Fluency of Text with Shallow Structural Features: Case Studies of Machine Translation and Human-Written Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter</title>
		<meeting>the 12th Conference of the European Chapter<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">213</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated essay scoring with string kernels and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cozma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butnaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="503" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Equilibrated adaptive learning rates for non-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1504" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic Features for Essay Scoring -An Empirical Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1072" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple Instance Learning for Content Feedback Localization without Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hellman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiemerslage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Derr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Seattle, WA, USA (Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="30" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>0899-7667</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Give Me More Feedback II: Annotating Thesis Strength and Related Attributes in Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inamdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3994" to="4004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ASAP++: Enriching the ASAP Automated Essay Grading Dataset with Essay Attribute Scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC-2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thank &quot;Goodness&quot;! A Way to Measure Style in Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</title>
		<meeting>the 5th Workshop on Natural Language Processing Techniques for Educational Applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can Neural Networks Automatically Score Essay Traits?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Seattle, WA, USA (Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2352" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Should You Fine-Tune BERT for Automated Essay Scoring?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Seattle, WA, USA (Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The imminence of... grading essays by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Phi Delta Kappan</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="238" to="243" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling Organization in Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling Thesis Clarity in Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling Prompt Adherence in Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling Argument Strength in Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="543" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling Stance in Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Handbook of Automated Essay Evaluation: Current Applications and New Directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Shermis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lexical Chaining for Measuring Discourse Coherence Quality in Test-taker Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Dublin City University and Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="950" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards Evaluating Narrative Quality In Student Writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gyawali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcculla</surname></persName>
		</author>
		<idno>2307-387X</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical Multi-task Learning for Organization Evaluation of Argumentative Student Essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>Bessiere, C.</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3875" to="3881" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Robust Trait-Specific Essay Scoring Using Neural Networks and Density Estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Taghipour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Neural Approach to Automated Essay Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1882" to="1891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>???? Skipflow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reducing Annotation Efforts in Supervised Short Answer Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="124" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Co-Attention Based Neural Network for Source-Dependent Essay Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Seattle, WA, USA (Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8569" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
