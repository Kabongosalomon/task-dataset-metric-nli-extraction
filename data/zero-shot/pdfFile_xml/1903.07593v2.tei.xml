<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Correspondence from the Cycle-consistency of Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<email>xiaolonw@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
							<email>ajabri@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
							<email>efros@eecs.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Correspondence from the Cycle-consistency of Time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We introduce a self-supervised method for learning visual correspondence from unlabeled video. The main idea is to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch. At training time, our model learns a feature map representation to be useful for performing cycle-consistent tracking. At test time, we use the acquired representation to find nearest neighbors across space and time. We demonstrate the generalizability of the representation -without finetuning -across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow. Our approach outperforms previous self-supervised methods and performs competitively with strongly supervised methods. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Motivation</head><p>It is an oft-told story that when a young graduate student asked Takeo Kanade what are the three most important problems in computer vision, Kanade replied: "Correspondence, correspondence, correspondence!" Indeed, most fundamental vision problems, from optical flow and tracking to action recognition and 3D reconstruction, require some notion of visual correspondence. Correspondence is the glue that links disparate visual percepts into persistent entities and underlies visual reasoning in space and time.</p><p>Learning representations for visual correspondence, from pixel-wise to object-level, has been widely explored, primarily with supervised learning approaches requiring large amounts of labelled data. For learning low-level correspondence, such as optical flow, synthetic computer graphics data is often used as supervision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62]</ref>, limiting generalization to real scenes. On the other hand, approaches for learning higher-level semantic correspondence rely on human annotations <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b64">65]</ref>, which becomes prohibitively expensive at large scale. In this work, our aim is to learn representations that support reasoning at various levels of visual correspondence <ref type="figure" target="#fig_0">(Figure 1</ref>) from scratch and without human supervision.</p><p>A fertile source of free supervision is video. Because the world does not change abruptly, there is inherent visual correspondence between observations adjacent in time. The problem is how to find these correspondences and turn Start Learning Signal End t -1 t -2 t <ref type="figure">Figure 2</ref>: A Cycle in Time. Given a video, tracking along the sequence formed by a cycle in time can be self-supervised: the target is simply the beginning of the cycle. The yellow arrow between the start and end represents the differentiable learning signal.</p><p>them into a learning signal. In a largely static world observed by a stationary camera, such as a webcam trained on the Eiffel Tower, correspondence is straightforward because nothing moves and capturing visual invariance (to weather, lighting) amounts to supervised metric learning. In the dynamic world, however, change in appearance is confounded by movement in space. Finding correspondence becomes more difficult because capturing visual invariance now requires learning to track, but tracking relies on a model of visual invariance. This paper proposes to learn to do both simultaneously, in a self-supervised manner.</p><p>The key idea is that we can obtain unlimited supervision for correspondence by tracking backward and then forward (i.e. along a cycle in time) and using the inconsistency between the start and end points as the loss function ( <ref type="figure">Figure 2</ref>). We perform tracking by template-matching in a learned deep feature space. To minimize the loss -i.e. to be cycle-consistent -the model must learn a feature representation that supports identifying correspondences across frames. As these features improve, the ability to track improves, inching the model toward cycle-consistency. Learning to chain correspondences in such a feature space should thus yield a visual similarity metric tolerant of local transformations in time, which can then be used at test-time as a stand-alone distance metric for correspondence.</p><p>While conceptually simple, implementing objectives based on cycle-consistency can be challenging. Without additional constraints, learning can take shortcuts, making correspondences cycle-consistent but wrong <ref type="bibr" target="#b87">[88]</ref>. In our case, a track that never moves is inherently cycle-consistent. We avoid this by forcing the tracker to re-localize the next patch in each successive frame. Furthermore, cycleconsistency may not be achievable due to sudden changes in object pose or occlusions; skip-cycles can allow for cycleconsistency by skipping frames, as in <ref type="figure" target="#fig_2">Figure 3</ref> (right). Finally, correspondence may be poor early in training, and shorter cycles may ease learning, as in <ref type="figure" target="#fig_2">Figure 3</ref> (left). Thus, we simultaneously learn from many kinds of cycles to induce a natural curriculum and provide better training data.</p><p>The proposed formulation can be used with any differentiable tracking operation, providing a general framework for learning representations for visual correspondence from raw video. Because the method does not rely on human annotation, it can learn from the near infinite video data available online. We demonstrate the usefulness of the learned features for tasks at various levels of visual correspondence, ranging from pose, keypoint, and segmentation propagation (of objects and parts) to optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Temporal Continuity in Visual Learning. Temporal structure serves as a useful signal for learning because the visual world is continuous and smoothly-varying. Spatiotemporal stability is thought to play a crucial role in the development of invariant representations in biological vision <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref>. For example, Wood <ref type="bibr" target="#b76">[77]</ref> showed that for newborn chicks raised in a visual world that was not temporally smooth, object recognition abilities were severely impaired. Computational approaches for unsupervised learning have sought to leverage this continuity, such as continuous transformation learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b69">70]</ref>, "slow" feature learning <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b24">25]</ref> and information maximization between neighbouring patches in time <ref type="bibr" target="#b65">[66]</ref>. Our work can be seen as slow feature learning with fixation, learned end-toend without supervision.</p><p>Self-supervised Representation Learning from Video. Learning representations from video using time as supervision has been extensively studied, both as future prediction task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42]</ref> as well as motion estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. Our approach is most related to the methods of Wang et al. <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74]</ref> and Pathak et al. <ref type="bibr" target="#b46">[47]</ref>, which use off-the-shelf tools for tracking and optical flow respectively, to provide supervisory signal for training. However, representations learned in this way are inherently limited by the power of these off-the-shelf tools as well as their failure modes. We address this issue by learning the representation and the tracker jointly, and find the two learning problems  Our solution is to optimize multiple cycles of different lengths simultaneously. This allows learning from shorter cycles when the full cycle is too difficult (left). This also allows cycles that skip frames, which can deal with momentary occlusions (right).</p><p>to be complementary. Our work is also inspired by the innovative approach of Vondrick et al <ref type="bibr" target="#b68">[69]</ref> where video colorization is used as a pretext self-supervised task for learning to track. While the idea is very intriguing, in Section 4 we find that colorization is a weaker source of supervision for correspondence than cycle-consistency, potentially due to the abundance of constant-color regions in natural scenes.</p><p>Tracking. Classic approaches to tracking treat it as a matching problem, where the goal is to find a given object/patch in the next frame (see <ref type="bibr" target="#b10">[11]</ref> for overview), and the key challenge is to track reliably over extended time periods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref>. Starting with the seminal work of Ramanan et al. <ref type="bibr" target="#b48">[49]</ref>, researchers largely turned to "tracking as repeated recognition", where trained object detectors are applied to each frame independently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b64">65]</ref>. Our work harks back to the classic tracking-bymatching methods in treating it as a correspondence problem, but uses learning to obtain a robust representation that is able to model wide range of appearance changes.</p><p>Optical Flow. Correspondence at the pixel level -mapping where each pixel goes in the next frame -is the optical flow estimation problem. Since the energy minimization framework of Horn and Schunck <ref type="bibr" target="#b19">[20]</ref> and coarseto-fine image warping by Lucas and Kanade <ref type="bibr" target="#b40">[41]</ref>, much progress has been made in optical flow estimation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62]</ref>. However, these methods still struggle to scale to long-range correspondence in dynamic scenes with partial observability. These issues have driven researchers to study methods for estimating long-range optical flow <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b33">34]</ref>. For example, Brox and Malik <ref type="bibr" target="#b4">[5]</ref> introduced a descriptor that matches region hierarchies and provides dense and subpixel-level estimation of flow. Our work can be viewed as enabling mid-level optical flow estimation.</p><p>Mid-level Correspondence. Given our focus on finding correspondence at the patch level, our method is also related to the classic SIFT Flow <ref type="bibr" target="#b38">[39]</ref> algorithm and other methods for finding mid-level correspondences between re-gions across different scenes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b86">87]</ref>. More recently, researchers have studied modeling correspondence in deep feature space <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>. In particular, our work draws from Rocco et al. <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>, who propose a differentiable soft inlier score for evaluating quality of alignment between spatial features and provides a loss for learning semantic correspondences. Most of these methods rely on learning from simulated or large-scale labeled datasets such as ImageNet, or smaller custom human-annotated data with narrow scope. We address the challenge of learning representations of correspondence without human annotations.</p><p>Forward-Backward and Cycle Consistency. Our work is influenced by the classic idea of forward-backward consistency in tracking <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref>, which has long been used as an evaluation metric for tracking <ref type="bibr" target="#b26">[27]</ref> as well as a measure of uncertainty <ref type="bibr" target="#b0">[1]</ref>. Recent work on optical flow estimation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b44">45]</ref> also utilizes forward-backward consistency as an optimization goal. For example, Meister et al. <ref type="bibr" target="#b44">[45]</ref> combines one-step forward and backward consistency check with pixel reconstruction loss for learning optical flows. Compared to pixel reconstruction, modeling correspondence in feature space allows us to follow and learn from longer cycles. Forward-backward consistency is a specific case of cycle-consistency, which has been widely applied as a learning objective for 3D shape matching <ref type="bibr" target="#b20">[21]</ref>, image alignment <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b87">88</ref>], depth estimation <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b83">84]</ref>, and image-to-image translation <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b3">4]</ref>. For example Zhou et al. <ref type="bibr" target="#b87">[88]</ref> used 3D CAD models to render two synthetic views for pairs of training images and construct a correspondence flow 4-cycle. To the best of our knowledge, our work is the first to employ cycle-consistency across multiple steps in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>An overview of the training procedure is presented in <ref type="figure" target="#fig_4">Figure 4a</ref>. The goal is to learn a feature space ? by tracking a patch p t extracted from image I t backwards and then forwards in time, while minimizing the cycle-consistency loss l ? (yellow arrow). Learning ? relies on a simple tracking A t-1,t operation T , which takes as inputs the features of a current patch and a target image, and returns the image feature region with maximum similarity. Our implementation of T is shown in <ref type="figure" target="#fig_4">Figure 4b</ref>: without information of where the patch came from, T must match features encoded by ? to localize the next patch. As shown in <ref type="figure" target="#fig_4">Figure 4a</ref>, T can be iteratively applied backwards and then forwards through time to track along an arbitrarily long cycle. The cycle-consistency loss l ? is the euclidean distance between the spatial coordinates of initial patch p t and the patch found at the end of the cycle in I t . In order to minimize l ? , the model must learn a feature space ? that allows for robustly measuring visual similarity between patches along the cycle.</p><formula xml:id="formula_0">x I t-1 : c x 30 x 30 ?t-1,t x p t-1 : c x 10 x 10 f (?) g(?) h(?) 900 x 10 x 10 Estimate Transform T 80 x 80 240 x 240 ! ! ! T T T T ! ! ! ! ! I t I t 1 I t 2 I t p t (a) Training ? by End-to-end Cycle-consistent Tracking Bilinear Sampler reshape transpose x p t : c x 10 x 10 A t-1,t x I t-1 : c x 30 x 30 ?t-1,t x p t-1 : c x 10 x 10 f (?) g(?) h(?) 900 x 10 x 10 Estimate Transform T 80 x 80 240 x 240 ! ! ! T T T T ! ! ! ! ! I t I t 1 I t 2 p t I t 1 (b) Differentiable Tracking Operation T</formula><p>Note that T is only used in training and is deliberately designed to be weak, so as to place the burden of representation on ?. At test time, the learned ? is used directly for computing correspondences. In the following, we first formalize cycle-consistent tracking loss functions and then describe our architecture for mid-level correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cycle-Consistency Losses</head><p>We describe a formulation of cycle-consistent tracking and use it to succinctly express loss functions based on temporal cycle-consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Recurrent Tracking Formulation</head><p>Consider as inputs a sequence of video frames I t?k:t and a patch p t taken from I t . These pixel inputs are mapped to a feature space by an encoder ?, such that</p><formula xml:id="formula_1">x I t?k:t = ?(I t?k:t ) and x p t = ?(p t ). Let T be a differentiable operation x I s ? x p t ? x p s</formula><p>, where s and t represent time steps. The role of T is to localize the patch features x p s in image features x I s that are most similar to x p t . We can apply T iteratively in a forward manner i times from t ? i to t ? 1:</p><formula xml:id="formula_2">T (i) (x I t?i , x p ) = T (x I t?1 , T (x I t?2 , ...T (x I t?i , x p )))</formula><p>By convention, the tracker T can be applied backwards i times from time t ? 1 to t ? i:</p><formula xml:id="formula_3">T (?i) (x I t?1 , x p ) = T (x I t?i , T (x I t?i+1 , ...T (x I t?1 , x p )))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Learning Objectives</head><p>The following learning objectives rely on a measure of agreement l ? (x p t ,x p t ) between the initial patch and relocalized patch (defined in Section 3.2).</p><p>Tracking: The cycle-consistent loss L i long is defined as</p><formula xml:id="formula_4">L i long = l ? (x p t , T (i) (x I t?i+1 , T (?i) (x I t?1 , x p t ))).</formula><p>The tracker attempts to follow features backward and then forward i steps in time to re-arrive to the initial query, as depicted in <ref type="figure" target="#fig_4">Figure 4a</ref>. Skip Cycle: In addition to cycles through consecutive frames, we also allow skipping through time. We define the loss on a two-step skip-cycle as L i skip :</p><formula xml:id="formula_5">L i skip = l ? (x p t , T (x I t , T (x I t?i , x p t ))).</formula><p>This attempts longer-range matching by skipping to the frame i steps away. Feature Similarity: We explicitly require the query patch x p t and localized patch T (x I t?i , x p t ) to be similar in feature space. This loss amounts to the negative Frobenius inner product between spatial feature tensors:</p><formula xml:id="formula_6">L i sim = ? x p t , T (x I t?i , x p t )</formula><p>In principle, this loss can further be formulated as the inlier loss from <ref type="bibr" target="#b53">[54]</ref>. The overall learning objective sums over the k possible cycles, with weight ? = 0.1:</p><formula xml:id="formula_7">L = k i=1 L i sim + ?L i skip + ?L i long .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture for Mid-level Correspondence</head><p>The learning objective thus described can be used to train arbitrary differentiable tracking models. In practice, the architecture of the encoder determines the type of correspondence captured by the acquired representation. In this work, we are interested in a model for mid-level temporal correspondence. Accordingly, we choose the representation to be a mid-level deep feature map, coarser than pixel space but with sufficient spatial resolution to support tasks that require localization. An overview is provided in <ref type="figure" target="#fig_4">Figure 4b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spatial Feature Encoder ?</head><p>We compute spatial features with a ResNet-50 architecture <ref type="bibr" target="#b17">[18]</ref> without res 5 (the final 3 residual blocks). We reduce the spatial stride of res 4 for larger spatial outputs. Input frames are 240 ? 240 pixels, randomly cropped from video frames re-scaled to have min(H, W ) = 256. The size of the spatial feature of the frame is thus 30 ? 30. Image patches are 80 ? 80, randomly cropped from the full 240 ? 240 frame, so that the feature is 10 ? 10. We perform l 2 normalization on the channel dimension of spatial features to facilitate computing cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Differentiable Tracker T</head><p>Given the representation from the encoder, we perform tracking with T . As illustrated in <ref type="figure" target="#fig_4">Figure 4b</ref>, the differentiable tracker is composed of three main components.</p><p>Affinity function f provides a measure of similarity between coordinates of spatial features x I and x p . We denote the affinity function as f (x I , x p ) := A, such that f : R c?30?30 ? R c?10?10 ? ? R 900?100 . A generic choice for computing the affinity is the dot product between embeddings, referred to in recent literature as attention <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b71">72]</ref> and more historically known as normalized cross-correlation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>. With spatial grid j in feature x I as x I (j) and the grid i in x p as x p (i),</p><formula xml:id="formula_8">A(j, i) = exp (x I (j) x p (i)) j exp (x I (j) x p (i))<label>(1)</label></formula><p>where the similarity A(j, i) is normalized by the softmax over the spatial dimension of x I , for each x p (i). Note that the affinity function is defined for any feature dimension.</p><p>Localizer g takes affinity matrix A as input and estimates localization parameters ? corresponding to the patch in feature x I which best matches x p . g is composed of two convolutional layers and one linear layer. We restrict g to output 3 parameters for the bilinear sampling grid (i.e. simpler than <ref type="bibr" target="#b22">[23]</ref>), corresponding to 2D translation and rotation: g(A) := ?, where g : R 900?100 ? ? R 3 . The expressiveness of g is intentionally limited so as to place the burden of representation on the encoder (see Appendix B).</p><p>Bilinear Sampler h uses the image feature x I and ? predicted by g to perform bilinear sampling to produce a new patch feature h(x I , ?) which is in the same size as x p , such that h : R c?30?30 ? R 3 ? ? R c?10?10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">End-to-end Joint Training</head><p>The composition of encoder ? and T forms a differentiable patch tracker, allowing for end-to-end training of ? and T :</p><p>x I , x p = ?(I), ?(p)</p><formula xml:id="formula_9">T (x I , x p ) = h(x I , g(f (x I , x p )).</formula><p>Alignment Objective l ? is applied in the cycle-consistent losses L i long and L i skip , measuring the error in alignment between two patches. We follow the formulation introduced by <ref type="bibr" target="#b52">[53]</ref>. Let M (? x p ) correspond to the bilinear sampling grids used to form a patch feature x p from image feature x I . Assuming M (? x p ) contains n sampling coordinates, the alignment objective is defined as:</p><formula xml:id="formula_10">l ? (x p * ,x p t ) = 1 n n i=1 ||M (? x p * ) i ? M (?x p t ) i || 2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We report experimental results for a model trained on the VLOG dataset <ref type="bibr" target="#b11">[12]</ref> from scratch; training on other large video datasets such as Kinetics gives similar results (see Appendix A.3). The trained representation is evaluated without fine-tuning on several challenging video propagation tasks: DAVIS-2017 <ref type="bibr" target="#b47">[48]</ref>, JHMDB <ref type="bibr" target="#b25">[26]</ref> and Video Instance-level Parsing (VIP) <ref type="bibr" target="#b84">[85]</ref>. Through various experiments, we show that the acquired representation generalizes to a range of visual correspondence tasks (see <ref type="figure" target="#fig_5">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Common Setup and Baselines</head><p>Training. We train the model on the VLOG dataset <ref type="bibr" target="#b11">[12]</ref> without using any annotations or pre-training. The VLOG dataset contains 114K videos and the total length of the videos is 344 hours. During training, we set the number of past frames as k = 4. We train on a 4-GPU machine with a mini-batch size of 32 clips (8 clips per GPU), for 30 epochs. The model is optimized with Adam <ref type="bibr" target="#b31">[32]</ref> with a learning rate of 0.0002 and momentum term ? 1 = 0.5, ? 2 = 0.999.</p><p>Inference. At test time, we use the trained encoder's representation to compute dense correspondences for video propagation. Given initial labels of the first frame, we propagate the labels to the rest of the frames in the video. Labels are given by specified targets for the first frame of each task, with instance segmentation masks for DAVIS-2017 <ref type="bibr" target="#b47">[48]</ref>, human pose keypoints JHMDB <ref type="bibr" target="#b25">[26]</ref>, and both instance-level and semantic-level masks for VIP <ref type="bibr" target="#b84">[85]</ref>. The labels of each pixel are discretized to C classes. For segmentation masks, C is the number of instance or semantic labels. For keypoints, C is the number of keypoints. We include a background class. We propagate the labels in the feature space. The labels in the first frame are one-hot vectors, while propagated labels are soft distributions. Propagation by k-NN. Given a frame I t and a frame I t?1 with labels, we compute their affinity in feature space: A t?1,t = f (?(I t?1 ), ?(I t )) (Eq. 1). We compute label y i of pixel i in I t as</p><formula xml:id="formula_11">y i = j A t?1,t (j, i)y j ,<label>(2)</label></formula><p>where A t?1,t (j, i) is the affinity between pixels i in I t and j in I t?1 . We propagate from the top-5 pixels with the greatest affinity A t?1,t (j, i) for each pixel i. Labels are propagated from I t?1:t?K , as well as I 1 , and averaged. Finally, we up-sample the label maps to image size. For segmentation, we use the argmax of the class distribution of each pixel. For keypoints, we choose the pixel with the maximum score for each keypoint type.</p><p>Baselines. We compare with the following baselines:</p><p>? Identity: Always copy the first frame labels.</p><p>? Optical Flow (FlowNet2 <ref type="bibr" target="#b21">[22]</ref>): A state-of-the-art method for predicting optical flow with neural networks <ref type="bibr" target="#b21">[22]</ref>. We adopt the open-source implementation which is trained with synthetic data in a supervised manner. For a target frame I t , we compute the optical flow from frame I t?1 to I t and warp the labels in I t?1 to I t . ? SIFT Flow <ref type="bibr" target="#b38">[39]</ref>: For a target frame I t , we compute the SIFT Flow between I t and its previous frames. We propagate the labels in K frames before I t and the first frame via SIFT Flow warping. The propagation results are averaged to compute the labels for I t .</p><p>? Transitive Invariance <ref type="bibr" target="#b73">[74]</ref>: A self-supervised approach that combines multiple objectives: (i) visual tracking on raw video <ref type="bibr" target="#b72">[73]</ref> and (ii) spatial context reasoning <ref type="bibr" target="#b8">[9]</ref>. We use the open-sourced pre-trained VGG-16 <ref type="bibr" target="#b57">[58]</ref> model and adopt our proposed inference procedure. ? DeepCluster [8]: A self-supervised approach which uses a K-means objective to iteratively update targets and learn a mapping from images to targets. It is trained on the ImageNet dataset without using annotations. We apply the trained model with VGG-16 and adopt the same inference procedure as our method. ? Video Colorization <ref type="bibr" target="#b68">[69]</ref>: A self-supervised approach for label propagation. Trained on the Kinetics <ref type="bibr" target="#b28">[29]</ref> dataset, it uses color propagation as self-supervision. The architecture is based on 3D ResNet-18. We report their results. ? ImageNet Pre-training <ref type="bibr" target="#b17">[18]</ref>: The conventional setup for supervised training of ResNet-50 on ImageNet. ? Fully-Supervised Methods: We report fully-supervised methods for reference, which not only use ImageNet pretraining but also fine-tuning on the target dataset. Note that these methods do not always follow the inference procedure used with method, and labels of the first frame are not used for JHMDB and VIP at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Instance Propagation on DAVIS-2017</head><p>We apply our model to video object segmentation on the DAVIS-2017 validation set <ref type="bibr" target="#b47">[48]</ref>. Given the initial masks of the first frame, we propagate the masks to the rest of the frames. Note that there can be multiple instances in the first frame. We follow the standard metrics including the region  <ref type="table">Table 1</ref>: Evaluation on instance mask propagation on DAVIS-2017 <ref type="bibr" target="#b47">[48]</ref>. We follow the standard metric on region similarity J and contour-based accuracy F. similarity J (IoU) and the contour-based accuracy F. We set K = 7, the number of reference frames in the past. We show comparisons in <ref type="table">Table 1</ref>. Comparing to the recent Video Colorization approach <ref type="bibr" target="#b68">[69]</ref>, our method is 7.3% in J and 6.7% in F. Note that although we are only 4.4% better than the DeepCluster baseline in J , we are better in contour accuracy F by 6.2%. Thus, DeepCluster does not capture dense correspondence on the boundary as well.</p><p>For fair comparisons, we also implemented our method with a ResNet-18 encoder, which has less parameters compared to the VGG-16 in <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b7">8]</ref> and the 3D convolutional ResNet-18 in <ref type="bibr" target="#b68">[69]</ref>. We observe that results are only around 2% worse than our model with ResNet-50, which is still better than the baselines.</p><p>While the ImageNet pre-trained network performs better than our method on this task, we argue it is easy for the Ima-geNet pre-trained network to recognize objects under large variation as it benefits from curated object-centric annotation. Though our model is only trained on indoor scenes without labels, it generalizes to outdoor scenes.</p><p>Although video segmentation is an important application, it does not necessarily show that the representation captures dense correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pose Keypoint Propagation on JHMDB</head><p>To see whether our method is learning more spatially precise correspondence, we apply our model on the task of keypoint propagation on the split 1 validation set of JH-MDB <ref type="bibr" target="#b25">[26]</ref>. Given the first frame with 15 labeled human keypoints, we propagate them through time. We follow the evaluation of the standard PCK metric <ref type="bibr" target="#b81">[82]</ref>, which measures the percentage of keypoints close to the ground truth in different thresholds of distance. We set the number of reference frames same as experiments in DAVIS-2017.</p><p>As shown in <ref type="table">Table 2</ref>, our method outperforms all selfsupervised baselines by a large margin. We observe that SIFT Flow actually performs better than other selfsupervised learning methods in PCK@.1. Our method outperforms SIFT Flow by 8.7% in PCK@. <ref type="bibr" target="#b0">1</ref>   <ref type="table">Table 2</ref>: Evaluation on pose propagation on JHMDB <ref type="bibr" target="#b25">[26]</ref>. We report the PCK in different thresholds.</p><p>PCK@.2. Notably, our approach is only 0.7% worse than ImageNet pre-trained features in PCK@.1 and performs better in PCK@.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semantic and Instance Propagation on VIP</head><p>We apply our approach on the Video Instance-level Parsing (VIP) dataset <ref type="bibr" target="#b84">[85]</ref>, which is densely labeled with semantic masks for different human parts (e.g., hair, right arm, left arm, coat). It also has instance labels that differentiate humans. Most interestingly, the duration of a video ranges from 10 seconds to 120 seconds in the dataset, which is much longer than aforementioned datasets.</p><p>We test our method on the validation set of two tasks in this dataset: (i) The first task is to propagate the semantic human part labels from the first frame to the rest of the video, and evaluate with the mean IoU metric; (ii) In the second task, the labels in the first frame are given with not only the semantic labels but also the instance identity. Thus, the model must differentiate the different arms of different human instances. We use the standard instance-level human parsing metric <ref type="bibr" target="#b36">[37]</ref>, mean Average Precision, for overlap thresholds varying from 0.1 to 0.9. Since part segments are relatively small (compared to objects in DAVIS-2017), we increase the input image size to 560?560 for inference, and use two reference frames, including the first frame.</p><p>Semantic Propagation. As shown with the mIoU metric in <ref type="table" target="#tab_3">Table 3</ref>, our method again exceeds all self-supervised baselines by a large margin (a [69] model is currently not available). ImageNet pre-trained models have the advantage of semantic annotation and thus do not necessarily have to perform tracking. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>(c), our method is able to handle occlusions and multiple instances.</p><p>Part Instance Propagation. This task is more challenging. We show the results in mean AP r vol in <ref type="table" target="#tab_3">Table 3</ref>. Our method performs close to the level of ImageNet pre-trained features. We show different radial thresholds for average precision (AP r vol ) in <ref type="table" target="#tab_4">Table 4</ref>. ImageNet pre-trained features performs better under smaller thresholds and worse under larger thresholds, suggesting that it has an advantage in finding coarse correspondence while our method is more capable of spatial precision.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Texture Propagation</head><p>The acquired representation allows for propagation of not only instance and semantic labels, but also textures. We visualize texture propagation in <ref type="figure" target="#fig_5">Figure 5</ref> (d); these videos are samples from DAVIS-2017 <ref type="bibr" target="#b47">[48]</ref>. We "paint" a texture of 6 colored stripes on an the object in the first frame and propagate it to the rest of the frames using our representation. We observe that the structure of the texture is well preserved in the following frames, demonstrating that the representation allows for finding precise correspondence smoothly though time. See the project page for video examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Video Frame Reconstructions</head><p>Though we do not optimize for pixel-level objectives at training time, we can evaluate how well our method performs on pixel-level reconstruction. Specifically, given two images I s and I t distant in time in a video, we compute coordinate-wise correspondences under the acquired representation and generate a flow field for pixel movement between I s and I t . We then upsample the flow field to the same size as the image and warp it on Image I s to generate a new image I t (as shown in <ref type="figure">Figure 7</ref>). We compare the L1 distance between I t and I t in RGB space and report the reconstruction errors in <ref type="table">Table 5</ref>.</p><p>For fair comparison, we perform this experiment on the DAVIS-2017 validation set, which none of the reported methods have seen. We experiment with two time gaps, 5 and 10 frames. For the smaller gap, FlowNet2 <ref type="bibr" target="#b21">[22]</ref> performs reasonably well, whereas reconstruction degrades for larger gaps. In both cases, our method performs better than FlowNet2 and the ImageNet pre-trained network. This is encouraging: our method is not trained with pixel-level losses, yet out-performs methods trained with pixel-level tasks and human supervision.</p><formula xml:id="formula_12">! " ! # Flow from ! " to ! #</formula><p>Warping ! " to ! # <ref type="figure">Figure 6</ref>: Given I1, I6 which have 5-frame gap, we compute the long-range flows between them with our representation. This flow can be used to warp I1 to generate image similar to I6.  <ref type="table">Table 5</ref>: We compute the long-range flow on two frames and warp the first one with the flow. We compare the warped frame with the second frame in L1 distance. The gaps are 5 or 10 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>While in principle our method should keep improving with more data, in practice, learning seems to plateau after a moderate amount of training (i.e. 30 epochs). An important next step is thus how to better scale to larger, noisier data. A crucial component is improving robustness to occlusions and partial observability, for instance, by using a better search strategy for finding cycles at training time. Another issue is deciding what to track at training time. Picking patches at random can result in issues such as stationary background patches and tracking ambiguity -e.g. how should one track a patch containing two objects that eventually diverge? Jointly learning what to track may also give rise to unsupervised object detection. Finally, incorporating more context for tracking both at training and test time may be important for learning more expressive models of spatial-temporal correspondence.</p><p>We hope this work is a step toward learning from the abundance of visual correspondence inherent in raw video in a scalable and end-to-end manner. While our experiments show promising results at certain levels of correspondence, much work remains to cover the full spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Removing Skip-Cycles</head><p>Removing the skip-cycle loss -i.e. keeping only the long tracking cycle loss and dense similarity loss -results in worse performance when applying the representations to the DAVIS-2017 dataset. This suggests the skip-cycle loss is useful in cases of occlusion or drift, and provides supplementary training data (c.f. <ref type="table" target="#tab_7">Table 6</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Effect of k in k-NN Label Propagation</head><p>We vary the number of nearest neighbors used in voting for label propagation (Eq. 2), finding that aggregating fewer nearest neighbors improves performance (c.f.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training with the Kinetics Dataset</head><p>Besides the VLOG dataset, we have also trained our model on the Kinetics Dataset <ref type="bibr" target="#b28">[29]</ref>, which contains around 230K training videos (with 10s per video). Compared to the VLOG dataset, the Kinetics dataset contains more videos under less environment constraints: There are videos with both indoor and outdoor scenes; some videos also have large camera motion. After applying the learned representation for label propagation on DAVIS, we observe similar performance by training with VLOG and Kinetics datasets (c.f.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Fine-tuning on the Test Domain</head><p>We emphasize that our method learns features that generalize even without fine-tuning. Here we study the effect of fine-tuning on the DAVIS training set before testing. We find this does not improve test set performance significantly (c.f. <ref type="table" target="#tab_13">Table 9</ref>). There is a risk of overfitting since datasets like DAVIS are so small; this is part of the reason why unsupervised methods are desirable.  Appendix B. Capacity of T As mentioned in Section 3.2, the tracking operation T is deliberately constrained in capacity in order to maximize the representational responsibility of ?. In our implementation, the only parameters learned by T are those of the localizer g, which processes the affinity tensor A to estimate the localization parameters. The affinity A (R 900?100 ) is first reshaped to a tensor with dimension R 900?10?10 as the input for g. The localizer g is a small ConvNet with two convolutional layers (3 ? 3 kernels with 512 channels) and one fully connected layer. The output of the ConvNet is a 3-dimension vector corresponding to 2D translation and rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Correspondence Visualization</head><p>In <ref type="figure">Fig. 7</ref> we visualize the correspondences (top-1 nearest neighbor) between regions with large movement in consecutive frames, comparing our features to ImageNet pretrained features. Our method produces more detailed correspondence. However, for certain object-level tasks (e.g. DAVIS), high-level semantics (captured by ImageNet) are more useful than good correspondences, which explains the difference in performance.</p><p>ImageNet Pre-trained Representation Our Representation <ref type="figure">Figure 7</ref>: Visualizations of correspondence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We propose to learn a representation for visual correspondence from raw video. Without any fine-tuning, the acquired representation generalizes to various tasks involving visual correspondence, allowing for propagation of: (a) Multiple Instance Masks; (b) Pose; (c) Semantic Masks; (d) Long-Range Optical Flow; (e) Texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Multiple Cycles and Skip Cycles. Cycle-consistency may not be achievable due to sudden changes in object pose or occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>: c x 10 x 10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Method Overview. (a) During training, the model learns a feature space encoded by ? to perform tracking using tracker T . By tracking backward and then forward, we can use cycle-consistency to supervise learning of ?. Note that only the initial patch pt is explicitly encoded by ?; other patch features along the cycle are obtained by localizing image features. (b) We show one step of tracking back in time from t to t ? 1. Given input image features x I t?1 and query patch features x p t , T localizes the patch x p t?1 in x I t?1 . This operation is performed iteratively to track along the cycle in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualizations of our propagation results. Given the labels as input in the first frame, our feature can propagate them to the rest of frames, without further fine-tuning. The labels include (a) instance masks in DAVIS-2017<ref type="bibr" target="#b47">[48]</ref>, (b) pose keypoints in JHMDB<ref type="bibr" target="#b25">[26]</ref>, (c) semantic masks in VIP<ref type="bibr" target="#b84">[85]</ref> and even (d) texture map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on propagating human part labels in Video Instance-level Parsing (VIP) dataset<ref type="bibr" target="#b84">[85]</ref>. We measure Semantic Propagation with mIoU and Part Instance Propagation in AP r vol .</figDesc><table><row><cell>model</cell><cell>AP r vol</cell><cell cols="2">IoU threshold 0.3 0.5</cell><cell>0.7</cell></row><row><cell>Ours (ResNet-50)</cell><cell>15.6</cell><cell>23.0</cell><cell cols="2">12.7 5.4</cell></row><row><cell>ImageNet (ResNet-50) [18]</cell><cell>16.1</cell><cell>24.2</cell><cell cols="2">11.9 4.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>A more detailed analysis of different thresholds for Part Instance Propagation on the VIP dataset<ref type="bibr" target="#b84">[85]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Removing Skip Cycles, test on DAVIS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>).</figDesc><table><row><cell>Experiment</cell><cell>J (Mean)</cell><cell>F (Mean)</cell></row><row><cell>Ours (5-NN)</cell><cell>41.9</cell><cell>39.4</cell></row><row><cell>Ours (20-NN)</cell><cell>40.8</cell><cell>38.5</cell></row><row><cell>Ours (10-NN)</cell><cell>41.5</cell><cell>39.1</cell></row><row><cell>Ours (1-NN)</cell><cell>41.0</cell><cell>38.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Effect of k in k-NN Label Propagation, test on DAVIS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>).</figDesc><table><row><cell>Experiment</cell><cell>J (Mean)</cell><cell>F (Mean)</cell></row><row><cell>Ours (VLOG)</cell><cell>41.9</cell><cell>39.4</cell></row><row><cell>Ours (Kinetics)</cell><cell>42.5</cell><cell>39.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Train with VLOG / Kinetics, test on DAVIS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Finetuning on DAVIS train before test.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We thank members of the BAIR community for helpful discussions and feedback, and Sasha Sax and Michael Janner for draft comments. AJ is supported by the P.D. Soros Fellowship. XW is supported by the Facebook PhD Fellowship. This work was also supported, in part, by NSF grant IIS-1633310 and Berkeley DeepDrive.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Recurrent Tracking using Multifold Consistency. CVPR Workshop on PETS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">People-tracking-bydetection and people-detection-by-tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recycle-gan: Unsupervised video retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Computer Vision -A Modern Approach, Second Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Pitman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From lifestyle vlogs to everyday interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Invariance from Transformation Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fldik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="200" />
			<date type="published" when="1991-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spatiotemporally coherent metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Consistent shape maps via semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Eurographics/ACMSIGGRAPH Symposium on Geometry Processing</title>
		<meeting>the Eleventh Eurographics/ACMSIGGRAPH Symposium on Geometry Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Spatial transformer networks. CoRR, abs/1506.02025</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning image representations tied to egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Forward-backward error: Automatic detection of tracking failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anchornet: A weakly supervised network to learn geometry-sensitive features for semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Track to the future: Spatio-temporal video segmentation with long-range motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised natural experience rapidly alters invariant object representation in visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">5895</biblScope>
			<biblScope unit="page" from="1502" to="1507" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Holistic, instance-level human parsing. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Sift flow: Dense correspondence across scenes and its applications. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Switchable temporal propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised learning of long-term motion dynamics for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Moving gradients: A path-based method for plausible image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH</title>
		<meeting>SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dense estimation and object-based segmentation of the optical flow with robust techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Strike a pose: Tracking people by finding stylized poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deepmatching: Hierarchical deformable dense matching. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end weaklysupervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Towards longer long-range motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Particle video: Long-range motion estimation using point trajectories. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Finding Trajectories of Feature Points in a Monocular Image Sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1987-01" />
			<biblScope unit="page" from="56" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Thin-slicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep semantic feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<title level="m">Sfm-net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Spatio-temporal influences at the neural level of object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation in Neural Systems</title>
		<imprint>
			<date type="published" when="1998-01" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Transitive invariance for selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Slow feature analysis: unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A smoothness constraint on the development of object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="145" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The development of newborn object recognition in fast and slow visual worlds. Proceedings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M W</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<date type="published" when="1829-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Situ Evaluation of Tracking Algorithms Using Time Reversed Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Spatiotemporal object continuity in human ventral visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Turk-Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Flombaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Scholl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="8840" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Adaptive temporal encoding network for video instance-level human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Multi-image matching via fast alternating minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Deep Learning of Invariant Features via Simulated Fixations in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3203" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
