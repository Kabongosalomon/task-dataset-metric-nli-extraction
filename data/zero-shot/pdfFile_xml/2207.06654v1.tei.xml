<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
							<email>zhengkjiang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>inter-class information into class-wise prototypes, and adopts the class-centered distribution alignment for adaptation. By considering the same class prototypes as positives and other class prototypes as negatives to achieve class-centered distribution alignment, ProCA achieves state-of-the-art performance on classical domain adaptation tasks, i.e., GTA5 ? Cityscapes and SYNTHIA ? Cityscapes. Code is available at ProCA.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Domain Adaptive Semantic Segmentation, Prototypical Con- trast Adaptation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised Domain Adaptation (UDA) aims to adapt the model trained on the labeled source domain to an unlabeled target domain. In this paper, we present Prototypical Contrast Adaptation (ProCA), a simple and efficient contrastive learning method for unsupervised domain adaptive semantic segmentation. Previous domain adaptation methods merely consider the alignment of the intra-class representational distributions across various domains, while the inter-class structural relationship is insufficiently explored, resulting in the aligned representations on the target domain might not be as easily discriminated as done on the source domain anymore. Instead, ProCA incorporates</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a fundamental computer vision task, which requires per-pixel predictions for a given image. Recently, with the development of deep neural networks (DNN) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>, semantic segmentation has achieved remarkable progress <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49]</ref>. However, state-of-the-art methods still suffer from significant performance drops when the distribution of testing data is different from training data owing to the domain shifts problem <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. At the same time, labeling pixel-wise large-scale semantic segmentation in the target domain is time-consuming and prohibitively expensive. Thus, Unsupervised Domain Adaptation (UDA) is a promising direction to solve such problem by adapting a model trained from largely labeled source domain to an unlabeled target domain without additional cost of annotations. Several works relying on adversarial training <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> have achieved remarkable progress for UDA semantic segmentation. These methods reduce the domain discrepancy between source and target domains by minimizing a series of adversarial training losses. Specifically, it is formulated as a two-player game, where a backbone network (i.e. ResNet-101 backbone) serves as the feature extractor, while a discriminator identifies which domain the features are derived from. To reach equilibrium in this minmax game, it requires the backbone network to produce the domain invariant representations for generalization. Such adversarial training will result in aligned and indistinguishable feature distributions between two domains. However, even though the global feature distributions across domains become closer, it is not guaranteed that pixels attributing to different semantic categories in the target domain are well separated, leading to poor generalization ability and even inferior performance.</p><p>To tackle the issues above, some works attempt to take the category-wise information into account. The idea of encouraging high-confidence predictions is achieved by minimizing the entropy of the output <ref type="bibr" target="#b41">[42]</ref>. The discrepancies between the outputs of two classifiers are utilized to achieve category-level alignment implicitly <ref type="bibr" target="#b26">[27]</ref>. In addition, a fine-grained adversarial learning framework <ref type="bibr" target="#b42">[43]</ref> is proposed to incorporate class information into domain discrimination, which helps to align features at a fine-grained level. However, prior approaches tend to apply such adversarial training in the intra-class, without considering the consistency of the representational structure between the source and target domains. Namely, to some extent, multiple categories on the target domain could be projected to a same group, which are usually well-discriminated on the source domain on the contrary. Therefore, merely considering the intra-class distributional alignment might be insufficient to make the best of the learned representations from labeled source data.</p><p>In order to fully exploit the class-level information, we propose Prototypical Contrast Adaptation (ProCA) for unsupervised domain adaptive semantic segmentation. Intuitively, the same category on different domains is supposed to share the high representational similarity. Therefore, multiple prototypes, i.e., the approximated representational centroid of various categories are utilized to depict the inter-class relationship for both source and target domains. Specifically, after acquiring the segmentation model trained only on the source domain, category-wise prototypes features are obtained by calculating the centroids of features on the source domain. Then, contrastive learning is introduced into domain adaptation process. In particular, a pixel on the target domain is pulled closer to its corresponding prototype with the same class as its estimated pseudo-label and pushed away from other prototypes. In addition, in order to be invariant to domains, category-wise prototypes would be further updated by the current features of two domains. Besides, such prototypical contrastive adaptation scheme is applied at the feature and output level simultaneously. Based on the self-training framework, we further improve the performance with class-aware pseudo-label thresholds.</p><p>Experimental results on the domain adaptation benchmarks for semantic segmentation, i.e., GTA5 ? Cityscapes and SYNTHIA ? Cityscapes further demonstrate the effectiveness of our approach, leading to the state-of-the-art performance. Specifically, with the DeepLab-v2 networks and ResNet-101 backbone, we achieve Cityscapes <ref type="bibr" target="#b3">[4]</ref> semantic segmentation mIoU by 56.3% and 53.0% when adapting from GTA5 <ref type="bibr" target="#b34">[35]</ref> and SYNTHIA <ref type="bibr" target="#b35">[36]</ref> datasets, largely outperforming previous state-of-the-arts.</p><p>We summarize the major contributions as follows:</p><p>-We propose Prototypical Contrastive Adaptation (ProCA) by explicitly introducing constraints on features of different categories for UDA problem in semantic segmentation. This is implemented by not only pulling closer to prototypes with the same class, but also pushing away from prototypes with different classes simultaneously. A multi-level variant is also designed to further improve the adaptation ability. -Online prototypes updating scheme is introduced to improve the domain invariance and discriminant ability of class-wise prototypes. -Combined with self-training method of class-wise adaptive thresholds, the proposed method achieves 56.3% and 52.6% mIoU when adapting GTA5 and SYNTHIA to Cityscapes, respectively, which outperforms previous state-ofthe-arts by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>Semantic segmentation is a fundamental computer vision task, which requires per-pixel predictions for a given image. Recently, with the help of convolution neural networks <ref type="bibr" target="#b23">[24]</ref>, semantic segmentation has achieved remarkable progress. Numerous approaches focus to enlarge receptive fields <ref type="bibr" target="#b1">[2]</ref> and capture context information <ref type="bibr" target="#b48">[49]</ref>. These methods generally require dense pixel-wise annotation datasets, such as Cityscapes <ref type="bibr" target="#b3">[4]</ref>, PASCAL VOC <ref type="bibr" target="#b5">[6]</ref> and ADE20K <ref type="bibr" target="#b50">[51]</ref>. Since perpixel level annotation of large amounts of data is time-consuming and expensive, some synthetic datasets are proposed such as GTA5 <ref type="bibr" target="#b34">[35]</ref> and SYNTHIA <ref type="bibr" target="#b35">[36]</ref> to generate largely labeled segmentation datasets at lower cost. However, when testing models trained on the synthetic datasets on the real-world datasets, significant performance drops are observed even for state-of-the-art methods. In presence of the domain shifts, we deal with the semantic segmentation task that aims to learn a well performing model on the target domain with only the source domain supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">UDA for Semantic Segmentation</head><p>Existing approaches for UDA of semantic segmentation can be primarily divided into three groups, including style transfer <ref type="bibr" target="#b30">[31]</ref>, feature alignment <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b51">52]</ref> and self-training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56]</ref>. Motivated by the recent progress of unpaired image-to-image translation works <ref type="bibr" target="#b54">[55]</ref>, researches on style transfer aim to learn the mapping from virtual to realistic data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>. Previous works on feature alignment minimize the discrepancy between source and target domains to obtain domain-invariant features. This can be achieved by directly minimizing the Maximum Mean Discrepancy (MMD) distances across domains over domain-specific layers <ref type="bibr" target="#b24">[25]</ref> or using discriminator to train the model in an adversarial way to avoid generating domain-aware discriminative features <ref type="bibr" target="#b12">[13]</ref>. There are also some works attempting to absorb class-wise information into feature alignment. The fine-grained adversarial learning framework <ref type="bibr" target="#b42">[43]</ref> is proposed to incorporate class information into the discriminator, which helps to align feature in a class-aware manner, resulting better feature adaptation and performance. Approaches on self-training mainly focus on assigning pseudo-labels on target domain. Iterative self-training method is proposed <ref type="bibr" target="#b55">[56]</ref> by alternatively generating pseudo-labels and retraining the model with a sampling module to deal with the category imbalanced issue. Uncertainty estimation <ref type="bibr" target="#b49">[50]</ref> is proposed to rectify pseudo-label generation. Consistency based methods <ref type="bibr" target="#b0">[1]</ref> have been adopted by enforcing consistency between predictions of different perturbations. In the work of <ref type="bibr" target="#b47">[48]</ref>, a prototype-based sample-wise pseudo-label correction scheme is proposed and embeded into a complicated multi-stage training framework to enhance segmentation performance. Nevertheless, the methods above neglect the explicit modeling of the relationship between clusters of different categories, on the contrary, we directly explore such constraints of different category centroids by prototypical contrastive adaptation. In this way, the categories with similar distributions on the target domain can be easier to distinguish, leading to superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>Contrastive learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b52">53]</ref> has lead remarkable performance in self-supervised representation learning. STC <ref type="bibr" target="#b16">[17]</ref> uses contrastive learning to learn association embeddings for video instance segmentation task. For UDA semantic segmentation, CLST <ref type="bibr" target="#b28">[29]</ref> attempts to leverage contrastive learning to learn finer adapted feature representation. The concurrent work SDCA <ref type="bibr" target="#b20">[21]</ref> proposes using highorder semantic information to conduct contrast adaptation for UDA segmentation, which we found that it is not necessary. In this paper, with the aid of contrastive learning, we explicitly model the relationships of pixel-wise features between different categories and domains to obtain domain-invariant representation for unsupervised domain adaptive semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>By minimizing the distributional distance between the source and target domains, previous approaches aim to obtain the domain-invariant representations for domain adaptation problem. However, the inter-class structural relationship is insufficiently explored. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a), after alignment within the intra-class across two domains, it could be much more challenging to distinguish different categories since the decision boundaries identified on source domain could hardly be maintained on the target domain. Therefore, we propose a novel category-aware prototypical contrast adaptation which introduces multiple prototypes to explicitly model the intra-class and inter-class relationships in a contrastive manner. Akin to previous state-of-the-art approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48</ref>], a segmentation model is first trained on source domain in the supervised manner. Meanwhile, multiple prototypes are initialized to represent each category. Contrast adaptation is then adopted to constrain the inter-class relationship. Besides, prototypes are updated on both source domain and target domain to enhance the domain-invariant representations. As last, we present a modified pseudo-label generation method with class-aware adaptive thresholds for self-training, leading to new state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Given the labeled source domain images D s = {(x s n , y s n )} Ns n=1 , as well as unlabeled target images D t = {(x t n )} Nt n=1 , the goal of UDA of semantic segmentation is to train a model on D s and D t ; and evaluate the performance on the target domain. The segmentation model consists of a feature extractor F and a classifier C, which predicts pixel-wise predictions for a given image.</p><p>Following previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43]</ref>, the segmentation model is first trained on the labeled source domain in a supervised manner by minimizing the loss between the prediction p s n and the ground-truth label Y s n ? L H?W , L = {1, 2, ? ? ? , C} annotated with C category labels, for a given image x s n ? R H?W . We use the standard cross-entropy loss, which can be formulated as:</p><formula xml:id="formula_0">L ce n = ? Ns n=1 H i=1 W j=1 C c=1 y s n,i,j,c log(p s n,i,j,c ),<label>(1)</label></formula><p>where N s is the number of source domain images, H and W denote the height and the width of an image, i, j are the pixel index of height and width, C is the number of categories. p s n ? R H?W ?C is the predicted probability of the image x s n , which is obtained by up-sampling the prediction C(F(x s n )). y s n ? {0, 1} H?W ?C is the one-hot representation of the ground-truth label Y s n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prototypical Contrast Adaptation</head><p>Here, intra-class and inter-class relations are simultaneously considered by prototypesbased contrastive learning as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Specifically, ProCA contains three stages, including prototypes initialization, contrast adaptation and prototypes updating. Prototypes Initialization. After obtaining the model trained on the labeled source domain, the initialized class-aware prototypes can be calculated as:</p><formula xml:id="formula_1">p f eat c = Ns n=1 H i=1 W j=1 F s n,i,j 1[Y s n,i,j = c] Ns n=1 H i=1 W j=1 1[Y s n,i,j = c] ,<label>(2)</label></formula><p>where F s n,i,j ? R d is the extracted source feature vector with dimension d, c is the index of categories number C, H and W denote the height and width of the features, 1[Y s n,i,j = c] is an indicator function, which equals to 1 if Y s n,i,j = c and 0 otherwise. Prototypes could be regarded as the approximated representational centroid of various categories. Contrast Adaptation. Given an image of target domain, the corresponding feature F t n is extracted by the shared backbone network F. Accordingly, its pseudo-label y t n ? {0, 1} H?W ?C could be produced by the classifier C trained on source domain. Here, pseudo-label could bridge the extracted features and their corresponding prototypes. Therefore, we could compute the similarity between features and each of prototypes, leading to a vector P t?s n,i,j = [P t?s n,i,j,1 , . . . , P t?s n,i,j,C ]:</p><formula xml:id="formula_2">P t?s n,i,j,c = exp(p f eat c ? F t n,i,j /? ) C c=1 exp(p f eat c ? F t n,i,j /? ) ,<label>(3)</label></formula><p>where ? is the temperature. Then, we minimize the cross entropy loss between P t?s n,i,j and pseudo-label y t n as:</p><formula xml:id="formula_3">L t?s n = ? H i=1 W j=1 C c=1 y t n,i,j,c log P s?t n,i,j,c .<label>(4)</label></formula><p>The goal of such objective is to enforce the pixels belonging to the same category are supposed to share high representational similarity. In addition to the crossdomain adaptation, we also use source-source contrastive loss L s?s n similarly:</p><formula xml:id="formula_4">L s?s n = ? H i=1 W j=1 C c=1 y s n,i,j,c log P s?s n,i,j,c ,<label>(5)</label></formula><p>where y s n is the ground-truth one-hot source domain label, P s?s n,i,j is calculated similarly as Equation <ref type="bibr" target="#b2">3</ref>. The final pixel-prototypes contrastive loss on featurelevel is:</p><formula xml:id="formula_5">L ContraFeat = Nt n=1 L t?s n + Ns n=1 L s?s n .<label>(6)</label></formula><p>Prototypes Updating. To enhance the domain-invariant representational ability of prototypes, we propose two schemes of prototype updating along with training to incorporate target-related information into prototypes. One is to update according to the computation of strict statistical mean of global data as:</p><formula xml:id="formula_6">p f eat c ? p f eat c n f eat c + p f eat c n f eat c n f eat c + n f eat c ,<label>(7)</label></formula><p>where n f eat c represents the accumulated number of pixels belonging to category c until the last update, p f eat c represents the online estimated prototypes for category c, and n f eat c represents the total number of pixels belonging to category c from a newly appended mini-batch during training.</p><p>In addition to source domain class-wise prototypes, we also leverage target features to update prototypes during feature adaptation process. This mixed prototypes scheme could be regarded as a bridge across two domains, which could naturally interact with each other. Thus, we further propose an alternative and more stable and robust way to directly update prototypes with a mixed domain scheme:</p><formula xml:id="formula_7">p f eat c ? mp f eat s c + (1 ? m)p f eat t c ,<label>(8)</label></formula><p>where m is a hyper-parameter, which defines a constant rate of source and target prototypes updating during training. p f eat s c is the estimated source prototype, and p f eat t c is the estimated target prototype. Label Space Adaptation. As we mentioned before, prototypes are initialized, calculated and updated at the feature level i.e., output of the backbone network F. Apart from this, we could also apply the proposed prototypical contrast adaptation in the label space i.e., the output of the classifier C. The major difference is that the dimension of prototypes becomes the number of categories rather than the hidden channels in the feature space. Accordingly, the overall prototypical contrast adaptation losses becomes:</p><formula xml:id="formula_8">L Contra = L ContraFeat + L ContraOut .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining ProCA with Self-Training</head><p>Since the proposed category-aware prototypical contrast adaptation is orthogonal to self-training based methods, we further improve the adaptation performance through the self-training strategy following previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Class-wise Adaptive Pseudo-Label Thresholds. After the prototypical contrast adaptation stage, we could obtain the sorted predicted confidence set ? c = [? c,1 , ? c,2 , ..., ? c,lc ] of each category c, the length of confidence set belonging to category c can be calculated as follows:</p><formula xml:id="formula_9">l c = Nt n=1 H i=1 W j=1 1[ Y t n,i,j = c],<label>(10)</label></formula><p>where Y t n,i,j ? L H?W , L = {1, 2, ? ? ? , C} is the predicted pseudo-label for the image x t n . Then, each class threshold of pseudo-labels can be obtained by fixed percentage of the ranked confidence sets, where the percentage is denoted as a hyper-parameter ?.</p><p>In addition to above self-training strategy, there are some works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50</ref>] focusing on self-training itself improvements, like ProDA <ref type="bibr" target="#b47">[48]</ref> which leverages prototypes to obtain accurate pseudo-label. Since our proposed ProCA mainly works during feature adaptation process, which is orthogonal to such self-training based improvements. Thus we could combine our ProCA with such self-training methods to achieve better performance, which is shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets and Evaluation Metrics: Following previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43]</ref>, we evaluate the model in common UDA of semantic segmentation benchmarks, GTA5 <ref type="bibr" target="#b34">[35]</ref> ? Cityscapes <ref type="bibr" target="#b3">[4]</ref> and SYNTHIA <ref type="bibr" target="#b35">[36]</ref> ? Cityscapes <ref type="bibr" target="#b3">[4]</ref>. GTA5 is an image dataset synthesized by a photo-realistic open-world computer game. <ref type="table">Table 1</ref>. Comparison results of GTA5 ? Cityscapes. All methods use DeepLab-v2 with ResNet-101 backbone for fair comparison. ? means that we report the first stage self-training result of ProDA <ref type="bibr" target="#b47">[48]</ref> for fair comparison, please see <ref type="table">Table 3</ref> of ProDA <ref type="bibr" target="#b47">[48]</ref> for details.  <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40]</ref> only report mIoU for 13 common categories in SYNTHIA ? CItyscapes setting, we also report the 13 common categories performance denoted as mIoU*. Implementation Details. Following most previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43]</ref>, we use the DeepLab-v2 framework <ref type="bibr" target="#b1">[2]</ref> with ResNet-101 <ref type="bibr" target="#b10">[11]</ref> encoder as our segmentation model for fair comparison. All models are pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>. Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b1">[2]</ref> is inserted after the last encoder layer with dilated rates {6, 12, 18, 24}. At last, an up-sampling layer is used to obtain the final per-pixel predictions with the same image size as input. We implement the proposed method with PyTorch [33] on NVIDIA Tesla V100. We apply SGD optimizer with the initial learning rate of 2.5 ? 10 ?4 , momentum 0.9 and weight decay of 5.0?10 ?4 . We use polynomial learning rate scheduling with the power of 0.9. During prototypical contrast adaptation, the pseudo-label threshold of target domain is set to 0.9. For self-training stage. we assign pseudo-labels based on the predicted category probabilities with the adaptive thresholds. The percentage ? of the number of pixels for each category is 0.6 in default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons with State-of-the-Art Methods</head><p>In order to compare with previous state-of-the-art methods comprehensively, we include two typical methods: 1) Domain alignment methods which aim to align the distribution between source and target domains by distribution distances or adversarial training, including LITR <ref type="bibr" target="#b18">[19]</ref>, PIT <ref type="bibr" target="#b27">[28]</ref>, WeakSeg <ref type="bibr" target="#b33">[34]</ref>, CrCDA <ref type="bibr" target="#b14">[15]</ref>, FADA <ref type="bibr" target="#b42">[43]</ref>, ASA <ref type="bibr" target="#b53">[54]</ref>, CLAN <ref type="bibr" target="#b25">[26]</ref>, ConTrans <ref type="bibr" target="#b19">[20]</ref>, SDCA <ref type="bibr" target="#b20">[21]</ref>, and CIRN <ref type="bibr" target="#b6">[7]</ref>. 2) Self-training approaches, including UIDA <ref type="bibr" target="#b31">[32]</ref>, LSE <ref type="bibr" target="#b37">[38]</ref>, IAST <ref type="bibr" target="#b29">[30]</ref>, DACS <ref type="bibr" target="#b38">[39]</ref>, RPLL <ref type="bibr" target="#b49">[50]</ref>, DAST <ref type="bibr" target="#b46">[47]</ref>, ESL <ref type="bibr" target="#b36">[37]</ref>, MetaCorrect <ref type="bibr" target="#b8">[9]</ref>, and ProDA <ref type="bibr" target="#b47">[48]</ref>.</p><p>Results on GTA5 ? Cityscapes. As shown in <ref type="table">Table 1</ref>, our approach achieves 56.3 % mIoU, outperforming prior methods by a large margin. In particular, the most challenging classes stated in <ref type="bibr" target="#b20">[21]</ref> including pole, person, rider, bike, and train, obtains the significant improvements, compared to previous work. It Arxiv'21 -  demonstrates our motivation that the inter-class modeling via prototypes indeed help the category recognition on the target domain, especially for the harder classes.</p><formula xml:id="formula_10">- - - - - - - - - - - - - - - - - 53.3</formula><p>Results on SYNTHIA ? Cityscapes. The comparisons of SYNTHIA ? Cityscapes are shown in <ref type="table" target="#tab_2">Table 4</ref>. Among all the 16 categories, we achieve the best scores on 6 categories, most of those are hard classes stated in <ref type="bibr" target="#b20">[21]</ref>, e.g., person, and bike. To be specific, the proposed method achieves the mIoU score by 53.0% and 59.6% over the 16 and 13 categories respectively, which obtains the gains over the baseline by 19.5% and 21.0%. <ref type="table">Table 6</ref>. Ablation studies of different self-training schemes for GTA5 ? Cityscapes. Naive Self-Training refers to fixed 0.9 threshold for pseudo-label generation; Adaptive Self-Training refers to adaptive pseudo-label generation, which is median of predicted confidence set of each class in default (Sec 3.3). Prototypes-based Self-Training refers to pseudo-label generation strategy by utilizing prototypes which is proposed by ProDA <ref type="bibr" target="#b47">[48]</ref>.</p><p>ProCA Naive Adaptive Prototypes-based mIoU</p><formula xml:id="formula_11">? 48.8 ? ? 55.2 ? ? 56.3 ? ? 57.5</formula><p>Discussion with ProDA. It should be noticeable that our proposed prototype contrastive learning method surpass a similar prototype-based method ProDA <ref type="bibr" target="#b47">[48]</ref> on both transferring scenarios under a fair comparison setting. Especially, in GTA5 ? Cityscapes, our adaptive method outperforms <ref type="bibr" target="#b47">[48]</ref> by a large margin of 1.4% mIOU. This is due to the fact that ProDA only utilizes prototypes to rectify pseudo-labels or align feature in a purely sample-wise manner, which is more vulnerable to the interference from outlier or noisy samples in the target domain, while our pipeline directly depicts the class-wise relation in a sample-to-prototype manner, making the learning process more robust and friendly to cross-domain transferring. Discussion with Other Contrastive Learning based Methods. It should also be noticed that compared with a similar patch-wise contrastive learning method PWCL <ref type="bibr" target="#b22">[23]</ref>, our approach achieves superiority of 4.2% and 5.4% mIOU improvement on both GTA5 ? Cityscapes and SYNTHIA ? Cityscapes respectively. This is due to the fact that PWCL only takes patch-wise features for contrastive feature adaptation, which is coarse to depict class-wise relation and ignores the fine-grained pixel-wise distribution variation during training process, resulting in less discriminative and general representation. How ProCA helps poor classes adaptation? As shown in Table1, the performance of train class could not be improved by state-of-the-art pseudo-label method ProDA. This is because initialized predictions are totally wrong, thus ProDA could not estimate accurate pseudo-label for train class. Different from ProDA, our ProCA first corrects the train class predictions by push aware from others class centroids, which progressively obtain more and more accurate feature representation of train class. After introducing such relationship between different classes, our proposed method achieves highest train class performance after combining with self-training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Effectiveness of Each Component. We conduct ablation studies to demonstrate the effectiveness of each component. We use the ResNet-101 backbone with DeepLab-v2 segmentation for GTA5 ? Cityscapes adaptation. As shown in <ref type="table">Table 2</ref>, the source-only baseline achieves 37.3% mIoU on Cityscapes val set. Further, we achieve 48.8% mIoU score after using the proposed prototypical contrast adaptation. At last, the performance can be improved to 55.1% mIoU through self-training with class-aware adaptive thresholds. Finally, we obtain 56.3% mIoU score by multi-scale testing following FADA <ref type="bibr" target="#b42">[43]</ref>. When directly using self-training after source-domain training, we could only obtain 43.9% mIoU, which is 11.2% mIoU lower than 55.1% mIoU score, demonstrating the effectiveness of ProCA. Effectiveness of ProCA. To verify the effectiveness of ProCA, we implement other feature alignment methods, e.g, class-wise adversarial training without inter-class modeling FADA <ref type="bibr" target="#b42">[43]</ref>, semantic-distribution modeling with categorywise information. As shown in <ref type="table">Table 3</ref>, FADA improves the baseline to 46.9% mIoU, which indicates the effectiveness of the adversarial training. SDCA <ref type="bibr" target="#b20">[21]</ref> obtains 47.2% mIoU by considering semantic-aware feature alignment. Memory Bank obtains 47.6% mIoU by introducing pixel-wise contrastive adaptation, which already achieves better performance than FADA and SDCA. Compared with above methods, our ProCA achieves the best mIoU score 48.8%, which demonstrates the superiority of the proposed class-aware prototypical contrast adaptation than pixel-wise memory bank scheme. Effectiveness of Mixed Updating. We conduce ablation studies to verify the effectiveness of mixed updating for prototypes. As shown in 5, a naive fixed prototype scheme only achieves 47.8% mIoU, while centroid updating way only in source domain obtains 48.3% mIoU, which has 0.5% gain compared with fixed-prototype scheme. Mixed updating scheme achieves best 48.8% mIoU score, which demonstrates the effectiveness of latest features during training. Effectiveness of Multi-Level Adaptation. We conduct ablation studies to verify the effectiveness of multi-level adaptation. The results are shown in <ref type="table">Table 2</ref>. when only using feature-level adaptation or output-level adaptation, we achieve 47.9% mIoU and 48.4% mIoU, respectively. After combining them, we obtain the best mIoU score 48.8%, demonstrating the superiority of multi-level adaptation.</p><p>Effectiveness of In-Domain Contrastive Adaptation and Cross-Domain Contrastive Adaptation. We conduct experiments to study the influence of different domain choices of prototypical contrastive adaptation. The results are shown in <ref type="table">Table 8</ref>. When only using source-to-source ProCA scheme, we could obtain 7.6% mIoU improvement. When only using cross-domain ProCA scheme, we could obtain 9.5% mIoU improvement. After combining both in-domain and cross-domain strategies, we finally obtain 48.8% mIoU, which verifies the effectiveness of the proposed method. Effectiveness of Different Percentages for Adaptive Self-training. We conduct experiments to study the influence of different percentages of pseudolabels generation during self-training stage. The results are shown in <ref type="table">Table 7</ref>.</p><p>Using 60 percentage to generate pseudo-labels, ProCA achieves the best mIoU 55.1%. And larger percentages harm the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose ProCA, which utilizes class-wise prototypes to align features in a fine-grained manner. Apart from feature-level adaptation, outputlevel prototypes are also exploited to boost the adaptation performance. The proposed method achieves the state-of-the-art performance on challenging benchmarks, outperforming previous methods by a large margin. Elaborate ablative studies demonstrate the advancement of our ProCA. We hope the proposed prototypical contrast adaptation could extend to more tasks, such as object detection and instance segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of inter-class modeling. ? means the adapted feature of target domain. With explicit inter-class constraints during adaptation, adapted features of target domain can appear at the right place of decision boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of proposed ProCA. For given source image I s and target image I t , features F s and F t of two domains are first obtained through a shared feature encoder F. Then, outputs O s and O t are obtained by a shared classifier C. After obtaining initialized prototypes, a pixel from two domains acts as a contrastive manner with class-aware prototypes to directly model inter-class constraints. We conduct such prototypical contrast adaption on both feature-level and output-level. At last, the initialized prototypes are also updated during training to enhance the domain-invariant representational ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MetaCorrect [9] CVPR'21 92.8 58.1 86.2 39.7 33.1 36.3 42.0 38.6 85.5 37.8 87.6 62.8 31.7 84.8 35.7 50.3 2.0 36.8 48.0 52.1 +14.8 ProDA ? [48] CVPR'21 91.5 52.3 82.9 42.0 35.7 40.0 44.4 43.2 87.0 43.8 79.5 66.4 31.3 86.7 41.1 52.5 0.0 45.4 53.8 53.7 +16.4 UPLR [45] ICCV'21 90.5 38.7 86.5 41.1 32.9 40.5 48.2 42.1 86.5 36.8 84.2 64.5 38.1 87.2 34.8 50.4 0.2 41.8 54.6 52.6 +15.3 Ours -91.9 48.4 87.3 41.5 31.8 41.9 47.9 36.7 86.5 42.3 84.7 68.4 43.1 88.1 39.6 48.8 40.6 43.6 56.9 56.3 +19.0 which shares 19 classes with Cityscapes. It has 24,966 images with the resolution 1914 ? 1052. SYNTHIA is a synthetic urban scene dataset. Following previous works [40], we use the subset SYNTHIA-RAND-CITYSCAPES sharing 16 common classes with Cityscapes. It contains 9400 images with the resolution 1280 ? 760. Cityscapes is a dataset of real urban scenes, which is collected from 50 cities in Germany and neighboring cities. It has 2,975 training images, 500 validation images, and 1,525 test images, with the resolution 2048 ? 1024. We report the results on Cityscapes validation set using the category-wise Intersection over Union (IoU). Specifically, we report the mean IoU (mIoU) of all 19 classes in GTA5 ? Cityscapes setting and the 16 common categories in SYNTHIA ? Cityscapes setting. In addition, since some works</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>trunk</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bike</cell><cell>mIoU gain</cell></row><row><cell>Source Only</cell><cell>-</cell><cell cols="20">53.8 15.6 69.3 28.1 18.8 27.6 34.9 18.2 82.5 27.8 71.6 59.4 35.3 44.1 25.9 37.5 0.1 28.9 24.9 37.3 +0.0</cell></row><row><cell cols="22">PatchAlign [41] CVPR'19 92.3 51.9 82.1 29.2 25.1 24.5 33.8 33.0 82.4 32.8 82.2 58.6 27.2 84.3 33.4 46.3 2.2 29.5 32.3 46.5 +9.2</cell></row><row><cell cols="22">ADVENT [42] CVPR'19 89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5 +8.2</cell></row><row><cell>BDL [22]</cell><cell cols="21">CVPR'19 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 +11.2</cell></row><row><cell>UIDA [32]</cell><cell cols="21">CVPR'20 90.6 37.1 82.6 30.1 19.1 29.5 32.4 20.6 85.7 40.5 79.7 58.7 31.1 86.3 31.5 48.3 0.0 30.2 35.8 46.3 +9.0</cell></row><row><cell>LTIR [19]</cell><cell cols="21">CVPR'20 92.9 55.0 85.3 34.2 31.1 34.9 40.7 34.0 85.2 40.1 87.1 61.0 31.1 82.5 32.3 42.9 0.3 36.4 46.1 50.2 +12.9</cell></row><row><cell>PIT [28]</cell><cell cols="21">CVPR'20 87.5 43.4 78.8 31.2 30.2 36.3 39.9 42.0 79.2 37.1 79.3 65.4 37.5 83.2 46.0 45.6 25.7 23.5 49.9 50.6 +13.2</cell></row><row><cell>LSE [38]</cell><cell cols="21">ECCV'20 90.2 40.0 83.5 31.9 26.4 32.6 38.7 37.5 81.0 34.2 84.6 61.6 33.4 82.5 32.8 45.9 6.7 29.1 30.6 47.5 +10.2</cell></row><row><cell cols="22">WeakSeg [34] ECCV'20 91.6 47.4 84.0 30.4 28.3 31.4 37.4 35.4 83.9 38.3 83.9 61.2 28.2 83.7 28.8 41.3 8.8 24.7 46.4 48.2 +10.9</cell></row><row><cell cols="22">CrCDA [15] ECCV'20 92.4 55.3 82.3 31.2 29.1 32.5 33.2 35.6 83.5 34.8 84.2 58.9 32.2 84.7 40.6 46.1 2.1 31.1 32.7 48.6 +11.3</cell></row><row><cell>FADA [43]</cell><cell cols="21">ECCV'20 92.5 47.5 85.1 37.6 32.8 33.4 33.8 18.4 85.3 37.7 83.5 63.2 39.7 87.5 32.9 47.8 1.6 34.9 39.5 49.2 +11.9</cell></row><row><cell>IAST [30]</cell><cell cols="21">ECCV'20 94.1 58.8 85.4 39.7 29.2 25.1 43.1 34.2 84.8 34.6 88.7 62.7 30.3 87.6 42.3 50.3 24.7 35.2 40.2 52.2 +14.9</cell></row><row><cell>ASA [54]</cell><cell cols="21">TIP'21 89.2 27.8 81.3 25.3 22.7 28.7 36.5 19.6 83.8 31.4 77.1 59.2 29.8 84.3 33.2 45.6 16.9 34.5 30.8 45.1 +7.8</cell></row><row><cell cols="22">CLAN [26] TPAMI'21 88.7 35.5 80.3 27.5 25.0 29.3 36.4 28.1 84.5 37.0 76.6 58.4 29.7 81.2 38.8 40.9 5.6 32.9 28.8 45.5 +8.2</cell></row><row><cell>DACS [39]</cell><cell cols="21">WACV'21 89.9 39.7 87.9 39.7 39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.2 0.0 27.3 34.0 52.1 +14.8</cell></row><row><cell>RPLL [50]</cell><cell cols="21">IJCV'21 90.4 31.2 85.1 36.9 25.6 37.5 48.8 48.5 85.3 34.8 81.1 64.4 36.8 86.3 34.9 52.2 1.7 29.0 44.6 50.3 +13.0</cell></row><row><cell>DAST [47]</cell><cell cols="21">AAAI'21 92.2 49.0 84.3 36.5 28.9 33.9 38.8 28.4 84.9 41.6 83.2 60.0 28.7 87.2 45.0 45.3 7.4 33.8 32.8 49.6 +12.3</cell></row><row><cell cols="22">ConTrans [20] AAAI'21 95.3 65.1 84.6 33.2 23.7 32.8 32.7 36.9 86.0 41.0 85.6 56.1 25.9 86.3 34.5 39.1 11.5 28.3 43.0 49.6 +13.2</cell></row><row><cell>CIRN [7]</cell><cell cols="21">AAAI'21 91.5 48.7 85.2 33.1 26.0 32.3 33.8 34.6 85.1 43.6 86.9 62.2 28.5 84.6 37.9 47.6 0.0 35.0 36.0 49.1 +11.8</cell></row><row><cell>SDCA [21]</cell><cell cols="21">Arxiv'21 92.8 52.5 85.9 34.8 28.1 40.3 44.4 33.4 86.7 41.7 87.1 67.4 37.3 88.1 39.9 52.5 1.4 34.2 55.0 52.9 +15.6</cell></row><row><cell>PWCL [23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Arxiv'21 93.3 54.2 83.0 25.9 28.1 37.2 41.1 39.3 83.1 38.9 78.2 61.3 36.2 84.2 35.8 54.0 18.1 26.7 47.5 50.9 +13.6 CLST [29] Arxiv'21 92.8 53.5 86.1 39.1 28.1 28.9 43.6 39.4 84.6 35.7 88.1 63.9 38.3 86.0 41.6 50.6 0.1 30.4 51.7 51.6 +14.3 ESL [37] CVPR'21 90.2 43.9 84.7 35.9 28.5 31.2 37.9 34.0 84.5 42.2 83.9 59.0 32.2 81.8 36.7 49.4 1.8 30.6 34.1 48.6 +11.3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3</head><label>23</label><figDesc>Ablation studies of each component for GTA5 ? Cityscapes. F refers to feature-level prototypical contrast adaptation; O refers to output-level prototypical contrast adaptation; Ada-ST refers to adaptive threshold self-training; MST refers to multi-scale testing. All methods use DeepLab-v2 with ResNet-101 backbone.</figDesc><table><row><cell></cell><cell cols="4">Source Only F O Ada-ST MST mIoU</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>37.3</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>47.9</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>48.4</cell></row><row><cell></cell><cell>?</cell><cell>? ?</cell><cell></cell><cell>48.8</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>43.9</cell></row><row><cell></cell><cell>?</cell><cell>? ?</cell><cell>?</cell><cell>55.1</cell></row><row><cell></cell><cell>?</cell><cell>? ?</cell><cell>?</cell><cell>? 56.3</cell></row><row><cell cols="5">Source Only FADA SDCA Memory Bank ProCA mIoU</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>37.3</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>46.9</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>47.2</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>47.6</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>48.8</cell></row></table><note>. Ablation studies of different domain alignment methods for GTA5 ? Cityscapes. FADA [43] refers fine-grained adversarial training for feature-level and output-level; SDCA [21] refers semantic distribution-aware adaptation; Memory Bank refers to pixel-level bank for contrast adaptation. ProCA refers to prototypical contrast adaptation. All methods use DeepLab-v2 with ResNet-101 backbone.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison results of SYNTHIA ? Cityscapes. mIoU* denotes the mean IoU of 13 classes, which excludes the classes marked by the asterisk. All methods use DeepLab-v2 with ResNet-101 backbone for fair comparison. ? means that we report the first stage self-training result of ProDA<ref type="bibr" target="#b47">[48]</ref> for fair comparison. The result of ProDA<ref type="bibr" target="#b47">[48]</ref> is from their released code. AAAI'21 93.<ref type="bibr" target="#b2">3</ref> 54.0 81.3 14.3 0.7 28.8 21.3 22.8 82.6 83.3 57.7 22.8 83.4 30.7 20.2 47.2 46.5 +13.0 53.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall*</cell><cell>fence*</cell><cell>pole*</cell><cell>light</cell><cell>sign</cell><cell>vegetation</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motorbike</cell><cell>bike</cell><cell cols="2">mIoU gain mIoU* gain*</cell></row><row><cell>Source Only</cell><cell>-</cell><cell cols="17">55.6 23.8 74.6 9.2 0.2 24.4 6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 13.7 25.0 33.5</cell><cell>0.0</cell><cell>38.6</cell><cell>0.0</cell></row><row><cell cols="6">PatchAlign [41] CVPR'19 82.4 38.0 78.6 -</cell><cell>-</cell><cell>-</cell><cell cols="10">9.9 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3</cell><cell>-</cell><cell>-</cell><cell>46.5 +7.9</cell></row><row><cell cols="20">ADVENT [42] CVPR'19 85.6 42.2 79.7 8.7 0.4 25.9 5.4 8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2 +7.7 48.0 +9.4</cell></row><row><cell>BDL [22]</cell><cell cols="5">CVPR'19 86.0 46.7 80.3 -</cell><cell>-</cell><cell cols="11">-14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>-</cell><cell>-</cell><cell>51.4 +12.8</cell></row><row><cell>UIDA [32]</cell><cell cols="19">CVPR'20 84.3 37.7 79.5 5.3 0.4 24.9 9.2 8.4 80.0 84.1 57.2 23.0 78.0 38.1 20.3 36.5 41.7 +8.2 48.9 +10.3</cell></row><row><cell>LTIR [19]</cell><cell cols="5">CVPR'20 92.6 53.2 79.2 -</cell><cell>-</cell><cell>-</cell><cell cols="10">1.6 7.5 78.6 84.4 52.6 20.0 82.1 34.8 14.6 39.4</cell><cell>-</cell><cell>-</cell><cell>49.3 +10.7</cell></row><row><cell>PIT [28]</cell><cell cols="19">CVPR'20 83.1 27.6 81.5 8.9 0.3 21.8 26.4 33.8 76.4 78.8 64.2 27.6 79.6 31.2 31.0 31.3 44.0 +10.5 51.8 +13.2</cell></row><row><cell>LSE [38]</cell><cell cols="19">ECCV'20 82.9 43.1 78.1 9.3 0.6 28.2 9.1 14.4 77.0 83.5 58.1 25.9 71.9 38.0 29.4 31.2 42.6 +9.1 49.4 +10.8</cell></row><row><cell cols="20">CrCDA [15] ECCV'20 86.2 44.9 79.5 8.3 0.7 27.8 9.4 11.8 78.6 86.5 57.2 26.1 76.8 39.9 21.5 32.1 42.9 +9.4 50.0 +11.4</cell></row><row><cell cols="20">WeakSeg [34] ECCV'20 92.0 53.5 80.9 11.4 0.4 21.8 3.8 6.0 81.6 84.4 60.8 24.4 80.5 39.0 26.0 41.7 44.3 +10.8 51.9 +13.3</cell></row><row><cell>IAST [30]</cell><cell cols="19">ECCV'20 81.9 41.5 83.3 17.7 4.6 32.3 30.9 28.8 83.4 85.0 65.5 30.8 86.5 38.2 33.1 52.7 49.8 +16.3 57.0 +18.4</cell></row><row><cell>FADA [43]</cell><cell cols="19">ECCV'20 84.5 40.1 83.1 4.8 0.0 34.3 20.1 27.2 84.8 84.0 53.5 22.6 85.4 43.7 26.8 27.8 45.2 +11.7 52.5 +13.9</cell></row><row><cell>ASA [54]</cell><cell cols="19">TIP'21 91.2 48.5 80.4 3.7 0.3 21.7 5.5 5.2 79.5 83.6 56.4 21.9 80.3 36.2 20.0 32.9 41.7 +8.2 49.3 +10.7</cell></row><row><cell cols="6">CLAN [26] TPAMI'21 82.7 37.2 81.5 -</cell><cell>-</cell><cell cols="11">-17.1 13.1 81.2 83.3 55.5 22.1 76.6 30.1 23.5 30.7</cell><cell>-</cell><cell>-</cell><cell>48.8 +10.2</cell></row><row><cell>DACS [39]</cell><cell cols="19">WACV'21 80.6 25.1 81.9 21.5 2.9 37.2 22.7 24.0 83.7 90.8 67.6 38.3 82.9 38.9 28.5 47.6 48.3 +14.8 54.8 +16.2</cell></row><row><cell>RPLL [50]</cell><cell cols="19">IJCV'21 87.6 41.9 83.1 14.7 1.7 36.2 31.3 19.9 81.6 80.6 63.0 21.8 86.2 40.7 23.6 53.1 47.9 +14.4 54.9 +16.3</cell></row><row><cell>CIRN [7]</cell><cell cols="19">AAAI'21 85.8 40.4 80.4 4.7 1.8 30.8 16.4 18.6 80.7 80.4 55.2 26.3 83.9 43.8 18.6 34.3 43.9 +10.4 51.1 +12.5</cell></row><row><cell>DAST [47]</cell><cell cols="19">AAAI'21 87.1 44.5 82.3 10.7 0.8 29.9 13.9 13.1 81.6 86.0 60.3 25.1 83.1 40.1 24.4 40.5 45.2 +11.7 52.5 +13.9</cell></row><row><cell cols="20">ConTrans [20] 9 +15.3</cell></row><row><cell>SDCA [21]</cell><cell cols="19">Arxiv'21 88.4 45.9 83.9 24.0 1.7 38.1 25.2 17.0 85.3 82.9 67.3 26.6 87.1 47.2 28.6 53.4 50.2 +16.7 56.8 +18.2</cell></row><row><cell>PWCL [23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>CVPR'21 92.6 52.7 81.3 8.9 2.4 28.1 13.0 7.3 83.5 85.0 60.1 19.7 84.8 37.2 21.5 43.9 45.1 +11.6 52.5 +13.9 ProDA ? [48] CVPR'21 87.1 44.0 83.2 26.9 0.0 42.0 45.8 34.2 86.7 81.3 68.4 22.1 87.7 50.0 31.4 38.6 51.9 +18.4 58.5 +19.9 UPLR [45] ICCV'21 79.4 34.6 83.5 19.3 2.8 35.3 32.1 26.9 78.8 79.6 66.6 30.3 86.1 36.6 19.5 56.9 48.0 +14.5 54.6 +16.0 Ours -90.5 52.1 84.6 29.2 3.3 40.3 37.4 27.3 86.4 85.9 69.8 28.7 88.7 53.7 14.8 54.8 53.0 +19.5 59.6 +21.0</figDesc><table><row><cell></cell><cell>+14.7</cell></row><row><cell>CLST [29]</cell><cell>Arxiv'21 88.0 49.2 82.2 16.3 0.4 29.2 31.8 23.9 84.1 88.0 59.1 27.2 85.5 46.4 28.9 56.5 49.8 +16.3 57.8 +19.2</cell></row><row><cell>ESL [37]</cell><cell>CVPR'21 84.3 39.7 79.0 9.4 0.7 27.7 16.0 14.3 78.3 83.8 59.1 26.6 72.7 35.8 23.6 45.8 43.5 +10.0 50.7 +12.1</cell></row><row><cell>MetaCorrect [9]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies of different prototypes updating scheme for GTA5 ?</figDesc><table><row><cell cols="3">Cityscapes. Fixed refers to no-updating for calculated prototypes; Source means updat-</cell></row><row><cell cols="3">ing in a strict statistical way on in source domain as Equation 7; Mixed refers updating</cell></row><row><cell cols="2">in Equation 8 in both source and target domain.</cell><cell></cell></row><row><cell cols="3">Source Only Fixed Source Mixed mIoU</cell></row><row><cell>?</cell><cell></cell><cell>37.3</cell></row><row><cell>?</cell><cell>?</cell><cell>47.8</cell></row><row><cell>?</cell><cell>?</cell><cell>48.3</cell></row><row><cell>?</cell><cell>?</cell><cell>48.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Ablation studies of different percentages determining class-wise thresholds during self-training process for GTA5 ? Cityscapes. All methods use DeepLab-v2 with ResNet-101 backbone. Ablation studies of different contrastive adaptation choices for GTA5 ? Cityscapes. s ? s means Eq. 5 and t ? s means Eq. 4.</figDesc><table><row><cell cols="4">? (%) 30 40 50 60 70 80 90</cell></row><row><cell cols="4">mIoU 54.3 54.7 54.9 55.1 54.4 54.1 52.5</cell></row><row><cell cols="4">Source Only s ? s t ? s mIoU</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>37.3</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>44.9</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>46.8</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>48.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Qualitative segmentation results for GTA5 ? Cityscapes. From the left to right: target image, ground-truth, predictions by Source Only, FADA<ref type="bibr" target="#b42">[43]</ref> and our proposed method are shown.</figDesc><table><row><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell cols="2">vegetation unlabled</cell></row><row><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motocycle</cell><cell>bike</cell></row><row><cell cols="2">(a) Target image</cell><cell cols="2">(b)Ground truth</cell><cell cols="2">(c)Source Only</cell><cell cols="2">(d)FADA</cell><cell cols="2">(e)Ours</cell></row><row><cell>Fig. 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15384" to="15394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Addressing domain gap via content invariant representation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7528" to="7536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pit: Position-invariant transform for cross-fov domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8761" to="8770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metacorrection: Domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="705" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video object detection with locally-weighted deformable neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03747</idno>
		<title level="m">Stc: Spatio-temporal contrastive learning for video instance segmentation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning where to focus for efficient video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12975" to="12984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation by content transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12545</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantic distribution-aware contrastive adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05013</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Domain adaptation for semantic segmentation via patch-wise contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zebedin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11056</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Category-level adversarial adaptation for semantic segmentation using purified features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-domain semantic segmentation via domain-invariant interactive relation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Contrastive learning and selftraining for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>D?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02001</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised intra-domain adaptation for semantic segmentation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation using weak labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="571" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Esl: Entropy-guided self-supervised learning for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08658</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from scale-invariant examples for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Subhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="290" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Classes matter: A finegrained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9092" to="9101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Dirl: Domain-invariant representation learning for generalizable semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dast: Unsupervised domain adaptation in semantic segmentation based on discriminator attention and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10754" to="10762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Self-adversarial disentangling for specific domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Domain adaptive semantic segmentation with regional contrastive consistency regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05170</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Affinity space adaptation for semantic segmentation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2549" to="2561" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
