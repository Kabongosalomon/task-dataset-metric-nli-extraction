<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Embedding of Words and Labels for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
							<email>r.henao@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
							<email>lcarin@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Embedding of Words and Labels for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed. U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9 o m 8 P e e / e U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9 o m 8 P e e / e U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9 o m 8 P e e / e</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is a fundamental problem in natural language processing (NLP). The task is to annotate a given text sequence with one (or multiple) class label(s) describing its textual content. A key intermediate step is the text representation. Traditional methods represent text with hand-crafted features, such as sparse lexical features (e.g., n-grams) <ref type="bibr" target="#b38">(Wang and Manning, 2012)</ref>. Recently, neural models have been employed to learn text representations, including convolutional neural networks (CNNs) <ref type="bibr">(Kalchbrenner * Corresponding author et al., 2014;</ref><ref type="bibr" target="#b46">Zhang et al., 2017b;</ref><ref type="bibr" target="#b31">Shen et al., 2017)</ref> and recurrent neural networks (RNNs) based on long short-term memory (LSTM) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b39">Wang et al., 2018)</ref>.</p><p>To further increase the representation flexibility of such models, attention mechanisms <ref type="bibr" target="#b2">(Bahdanau et al., 2015)</ref> have been introduced as an integral part of models employed for text classification <ref type="bibr" target="#b41">(Yang et al., 2016)</ref>. The attention module is trained to capture the dependencies that make significant contributions to the task, regardless of the distance between the elements in the sequence. It can thus provide complementary information to the distance-aware dependencies modeled by RNN/CNN. The increasing representation power of the attention mechanism comes with increased model complexity.</p><p>Alternatively, several recent studies show that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings <ref type="bibr" target="#b13">(Joulin et al., 2016;</ref><ref type="bibr" target="#b40">Wieting et al., 2016;</ref><ref type="bibr" target="#b1">Arora et al., 2017;</ref><ref type="bibr" target="#b30">Shen et al., 2018a)</ref>. Particularly, <ref type="bibr" target="#b30">Shen et al. (2018a)</ref> quantitatively show that the word-embeddings-based text classification tasks can have the similar level of difficulty regardless of the employed models, using the concept of intrinsic dimension <ref type="bibr" target="#b20">(Li et al., 2018)</ref>. Thus, simple models are preferred. As the basic building blocks in neural-based NLP, word embeddings capture the similarities/regularities between words <ref type="bibr" target="#b26">Pennington et al., 2014)</ref>. This idea has been extended to compute embeddings that capture the semantics of word sequences (e.g., phrases, sentences, paragraphs and documents) <ref type="bibr" target="#b19">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b18">Kiros et al., 2015)</ref>. These representations are built upon various types of compositions of word vectors, ranging from simple averaging to sophisticated architectures. Further, they suggest that simple models are efficient and interpretable, and have the poten-arXiv:1805.04174v1 [cs.CL] 10 May 2018 tial to outperform sophisticated deep neural models.</p><p>It is therefore desirable to leverage the best of both lines of works: learning text representations to capture the dependencies that make significant contributions to the task, while maintaining low computational cost. For the task of text classification, labels play a central role of the final performance. A natural question to ask is how we can directly use label information in constructing the text-sequence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contribution</head><p>Our primary contribution is therefore to propose such a solution by making use of the label embedding framework, and propose the Label-Embedding Attentive Model (LEAM) to improve text classification. While there is an abundant literature in the NLP community on word embeddings (how to describe a word) for text representations, much less work has been devoted in comparison to label embeddings (how to describe a class). The proposed LEAM is implemented by jointly embedding the word and label in the same latent space, and the text representations are constructed directly using the text-label compatibility.</p><p>Our label embedding framework has the following salutary properties: (i) Label-attentive text representation is informative for the downstream classification task, as it directly learns from a shared joint space, whereas traditional methods proceed in multiple steps by solving intermediate problems.</p><p>(ii) The LEAM learning procedure only involves a series of basic algebraic operations, and hence it retains the interpretability of simple models, especially when the label description is available. (iii) Our attention mechanism (derived from the text-label compatibility) has fewer parameters and less computation than related methods, and thus is much cheaper in both training and testing, compared with sophisticated deep attention models. (iv) We perform extensive experiments on several text-classification tasks, demonstrating the effectiveness of our label-embedding attentive model, providing state-of-the-art results on benchmark datasets. (v) We further apply LEAM to predict the medical codes from clinical text. As an interesting by-product, our attentive model can highlight the informative key words for prediction, which in practice can reduce a doctor's burden on reading clinical notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Label embedding has been shown to be effective in various domains and tasks. In computer vision, there has been a vast amount of research on leveraging label embeddings for image classification <ref type="bibr" target="#b0">(Akata et al., 2016)</ref>, multimodal learning between images and text <ref type="bibr" target="#b9">(Frome et al., 2013;</ref><ref type="bibr" target="#b17">Kiros et al., 2014)</ref>, and text recognition in images <ref type="bibr" target="#b28">(Rodriguez-Serrano et al., 2013)</ref>. It is particularly successful on the task of zero-shot learning <ref type="bibr" target="#b25">(Palatucci et al., 2009;</ref><ref type="bibr" target="#b43">Yogatama et al., 2015;</ref><ref type="bibr" target="#b21">Ma et al., 2016)</ref>, where the label correlation captured in the embedding space can improve the prediction when some classes are unseen. In NLP, labels embedding for text classification has been studied in the context of heterogeneous networks in <ref type="bibr" target="#b36">(Tang et al., 2015)</ref> and multitask learning in <ref type="bibr" target="#b44">(Zhang et al., 2017a)</ref>, respectively. To the authors' knowledge, there is little research on investigating the effectiveness of label embeddings to design efficient attention models, and how to joint embedding of words and labels to make full use of label information for text classification has not been studied previously, representing a contribution of this paper.</p><p>For text representation, the currently bestperforming models usually consist of an encoder and a decoder connected through an attention mechanism <ref type="bibr" target="#b37">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>, with successful applications to sentiment classification , sentence pair modeling <ref type="bibr" target="#b42">(Yin et al., 2016)</ref> and sentence summarization <ref type="bibr">(Rush et al., 2015)</ref>. Based on this success, more advanced attention models have been developed, including hierarchical attention networks <ref type="bibr" target="#b41">(Yang et al., 2016)</ref>, attention over attention <ref type="bibr" target="#b6">(Cui et al., 2016)</ref>, and multi-step attention <ref type="bibr" target="#b10">(Gehring et al., 2017)</ref>. The idea of attention is motivated by the observation that different words in the same context are differentially informative, and the same word may be differentially important in a different context. The realization of "context" varies in different applications and model architectures. Typically, the context is chosen as the target task, and the attention is computed over the hidden layers of a CNN/RNN. Our attention model is directly built in the joint embedding space of words and labels, and the context is specified by the label embedding.</p><p>Several recent works <ref type="bibr" target="#b37">(Vaswani et al., 2017;</ref><ref type="bibr">Shen et al., 2018b,c)</ref> have demonstrated that sim-ple attention architectures can alone achieve stateof-the-art performance with less computational time, dispensing with recurrence and convolutions entirely. Our work is in the same direction, sharing the similar spirit of retaining model simplicity and interpretability. The major difference is that the aforementioned work focused on self attention, which applies attention to each pair of word tokens from the text sequences. In this paper, we investigate the attention between words and labels, which is more directly related to the target task. Furthermore, the proposed LEAM has much less model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Throughout this paper, we denote vectors as bold, lower-case letters, and matrices as bold, uppercase letters. We use for element-wise division when applied to vectors or matrices. We use ? for function composition, and ? p for the set of one hot vectors in dimension p.</p><p>Given a training set S = {(X n , y n )} N n=1 of pair-wise data, where X ? X is the text sequence, and y ? Y is its corresponding label. Specifically, y is a one hot vector in single-label problem and a binary vector in multi-label problem, as defined later in Section 4.1. Our goal for text classification is to learn a function f : X ? Y by minimizing an empirical risk of the form:</p><formula xml:id="formula_0">min f ?F 1 N N n=1 ?(y n , f (X n ))<label>(1)</label></formula><p>where ? : Y ? Y ? R measures the loss incurred from predicting f (X) when the true label is y, where f belongs to the functional space F. In the evaluation stage, we shall use the 0/1 loss as a target loss: ?(y, z) = 0 if y = z, and 1 otherwise.</p><p>In the training stage, we consider surrogate losses commonly used for structured prediction in different problem setups (see Section 4.1 for details on the surrogate losses used in this paper). More specifically, an input sequence X of length L is composed of word tokens: X = {x 1 , ? ? ? , x L }. Each token x l is a one hot vector in the space ? D , where D is the dictionary size. Performing learning in ? D is computationally expensive and difficult. An elegant framework in NLP, initially proposed in <ref type="bibr" target="#b19">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b26">Pennington et al., 2014;</ref><ref type="bibr" target="#b18">Kiros et al., 2015)</ref>, allows to concisely perform learning by mapping the words into an embedding space. The framework relies on so called word embedding: ? D ? R P , where P is the dimensionality of the embedding space. Therefore, the text sequence X is represented via the respective word embedding for each token: V = {v 1 , ? ? ? , v L }, where v l ? R P . A typical text classification method proceeds in three steps, endto-end, by considering a function decomposition f = f 0 ? f 1 ? f 2 as shown in <ref type="figure" target="#fig_7">Figure 1</ref>(a):</p><p>? f 0 : X ? V, the text sequence is represented as its word-embedding form V, which is a matrix of P ? L.</p><p>? f 1 : V ? z, a compositional function f 1 aggregates word embeddings into a fixed-length vector representation z.</p><p>? f 2 : z ? y, a classifier f 2 annotates the text representation z with a label.</p><p>A vast amount of work has been devoted to devising the proper functions f 0 and f 1 , i.e., how to represent a word or a word sequence, respectively. The success of NLP largely depends on the effectiveness of word embeddings in f 0 <ref type="bibr" target="#b3">(Bengio et al., 2003;</ref><ref type="bibr" target="#b4">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b26">Pennington et al., 2014)</ref>. They are often pre-trained offline on large corpus, then refined jointly via f 1 and f 2 for task-specific representations. Furthermore, the design of f 1 can be broadly cast into two categories. The popular deep learning models consider the mapping as a "black box," and have employed sophisticated CNN/RNN architectures to achieve state-of-theart performance <ref type="bibr" target="#b45">(Zhang et al., 2015;</ref><ref type="bibr" target="#b41">Yang et al., 2016)</ref>. On the contrary, recent studies show that simple manipulation of the word embeddings, e.g., mean or max-pooling, can also provide surprisingly excellent performance <ref type="bibr" target="#b13">(Joulin et al., 2016;</ref><ref type="bibr" target="#b40">Wieting et al., 2016;</ref><ref type="bibr" target="#b1">Arora et al., 2017;</ref><ref type="bibr" target="#b30">Shen et al., 2018a)</ref>. Nevertheless, these methods only leverage the information from the input text sequence.</p><p>4 Label-Embedding Attentive Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>By examining the three steps in the traditional pipeline of text classification, we note that the use of label information only occurs in the last step, when learning f 2 , and its impact on learning the representations of words in f 0 or word sequences in f 1 is ignored or indirect. Hence, we propose a new pipeline by incorporating label information in every step, as shown in <ref type="figure" target="#fig_7">Figure 1</ref>   </p><formula xml:id="formula_1">V f U = " &gt; A A A C C X i c b V C 7 T s M w F H V 4 l v I K M L I Y K i S m K k V I w F b B U M Y i E V q p i S r H c V q r t h P Z D q i K M r P w K y w M g F j 5 A z b + B j f N A C 1 H s n R 8 z r 3 2 v S d I G F X a c b 6 t h c W l 5 Z X V y l p 1 f W N z a 9 v e 2 b 1 T c S o x c X H M Y t k N k C K M C u J q q h n p J p I g H j D S C U Z X E 7 9 z T 6 S i s b j V 4 4 T 4 H A 0 E j S h G 2 k h 9 + 8 A T 5 A H H n C M R Z l 6 L I 5 1 n m R d E s J X n 1 e L e t 2 t O 3 S k A 5 0 m j J D V Q o t 2 3 v 7 w w x i k n Q m O G l O o 1 n E T 7 G Z K a Y k b M q 6 k i C c I j N C A 9 Q w X i R P l Z s U o O j 4 w S w i i W 5 g g N C / V 3 R 4 a 4 U m M e m E o z 2 1 D N e h P x P 6 + X 6 u j c z 6 h I U k 0 E n n 4 U p Q z q G E 5 y g S G V B G s 2 N g R h S c 2 s E A + R R F i b 9 K o m h M b s y v P E P a l f 1 J 2 b 0 1 r z s k y j A v b B I T g G D X A G m u A a t I E L M H g E z + A V v F l P 1 o v 1 b n 1 M S x e</formula><formula xml:id="formula_2">V f U = " &gt; A A A C C X i c b V C 7 T s M w F H V 4 l v I K M L I Y K i S m K k V I w F b B U M Y i E V q p i S r H c V q r t h P Z D q i K M r P w K y w M g F j 5 A z b + B j f N A C 1 H s n R 8 z r 3 2 v S d I G F X a c b 6 t h c W l 5 Z X V y l p 1 f W N z a 9 v e 2 b 1 T c S o x c X H M Y t k N k C K M C u J q q h n p J p I g H j D S C U Z X E 7 9 z T 6 S i s b j V 4 4 T 4 H A 0 E j S h G 2 k h 9 + 8 A T 5 A H H n C M R Z l 6 L I 5 1 n m R d E s J X n 1 e L e t 2 t O 3 S k A 5 0 m j J D V Q o t 2 3 v 7 w w x i k n Q m O G l O o 1 n E T 7 G Z K a Y k b M q 6 k i C c I j N C A 9 Q w X i R P l Z s U o O j 4 w S w i i W 5 g g N C / V 3 R 4 a 4 U m M e m E o z 2 1 D N e h P x P 6 + X 6 u j c z 6 h I U k 0 E n n 4 U p Q z q G E 5 y g S G V B G s 2 N g R h S c 2 s E A + R R F i b 9 K o m h M b s y v P E P a l f 1 J 2 b 0 1 r z s k y j A v b B I T g G D X A G m u A a t I E L M H g E z + A V v F l P 1 o v 1 b n 1 M S x e</formula><formula xml:id="formula_3">V f U = " &gt; A A A C C X i c b V C 7 T s M w F H V 4 l v I K M L I Y K i S m K k V I w F b B U M Y i E V q p i S r H c V q r t h P Z D q i K M r P w K y w M g F j 5 A z b + B j f N A C 1 H s n R 8 z r 3 2 v S d I G F X a c b 6 t h c W l 5 Z X V y l p 1 f W N z a 9 v e 2 b 1 T c S o x c X H M Y t k N k C K M C u J q q h n p J p I g H j D S C U Z X E 7 9 z T 6 S i s b j V 4 4 T 4 H A 0 E j S h G 2 k h 9 + 8 A T 5 A H H n C M R Z l 6 L I 5 1 n m R d E s J X n 1 e L e t 2 t O 3 S k A 5 0 m j J D V Q o t 2 3 v 7 w w x i k n Q m O G l O o 1 n E T 7 G Z K a Y k b M q 6 k i C c I j N C A 9 Q w X i R P l Z s U o O j 4 w S w i i W 5 g g N C / V 3 R 4 a 4 U m M e m E o z 2 1 D N e h P x P 6 + X 6 u j c z 6 h I U k 0 E n n 4 U p Q z q G E 5 y g S G V B G s 2 N g R h S c 2 s E A + R R F i b 9 K o m h M b s y v P E P a l f 1 J 2 b 0 1 r z s k y j A v b B I T g G D X A G m u A a t I E L M H g E z + A V v F l P 1 o v 1 b n 1 M S x</formula><p>e s s m c P / I H 1 + Q P m w p q U &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 a r K u f y R 9 c N 9 Z i N s z n h Y 2 l 3 6 Q</p><formula xml:id="formula_4">x I = " &gt; A A A C U n i c b V J N T w I x E O 3 i F y I q 6 t F L I z H x R B Z j o t 6 I X j x i F C U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9</formula><p>o m 8 P e e / e V 8 H 9 k l 9 r w Z v l H K C 5 K J R / A H p C t 3 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 a r K u f y R 9 c N 9 Z i N s z n h Y 2 l 3 6 Q</p><formula xml:id="formula_5">x I = " &gt; A A A C U n i c b V J N T w I x E O 3 i F y I q 6 t F L I z H x R B Z j o t 6 I X j x i F C U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9</formula><p>o m 8 P e e / e V 8 H 9 k l 9 r w Z v l H K C 5 K J R / A H p C t 3 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 a r K u f y R 9 c N 9 Z i N s z n h Y 2 l 3 6 Q     We focus on learning label embedding C (how to embed class labels in a Euclidean space), and leveraging the "compatibility" G between embedded words and labels to derive the attention score ? for improved z. Note that ? denotes the cosine similarity between C and V. In this figure, there are K=2 classes.</p><formula xml:id="formula_6">x I = " &gt; A A A C U n i c b V J N T w I x E O 3 i F y I q 6 t F L I z H x R B Z j o t 6 I X j x i F C U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R</formula><formula xml:id="formula_7">R s 1 X O p Y U a 1 y i u V 8 L T b i U = " &gt; A A A C M 3 i c b V D L S g M x F M 3 4 r P V V d e k m W A R X Z S q C u h P d C G 4 q d V R o S 8 l k 7 t R g H k O S U c r Q j 3 L j h 7 g R w Y W K W / / B z H Q W V r 0 Q c j j n 3 u S e E y a c G e v 7 L 9 7 U 9 M z s 3 H x l o b q 4 t L y y W l t b v z Q q 1 R Q C q r j S 1 y E x w J m E w D L L 4 T r R Q E T I 4 S q 8 P c n 1 q z v Q h i l 5 Y Y c J 9 A Q Z S B Y z S q y j + r W z r o R 7 q o Q g M s q 6 b U H s K M u 6 Y Y z b o 1 F 1 Q g v B k r t C V D w y Q + E u X J B 5 Y y H 2 a 3 W / 4 R e F / 4 J m C e q o r F a / 9 t S N F E 0 F S E s 5 M a b T 9 B P b y 4 i 2 j H J w z 6 Y G E k J v y Q A 6 D k o i w P S y w v Q I b z s m w r H S 7 k i L C / b n R E a E y d d 0 n c 7 V j f m t 5 e R / W i e 1 8 U E v Y z J J L U g 6 / i h O O b Y K 5 w n i i G m g l g 8 d I F Q z t y u m N 0 Q T a l 3 O V R d C 8 7 f l v y D Y b R w 2 / P O 9 + t F x m U Y F b a I t t I O a a B 8 d o V P U Q g G i 6 A E</formula><formula xml:id="formula_8">U Z h 7 Y j k s Y l F N c + x w F 1 G V x 2 T 7 S F E = " &gt; A A A C C X i c b V C 7 T s M w F H X K q 5 R X g J H F U C E x V S l C A r Y K F s Y i k b Z S E 1 W O 4 7 R W b S e y H V A V Z W b h V 1 g Y A L</formula><formula xml:id="formula_9">U Z h 7 Y j k s Y l F N c + x w F 1 G V x 2 T 7 S F E = " &gt; A A A C C X i c b V C 7 T s M w F H X K q 5 R X g J H F U C E x V S l C A r Y K F s Y i k b Z S E 1 W O 4 7 R W b S e y H V A V Z W b h V 1 g Y A L</formula><formula xml:id="formula_10">U Z h 7 Y j k s Y l F N c + x w F 1 G V x 2 T 7 S F E = " &gt; A A A C C X i c b V C 7 T s M w F H X K q 5 R X g J H F U C E x V S l C A r Y K F s Y i k b Z S E 1 W O 4 7 R W b S e y H V A V Z W b h V 1 g Y A L</formula><p>? f 0 : Besides embedding words, we also embed all the labels in the same space, which act as the "anchor points" of the classes to influence the refinement of word embeddings.</p><p>? f 1 : The compositional function aggregates word embeddings into z, weighted by the compatibility between labels and words.</p><p>? f 2 : The learning of f 2 remains the same, as it directly interacts with labels.</p><p>Under the proposed label embedding framework, we specifically describe a label-embedding attentive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Embeddings of Words and Labels</head><p>We propose to embed both the words and the labels into a joint space i.e., ? D ? R P and Y ? R P . The label embeddings are C = [c 1 , ? ? ? , c K ], where K is the number of classes. A simple way to measure the compatibility of label-word pairs is via the cosine similarity</p><formula xml:id="formula_11">G = (C V) ? ,<label>(2)</label></formula><p>where? is the normalization matrix of size K?L, with each element obtained as the multiplication of 2 norms of the c-th label embedding and l-th word embedding:? kl = c k v l .</p><p>To further capture the relative spatial information among consecutive words (i.e., phrases 1 ) and introduce non-linearity in the compatibility measure, we consider a generalization of (2). Specifically, for a text phase of length 2r + 1 centered at l, the local matrix block G l?r:l+r in G measures the label-to-token compatibility for the "label-phrase" pairs. To learn a higher-level compatibility stigmatization u l between the l-th phrase and all labels, we have:</p><formula xml:id="formula_12">u l = ReLU(G l?r:l+r W 1 + b 1 ),<label>(3)</label></formula><p>where W 1 ? R 2r+1 and b 1 ? R K are parameters to be learned, and u l ? R K . The largest compatibility value of the l-th phrase wrt the labels is collected:</p><formula xml:id="formula_13">m l = max-pooling(u l ).<label>(4)</label></formula><p>Together, m is a vector of length L. The compatibility/attention score for the entire text sequence is:</p><formula xml:id="formula_14">? = SoftMax(m),<label>(5)</label></formula><p>where the l-th element of SoftMax is</p><formula xml:id="formula_15">? l = exp(m l ) L l =1 exp(m l )</formula><p>.</p><p>The text sequence representation can be simply obtained via averaging the word embeddings, weighted by label-based attention score:</p><formula xml:id="formula_16">z = l ? l v l .<label>(6)</label></formula><p>Relation to Predictive Text Embeddings Predictive Text Embeddings (PTE) <ref type="bibr" target="#b36">(Tang et al., 2015)</ref> is the first method to leverage label embeddings to improve the learned word embeddings. We discuss three major differences between PTE and our LEAM: (i) The general settings are different. PTE casts the text representation through heterogeneous networks, while we consider text representation through an attention model. (ii) In PTE, the text representation z is the averaging of word embeddings. In LEAM, z is weighted averaging of word embeddings through the proposed labelattentive score in (6). (iii) PTE only considers the linear interaction between individual words and labels. LEAM greatly improves the performance by considering nonlinear interaction between phrase and labels. Specifically, we note that the text embedding in PTE is similar with a very special case of LEAM, when our window size r = 1 and attention score ? is uniform. As shown later in <ref type="figure">Figure 2(c)</ref> of the experimental results, LEAM can be significantly better than the PTE variant.</p><p>Training Objective The proposed joint embedding framework is applicable to various text classification tasks. We consider two setups in this paper. For a learned text sequence representation z = f 1 ?f 0 (X), we jointly optimize f = f 0 ?f 1 ?f 2 over F, where f 2 is defined according to the specific tasks:</p><p>? Single-label problem: categorizes each text instance to precisely one of K classes, y ?</p><formula xml:id="formula_17">? K min f ?F 1 N N n=1 CE(y n , f 2 (z n )),<label>(7)</label></formula><p>where CE(?, ?) is the cross entropy between two probability vectors, and f 2 (z n ) = SoftMax (z n ), with z n = W 2 z n + b 2 and W 2 ? R K?P , b 2 ? R K are trainable parameters.</p><p>? Multi-label problem: categorizes each text instance to a set of K target labels {y k ? ? 2 |k = 1, ? ? ? , K}; there is no constraint on how many of the classes the instance can be assigned to, and</p><formula xml:id="formula_18">min f ?F 1 N K N n=1 K k=1 CE(y nk , f 2 (z nk ),<label>(8)</label></formula><p>where f 2 (z nk ) = 1 1+exp(z nk ) , and z nk is the kth column of z n .</p><p>To summarize, the model parameters ? = {V, C, W 1 , b 1 , W 2 , b 2 }. They are trained endto-end during learning. {W 1 , b 1 } and {W 2 , b 2 } are weights in f 1 and f 2 , respectively, which are treated as standard neural networks. For the joint embeddings {V, C} in f 0 , the pre-trained word embeddings are used as initialization if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning &amp; Testing with LEAM</head><p>Learning and Regularization The quality of the jointly learned embeddings are key to the model performance and interpretability. Ideally, we hope that each label embedding acts as the "anchor" points for each classes: closer to the word/sequence representations that are in the same classes, while farther from those in different classes. To best achieve this property, we consider to regularize the learned label embeddings c k to be on its corresponding manifold. This is imposed by the fact c k should be easily classified as the correct label y k :</p><formula xml:id="formula_19">min f ?F 1 K K n=1 CE(y k , f 2 (c k )),<label>(9)</label></formula><p>where f 2 is specficied according to the problem in either <ref type="formula" target="#formula_17">(7)</ref> or <ref type="formula" target="#formula_18">(8)</ref>. This regularization is used as a penalty in the main training objective in <ref type="formula" target="#formula_17">(7)</ref> or <ref type="formula" target="#formula_18">(8)</ref>, and the default weighting hyperparameter is set as 1. It will lead to meaningful interpretability of learned label embeddings as shown in the experiments.</p><p>Interestingly in text classification, the class itself is often described as a set of E words {e i , i = 1, ? ? ? , E}. These words are considered as the most representative description of each class, and highly distinguishing between different classes. For example, the Yahoo! Answers Topic dataset <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref> contains ten classes, most of which have two words to precisely describe its class-specific features, such as "Computers &amp; Internet", "Business &amp; Finance" as well as "Politics &amp; Government" etc. We consider to use each label's corresponding pre-trained word embeddings as the initialization of the label embeddings. For the datasets without representative class descriptions, one may initialize the label embeddings as random samples drawn from a standard Gaussian distribution.</p><p>Testing Both the learned word and label embeddings are available in the testing stage. We clarify that the label embeddings C of all class candidates Y are considered as the input in the testing stage; one should distinguish this from the use of groundtruth label y in prediction. For a text sequence X, one may feed it through the proposed pipeline for prediction: (i) f 1 : harvesting the word embeddings V, (ii) f 2 : V interacts with C to obtain G, pooled as ?, which further attends V to derive z, and (iii) f 3 : assigning labels based on the tasks. To speed up testing, one may store G offline, and avoid its online computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>Complexity We compare CNN, LSTM, Simple Word Embeddings-based Models (SWEM) <ref type="bibr" target="#b30">(Shen et al., 2018a)</ref> and our LEAM wrt the parameters and computational speed. For the CNN, we assume the same size m for all filters. Specifically, h represents the dimension of the hidden units in the LSTM or the number of filters in the CNN; R denotes the number of blocks in the Bi-BloSAN; P denotes the final sequence representation dimension. Similar to <ref type="bibr" target="#b37">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b30">Shen et al., 2018a)</ref>, we examine the number of compositional parameters, computational complexity and sequential steps of the four methods. As shown in <ref type="table" target="#tab_0">Table 1</ref>, both the CNN and LSTM have a large number of compositional parameters. Since K m, h, the number of parameters in our models is much smaller than for the CNN and LSTM models. For the computational complexity, our model is almost same order as the most simple SWEM model, and is smaller than the CNN or LSTM by a factor of mh/K or h/K.</p><formula xml:id="formula_20">Seq. Operation CNN m ? h ? P O(m ? h ? L ? P ) O(1) LSTM 4 ? h ? (h + P ) O(L ? h 2 + h ? L ? P ) O(L) SWEM 0 O(L ? P ) O(1) Bi-BloSAN 7?P 2 +5?P O(P 2 ?L 2 /R+P 2 ?L+P 2 ?R 2 ) O(1) Our model K ? P O(K ? L ? P ) O(1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Setup We use 300-dimensional GloVe word embeddings <ref type="bibr" target="#b26">Pennington et al. (2014)</ref> as initialization for word embeddings and label embeddings in our model. Out-Of-Vocabulary (OOV) words are initialized from a uniform distribution with range [?0.01, 0.01]. The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task. We train our model's parameters with the Adam Optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.001, and a minibatch size of 100. Dropout regularization <ref type="bibr" target="#b35">(Srivastava et al., 2014)</ref> is employed on the final MLP layer, with dropout rate 0.5. The model is implemented using Tensorflow and is trained on GPU Titan X.</p><p>The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM  <ref type="table">Table 2</ref>: Summary statistics of five datasets, including the number of classes, number of training samples and number of testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification on Benchmark Datasets</head><p>We test our model on the same five standard benchmark datasets as in <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref>. The summary statistics of the data are shown in <ref type="table">Table  2</ref>, with content specified below:</p><p>? AGNews: Topic classification over four categories of Internet news articles <ref type="bibr" target="#b8">(Del Corso et al., 2005)</ref> composed of titles plus description classified into: World, Entertainment, Sports and Business.</p><p>? Yelp Review Full: The dataset is obtained from the Yelp Dataset Challenge in 2015, the task is sentiment classification of polarity star labels ranging from 1 to 5.</p><p>? Yelp Review Polarity: The same set of text reviews from Yelp Dataset Challenge in 2015, except that a coarser sentiment definition is considered: 1 and 2 are negative, and 4 and 5 as positive.</p><p>? DBPedia: Ontology classification over fourteen non-overlapping classes picked from DBpedia 2014 (Wikipedia).</p><p>? Yahoo! Answers Topic: Topic classification over ten largest main categories from Yahoo! Answers Comprehensive Questions and Answers version 1.0, including question title, question content and best answer.</p><p>We compare with a variety of methods, including (i) the bag-of-words in <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref>; (ii) sophisticated deep CNN/RNN models: large/small word CNN, LSTM reported in <ref type="bibr" target="#b45">(Zhang et al., 2015;</ref><ref type="bibr" target="#b7">Dai and Le, 2015)</ref> and deep CNN (29 layer) <ref type="bibr" target="#b5">(Conneau et al., 2017)</ref>; (iii) simple compositional methods: fastText <ref type="bibr" target="#b13">(Joulin et al., 2016)</ref> and simple word embedding models (SWEM) <ref type="bibr" target="#b30">(Shen et al., 2018a)</ref>; (iv) deep attention models: hierarchical attention network (HAN) (Yang et al.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Yahoo DBPedia AGNews Yelp P. Yelp F. Bag-of-words <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref> 68.90 96.60 88.80 92.20 58.00 Small word CNN <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref> 69.98 98.15 89.13 94.46 58.59 Large word CNN <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref> 70.94 98.28 91.45 95.11 59.48 LSTM <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref> 70  <ref type="bibr" target="#b13">(Joulin et al., 2016)</ref> 72.30 98.60 92.50 95.70 63.90 HAN <ref type="bibr" target="#b41">(Yang et al., 2016)</ref> 75.80 ----Bi-BloSAN <ref type="bibr" target="#b33">(Shen et al., 2018c)</ref> 76   <ref type="bibr" target="#b33">(Shen et al., 2018c)</ref>. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Testing accuracy Simple compositional methods indeed achieve comparable performance as the sophisticated deep CNN/RNN models. On the other hand, deep hierarchical attention model can improve the pure CNN/RNN models. The recently proposed self-attention network generally yield higher accuracy than previous methods. All approaches are better than traditional bag-of-words method. Our proposed LEAM outperforms the state-of-the-art methods on two largest datasets, i.e., Yahoo and DBPedia. On other datasets, LEAM ranks the 2nd or 3rd best, which are similar to top 1 method in term of the accuracy. This is probably due to two reasons: (i) the number of classes on these datasets is smaller, and (ii) there is no explicit corresponding word embedding available for the label embedding initialization during learning. The potential of label embedding may not be fully exploited. As the ablation study, we replace the nonlinear compatibility <ref type="formula" target="#formula_12">(3)</ref> to the linear one in (2) . The degraded performance demonstrates the necessity of spatial dependency and nonlinearity in constructing the attentions. Nevertheless, we argue LEAM is favorable for text classification, by comparing the model size and time cost <ref type="table" target="#tab_6">Table 4</ref>, as well as convergence speed in <ref type="figure">Figure 2(a)</ref>. The time cost is reported as the wall-clock time for 1000 iterations. LEAM maintains the simplicity and low cost of SWEM, compared with other models. LEAM uses much less model parameters, and converges significantly  faster than Bi-BloSAN. We also compare the performance when only a partial dataset is labeled, the results are shown in <ref type="figure">Figure 2(b)</ref>. LEAM consistently outperforms other methods with different proportion of labeled data.</p><p>Hyper-parameter Our method has an additional hyperparameter, the window size r to define the length of "phase" to construct the attention. Larger r captures long term dependency, while smaller r enforces the local dependency. We study its impact in <ref type="figure">Figure 2</ref>(c). The topic classification tasks generally requires a larger r, while sentiment classification tasks allows relatively smaller r. One may safely choose r around 50 if not finetuning. We report the optimal results in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Representational Ability</head><p>Label embeddings are highly meaningful To provide insight into the meaningfulness of the learned representations, in <ref type="figure" target="#fig_8">Figure 3</ref> we visualize the correlation between label embeddings and document embeddings based on the Yahoo dateset. First, we compute the averaged document embeddings per class:z k = 1  text manifold for class k. Ideally, the perfect label embedding c k should be the representative anchor point for class k. We compute the cosine similarity betweenz k and c k across all the classes, shown in <ref type="figure" target="#fig_8">Figure 3</ref>(a). The rows are averaged per-class document embeddings, while columns are label embeddings. Therefore, the on-diagonal elements measure how representative the learned label embeddings are to describe its own classes, while off-diagonal elements reflect how distinctive the label embeddings are to be separated from other classes. The high on-diagonal elements and low off-diagonal elements in <ref type="figure" target="#fig_8">Figure 3(a)</ref> indicate the superb ability of the label representations learned from LEAM. Further, since both the document and label embeddings live in the same high-dimensional space, we use t-SNE (Maaten and Hinton, 2008) to visualize them on a 2D map in <ref type="figure" target="#fig_8">Figure 3(b)</ref>. Each color represents a different class, the point clouds are document embeddings, and the label embeddings are the large dots with black circles. As can be seen, each label embedding falls into the inter-nal region of the respective manifold, which again demonstrate the strong representative power of label embeddings.</p><formula xml:id="formula_21">|S k | i?S k z i ,</formula><p>Interpretability of attention Our attention score ? can be used to highlight the most informative words wrt the downstream prediction task. We visualize two examples in <ref type="figure" target="#fig_9">Figure 4(a)</ref> for the Yahoo dataset. The darker yellow means more important words. The 1st text sequence is on the topic of "Sports", and the 2nd text sequence is "Entertainment". The attention score can correctly detect the key words with proper scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Applications to Clinical Text</head><p>To demonstrate the practical value of label embeddings, we apply LEAM for a real health care scenario: medical code prediction on the Electronic Health Records dataset. A given patient may have multiple diagnoses, and thus multi-label learning is required.</p><p>Specifically, we consider an open-access dataset, MIMIC-III (Johnson et al., 2016), which AUC F1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Macro Micro Macro Micro P@5 Logistic Regression 0.829 0.864 0.477 0.533 0.546 Bi-GRU 0.828 0.868 0.484 0.549 0.591 CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 0.876 0.907 0.576 0.625 0.620 C-MemNN <ref type="bibr" target="#b27">(Prakash et al., 2017)</ref> 0.833 ---0.42 Attentive LSTM <ref type="bibr" target="#b34">(Shi et al., 2017)</ref> -0.900 -0.532 -CAML <ref type="bibr" target="#b24">(Mullenbach et al., 2018)</ref> 0.875 0.909 0.532 0.614 0.609 LEAM 0.881 0.912 0.540 0.619 0.612 <ref type="table">Table 5</ref>: Quantitative results for doctor-notes multi-label classification task.</p><p>contains text and structured records from a hospital intensive care unit. Each record includes a variety of narrative notes describing a patients stay, including diagnoses and procedures. They are accompanied by a set of metadata codes from the International Classification of Diseases (ICD), which present a standardized way of indicating diagnoses/procedures. To compare with previous work, we follow <ref type="bibr" target="#b34">(Shi et al., 2017;</ref><ref type="bibr" target="#b24">Mullenbach et al., 2018)</ref>, and preprocess a dataset consisting of the most common 50 labels. It results in 8,067 documents for training, 1,574 for validation, and 1,730 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare against the three baselines: a logistic regression model with bag-ofwords, a bidirectional gated recurrent unit (Bi-GRU) and a single-layer 1D convolutional network <ref type="bibr" target="#b15">(Kim, 2014)</ref>. We also compare with three recent methods for multi-label classification of clinical text, including Condensed Memory Networks (C-MemNN) <ref type="bibr" target="#b27">(Prakash et al., 2017)</ref>, Attentive LSTM <ref type="bibr" target="#b34">(Shi et al., 2017)</ref> and Convolutional Attention (CAML) <ref type="bibr" target="#b24">(Mullenbach et al., 2018)</ref>. To quantify the prediction performance, we follow <ref type="bibr" target="#b24">(Mullenbach et al., 2018)</ref> to consider the micro-averaged and macro-averaged F1 and area under the ROC curve (AUC), as well as the precision at n (P@n). Micro-averaged values are calculated by treating each (text, code) pair as a separate prediction. Macro-averaged values are calculated by averaging metrics computed per-label. P@n is the fraction of the n highestscored labels that are present in the ground truth.</p><p>The results are shown in <ref type="table">Table 5</ref>. LEAM provides the best AUC score, and better F1 and P@5 values than all methods except CNN. CNN consistently outperforms the basic Bi-GRU architecture, and the logistic regression baseline performs worse than all deep learning architectures. We emphasize that the learned attention can be very useful to reduce a doctor's reading burden. As shown in <ref type="figure" target="#fig_9">Figure 4(b)</ref>, the health related words are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we first investigate label embeddings for text representations, and propose the label-embedding attentive models. It embeds the words and labels in the same joint space, and measures the compatibility of word-label pairs to attend the document representations. The learning framework is tested on several large standard datasets and a real clinical text application. Compared with the previous methods, our LEAM algorithm requires much lower computational cost, and achieves better if not comparable performance relative to the state-of-the-art. The learned attention is highly interpretable: highlighting the most informative words in the text sequence for the downstream classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " u s N I E C u G P 8 Y y D G t y I d e s r n E +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>s s m c P / I H 1 + Q P m w p q U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u s N I E C u G P 8 Y y D G t y I d e s r n E +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s s m c P / I H 1 + Q P m w p q U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u s N I E C u G P 8 Y y D G t y I d e s r n E +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9 o m 8 P e e / e V 8 H 9 k l 9 r w Z v l H K C 5 K J R / A H p C t 3 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J P S 9 0 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>9 o z f 0 7 j 1 6 r 9 6 H 9 z l u n f L K m Q 0 0 U d 7 X N 5 I 5 r W k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J P S 9 0 0 R s 1 X O p Y U a 1 y i u V 8 L Tb i U = " &gt; A A A C M 3 i c b V D L S g M x F M 3 4 r P V V d e k m W A R X Z S q C u h P d C G 4 q d V R o S 8 l k 7 t R g H k O S U c r Q j 3 L j h 7 g R w Y W K W / / B z H Q W V r 0 Q c j j n 3 u S e E y a c G e v 7 L 9 7 U 9 M z s 3 H x l o b q 4 t L y y W l t b v z Q q 1 R Q C q r j S 1 y E x w J m E w D L L 4 T r R Q E T I 4 S q 8 P c n 1 q z v Q h i l 5 Y Y c J 9 A Q Z S B Y z S q y j + r W z r o R 7 q o Q g M s q 6 b U H s K M u 6 Y Y z b o 1 F 1 Q g v B k r t C V D w y Q + E u X J B 5 Y y H 2 a 3 W / 4 R e F / 4 J m C e q o r F a / 9 t S N F E 0 F S E s 5 M a b T 9 B P b y 4 i 2 j H J w z 6 Y G E k J v y Q A 6 D k o i w P S y w v Q I b z s m w r H S 7 k i L C / b n R E a E y d d 0 n c 7 V j f m t 5 e R / W i e 1 8 U E v Y z J J L U g 6 / i h O O b Y K 5 w n i i G m g l g 8 d I F Q z t y u m N 0 Q T a l 3 O V R d C 8 7 f l v y D Y b R w 2 / P O 9 + t F x m U Y F b a I t t I Oa a B 8 d o V P U Q g G i 6 A E 9 o z f 0 7 j 1 6 r 9 6 H 9 z l u n f L K m Q 0 0 U d 7 X N 5 I 5 r W k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J P S 9 0 0 R s 1 X O p Y U a 1 y i u V 8 L T b i U = " &gt; A A A C M 3 i c b V D L S g M x F M 3 4 r P V V d e k m W A R X Z S q C u h P d C G 4 q d V R o S 8 l k 7 t R g H k O S U c r Q j 3 L j h 7 g R w Y W K W / / B z H Q W V r 0 Q c j j n 3 u S e E y a c G e v 7 L 9 7 U 9 M z s 3 H x l o b q 4 t L y y W l t b v z Q q 1 R Q C q r j S 1 y E x w J m E w D L L 4 T r R Q E T I 4 S q 8 P c n 1 q z v Q h i l 5 Y Y c J 9 A Q Z S B Y z S q y j + r W z r o R 7 q o Q g M s q 6 b U H s K M u 6 Y Y z b o 1 F 1 Q g v B k r t C V D w y Q + E u X J B 5 Y y H 2 a 3 W / 4 R e F / 4 J m C e q o r F a / 9 t S N F E 0 F S E s 5 M a b T 9 B P b y 4 i 2 j H J w z 6 Y G E k J v y Q A 6 D k o i w P S y w v Q I b z s m w r H S 7 k i L C / b n R E a E y d d 0 n c 7 V j f m t 5 e R / W i e 1 8 U E v Y z J J L U g 6 / i h O O b Y K 5 w n i i G m g l g 8 d I F Q z t y u m N 0 Q T a l 3 O V R d C 8 7 f l v y D Y b R w 2 / P O 9 + t F x m U Y F b a I t t I O a a B 8 d o V P U Q g G i 6 A E 9 o z f 0 7 j 1 6 r 9 6 H 9 z l u n f L K m Q 0 0 U d 7 X N 5 I 5 r W k = &lt; / l a t e x i t &gt; V &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>H y B 2 z 8 D W 6 a A V q O Z O n 4 n H v t e 0 + Q M K q 0 4 3 x b l a X l l d W 1 6 n p t Y 3 N r e 8 f e 3 e u o O J W Y u D h m s e w F S B F G B X E 1 1 Y z 0 E k k Q D x j p B u P r q d + 9 J 1 L R W N z p S U J 8 j o a C R h Q j b a S B f e g J 8 o B j z p E I M 6 / D k c 6 z z A s i 2 M n z W n E f 2 H W n 4 R S A i 6 R Z k j o o 0 R 7 Y X 1 4 Y 4 5 Q T o T F D S v W b T q L 9 D E l N M S P m 1 V S R B O E x G p K + o Q J x o v y s W C W H x 0 Y J Y R R L c 4 S G h f q 7 I 0 N c q Q k P T K W Z b a T m v a n 4 n 9 d P d X T h Z 1 Q k q S Y C z z 6 K U g Z 1 D K e 5 w J B K g j W b G I K w p G Z W i E d I I q x N e j U T Q n N + 5 U X i n j Y u G 8 7 t W b 1 1 V a Z R B Q f g C J y A J j g H L X A D 2 s A F G D y C Z / A K 3 q w n 6 8 V 6 t z 5 m p R W r 7 N k H f 2 B 9 / g A s m p r B &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>H y B 2 z 8 D W 6 a A V q O Z O n 4 n H v t e 0 + Q M K q 0 4 3 x b l a X l l d W 1 6 n p t Y 3 N r e 8 f e 3 e u o O J W Y u D h m s e w F S B F G B X E 1 1 Y z 0 E k k Q D x j p B u P r q d + 9 J 1 L R W N z p S U J 8 j o a C R h Q j b a S B f e g J 8 o B j z p E I M 6 / D k c 6 z z A s i 2 M n z W n E f 2 H W n 4 R S A i 6 R Z k j o o 0 R 7 Y X 1 4 Y 4 5 Q T o T F D S v W b T q L 9 D E l N M S P m 1 V S R B O E x G p K + o Q J x o v y s W C W H x 0 Y J Y R R L c 4 S G h f q 7 I 0 N c q Q k P T K W Z b a T m v a n 4 n 9 d P d X T h Z 1 Q k q S Y C z z 6 K U g Z 1 D K e 5 w J B K g j W b G I K w p G Z W i E d I I q x N e j U T Q n N + 5 U X i n j Y u G 8 7 t W b 1 1 V a Z R B Q f g C J y A J j g H L X A D 2 s A F G D y C Z / A K 3 q w n 6 8 V 6 t z 5 m p R W r 7 N k H f 2 B 9 / g A s m p r B &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>H y B 2 z 8 D W 6 a A V q O Z O n 4 n H v t e 0 + Q M K q 0 4 3 x b l a X l l d W 1 6 n p t Y 3 N r e 8 f e 3 e u o O J W Y u D h m s e w F S B F G B X E 1 1 Y z 0 E k k Q D x j p B u P r q d + 9 J 1 L R W N z p S U J 8 j o a C R h Q j b a S B f e g J 8 o B j z p E I M 6 / D k c 6 z z A s i 2 M n z W n E f 2 H W n 4 R S A i 6 R Z k j o o 0 R 7 Y X 1 4 Y 4 5 Q T o T F D S v W b T q L 9 D E l N M S P m 1 V S R B O E x G p K + o Q J x o v y s W C W H x 0 Y J Y R R L c 4 S G h f q 7 I 0 N c q Q k P T K W Z b a T m v a n 4 n 9 d P d X T h Z 1 Q k q S Y C z z 6 K U g Z 1 D K e 5 w J B K g j W b G I K w p G Z W i E d I I q x N e j U T Q n N + 5 U X i n j Y u G 8 7 t W b 1 1 V a Z R B Q f g C J y A J j g H L X A D 2 s A F G D y C Z / A K 3 q w n 6 8 V 6 t z 5 m p R W r 7 N k H f 2 B 9 / g A s m p r B &lt; / l a t e x i t &gt; Illustration of different schemes for document representations z. (a) Much work in NLP has been devoted to directly aggregating word embedding V for z. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Correlation between the learned text sequence representation z and label embedding V. (a) Cosine similarity matrix between averagedz per class and label embedding V, and (b) t-SNE plot of joint embedding of text z and labels V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of learned attention ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of CNN, LSTM, SWEM and our model architecture.</figDesc><table><row><cell>Columns correspond</cell></row><row><cell>to the number of compositional parameters, com-</cell></row><row><cell>putational complexity and sequential operations</cell></row><row><cell>4.3 Model Complexity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Test Accuracy on document classification tasks, in percentage. We ran Bi-BloSAN using the</cell></row><row><cell>authors' implementation; all other results are directly cited from the respective papers.</cell></row><row><cell>2016); (v) simple attention models: bi-directional</cell></row><row><cell>block self-attention network (Bi-BloSAN)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of model size and speed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>where S k is the set of sample indices belonging to class k. Intuitively,z k represents the center of embedded</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Yahoo!</cell><cell></cell><cell>99.0</cell><cell>DBPedia</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>50 60 70</cell><cell></cell><cell>LEAM CNN LSTM Bi-Blosa</cell><cell>Accuracy (%)</cell><cell>40 60</cell><cell cols="2">LEAM SWEM LSTM CNN</cell><cell>76.5 94.0 94.5 76.0 95.0 Accuracy (%)</cell><cell cols="3">Yelp Polarity</cell><cell>98.8 62 98.6 63 64</cell><cell>Yelp Full</cell></row><row><cell></cell><cell>0</cell><cell>2K # Iteration</cell><cell>4K</cell><cell></cell><cell>0.1</cell><cell>1 Proportion (%) of labeled data 10</cell><cell>100</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell cols="2">75 # Window Size 0 25 61</cell><cell>50</cell><cell>75</cell></row><row><cell></cell><cell cols="3">(a) Convergence speed</cell><cell></cell><cell cols="2">(b) Partially labeled data</cell><cell></cell><cell cols="5">(c) Effects of window size</cell></row><row><cell cols="13">Figure 2: Comprehensive study of LEAM, including convergence speed, performance vs proportion of</cell></row><row><cell cols="6">labeled data, and impact of hyper-parameter</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Society Culture</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Science Mathematics</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Health</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Education Reference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Computers Internet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sports</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Business Finance</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Entertainment Music</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Family Relationships</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Politics Government</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We call it "phrase" for convenience; it could be any longer word sequence such as a sentence and paragraph etc. when a larger window size r is considered.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research was supported by DARPA, DOE, NIH, ONR and NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ranking a stream of news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianna M Del</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Gulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Romani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on World Wide Web</title>
		<meeting>the 14th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Scientific data</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 deep learning workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Label embedding for zero-shot fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Explainable prediction of medical codes from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05695</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Condensed memory networks for clinical diagnostic inferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Label embedding for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Rodriguez-Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deconvolutional latent-variable model for text sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bi-directional block selfattention for fast and memory-efficient sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04075</idno>
		<title level="m">Towards automated icd coding using deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<title level="m">Topic compositional neural language model</title>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Embedding methods for fine grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07210</idno>
		<title level="m">Multitask label embedding for text classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deconvolutional paragraph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention-based lstm network for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
