<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Best Loss Function for DNN-Based Low-latency Speech Enhancement with Temporal Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Koyama</surname></persName>
							<email>yuichiro.koyama@sony.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Vuong</surname></persName>
							<email>tvuong@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
							<email>stefan.uhlich@sony.com</email>
							<affiliation key="aff2">
								<orgName type="department">Sony Europe B.V</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
							<email>bhiksha@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the Best Loss Function for DNN-Based Low-latency Speech Enhancement with Temporal Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: deep learning</term>
					<term>speech enhancement</term>
					<term>perceptual quality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep neural networks (DNNs) have been successfully used for speech enhancement, and DNN-based speech enhancement is becoming an attractive research area. While time-frequency masking based on the short-time Fourier transform (STFT) has been widely used for DNN-based speech enhancement over the last years, time domain methods such as the time-domain audio separation network (TasNet) have also been proposed. The most suitable method depends on the scale of the dataset and the type of task. In this paper, we explore the best speech enhancement algorithm on two different datasets. We propose a STFT-based method and a loss function using problem-agnostic speech encoder (PASE) features to improve subjective quality for the smaller dataset. Our proposed methods are effective on the Voice Bank + DEMAND dataset and compare favorably to other state-of-the-art methods. We also implement a low-latency version of TasNet, which we submitted to the DNS Challenge and made public by open-sourcing it. Our model achieves excellent performance on the DNS Challenge dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech enhancement, which removes noise signals from the observed signal in order to enhance the speech signal, is one of the most useful technologies not only to improve the performance of various speech related applications such as speech recognition but also to enable people to listen and understand recorded speech more clearly. Recently, deep neural networks (DNNs) have been successfully used in speech enhancement, and DNNbased speech enhancement is becoming an attractive research area. Due to these recent developments, the Deep Noise Suppression (DNS) Challenge <ref type="bibr" target="#b0">[1]</ref> was organized together with this year's Interspeech. This competition compared single-channel speech enhancement systems with respect to their (perceptual) quality of the enhanced speech.</p><p>Over the last years, time-frequency masking based on the short-time Fourier transform (STFT) has been widely used for DNN-based speech enhancement <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. While approaches that estimate not only the amplitude but also the phase have been proposed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, in <ref type="bibr" target="#b8">[9]</ref>, a framework that optimizes the network such that the separated signals satisfy both STFT consistency and mixture consistency was proposed for real-valued as well as complex-valued masks. Also, the Phase-and-Harmonics-Aware Speech Enhancement Network (PHASEN) <ref type="bibr" target="#b9">[10]</ref> has achieved high performance using a two-stream architecture, which efficiently estimates the amplitude and phase while the streams exchange information with each other. All these approaches are based on processing the signals in the STFT domain.</p><p>On the other hand, time domain methods have also been studied <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. In particular, the Time-domain audio sepa-ration network (TasNet) <ref type="bibr" target="#b13">[14]</ref> and its fully convolutional version (Conv-TasNet) <ref type="bibr" target="#b14">[15]</ref>, which have a trainable encoder-decoder architecture and do not depend on the STFT, have surpassed the performance of the ideal binary mask (IBM) for speech separation; in other words, it has exceeded the upper limit of the performance of time-frequency masking. Moreover, architectures using a temporal convolutional network (TCN) <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> such as Conv-TasNet are considered to be more suitable than recurrent neural network architectures for real-time implementation as they are unaffected by the start point and can be operated efficiently <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The advantage of using the STFT is that the knowledge obtained by time-frequency analysis in past research can be applied to the architecture. For example, PHASEN takes into account the harmonic characteristic of speech <ref type="bibr" target="#b9">[10]</ref>. If enough data is available, on the other hand, the performance of time domain methods such as Conv-TasNet is expected to be higher than that of time-frequency masking because a projection to an appropriate domain can be learned from the distribution of the training data. Therefore, it is necessary to determine which method is the best by considering the scale of the dataset.</p><p>In this paper, we explore the best speech enhancement algorithm and the best loss function on both the Voice Bank + DEMAND (VBD) dataset <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, which is widely used to evaluate speech enhancement algorithms, and the dataset provided for the DNS Challenge (DNS dataset) <ref type="bibr" target="#b0">[1]</ref>. We also compare several types of loss functions including the use of the problem-agnostic speech encoder (PASE) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> to improve objective measures highly related to the subjective quality <ref type="bibr" target="#b23">[24]</ref>. The contribution of this paper is threefold. First, we propose a novel method called STFT-TCN, which is based on Conv-TasNet and utilizes the STFT/ISTFT for the encoder/decoder instead of trainable parameters, similarly to <ref type="bibr" target="#b24">[25]</ref>. Since the size of the VBD dataset is relatively small, we can expect that fixing the encoder and decoder can simplify the problem that the DNN has to solve and achieve better performance. It achieved comparable performance to other state-of-the-art methods on the VBD dataset in terms of the metrics highly correlated with the subjective quality. Second, we found that using PASE in the loss function improves the metrics correlated with subjective quality on the VBD dataset. Previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> use PASE as a feature extractor for downstream tasks such as speech recognition, however we only use PASE during training when computing the loss to optimize our model. Third, we extend our best scheme that was found on the VBD dataset for the DNS Challenge and our modified Conv-TasNet, which we submitted to the DNS Challenge, achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>The discrete-time signal captured by a microphone can be written as x(n) = s(n) + v(n), where x(n) is the observed signal, s(n) the speech signal, and v(n) the noise signal. x(n) can be divided into overlapping frames of length L, represented by x t ? R L , where t = 1, . . . , T represents the frame index and T represents the total number of frames. A matrix X ? R L?T can then be formed by concatenating x t for all frames t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conv-TasNet</head><p>We will now review Conv-TasNet <ref type="bibr" target="#b14">[15]</ref>. X is transformed into Ndimensional representations W ? R N?T for all frames by multiplying by a trainable linear encoder U ? R N?L as follows:</p><formula xml:id="formula_0">W = UX.<label>(1)</label></formula><p>W is fed into a TCN-based separation block F , which is composed of a bottleneck layer, TCN blocks, and a mask estimation block <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_1">M k = F k (W),<label>(2)</label></formula><p>where M k ? R N?T (k = 1, . . . , K) are the masks for K sources. Then, M k ? R N?T is multiplied by W,</p><formula xml:id="formula_2">Z k = M k W,<label>(3)</label></formula><p>where Z k ? R N?T is the N-dimensional representation of each source and is the Hadamard product. Z k is multiplied by a trainable linear decoder V ? R L?N ,</p><formula xml:id="formula_3">S k = VZ k ,<label>(4)</label></formula><p>where? k ? R L?T contains the estimated frames for each source. Each estimated source? k (n) is finally reconstructed by overlapping and adding the T columns in? k . The parameters of Conv-TasNet are learned by minimizing the SI-SNR loss:</p><formula xml:id="formula_4">L SI-SNR = ? 1 K K k=1 10 log 10 ( ?s k 2 / ?s k ?? k 2 ),<label>(5)</label></formula><p>where s k and? k are vector representations of s k (n) and? k (n), respectively, and ? = s k ,? k / s k 2 . Permutation-invariant training (PIT) is utilized to determine an appropriate permutation in terms of the order of sources <ref type="bibr" target="#b26">[27]</ref>.</p><p>There are two options when applying Conv-TasNet to speech enhancement. The first option is to estimate only the speech signal, corresponding to K = 1, which obviously does not require PIT. The second option is to estimate both speech and noise signals, corresponding to K = 2, defining k = 1 as speech signal and k = 2 as noise signal. This option also does not require PIT. In <ref type="bibr" target="#b27">[28]</ref>, it was shown that the second option gives better speech recognition performance than the first option. Therefore, we evaluate these options in terms of metrics highly correlated with subjective quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">PASE</head><p>Although self-supervised learning applied to speech related tasks have gained popularity, there remains a challenge in designing proxy self-supervised learning tasks. In <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, the authors designed a self-supervised learning framework to train a problem-agnostic speech encoder (PASE), a deep neural network based encoder that maps a raw waveform into an encoded speech representation. The encoded speech representation is learned by having many small multi-layer perceptrons (workers), each taking the encoded speech representation as input and trying to predict a known audio transformation of the original input waveform. For example, one worker tries to predict the log-power spectra (LPS), another worker tries to predict the Mel Frequency Cepstral Coefficients (MFCCs) and another worker tries to predict the original waveform from the encoded speech representation obtained by PASE. Since the ground-truth LPS, MFCCs, and waveform can be computed from the original raw waveform, each worker is trained using the mean-squared error between the prediction and the computed ground-truth features. Both PASE and all the workers are optimized together using backpropagation. In <ref type="bibr" target="#b22">[23]</ref>, they introduced PASE+ where the total number of workers increased to 12. We will show in Sec. 3.3 how PASE+ can be used to define a new loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation metrics</head><p>For the evaluation of our systems, we use composite objective measures (i.e., CSIG, CBAK, and COVL) <ref type="bibr" target="#b23">[24]</ref> and the perceptual evaluation of the speech quality (PESQ) measure <ref type="bibr" target="#b28">[29]</ref>, as they are considered to be highly correlated with subjective quality <ref type="bibr" target="#b23">[24]</ref>. We will refer to these metrics as perceptual quality metrics hereafter. In addition to these metrics, the scaleinvariant signal-to-noise ratio (SI-SNR) and segmental SNR (SSNR) are used for multiaspect comparison in some of our experiments. Note that SSNR is considered to have a lower correlation with subjective quality <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">STFT-based approach using TCN</head><p>We propose to replace the trainable encoder and decoder U, V of Conv-TasNet with discrete Fourier basis functions (nontrainable), which makes W regular complex spectrograms. Eqs. (1)-(4) are rewritten as</p><formula xml:id="formula_5">W SPEC = U STFT X,? k = V ISTFT [F k (W INPUT ) W SPEC ] ,<label>(6)</label></formula><p>where U STFT , W SPEC , and V ISTFT are real-valued matrices, although their first half (1, . . . , N/2 in the first dimension) represents real values and their latter half (N/2 + 1, . . . , N in the first dimension) represents imaginary values of the original complex representation. We propose two ways of defining W INPUT . The first way is to simply assign W SPEC as W INPUT W SPEC . The second way defines W INPUT W AP , where W AP is the realvalued matrix obtained by transforming W SPEC to an amplitude and phase representation. We also remove the sigmoid function of the mask estimation block of F such that the masks can take negative values. This is important as the masks are applied to the real and imaginary part of the spectrum. We will refer to this method as STFT-TCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Low-latency algorithm</head><p>For the methods based on a TCN, it is reported that the network can be easily modified to a low-latency algorithm by applying causal convolutions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. When access to a limited num-ber of future frames is allowed, as was the case for the DNS Challenge, it is better to utilize as much future information as possible. Therefore, we propose to let the first several layers be noncausal and the other layers be causal as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. When the overlapping utilized in the encoder and the decoder is 1/2 and the delay of noncausal layers is three frames, for instance, the output frame at t = t input ? 4 is computed when the input frame at t = t input is available, indicating that the algorithm accesses five future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss functions</head><p>In addition to the SI-SNR loss from (5), we use two more loss functions from literature and propose our own loss function based on PASE.</p><p>First, in <ref type="bibr" target="#b27">[28]</ref>, it was reported that the classical SNR is also a suitable loss function, which preserves the scale of estimated signal and is calculated as</p><formula xml:id="formula_6">L SNR = ? 1 K K k=1 10 log 10 ( s k 2 / s k ?? k 2 ).<label>(7)</label></formula><p>Second, power-compressed MSE (PCMSE) was utilized in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. To satisfy STFT consistency <ref type="bibr" target="#b8">[9]</ref>, we apply the STFT to estimated signals? k and clean signals S k , which are created by dividing, overlapping and concatenating? k and s k , as? SPEC,k = U STFT?k , W SPEC,k = U STFT S k . Then, the powercompressed MSE loss is calculated as</p><formula xml:id="formula_7">L PCMSE = 1 K f,t ? ? k (t, f ) 0.3 ? |W k (t, f )| 0.3 2 +(1 ? ?) ? k (t, f ) 0.3 ? W k (t, f ) 0.3 2 ,<label>(8)</label></formula><p>where? k (t, f ) and W k (t, f ) are complex representations of W SPEC,k and W SPEC,k at frame index t and frequency bin f , respectively, and W 0.3 |W| 0.3 e j?W . Third, we propose to utilize PASE+ <ref type="bibr" target="#b22">[23]</ref> as a feature extractor and will refer to the encoded speech representations as PASE features. The PASE features are calculated as</p><formula xml:id="formula_8">P k = PASE(s k ),P k = PASE(? k ),<label>(9)</label></formula><p>where P k ? R Q?R andP k ? R Q?R are PASE features calculated from clean and estimated signals, respectively. The PASE encoder is mainly composed of several convolutional layers and a quasi-recurrent neural network, and pretrained by selfsupervised learning using clean speech signals as we mentioned in Sec. 2.2. The optimization based on only PASE features cannot estimate the phase of signals accurately as most of the worker tasks utilized to learn PASE features are independent of phase. Therefore, we propose to utilize the power-compressed MSE loss simultaneously as</p><formula xml:id="formula_9">L PASEMSE = ?L PASE + L PCMSE ,<label>(10)</label></formula><p>where L PASE is the mean squared error (MSE) between P k and P k . We will refer to this loss function as PASE-feature MSE (PASEMSE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We conduct experiments on two different datasets: the Voice-Bank + DEMAND (VBD) dataset 1 <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, which is relatively small, and the DNS Challenge dataset <ref type="bibr" target="#b0">[1]</ref>, which is a large-scale   dataset provided by the DNS Challenge. Several ablation studies are conducted on the VBD dataset, and the most promising methods are applied to the DNS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">VoiceBank + DEMAND dataset</head><p>In the VBD dataset, 11,572 and 824 noisy-clean speech pairs are provided as the training and test set, respectively. We extract 300 pairs from the training set and utilize them as the validation set. In Conv-TasNet, L = 32 and N = 512 are applied with 1/2 overlapping, and other hyperparameters are those of the best configuration in the original paper <ref type="bibr" target="#b14">[15]</ref>. In STFT-TCN, L = 192 and N = 512 are applied with 2/3 overlapping such that the future information that the algorithm accesses is just 40 ms when the number of noncausal layers is three. To calculate the loss function <ref type="formula" target="#formula_7">(8)</ref> and <ref type="formula" target="#formula_0">(10)</ref>, ? = 0.5 and ? = 0.25 are used, which are the best configuration in our preliminary experiment. All networks in our experiment are trained with the Adam optimizer <ref type="bibr" target="#b34">[35]</ref>. The learning rate is initially set to 0.001 and halved if the output of the loss function in the validation set is not improved in three consecutive epochs. We first compare two types of W INPUT in (6), namely, W SPEC and W AP , in terms of STFT-TCN (SI-SNR is assigned for the loss function and K = 1). As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the performance of the method using W AP is slightly better than that of the method using W SPEC in terms of perceptual quality metrics (i.e., PESQ, CSIG, CBAK, and COVL). On the basis of this result, hereafter W AP is utilized as W INPUT in STFT-TCN.</p><p>Next, the number of output sources, K, which determines whether the noise signal is estimated simultaneously or not, is compared for both Conv-TasNet and STFT-TCN. Both are trained with the SI-SNR loss <ref type="bibr" target="#b4">(5)</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref> shows that the performance of both methods with K = 2 is consistently higher than that with K = 1 in terms of all metrics as was also noted  <ref type="figure">Figure 4</ref>: Comparison of loss functions on VBD dataset. The performance of the method using PASEMSE loss is very high in terms of the metrics correlated with subjective quality (PESQ, CSIG, CBAK, and COVL). in <ref type="bibr" target="#b27">[28]</ref>. Thus, learning the noise mask together with the speech mask improves performance and we adopt K = 2 hereafter. It is also shown that STFT-TCN surpasses Conv-TasNet in terms of perceptual quality metrics. <ref type="figure">Fig. 4</ref> shows the result of comparing loss functions for Conv-TasNet and STFT-TCN. It also shows that SNR loss is more suitable than SI-SNR loss for the perceptual quality metrics, which is a similar tendency to that with <ref type="bibr" target="#b27">[28]</ref>. PCMSE can improve the performance of STFT-TCN in terms of metrics correlated with subjective quality, while it does not work well for Conv-TasNet. The most important finding in <ref type="figure">Fig. 4</ref> is that the performance of the method using PASEMSE is very high for the perceptual quality metrics.</p><p>Finally, we compared our proposed methods using PASEMSE with other existing methods whose performance on the VBD dataset was previously reported. As shown in Table 1, STFT-TCN outperforms Conv-TasNet, DFL <ref type="bibr" target="#b12">[13]</ref>, and MDPhD <ref type="bibr" target="#b32">[33]</ref> in terms of all perceptual quality metrics. From the table, it can be seen that STFT-TCN performs slightly worse than PHASEN <ref type="bibr" target="#b9">[10]</ref>. We implemented PHASEN ourselvessee "PHASEN (our impl.)" in <ref type="table" target="#tab_1">Table 1</ref> -but could not reproduce the reported results. It seems that PHASEN is sensitive to the chosen hyperparameter values. In addition, STFT-TCN surpassed SDR-PESQ <ref type="bibr" target="#b33">[34]</ref>, which uses PESQ for its loss function, in terms of CSIG and COVL. Note that our proposed methods achieve such performance without training explicitly for PESQ. Moreover, it was proven that future information obtained by the noncausal layer contributes to the performance even if the range is very short such as 33 ms (Conv-TasNet with five noncausal layers) or 40 ms (STFT-TCN with three noncausal layers), and they are still comparable to other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">DNS Challenge dataset</head><p>In the DNS dataset, over 60,000 speech and noise samples are provided as the training set. Synthetic and real recorded test sets are also provided, and the synthetic set is further divided into "No reverb" and "With reverb" sets, according to whether reverberation exists. We focus on the synthetic set for the quantitative evaluation. We first roughly screened out noisy speech using a speech enhancement model trained in our preliminary experiment. Then, 150 speech and noise data are extracted from the training set and utilized for the validation set. To adapt the model to the "With reverb" set, we augment the speech data by convolving the impulse response generated by the image-source method <ref type="bibr" target="#b35">[36]</ref>. The reverberation time T60 is randomly sampled from 0.2 s to 0.8 s. Augmented speech and noise are randomly and independently sampled and mixed to create pairs of data for the training (i.e., on the fly <ref type="bibr" target="#b29">[30]</ref>), and 10,000 iterations are defined as 1 epoch. As the algorithm is allowed to access only 40ms of future frames in the DNS Challenge, we evaluate Conv-TasNet with access to 33 ms of future frames and STFT-TCN with access to 40 ms of future frames introduced in <ref type="table" target="#tab_1">Table 1</ref>. All other configurations are the same as those in the experiment on the VBD dataset. <ref type="table" target="#tab_2">Table 2</ref> shows the evaluation results on DNS dataset. We implement our methods with PyTorch <ref type="bibr" target="#b36">[37]</ref> as each frame is processed 2 (i.e., "1-frame-input-1-frame-output") and measure the time to infer a frame using an Intel Core i5-6200U clocked at 2.4 GHz by taking the average over the whole utterance of the first sample in the test set. We find that the data augmentation using impulse responses significantly improves the performance on the "With reverb" set, while a slight degradation in STFT-TCN can be found on the "No reverb" set. The most notable finding is that the method based on the Conv-TasNet outperforms STFT-TCN on the DNS dataset but not on the VBD dataset. This can be interpreted to mean that the large training dataset enables the network to learn appropriate encoders and decoders. Also, the method using PASE does not work well on the DNS dataset. We conclude that STFT-TCN and the loss function using PASE are effective only for smaller dataset.</p><p>Since the computational complexity of our method does not satisfy the requirement of the real-time track, we have submitted our best method (marked in <ref type="table" target="#tab_2">Table 2</ref> by *) to the non-real-time track in the DNS Challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed and evaluated two TCN-based approaches, Conv-TasNet and STFT-TCN, on two different datasets, VBD and DNS datasets. For the VBD dataset, STFT-TCN utilizing PASE for the loss function outperformed Conv-TasNet and other existing approaches in terms of perceptual quality metrics, while Conv-TasNet surpassed STFT-TCN on the DNS dataset. In future work, we will explore how to shrink our networks and implement them in a real-time system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Noncausal layers and causal layers. When the overlapping utilized in the encoder and the decoder is 1/2 and the delay of the noncausal layers is three frames, for instance, the algorithm accesses five future frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the input type of the TCN-based separation block in STFT-TCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the number of output sources K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with existing methods on VBD dataset.</figDesc><table><row><cell>Method</cell><cell>Causality</cell><cell cols="4">PESQ CSIG CBAK COVL</cell></row><row><cell>Noisy</cell><cell>-</cell><cell>1.97</cell><cell>3.35</cell><cell>2.44</cell><cell>2.63</cell></row><row><cell>Open-Unmix [31, 32]</cell><cell>noncausal</cell><cell>2.39</cell><cell>3.12</cell><cell>3.19</cell><cell>2.73</cell></row><row><cell>DFL [13]</cell><cell>noncausal</cell><cell></cell><cell>3.86</cell><cell>3.33</cell><cell>3.22</cell></row><row><cell>MDPhD [33]</cell><cell>noncausal</cell><cell>2.70</cell><cell>3.85</cell><cell>3.39</cell><cell>3.27</cell></row><row><cell>PHASEN [10]</cell><cell>noncausal</cell><cell>2.99</cell><cell>4.21</cell><cell>3.55</cell><cell>3.62</cell></row><row><cell>SDR-PESQ [34]</cell><cell>noncausal</cell><cell>3.01</cell><cell>4.09</cell><cell>3.54</cell><cell>3.55</cell></row><row><cell>PHASEN (our impl.)</cell><cell>noncausal</cell><cell>2.58</cell><cell>3.91</cell><cell>3.20</cell><cell>3.23</cell></row><row><cell>Conv-TasNet</cell><cell>noncausal</cell><cell>2.66</cell><cell>4.06</cell><cell>3.28</cell><cell>3.35</cell></row><row><cell>Conv-TasNet</cell><cell>33 ms look ahead</cell><cell>2.63</cell><cell>4.02</cell><cell>3.19</cell><cell>3.32</cell></row><row><cell>Conv-TasNet</cell><cell>1 ms look ahead</cell><cell>2.53</cell><cell>3.95</cell><cell>3.08</cell><cell>3.23</cell></row><row><cell>STFT-TCN</cell><cell>noncausal</cell><cell>2.89</cell><cell>4.24</cell><cell>3.40</cell><cell>3.56</cell></row><row><cell>STFT-TCN</cell><cell>40 ms look ahead</cell><cell>2.80</cell><cell>4.17</cell><cell>3.30</cell><cell>3.49</cell></row><row><cell>STFT-TCN</cell><cell>4 ms look ahead</cell><cell>2.73</cell><cell>4.11</cell><cell>3.25</cell><cell>3.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on DNS dataset. Conv-TasNet using SNR loss achieved the best performance.</figDesc><table><row><cell>Method</cell><cell># of params [million]</cell><cell>Time per frame [ms]</cell><cell>Time to infer a frame [ms]</cell><cell>Loss function</cell><cell>Reverberant augmentation</cell><cell cols="8">No reverb PESQ CSIG CBAK COVL PESQ CSIG CBAK COVL With reverb</cell></row><row><cell>Noisy (Raw)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.58</cell><cell>3.03</cell><cell>2.53</cell><cell>2.27</cell><cell>1.82</cell><cell>3.50</cell><cell>2.80</cell><cell>2.64</cell></row><row><cell>Baseline (NSNet) [6]</cell><cell>1.26</cell><cell>10.0</cell><cell>-</cell><cell>[6]</cell><cell>[6]</cell><cell>1.83</cell><cell>2.88</cell><cell>2.75</cell><cell>2.32</cell><cell>1.52</cell><cell>2.71</cell><cell>2.51</cell><cell>2.09</cell></row><row><cell>STFT-TCN</cell><cell>5.03</cell><cell>4.0</cell><cell>350.1</cell><cell>PASEMSE</cell><cell></cell><cell>1.51</cell><cell>2.99</cell><cell>2.25</cell><cell>2.22</cell><cell>1.51</cell><cell>3.20</cell><cell>2.31</cell><cell>2.33</cell></row><row><cell>STFT-TCN</cell><cell>5.03</cell><cell>4.0</cell><cell>348.1</cell><cell>PCMSE</cell><cell></cell><cell>2.24</cell><cell>3.63</cell><cell>3.12</cell><cell>2.93</cell><cell>1.46</cell><cell>2.89</cell><cell>2.45</cell><cell>2.15</cell></row><row><cell>STFT-TCN</cell><cell>5.03</cell><cell>4.0</cell><cell>353.8</cell><cell>PCMSE</cell><cell></cell><cell>2.16</cell><cell>3.54</cell><cell>3.04</cell><cell>2.84</cell><cell>2.19</cell><cell>3.75</cell><cell>3.10</cell><cell>2.97</cell></row><row><cell>Conv-TasNet</cell><cell>5.08</cell><cell>1.0</cell><cell>355.5</cell><cell>PASEMSE</cell><cell></cell><cell>1.97</cell><cell>3.49</cell><cell>2.88</cell><cell>2.71</cell><cell>2.11</cell><cell>3.77</cell><cell>3.02</cell><cell>2.92</cell></row><row><cell>Conv-TasNet</cell><cell>5.08</cell><cell>1.0</cell><cell>355.6</cell><cell>SNR</cell><cell></cell><cell>2.72</cell><cell>4.05</cell><cell>3.65</cell><cell>3.39</cell><cell>1.85</cell><cell>3.25</cell><cell>2.78</cell><cell>2.53</cell></row><row><cell>Conv-TasNet (*)</cell><cell>5.08</cell><cell>1.0</cell><cell>351.9</cell><cell>SNR</cell><cell></cell><cell>2.73</cell><cell>4.07</cell><cell>3.64</cell><cell>3.41</cell><cell>2.71</cell><cell>4.21</cell><cell>3.67</cell><cell>3.47</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ykoyama58/tcnse</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A structure-preserving training target for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6107" to="6111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Time-frequency maskingbased speech enhancement using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5039" to="5043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weighted speech distortion losses for neural-networkbased real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="871" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Phasenet: Discretized phase modeling with deep neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2713" to="2717" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Differentiable consistency constraints for improved deep speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phasen: A phase-andharmonics-aware speech enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA INTERSPEECH 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10522</idno>
		<title level="m">Speech denoising with deep feature losses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tasnet: Time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meetings on Acoustics ICA2013</title>
		<meeting>Meetings on Acoustics ICA2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">35081</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Investigating rnn-based speech enhancement methods for noiserobust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th ISCA Speech Synthesis Workshop</title>
		<meeting>9th ISCA Speech Synthesis Workshop</meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="159" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2605</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-2605" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. of the Int. Speech Communication Association (INTERSPEECH)</title>
		<meeting>of the Conf. of the Int. Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="6989" to="6993" />
		</imprint>
	</monogr>
	<note>in 2020 ICASSP, 2020</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASLP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Demystifying tasnet: A dissecting approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jakobeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08895</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Problem-Agnostic Speech Embeddings for Multi-Speaker Text-to-Speech with SampleRNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<idno type="DOI">10.21437/SSW.2019-7</idno>
		<ptr target="http://dx.doi.org/10.21437/SSW.2019-7" />
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ISCA Speech Synthesis Workshop</title>
		<meeting>10th ISCA Speech Synthesis Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving noise robust automatic speech recognition with single-channel time-domain enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="7009" to="7013" />
		</imprint>
	</monogr>
	<note>in 2020 ICASSP, 2020</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Investigations on data augmentation and loss functions for deep learning based speechbackground separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3499" to="3503" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">1667</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Open-unmix for speech enhancement (umx se)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3786908</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3786908" />
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-domain processing via hybrid denoising networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08914</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">End-to-end multi-task denoising for the joint optimization of perceptual speech metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10707</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prediction of energy decay in room impulse responses simulated with an image-source model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="277" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
