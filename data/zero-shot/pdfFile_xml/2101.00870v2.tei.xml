<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight representation learning for efficient and scalable recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Koch</surname></persName>
							<email>o.koch@criteo.com</email>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Criteo</forename><surname>France</surname></persName>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
							<email>ma.benhalloum@criteo.com</email>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Genthial</surname></persName>
							<email>g.genthial@criteo.com</email>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Criteo</forename><surname>France</surname></persName>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Kuzin</surname></persName>
							<email>d.kuzin@criteo.com</email>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Criteo</forename><surname>France</surname></persName>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Parfenchik</surname></persName>
							<email>d.parfenchik@criteo.com</email>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Criteo</forename><surname>France</surname></persName>
							<affiliation key="aff0">
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight representation learning for efficient and scalable recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>ACM Reference Format: Olivier Koch, Amine Benhalloum, Guillaume Genthial, Denis Kuzin, and Dmitry Parfenchik. 2018. Lightweight representation learning for efficient and scal-able recommendation. In Open research. ACM, New York, NY, USA, 11 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Display advertising</term>
					<term>? Computer systems organization ? Real-time system architecture</term>
					<term>? Com- puting methodologies ? Machine learning algorithms KEYWORDS online advertising, recommender systems, representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past decades, recommendation has become a critical component of many online services such as media streaming and ecommerce. Billion-scale recommendation engines are becoming common place. Meanwhile, new state-of-the-art methods such as variational auto-encoders have appeared, raising the bar in performance.</p><p>We propose to go a step further with a simple and efficient model (LED, for Lightweight Encoder-Decoder). By combining pretraining, sampled losses and amortized inference, LED brings a 30? inference speed-up compared to the best system known so far, while reaching the performance of variational auto-encoders on standard recommendation metrics.</p><p>LED has been deployed in production at Criteo and serves billions of users across hundreds of millions of items in a few milliseconds using standard hardware. We provide a detailed description of our system and illustrate its operation over two months of experiment. We also release the code for LED. Our work should be of significant interest to practitioners wishing to deploy an efficient large-scale recommendation system in the real-world.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Online advertising offers a unique test bed for recommendation at scale. Every day, billions of users interact with millions of products on thousands of retailers' websites in real-time. Systems designed to address this challenge must provide a recommendation for any user in a few milliseconds while leveraging the massive amounts of data available for training.</p><p>Building a successful recommendation engine in this context requires a scalable and deep understanding of the users, the merchants, the products, and their respective interactions. Representation learning is an attractive approach as it provides a way to embed various entities into the same space which can be then searched through nearest-neighbor efficiently. Recent years have seen tremendous progress in the field, giving birth to a large variety of approaches such as variational auto-encoders <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b39">37]</ref>, reinforcement learning <ref type="bibr" target="#b25">[23]</ref> and graph networks <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b45">43]</ref>, to name a few. Meanwhile, billion-scale recommender system have become common place <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b45">43]</ref>.</p><p>Making these systems efficient is critical in order to make stateof-the-art methods accessible to many in the real world and reduce their computational footprint. In addition, the end-to-end problem of large-scale training and real-time inference is rarely addressed.</p><p>Our contribution is to make a new leap forward in terms of simplicity and computing efficiency while maintaining state-of-the-art performance at scale. We analyze the fundamental bottlenecks of existing state-of-the-art algorithms. We then devise several practical design choices which, once applied to these algorithms, allows us to design a new method (LED, for Lightweight Encoder-Decoder). Our experiments show that LED brings a 30? speed-up in latency against the best well-known baseline <ref type="bibr" target="#b5">[5]</ref> while reaching the algorithmic performance of state-of-the-art methods <ref type="bibr" target="#b22">[21]</ref>.</p><p>LED has been deployed at Criteo and powers our recommendation engine in production. We provide a detailed description of our system architecture, from offline training to online inference, delivering 3200 queries per second with sub-millisecond latency. We further demonstrate its operation over several months at scale and open-source the code. Our work should be of interest to practitioners wishing to deploy an efficient large-scale recommendation system in the real-world. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Definitions</head><p>We consider the general problem of recommending products to users on the internet. Products are shown on banners displayed on publisher websites at the scale of billions of users, dozens of millions of items, and billions of displays per day.</p><p>A product is any item a user can purchase: retail, travel, etc. We assume that events coming from the same user across merchants and banners are aggregated into a single series of events called a user timeline. We also assume that an attribution mechanism associates a product sale on a merchant to the last product clicked on a previously displayed banner (if such an event ever occurred).</p><p>The goal of the recommendation engine is to show products that will be of interest to the user, captured either through clicks on the banners or through sales on the merchants' websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Requirements</head><p>Our system needs to address the following requirements: Scale The recommendation system must work at the scale of billions of users and hundreds of millions of items.</p><p>Latency The system should respond within a few milliseconds to fit the need for banner display on mobile and web applications.</p><p>Churn Users enter and exit the system at a far higher rate than changes in the product catalog. The system should avoid having to recompute its parameters for each update in the user base.</p><p>Multiple feedbacks Collected data usually involves different types of feedback (product views and sales, clicks on banners). We seek a design that will extract as much information as possible from the available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Representation learning for recommendation A classical approach to recommendation uses collaborative filtering through factorization of the user-item matrix <ref type="bibr" target="#b11">[10]</ref>. This approach leverages decades of research and produces robust results at scale <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b36">34]</ref>. The field went through a significant renewal when randomized algorithms helped scale to large dimensions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">7]</ref>. SLIM <ref type="bibr" target="#b30">[28]</ref> and its variants <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b39">37]</ref> differ from standard matrix factorization by solving a constrained linear problem to learn a sparse item-item matrix. Recent improvements like EASE <ref type="bibr" target="#b39">[37]</ref> relax some of the constraints to find a dense closed-form solution. These methods are slow to train at a large scale <ref type="bibr" target="#b22">[21]</ref>, even if quantization and fast nearest-neighbor techniques can speed up inference.</p><p>A vast body of literature recently grew around neural networks, leveraging various architectures such as convolutional, recurrent and graph networks <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b44">[42]</ref><ref type="bibr" target="#b45">[43]</ref><ref type="bibr" target="#b46">[44]</ref>. These models allow to seamlessly fit into a single loss a number of constraints or metadata available in the input. The affinity between a user and an item is usually modelled as a score function?( , ; ), where?is a neural network with parameters . While most methods express the final score via inner product <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b21">20]</ref>, or euclidean distance <ref type="bibr" target="#b9">[9]</ref>, making it possible to use fast-KNN retrieval techniques, some <ref type="bibr" target="#b8">[8]</ref> require to rank all products, making inference cost linear in the number of products.</p><p>More recently, Variational Auto-Encoders <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b37">35]</ref> generalizing latent factor models have established a new standard. While integrating new users is made possible by using bag-of-words representations, the multinomial likelihood commonly used as objective involves a softmax over all items, which becomes too computationally expensive at the scale at which we operate. Improvements <ref type="bibr" target="#b25">[23]</ref> using reinforcement learning have further increased their performance, but lack the efficiency needed at large scale.</p><p>Billion-scale recommendation systems. With the advent of ever newer recommendation algorithms, a major concern of the community has been to bring them to the scale of the internet. Early work demonstrates a deep learning architecture operating at the scale of Youtube <ref type="bibr" target="#b4">[4]</ref>. Compact hierarchical networks <ref type="bibr" target="#b27">[25]</ref> address real-time and diverse metadata needs and have been deployed at Amazon Web Services. Neural input search <ref type="bibr" target="#b12">[11]</ref> learns vocabulary and embedding sizes and allows the embedding dimension to vary for different values of the feature. Recent work investigates how to correct biases induced by two-stage recommender systems with off-policy reinforcement learning at scale <ref type="bibr" target="#b26">[24]</ref>.</p><p>Advances at scale are particularly notable on graph networks. A data-efficient Graph Convolutional Network (GCN) combines efficient random walks and graph convolutions incorporating both graph structure and node information at the scale of billions of users and items <ref type="bibr" target="#b44">[42]</ref>. An efficient graph convolutional model <ref type="bibr" target="#b45">[43]</ref> captures both explicit user preferences and heterogeneous relationships of side information over hundreds of millions of users and items. Multi-task multi-view graph representation learning fuses multiple representations for 57 billion examples <ref type="bibr" target="#b42">[40]</ref>. Finally, graph embeddings with side information <ref type="bibr" target="#b41">[39]</ref> leverage unsupervised feature learning at the scale of billions of users and billions of items.</p><p>Latency and load are rarely described in the literature, yet represent major constraints in real-world applications. The best performance we are aware of is reached by Pixie at Pinterest, leveraging graph networks to handle 1, 200 recommendation queries per second (qps) with 60 millisecond latency <ref type="bibr" target="#b5">[5]</ref>. Collaborative Metric Learning <ref type="bibr" target="#b9">[9]</ref> leverages LSH to achieve 10, 000 qps on a catalog of 260, 000 items. Our contribution is to show that a lightweight encoder-decoder model (LED) reaches the performance of VAEs while allowing a 30? speed-up in latency and high throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EFFICIENT MODELS</head><p>We present here the main design choices enabling efficient representation learning at scale. We use ? {1, ..., } to index items, ? {1, ..., } to index users, and ( ) = ( 1 , ..., ) to index the sequence of items one user interacted with. We associate each item and user with their -dimensional vector representations ? ? ? R and ? ? ? R .</p><p>Fast nearest-neighbor search For a given user , the system should rank items with a scoring function expressed as an inner product ( , ) = ??, ? ?. Finding the best items for user is thus equivalent to finding the nearest neighbors of ? with maximum inner product. Thanks to this formulation, we can leverage efficient approximate nearest neighbor techniques (see Section 6.2) at inference.</p><p>Amortized inference The use of amortized inference <ref type="bibr" target="#b6">[6]</ref> consists in sharing the same procedure to compute user representations, effectively making the number of parameters to learn independent from the number of users and addressing user churn. This is one of the strengths of the VAE framework <ref type="bibr" target="#b25">[23]</ref>.</p><p>Sampling-based losses The computational complexity of comprehensive losses like the multinomial and gaussian likelihoods is linear in the number of items which makes them unusable under our requirements. Instead, we can use ranking-based losses, such as Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b33">[31]</ref>, Negative Sampling (NS) <ref type="bibr" target="#b29">[27]</ref>, or approximation-based methods like Complementarity Sum Sampling (CSS) <ref type="bibr" target="#b0">[1]</ref>.</p><p>Pre-training While the final recommendation system is evaluated on click events, we can leverage the large amount of historical data (mainly view events) to pre-train the model. In practice, we pretrain item embeddings ? on view events using large-scale matrix factorization (see Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-training with large-scale matrix factorization Randomized SVD</head><p>We first present a pre-training method to get item embeddings consisting in factorizing an item-item matrix. We depart from the usual user-item rating matrix factorization since having latent representations for users would not be robust to churn. Also, the number of users is the limiting factor regarding scalability as it greatly exceeds the number of items. Instead, we factorize the Pointwise Mutual Information (PMI [2]) matrix. Writing ( ) the probability of item to appear in the data, ( , ) the probability of items and to co-occur in one user timeline, the PMI coefficient associated with items and is defined as PMI , = log( ( , )/( ( ) ( ))). This allows us to capture similarities between all items, since the coefficients are normalized by popularity. The number of views being unevenly distributed, factorizing the co-occurrence matrix would give too much importance to popular items.</p><p>Since we collect cross-merchant interactions, the resulting matrix cannot be factorized by block and we need to leverage factorization methods that can operate on sparse matrices with hundreds of millions of rows and columns. We use Randomized Singular Value Decomposition (RSVD). RSVD first projects the input matrix into a low-dimensional space which captures most of the norm of the input matrix. Second, using the QR-decomposition of this thin matrix <ref type="bibr" target="#b2">[3]</ref> it computes its left singular vectors which we use as item embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lightweight Encoder-Decoder (LED)</head><p>We derive a simple and efficient model, the Lightweight Encoder-Decoder (LED), from a common yet powerful baseline <ref type="bibr" target="#b43">[41]</ref>. This model has few parameters, one embedding ? R and bias ? R per item. It encodes each user timeline 1 , ... to derive the user representation ? via a simple average, respecting the amortized inference guideline:</p><formula xml:id="formula_0">? ? = 1 ?? =1 ? ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 LED: Training with project fine-tuning</head><p>Input: User Timelines (views and clicks on items)</p><p>Output: Item embeddings = ? ? ??? ? RSVD ? R and biases 1: Build the PMI matrix ? R ? on view events 2: Run RSVD on the PMI matrix to get pre-trained item embed-</p><formula xml:id="formula_1">dings ? ??? ? RSVD ? R 3:</formula><p>Randomly initialize parameters ? R ? (projection matrix) and ? R (items' biases). Given a user and item , define It then "decodes" the user representation to retrieve recommendation scores for each item as follows:</p><formula xml:id="formula_2">( , ) = ??, ? ? ??? ? RSVD ? + with ? = 1 ?? ? ? ??? ?</formula><formula xml:id="formula_3">( , ) = ? ? ? , ? ? ? + .</formula><p>Because some items tend to be more popular than others, we capture the global tendency with biases . As a result, the inner product can focus on modeling the true interaction between a user and an item. While using biases as a first-order approximation of the user-item matrix is standard in the matrix factorization literature <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b32">30]</ref>, we differ from it by jointly training embeddings and biases instead of using a pre-computed average.</p><p>Note that the score can be rewritten as a plain inner product if we add an extra dimension to the vectors ? and ? ? with values 1 for the user and for the item, making it compatible with our fast nearest-neighbor search guideline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sampling-based losses</head><p>We split the timelines into two parts, input and target. The model is trained to predict the content of the target period given the input period. In the case where we have two types of events (view and click), we use all events as inputs and only include click events in the target. Similarly to the Mult-DAE, we randomly drop a fraction of the input items. It reduces over-fitting and leads to competitive results, as shown in <ref type="bibr" target="#b22">[21]</ref>. In all generality, we want ( , ) to be high for the items in the target period. We consider the following loss functions.</p><p>Approximated Mult-Likelihood A common approach consists in treating the recommendation problem as a multi-class multilabel classification problem, where the labels are the items in the target. Traditionally used in the Auto-Encoder literature, the gaussian likelihood has been shown to be less effective than the Multinomial Likelihood <ref type="bibr" target="#b22">[21]</ref>, which models each item's probability using a softmax such that ( | ) ? exp ( ( , )) and assumes that the user's items are drawn from according to a multinomial distribution. Unfortunately, using a softmax involves a partition function ( ) = =1 exp ( ( , )) whose computation is too costly. We circumvent this problem using Complementarity Sum Sampling (CSS) <ref type="bibr" target="#b0">[1]</ref> which provides a robust approach to estimate ( ) via sampling. In practice, we sample negatives for each target item and estimate its partition function as:</p><formula xml:id="formula_4">= exp ( ( , )) + ? 1 ?? =1 exp( ( , )).</formula><p>Ranking Another approach is to see the task as a ranking problem. In other words, the objective is to differentiate the positive item from sampled negative items. In our settings, we use the Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b34">[32]</ref>, which maximizes the ranking</p><formula xml:id="formula_5">?? =1 log ( ( , ) ? ( , ))</formula><p>where is the sigmoid function. We note that the CSS method with one negative is equivalent to a BPR loss with margin log( ? 1).</p><p>Negative Sampling Traditionally used in Natural Language Processing <ref type="bibr" target="#b29">[27]</ref> and similar in its form to the ranking formulation, it consists in training the model to distinguish positives from negatives by maximizing</p><formula xml:id="formula_6">log ( ( , )) + ?? =1 log (1 ? ( ( , ))) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Fine-tuning RSVD Embeddings</head><p>Item embeddings can be initialized with the RSVD embeddings.</p><p>Since these embeddings are trained on view events, they would benefit from additional training on click events. The classical approach is to train them alongside the other model's parameters. We refer to this fine-tuning method as full.</p><p>We consider a variant called project (see <ref type="figure" target="#fig_2">Figure 1</ref>), which consists in computing the item embeddings from the RSVD embeddings RSVD , freezed during training. More specifically, the model learns</p><formula xml:id="formula_7">a projection matrix ? R ? such that ? = ? ? ??? ? RSVD .</formula><p>Using such a simple transformation allows us to rewrite the scoring function using a transposition trick:</p><formula xml:id="formula_8">( , ) = ? ? ? ? ???? ? RSVD , ? ??? ? RSVD ? + .</formula><p>Because we have multiple positives and negatives for a given user, we reduce the number of matrix multiplications. This yields a significant speedup during training (x3 in our setup).</p><p>The project fine-tuning method is a key factor in the performance of our method. It can be applied to other models (like VAEs), though the transposition trick is only possible in our formulation. By reducing the number of trainable parameters, it speeds up training and decreases exposure to over-fitting. At retrieval time, the computational cost of the project method is identical to the full method, since we can directly use the projected embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Complexity analysis</head><p>The number of parameters of LED is linear in the number of items ( ? item embeddings and biases), but independent from the number of users (contrary to NCF <ref type="bibr" target="#b8">[8]</ref> or matrix factorization methods <ref type="bibr" target="#b11">[10]</ref>). When using the Project fine-tuning method, the number of trainable parameters is even smaller, ? + . Training requires multiple updates on each user timeline. Each update using one of the sampled losses is linear in the number of items in the timeline and the number of negatives . As a result, the final training complexity is O ( ? ? ), which has the advantage of not being quadratic in or (unlike EASE <ref type="bibr" target="#b39">[37]</ref> in O ( 2.376 ) or the Mult-VAE <ref type="bibr" target="#b22">[21]</ref> in O ( ? )).</p><p>At inference, computing the user representation ? ? is only linear in the number of items in the input timeline while finding the topbest recommendations is roughly O (log( )) thanks to efficient approximate nearest neighbors techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OFFLINE EVALUATION</head><p>We now evaluate the impact of model choices described in Section 4. We insist on the fact that our goal is not to outperform state-of-the-art methods on performance but to reach state-of-theart performance while operating at much better latency and scale. We compare the performance of LED to Mult-VAE <ref type="bibr" target="#b22">[21]</ref> on two datasets using two standard metrics, recall@k and click rank. In particular, we assess the impact of sampling-based losses as well as pre-training. The findings of this section can be summarized as follows:</p><p>? Training models using a sampling-based loss (Mult-CSS <ref type="bibr" target="#b0">[1]</ref>, BPR <ref type="bibr" target="#b33">[31]</ref> or NS <ref type="bibr" target="#b29">[27]</ref>) gives results close to the state-of-the-art, while enabling training at scale. ? The LED model achieves competitive performance compared to the state-of-the-art despite its simplicity. ? Pre-training embeddings on view events and fine-tuning them using a projection matrix is an effective way to transfer knowledge to the click prediction task. Section 9.2 in Appendix details the parameters and experiment setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use two datasets for the offline evaluation of our system. The public dataset ML20M provides a benchmark that anchors our work within the existing literature. The large-scale dataset Production demonstrates the ability of our approach to scale. <ref type="table" target="#tab_0">Table 1</ref> reports statistics about each dataset.</p><p>? ML20M: A user-movie ratings dataset commonly used as a benchmark in the recommendation literature. We use the same preprocessing as <ref type="bibr" target="#b22">[21]</ref> by keeping only ratings of four and higher and users with at least five ratings. ? Production: We collect user interactions with products on merchant websites and banners on publisher websites using the production system already in place for a period of three months. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sampling loss</head><p>In this experiment, we compare the different losses of the LED model on the ML20M dataset. Unless specified otherwise, we use = 1000 negatives per user, shared across target positives. For all models, initialization is random, and Full fine-tuning is enabled. For the Mult-VAE, we use our scalable implementation using embedding lookups instead of one-hot vectors and observe the same results as those reported in the paper. We also include the performance of the Mult-VAE trained with the approximated softmax (Mult-CSS). All losses except the BPR use the same items in both the input and target. For the BPR, we shuffle and randomly split the user timeline into input (80%) and target (20%). For the sake of comparison, we also show the results reported for other methods such as EASE <ref type="bibr" target="#b39">[37]</ref>, WMF <ref type="bibr" target="#b11">[10]</ref>, and SLIM <ref type="bibr" target="#b30">[28]</ref>.  <ref type="bibr" target="#b39">[37]</ref> 0.391 (-1.26%) 0.521 (-2.98%) WMF <ref type="bibr" target="#b11">[10]</ref> 0.360 (-9.09%) 0.498 (-7.26%) SLIM <ref type="bibr" target="#b30">[28]</ref> 0.370 (-6.57%) 0.495 (-7.82%) <ref type="table" target="#tab_1">Table 2</ref> reports the results. From this experiment, we conclude that (1) training models using a sampling-based loss (Multi-CSS <ref type="bibr" target="#b0">[1]</ref>, BPR <ref type="bibr" target="#b33">[31]</ref> or NS <ref type="bibr" target="#b29">[27]</ref>) degrades the performance in a minimal way while enabling training at scale (2) the LED model reaches close to state-of-the-art performance while bringing simplicity in the design. Similar results were obtained on the Production dataset. We select the BPR loss for further experiments on the Production dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Fine-tuning the embeddings</head><p>Section 4.4 discusses two ways of fine-tuning pre-trained SVD embeddings. We start by artificially evaluating their efficiency on the ML20M dataset. Since this dataset contains only one type of events and is small enough for a model to perform multiple epochs on it, we emulate the large-scale scenario with the following procedure. We first train SVD embeddings on the full training set. Then, we train a LED with multinomial likelihood on a small fraction of the training set and evaluate the effects of initialization and fine-tuning methods. <ref type="figure" target="#fig_3">Figure 2</ref> shows that the Project method outperforms both random initialization and classical fine-tuning for models trained on 0.1% and 1% of the dataset. However, when the dataset size increases, training the embeddings yields better performance. The project method still achieves decent results, which is impressive considering that it learns a ? matrix instead of the full embedding matrix ? R ? . We go on to the experiments on the Production dataset for whose scale the pre-training method was designed. <ref type="table" target="#tab_2">Table 3</ref> summarizes the results. First, we find that on this dataset, the LED model performs better than the VAE. When training the embeddings (using either random or SVD initialization), we observe that the VAE is more prone to over-fitting compared to the LED model. The Project finetuning method proves to be effective, yielding the best click-ranks for both models. We propose an architecture that incorporates the methods described in the previous section suited for a real-world, large-scale deployment. We address scale and robustness through modularity by splitting the system into components that can be tested and improved independently. At the highest level, our system is made of two components. First, a candidate selection step retrieves a list of candidate items from different algorithms (such as LED). Second, a ranking model predicts which of the candidates have the highest probability of being clicked or purchased and builds the final banner. The ranking model uses a logistic regression trained to predict clicks or sales.</p><p>Splitting the recommendation task into these two independent tasks is typical <ref type="bibr" target="#b18">[17]</ref> but also known to be sub-optimal <ref type="bibr" target="#b26">[24]</ref>. Endto-end training of two-stage systems remains impractical given our requirements (Section 2.2). This approach, on the other hand, offers modularity and allows us to "fuse" different recommendation signals in the same system. Our online experiments show that this feature is useful in practice (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Offline Pipeline</head><p>An offline component ( <ref type="figure" target="#fig_5">Figure 3</ref>) computes product embeddings from user timelines using RSVD which are then fine-tuned by training the LED model. Embeddings are further indexed into an appropriate data structure for fast retrieval (Maximum Inner Product Search).</p><p>The data and models are split by country. The PMI, RSVD and dataset creation are distributed using spark, while the LED training is run on a single machine with Tensorflow. The model is exported as a Tensorflow Protocol Buffer file.</p><p>Saving computational graphs as Protocol Buffer files makes the pipeline generic as it can support any other model. Thanks to XLA: Ahead of Time Compilation (XLA AoT) 1 , these files can be further optimized for inference. XLA AoT compiles the model graph into machine instructions for various architectures through intermediate LLVM representation 2 . This yields significant performance improvement compared to a hand-coded implementation (x5 in our setting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Real-time retrieval at scale</head><p>An online JVM service loads both the product embeddings' indices and the user model in memory for several countries and starts processing requests via gRPC <ref type="bibr" target="#b2">3</ref> . For each request, the online service receives the user's history and performs the following steps:</p><p>(1) Fetch relevant product embeddings from hnsw.  This architecture leverages offline computation for the product embeddings and model training while using fresh user data to compute recommendations. Regarding the nearest-neighbor search, the scale of our problem precludes the use of exact search. Approximate nearest neighbor is an attractive alternative and has been extensively studied. We built on top of existing benchmarks 5 and chose hnswlib <ref type="bibr" target="#b28">[26]</ref>.</p><p>This design has several advantages. First, it enables to leverage the full power of representation learning throughout the whole pipeline. Indeed, product and user representations remain accessible in their full vector state until the final recommendation is done. Second, the memory footprint is independent of the number of users and only scales linearly with the number of products. Third, it fails gracefully in case of user cold start: users with no history get recommendations by skipping the user embedding computing step and doing a nearest neighbour search with a null vector which is equivalent to retrieving the products with the highest popularity bias for the merchant. Section 6.3 shows that minimal hardware is required to serve billions of users with this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Computation resources</head><p>In production, the offline pipeline is made of data pre-processing, RSVD, model training, and hnsw indexing. It runs on our internal Hadoop cluster independently for each country. The RSVD and indexing jobs are parallelized using Spark on a few dozen executors with 16 CPU and 30-GB RAM, depending on the number of items in a country's catalog. Each Tensorflow model is trained on a single machine with 48 CPU and 48-GB RAM. The whole pipeline, including training on the full dataset, runs in a few hours and is scheduled every day to incorporate new data. The online service runs as a pool of instances within an Apache Mesos cluster. Each instance has 4 CPU, 30 GB RAM, and serves all countries.</p><p>The system must handle billions of requests per day. Hence, we monitor the long tail of inference time distribution to minimize the number of servers needed and production incidents. Some useful numbers describing the system scalability are listed in <ref type="table" target="#tab_3">Table 4</ref>. We report latency including request deserialization and network overheads. LED provides an end-to-end recommendation in just 2 ms on a single server, i.e. 30? faster than the best system known so far <ref type="bibr" target="#b5">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ONLINE EVALUATION</head><p>We present here the results of live testing of our system on real traffic. This experiment serves two purposes: first, it demonstrates the ability of our system to operate at the scale specified in our requirements; second, it proves that the performance gain observed in offline experiments translates into actual business uplift once tested with real users. We use a classical A/B testing approach where subsets of randomly selected users are exposed to various algorithms. In these experiments, the ranking model is trained continuously several times a day and is the same for all populations. In particular, no feature is added to the ranking model that would benefit a particular method or population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">A/B test setup</head><p>We use three algorithms for the A/B test:</p><p>? Global Best Of (GBO): This algorithm recommends the most popular items of the merchant to each user, independently from their history. This algorithm is simple and provides a reference for a non-personalized recommendation. ? Clustering Best Of (CBO): This algorithm corresponds to the reference algorithm that was running in production before the project started. The algorithm leverages user clustering and computes a top-k for sets of users sharing similar interests in items. CBO leverages cross-merchant information and was developed and optimized over several years, thus representing a strong baseline for our work. ? LED: This is the algorithm chosen after offline experimentation as described in Section 5, i.e. with RSVD initialization and fine-tuning by Projection.</p><p>Before the A/B test, all users are exposed to the baseline algorithms, i.e. GBO + CBO. During the A/B test, we split users at random into three groups: A, B, and C. Users in group A are exposed to GBO products only. Users in group B are exposed to GBO + CBO products. Finally, users in group C are exposed to products coming from GBO + CBO + LED. We use sets of algorithms instead of individual algorithms as it lets the ranking algorithm learn which algorithm works best for each population dynamically. This approach is typical in an industrial setting where a new algorithm (LED) is evaluated against an existing baseline running in production (GBO + CBO).</p><p>We consider three evaluation metrics: clicks on banner products, sales associated with a display, and total sales amount (order value). Sales are sparser than clicks but better capture the interest of the user. The large volume of our test allows us to draw conclusions on sales well out of noise.</p><p>To better capture the interest of the user, we used landed clicks, i.e. clicks that actually drove the user to a product page on the merchant website. This definition of click is sparser but less noisy and better represents true user interest, since unintentional clicks and failed landing pages are discarded. Sales correspond to actual sales on the merchants' websites.</p><p>The test lasts two months and collects 2 billion displays. The displayed products contain a mixture of goods, services, travel and classified ads and covers thousands of merchants across Europe, America and Asia. <ref type="figure" target="#fig_1">Figure 4</ref> shows the uplift of GBO + CBO + LED over GBO normalized by the uplift of CBO over GBO. Error bars represent 95% confidence intervals. We observe a significant increase in clicks, sales and order value (sales amount). Specifically, LED brings twice more landed clicks and 3.5 times more sales than GPO + CBO did over GBO. In other words, representation learning increased multipled times the added value of recommendation in our system. The uplift is observed continuously over the A/B test period ( <ref type="figure" target="#fig_7">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">A/B test results</head><p>We also analyze the uplift for a specific set of merchants that possess large and diverse catalogs. Intuitively, we expect a larger uplift of our method for these merchants for which the room for recommendation is larger. In practice, merchants belong to this group if they have more than 10 items and more than 10 different categories in their catalog. Categories are associated with each product based on the Google taxonomy using an independent method. With this definition, we indeed observe a higher uplift for these merchants. The uplift in clicks increases four-fold while the uplift in sales increases eight-fold. <ref type="table" target="#tab_4">Table 5</ref> shows the share of products displayed for each algorithm in each population. We remind that the ranking algorithm does not favor a particular algorithm (GBO, CBO, or LED) and only selects the best product to optimize clicks and sales. We observe that LED gains a significant portion of the displayed products (68%) at the expense of GBO and CBO. This testifies of the strong interest of users for the products recommended by LED. GBO and CBO maintain a significant share of voice in situations where popularity is a more effective approach than personalization. The advantage of a modular and two-step architecture is obvious here. <ref type="figure" target="#fig_8">Figure 6</ref> shows the distribution of product popularity for each algorithm. Unsurprisingly, GBO products have the highest popularity overall. CBO products are 3x less popular on average, while LED returns products that are 10x less popular than GBO. LED recommendations venture deep into the long tail of the popularity distribution, while still achieving state-of-the-art performance as seen in Section 7.1. This result is particularly interesting since the popularity bias of LED is learned by the model and not tuned manually (Section 4). Showing less popular products is a valuable behavior from a user experience point of view, resulting in more product discovery for end users.</p><p>From these results, we conclude that LED generates recommendations that are significantly more interesting for the users and the merchants. The increase is significant both in clicks and in sales.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS AND FUTURE WORK</head><p>Recommendation engines have become an essential part of online services. Systems operating at the scale of billions of users have been extensively described. Yet, new algorithms such as variational autoencoders have raised the bar in terms of algorithmic performance but present significant limitations of scale, both for training and inference. We propose a simple and efficient model (LED, for Lightweight Encoder-Decoder) reaching a new trade-off between complexity, scale and performance. By combining pre-training, sampled losses and amortized inference, LED brings a 30? speed-up in latency while reaching the same performance as variational auto-encoders <ref type="bibr" target="#b22">[21]</ref> on standard recommendation metrics.</p><p>The model combines several key design choices such as pretraining, amortized inference, sampling-based losses, and fast nearestneighbor search. We provided a detailed description of a system as deployed at Criteo and open source our code, making our work useful for practitioners wishing to deploy an efficient large-scale recommendation system in the real-world.</p><p>Future work will examine the added value of side information in the architecture. The ability to inject more diversity in the recommendation is also a valuable area of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">REPRODUCIBILITY 9.1 Code</head><p>We release the code used for our experiments under the Apache 2.0 License. <ref type="bibr" target="#b6">6</ref> . The ML20M experiments can be reproduced on a standard machine with Python 3.x and standard libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Experimental setup</head><p>We describe the parameters used to obtain the results described in Sections 5.2 and 5.3.</p><p>ML20M dataset We use the same experimental setup as the Mult-VAE paper <ref type="bibr" target="#b22">[21]</ref> and split the users into train / validation / test, which means one user does not appear simultaneously in train and validation / test. At test time, we randomly split timelines into input (80%) and target (20%) and evaluate the model's recommendations as a top-retrieval task by reporting recall at = 20 and = 50.</p><p>For the VAE, we use embeddings of dimension = 600, hidden layers of dimension 200, tanh activation, and anneal the KL divergence from 0 to 0.2 during training. The embeddings' dimension is set to = 600 for the LED as well and we apply denoising with probability 0.5 to both models. We use the Adam <ref type="bibr" target="#b15">[14]</ref> optimizer with learning rate 0.001, batch size 512, and select the best checkpoint using NDCG@100 on the validation users. All models are trained for 50k steps (? 200 epochs) with a checkpoint frequency of 230 steps (roughly every epoch). Unless specified otherwise, we use = 1000 negatives sampled uniformly.</p><p>Production dataset We keep the last 7 days of each user activity for testing, and split the remaining days by user into 90% for training and 10% for validation. For each user in training and validation, we use the last 7 days as the target part and the first days as the input part. Because we only keep timelines with at least one click in the target period, the resulting pre-processed dataset is only a fraction of the original dataset. Although we are interested in the top-retrieval task, we are also focused on the quality of the top recommendations and report click-rank in addition to recall at 20. Click-rank is defined as the normalized rank of a clicked item among other items in one banner sorted by score and typically goes from 0.5 (random system) to 0 (perfect system, clicked item has the highest score returned by the model).</p><p>The embedding size is an important hyper-parameter. We experimented with various values ranging from 10 to 1000 and found little performance improvement beyond 100. We choose an embedding size of 100 for both the LED and VAE. We use LazyAdam with learning rate 0.001 and batch size 512. We train models for 100k steps (? 30 epochs) with a checkpoint frequency of 2k. We also reuse the same embeddings for the input and output layers of the VAE. Unless specified otherwise, the VAE is trained with approximated multinomial likelihood (Mult-CSS), while the LED is trained with BPR. Using denoising did not seem to increase performance. As banners usually contain a few items (typically between 2 and 10), we use the non-clicked items as negatives. <ref type="bibr" target="#b6">6</ref> https://github.com/criteo/deepr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Significant parameters</head><p>The sampling of negatives is a significant parameter to tune and should be carefully set as more negatives come with a cost in training time. The following experiment evaluates the impact of the number of negatives on performance. We train a LED with BPR for a varying number of negatives and compare the resulting metrics with the same model trained with a multinomial likelihood.</p><p>Unsurprisingly, increasing the number of negatives yields better performance. With = 1000 negatives, the BPR is within 1% of the metrics of the Multinomial Likelihood. However, even with low values ( = 10), the relative difference does not exceed 5%. We observe similar trends with other sampling-based losses or models. In Section 4.2, the normalization factor 1 is less common than 1 ? . In practice, we obtained slightly better results with it. Intuitively, it controls the relative importance with the biases. On one hand, using 1 means that the bias will have more importance when the user's history is diverse, while short and long homogeneous timelines will be treated equally (consider a timeline made of times the same item). On the other hand, using 1 ? , decreases the importance of the bias for long homogeneous timelines compared to short ones, while treating diverse timelines similarly to short but homogeneous ones.</p><p>In Section 4.1, we use an implementation of distributed randomized SVD on Spark already open sourced by Criteo 7 . As in <ref type="bibr" target="#b20">[19]</ref>, we observe that smoothing the distribution of context products improves performance. We implement this by raising the probability ( ) to a power strictly lower than 1 (we choose 0.75 as is commonly done in the literature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Qualitative evaluation</head><p>Qualitative evaluation helps understanding the behavior of the algorithm. We do this by selecting several real user histories at random and inspecting the algorithm output. <ref type="figure" target="#fig_10">Figure 8</ref> shows products recommended by GBO, CBO and LED for two merchants and two real users. Merchant 1 sells mostly furniture and clothes. Merchant 2 is a more general retailer selling a larger variety of items such as furniture, electronics, hardware, and software. User 1 is mostly interested in desks, traveling and children clothes; User 2 browsed women's clothes, sewing machine and TVs. For each user, we show six products sampled from their browsing history. The last row of each set shows the final recommendation output by the ranking model. A colored label identifies the algorithm which recommended each product. We extract several lessons from this analysis:</p><p>? CBO manages to capture some of the interests of the user; by design, this algorithm is less personalized. On the other hand, LED leverages product-level representations and yields more personalized recommendations. ? LED is surprisingly robust and manages to extract useful recommendations even though some items from the user history are irrelevant to the merchant. This is particularly visible for User 1 and Merchant 1 where LED recommends desks despite travel items being present in the user history, and User 2 and Merchant 2 where LED extracts an interest in coats and shoes among diverse user interests. ? LED recommends items that are not necessarily similar to the items browsed by the user but also complementary (e.g. desk board or trestles for User 1 and Merchant 1). ? LED recommendations are fairly narrow and focused on a specific type of product. For instance, for User 2 and Merchant 1, recommendations for bed covers (which are available for this merchant as seen in GBO) could be considered as relevant. An interesting area for future work is to build multi-modal user representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Relationship to other methods</head><p>In the aim of clarifying our algorithm as much as possible, we explain the differences it bears with similar algorithms existing in the literature. When trained as an auto-encoder with multinomial likelihood, the LED model is the lightest possible Mult-DAE <ref type="bibr" target="#b22">[21]</ref>. It has no hidden layers and shares both encoder and decoder embeddings. The LED design is motivated by the unique scale at which we operate, constraining us to reduce compute and memory footprint to the minimum. We note that both the Mult-DAE and Mult-VAE <ref type="bibr" target="#b22">[21]</ref> satisfy the amortized inference and fast nearest neighbor search guidelines since the model parameters are shared by all users and the weights of the last softmax layer can be interpreted as item embeddings.</p><p>The LED model bears a lot of similarities with other latent factors models. If we omit the normalization factor, the biases, and use a gaussian likelihood as objective, it is equivalent to minimizing ? ? ? ? ? 2 , where ? N ? is the user-item interaction matrix, ? R ? the item embeddings matrix, and ? ? ? the Frobenius norm. This formulation is closely related to standard SVD of the user-item matrix , since its closed-form solution is the matrix formed of the top-right singular vectors. It is also similar to the SLIM <ref type="bibr" target="#b30">[28]</ref> objective, with the sparsity constraint replaced with a low-rank constraint. This does not come as a surprise since the VAE framework is known to be a generalization of latent factors models <ref type="bibr" target="#b22">[21]</ref>.</p><p>Previous attempts to simplify matrix factorization methods include NSVD <ref type="bibr" target="#b32">[30]</ref>, which also represents users using their history. In particular, it uses a Gaussian likelihood to reconstruct recommendation scores as ( , ) = + + =1 ? ? , ? ? , where (resp. ) are user (resp. item) biases. The LED differs mostly by the absence of user biases, the normalization factor, as well as the training procedure. As an item-based model, our approach can also be seen as a simple neighborhood-based method <ref type="bibr" target="#b40">[38]</ref>. Building on the related NSVD <ref type="bibr" target="#b32">[30]</ref> method, more powerful variants <ref type="bibr" target="#b16">[15]</ref> intersecting matrix factorization with neighborhood-based techniques have been explored.</p><p>If we apply a softmax to the scores to get a distribution over items, this model is a one-hidden-layer neural network, whose input is a normalized one-hot encoding of the user's history and output a distribution over items. If we substitute items with words, we recognize a log-linear model, similar to Word2Vec <ref type="bibr" target="#b29">[27]</ref>, where context and target parameters are shared and inputs normalized.</p><p>In Section 4.4, ? is the Gram matrix associated with the kernel ( , ) =&lt; , &gt;. Previous work <ref type="bibr" target="#b35">[33]</ref> proposed a regularized matrix factorization method using kernels and derived an online update rule to solve the new user/item problem. Instead of updating the user/item matrices, our method focuses on updating the kernel itself to adapt to a new type of feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2101.00870v2 [cs.IR] 29 Nov 2021 2 PROBLEM STATEMENT 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>RSVD 4 :</head><label>4</label><figDesc>For each user associated with a positive click event , sample a negative item and update and to maximize the BPR log ( ( , ) ? ( , ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>LED architecture. P is a trainable projection matrix ? R ? . Popularity bias omitted for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Recall@50 of LED for different initialization and fine-tuning methods on ML20M. The SVD embeddings are always pre-trained on the full training set. The project method outperforms both random initialization and classical finetuning for models trained on small fractions of the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 2 )</head><label>2</label><figDesc>Pass embeddings and other features to LED and compute a user embedding using TensorFlow Java bindings 4 (3) Run approximate KNN search (hnswlib) with the user embedding as a query (4) Return the approximate nearest neighbors as recommendations 1 https://www.tensorflow.org/xla/tfcompile 2 http://llvm.org/ 3 https://grpc.io/ 4 https://www.tensorflow.org/install/lang_java</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>LED: Overview of the offline pipeline to train the retrieval system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>A/B test results: uplift of GBO + CBO and GBO + CBO + LED versus GBO. The uplift of GBO + CBO is scaled to 1. Error bars represent confidence intervals at 95%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Number of sales per day for each population of the A/B test. The test lasts two months. Y-axis scaled to 1 for the first day of GBO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Distribution of product popularity per algorithm. x-axis: number of views per month on a log scale normalized to 1 for GBO. Average value in dotted line. Despite showing less popular products, LED generates more clicks and sales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Relative performance drop of LED trained with BPR instead of multinomial likelihood (smaller is better) on ML20M. With only 10 negatives, the drop is less than 5 %.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Sample recommendations for two users and two merchants. LED recommends more personalized products thanks to its fine-grain product-level user representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. Density refers to the density of the item-item matrix.</figDesc><table><row><cell></cell><cell cols="4">users items events density %</cell></row><row><cell>ML20M</cell><cell>136K</cell><cell>20K</cell><cell>10M</cell><cell>2.47</cell></row><row><cell>Production</cell><cell>587M</cell><cell>5M</cell><cell>19B</cell><cell>0.076</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of VAE and LED models with and without sampling on ML20M. Percentages measure relative difference with the Mult-VAE. Only lines marked with a ? are scalable. The LED model trained with BPR achieves results close to the Mult-VAE while enabling training at scale.</figDesc><table><row><cell></cell><cell cols="2">ML20M dataset</cell><cell></cell></row><row><cell>Model</cell><cell>Loss</cell><cell>Recall@20</cell><cell>Recall@50</cell></row><row><cell>VAE [21]</cell><cell>Mult</cell><cell>0.396</cell><cell>0.537</cell></row><row><cell>VAE</cell><cell>Mult-CSS  ?</cell><cell cols="2">0.382 (-3.54%) 0.523 (-2.61%)</cell></row><row><cell>DAE [21]</cell><cell>Mult</cell><cell cols="2">0.387 (-2.27%) 0.524 (-2.42%)</cell></row><row><cell>LED</cell><cell>Mult</cell><cell cols="2">0.379 (-4.29%) 0.517 (-3.72%)</cell></row><row><cell>LED</cell><cell>Mult-CSS  ?</cell><cell cols="2">0.368 (-7.07%) 0.506 (-5.77%)</cell></row><row><cell>LED</cell><cell>BPR  ?</cell><cell cols="2">0.375 (-5.30%) 0.516 (-3.91%)</cell></row><row><cell>LED</cell><cell>NS  ?</cell><cell cols="2">0.375 (-5.30%) 0.514 (-4.28%)</cell></row><row><cell>EASE</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Impact of pre-training and fine-tuning methods on the Production dataset. Despite its simplicity, LED outperforms the VAE. Pre-training is particularly effective, yielding better results than random initialization.</figDesc><table><row><cell></cell><cell cols="2">Production dataset</cell><cell></cell></row><row><cell>Model</cell><cell>Init</cell><cell>Tuning</cell><cell>R@20 ClickRank</cell></row><row><cell>VAE</cell><cell>Random</cell><cell>Train</cell><cell>0.078 0.471</cell></row><row><cell>VAE</cell><cell>SVD</cell><cell>Train</cell><cell>0.083 0.457</cell></row><row><cell>VAE</cell><cell>SVD</cell><cell>Proj</cell><cell>0.091 0.454</cell></row><row><cell>LED</cell><cell>Random</cell><cell>Train</cell><cell>0.099 0.468</cell></row><row><cell>LED</cell><cell>SVD</cell><cell>Train</cell><cell>0.109 0.454</cell></row><row><cell>LED</cell><cell>SVD</cell><cell>Proj</cell><cell>0.104 0.450</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Real-time computing performance of our system with the LED model</figDesc><table><row><cell>Max Queries Per Second (QPS) per instance</cell><cell>3200</cell></row><row><cell>Latency @ 50th pct</cell><cell>500 s</cell></row><row><cell>Latency @ 99th pct</cell><cell>2ms</cell></row><row><cell cols="2">Latency of user embedding computation @ 50th pct 30 s</cell></row><row><cell cols="2">Latency of user embedding computation @ 99th pct 65 s</cell></row><row><cell>Latency of KNN search @ 50th pct</cell><cell>160 s</cell></row><row><cell>Latency of KNN search @ 99th pct</cell><cell>450 s</cell></row><row><cell>Instances used in production</cell><cell>200</cell></row><row><cell>Recommendations served per day</cell><cell>4B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Share of each algorithm in the displayed products after ranking. The ranking model predicts clicks and sales independently from the product origin.</figDesc><table><row><cell>A/B test population</cell><cell cols="3">GBO CBO LED</cell></row><row><cell>A</cell><cell>100%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>B</cell><cell>61%</cell><cell>39%</cell><cell>0%</cell></row><row><cell>C</cell><cell>22%</cell><cell cols="2">11% 68%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://ann-benchmarks.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/criteo/Spark-RSVD</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Complementary Sum Sampling for Likelihood Approximation in Large Scale Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v54/botev17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<editor>Aarti Singh and Jerry Zhu</editor>
		<meeting>Machine Learning Research<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Normalized (pointwise) mutual information in collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Form to Meaning: Processing Texts Automatically, Proceedings of the Biennial GSCL Conference</title>
		<meeting><address><addrLine>T?bingen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tall and Skinny QR Factorizations in MapReduce Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Constantine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on MapReduce and Its Applications</title>
		<meeting>the Second International Workshop on MapReduce and Its Applications<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/1996092.1996103</idno>
		<ptr target="https://doi.org/10.1145/1996092.1996103" />
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="43" to="50" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2959100.2959190</idno>
		<ptr target="https://doi.org/10.1145/2959100.2959190" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems<address><addrLine>Boston, Massachusetts, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
	<note>RecSys &apos;16</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sugnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3186183</idno>
		<ptr target="https://doi.org/10.1145/3178876.3186183" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference<address><addrLine>Lyon, France; Republic and Canton of Geneva, CHE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1775" to="1784" />
		</imprint>
	</monogr>
	<note>WWW &apos;18). International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Amortized Inference in Probabilistic Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<idno type="DOI">10.1137/090771806</idno>
		<ptr target="https://doi.org/10.1137/090771806" />
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WWW &apos;17). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052569</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052569" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>Neural Collaborative Filtering</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th</title>
		<meeting>the 26th</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WWW &apos;17). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE</title>
		<idno type="DOI">10.1145/3038912.3052639</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052639" />
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
		<meeting><address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2008.22</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2008.22" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Eighth IEEE International Conference on Data Mining (ICDM &apos;08)</title>
		<meeting>the 2008 Eighth IEEE International Conference on Data Mining (ICDM &apos;08)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Input Search for Large Scale Recommendation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taibai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403288</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403288" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event, CA, USA) (KDD &apos;20)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event, CA, USA) (KDD &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2387" to="2397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3109859.3109933</idno>
		<ptr target="https://doi.org/10.1145/3109859.3109933" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
		<meeting>the Eleventh ACM Conference on Recommender Systems<address><addrLine>Como, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="396" to="397" />
		</imprint>
	</monogr>
	<note>RecSys &apos;17</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhancing VAEs for Collaborative Filtering: Flexible Priors &amp; Gating Mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongwon</forename><surname>Suh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3298689.3347015</idno>
		<ptr target="https://doi.org/10.1145/3298689.3347015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems<address><addrLine>Copenhagen, Denmark; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="407" />
		</imprint>
	</monogr>
	<note>RecSys &apos;19). Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<idno type="DOI">10.1145/1401890.1401944</idno>
		<ptr target="https://doi.org/10.1145/1401890.1401944" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Nevada, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient Training on Very Large Corpora via Gramian Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mayoraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Large-scale Collaborative Filtering with Product Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thom</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinead</forename><forename type="middle">A</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">T</forename><surname>Hawk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">C</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Wing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04321</idno>
		<ptr target="http://arxiv.org/abs/1901.04321" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jack</surname></persName>
		</author>
		<title level="m">Efficient Top-N Recommendation by Linear Regression. RecSys Large Scale Recommender Systems Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LightRec: A Memory and Search-Efficient Recommender System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380151</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380151" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020<address><addrLine>Taipei, Taiwan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="695" to="705" />
		</imprint>
	</monogr>
	<note>WWW &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">WWW &apos;18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE</title>
		<idno type="DOI">10.1145/3178876.3186150</idno>
		<ptr target="https://doi.org/10.1145/3178876.3186150" />
	</analytic>
	<monogr>
		<title level="m">World Wide Web Conference</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Amazon.Com Recommendations: Item-to-Item Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>York</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIC.2003.1167344</idno>
		<ptr target="https://doi.org/10.1109/MIC.2003.1167344" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Lobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/ract-toward-amortized-ranking-critical-training-for-collaborative-filtering/" />
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Off-Policy Learning in Two-Stage Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380130</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380130" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020<address><addrLine>Taipei, Taiwan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="463" to="473" />
		</imprint>
	</monogr>
	<note>WWW &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal-Contextual Recommendation in Real-Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403278</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403278" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Virtual Event, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2291" to="2299" />
		</imprint>
	</monogr>
	<note>KDD &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2889473</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2889473" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe, Nevada; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SLIM: Sparse Linear Methods for Top-N Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2011.134</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2011.134" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM &apos;11)</title>
		<meeting>the 2011 IEEE 11th International Conference on Data Mining (ICDM &apos;11)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Content-based Music Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2999792.2999907" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe, Nevada; USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving regularized singular value decomposition for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Paterek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD Cup and Workshop</title>
		<meeting>KDD Cup and Workshop</meeting>
		<imprint>
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Montreal, Quebec, Canada; Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>UAI &apos;09)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
		<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="DOI">10.1145/1454008.1454047</idno>
		<ptr target="https://doi.org/10.1145/1454008.1454047" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM Conference on Recommender Systems</title>
		<meeting>the 2008 ACM Conference on Recommender Systems<address><addrLine>Lausanne, Switzerland; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
	<note>RecSys &apos;08</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Item-based Collaborative Filtering Recommendation Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
		<idno type="DOI">10.1145/371920.372071</idno>
		<ptr target="https://doi.org/10.1145/371920.372071" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on World Wide Web</title>
		<meeting>the 10th International Conference on World Wide Web<address><addrLine>Hong Kong, Hong Kong; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">RecVAE: A New Variational Autoencoder for Top-N Recommendations with Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Shenbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Alekseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Tutubalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371831</idno>
		<ptr target="https://doi.org/10.1145/3336191.3371831" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining<address><addrLine>Houston, TX, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
	<note>WSDM &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Use of Deep Learning in Modern Recommendation System: A Summary of Recent Works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Pant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07525</idno>
		<ptr target="http://arxiv.org/abs/1712.07525" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Embarrassingly Shallow Autoencoders for Sparse Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313710</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313710" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3251" to="3257" />
		</imprint>
	</monogr>
	<note>WWW &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unifying Nearest Neighbors Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Verstrepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Goethals</surname></persName>
		</author>
		<idno type="DOI">10.1145/2645710.2645731</idno>
		<ptr target="https://doi.org/10.1145/2645710.2645731" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender Systems</title>
		<meeting>the 8th ACM Conference on Recommender Systems<address><addrLine>Foster City, Silicon Valley, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
	<note>RecSys &apos;14)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dik Lun</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219869</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219869" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">M2GRL: A Multi-Task Multi-View Graph Representation Learning Framework for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guli</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403284</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403284" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event, CA, USA) (KDD &apos;20)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (Virtual Event, CA, USA) (KDD &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03856</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">StarSpace: Embed All The Things! arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219890</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219890" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
	<note>KDD &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">IntentGC: A Scalable Graph Convolution Framework Fusing Heterogeneous Information for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330686</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330686" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, AK, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2347" to="2357" />
		</imprint>
	</monogr>
	<note>KDD &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep Interest Network for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219823</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219823" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
	<note>KDD &apos;18)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
