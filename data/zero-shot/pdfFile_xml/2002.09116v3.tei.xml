<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deep Kernels for Non-Parametric Two-Sample Tests</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangquan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
						</author>
						<title level="a" type="main">Learning Deep Kernels for Non-Parametric Two-Sample Tests</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two sample tests is available at github.com/fengliu90/DK-for-TST.</p><p>We use the null hypothesis testing framework, where the null hypothesis H 0 : P = Q is tested against the alternative hypothesis H 1 : P = Q. We perform a two-sample test in four steps: select a significance level ? ? [0, 1]; compute a test statistict(S P , S Q ); compute the p-valuep = Pr H0 (T &gt; t), the probability of the two-sample test returning a statistic as large ast when H 0 is true; finally, reject H 0 ifp &lt; ?.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Two sample tests are hypothesis tests aiming to determine whether two sets of samples are drawn from the same distribution. Traditional methods such as t-tests and Kolmogorov-Smirnov tests are mainstays of statistical applications, but require strong parametric assumptions about the distributions being studied and/or are only effective on data in ex-tremely low-dimensional spaces. A broad set of recent work in statistics and machine learning has focused on relaxing these assumptions, with methods either generally applicable or specific to various more complex domains <ref type="bibr" target="#b18">(Gretton et al., 2012a;</ref><ref type="bibr">Sz?kely &amp; Rizzo, 2013;</ref><ref type="bibr" target="#b21">Heller &amp; Heller, 2016;</ref><ref type="bibr" target="#b23">Jitkrittum et al., 2016;</ref><ref type="bibr">Ramdas et al., 2017;</ref><ref type="bibr">Lopez-Paz &amp; Oquab, 2017;</ref><ref type="bibr" target="#b7">Chen &amp; Friedman, 2017;</ref><ref type="bibr" target="#b12">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Ghoshdastidar et al., 2017;</ref><ref type="bibr" target="#b13">Ghoshdastidar &amp; von Luxburg, 2018;</ref><ref type="bibr">Li &amp; Wang, 2018;</ref><ref type="bibr">Kirchler et al., 2020)</ref>. These tests have also allowed application in various machine learning problems such as domain adaptation, generative modeling, and causal discovery <ref type="bibr" target="#b16">Gong et al., 2016;</ref><ref type="bibr">Stojanov et al., 2019;</ref><ref type="bibr">Lopez-Paz &amp; Oquab, 2017)</ref>.</p><p>A popular class of non-parametric two-sample tests is based on kernel methods <ref type="bibr">(Smola &amp; Sch?lkopf, 2001)</ref>: such tests construct a kernel mean embedding <ref type="bibr" target="#b3">(Berlinet &amp; Thomas-Agnan, 2004;</ref><ref type="bibr">Muandet et al., 2017)</ref> for each distribution, and measure the difference in these embeddings. For any characteristic kernel, two distributions are the same if and only if their mean embeddings are the same; the distance between mean embeddings is the maximum mean discrepancy (MMD) <ref type="bibr" target="#b18">(Gretton et al., 2012a)</ref>. There are also several closely related methods, including tests based on checking for differences in mean embeddings evaluated at specific locations <ref type="bibr" target="#b9">(Chwialkowski et al., 2015;</ref><ref type="bibr" target="#b23">Jitkrittum et al., 2016)</ref> and kernel Fisher discriminant analysis <ref type="bibr" target="#b20">(Harchaoui et al., 2007)</ref>. These tests all work well for samples from simple distributions when using appropriate kernels.</p><p>Problems that we care about, however, often involve distributions with complex structure, where simple kernels will often map distinct distributions to nearby (and hence hard to distinguish) mean embeddings. <ref type="figure" target="#fig_0">Figure 1a</ref> shows an example of a multimodal dataset, where the overall modes align but the sub-mode structure varies differently at each mode. A translation-invariant Gaussian kernel only "looks at" the data uniformly within each mode (see <ref type="figure" target="#fig_0">Figure 1b</ref>), requiring many samples to correctly distinguish the two distributions. The distributions can be distinguished more effectively if we understand the structure of each mode, as with the more complex kernel illustrated in <ref type="figure" target="#fig_0">Figure 1c</ref>.</p><p>To model these complex functions, we adopt a deep kernel approach <ref type="bibr">(Wilson et al., 2016;</ref><ref type="bibr">Sutherland et al., 2017;</ref><ref type="bibr">Li et al., 2017;</ref><ref type="bibr" target="#b22">Jean et al., 2018;</ref><ref type="bibr">Wenliang et al., 2019</ref> building a kernel with a deep network. In this paper, we use</p><formula xml:id="formula_0">k ? (x, y) = [(1 ? )?(? ? (x), ? ? (y)) + ]q(x, y),<label>(1)</label></formula><p>where the deep neural network ? ? extracts features of samples, and ? is a simple kernel (e.g., a Gaussian) on those features, while q is a simple characteristic kernel (e.g. Gaussian) on the input space. With an appropriate choice of ? ? , this allows for extremely flexible kernels which can learn complex behavior very different in different parts of space. This choice is discussed further in Section 5.</p><p>These complex kernels, though, cannot feasibly be specified by hand or simple heuristics, as is typical practice in kernel methods. We select the parameters ? by maximizing the ratio of the MMD to its variance, which maximizes test power at large sample sizes. This procedure was proposed by <ref type="bibr">Sutherland et al. (2017)</ref>, but we establish for the first time that it gives consistent selection of the best kernel in the class, whether optimizing our deep kernels with hundreds of thousands of parameters or simply choosing lengthscales of a Gaussian as did Sutherland et al. Previously, there were no guarantees this procedure would yield a kernel which generalized at all from the training set to a test set.</p><p>Another way to compare distributions is to train a classifier between them, and evaluate its accuracy <ref type="bibr">(Lopez-Paz &amp; Oquab, 2017)</ref>. We show, perhaps surprisingly, that our framework encompasses this approach, but deep kernels allow for more general model classes which can use the data more efficiently. We also train representations directly to maximize test power, rather than a cross-entropy surrogate.</p><p>We test our method on several simulated and real-world datasets, including complex synthetic distributions, highenergy physics data, and challenging image problems. We find convincingly that learned deep kernels outperform simple shallow methods, and learning by maximizing test power outperforms learning through a cross-entropy surrogate loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MMD Two-Sample Tests</head><p>Two-sample testing. Let X be a separable metric spacein this paper, typically a subset of R d -and P, Q be Borel probability measures on X . We observe independent identically distributed (i.i.d.) samples S P = {x i } n i=1 ? P n and S Q = {y j } m j=1 ? Q m . We wish to know whether S P and S Q come from the same distribution: does P = Q? 1 n(n ? 1) i =j H ij</p><p>(2)</p><formula xml:id="formula_1">H ij := k(X i , X j ) + k(Y i , Y j ) ? k(X i , Y j ) ? k(Y i , X j ).</formula><p>The similar MMD 2 b := 1 n 2 ij H ij is the squared MMD between the empirical distributions of S P and S Q . 1</p><p>Testing with the MMD. It can be shown that under H 0 , n MMD 2 u converges to a distribution depending on P and k; we thus use this as our test statistic.</p><p>Proposition 2 (Asymptotics of MMD 2 u ). Under the null hypothesis, H 0 : P = Q, we have if Z i ? N (0, 2),</p><formula xml:id="formula_2">n MMD 2 u d ? i ? i (Z 2 i ? 2);</formula><p>here ? i are the eigenvalues of the P-covariance operator of the centered kernel <ref type="bibr">(Gretton et al., 2012a, Theorem 12)</ref>, and d ? denotes convergence in distribution.</p><p>Under the alternative, H 1 : P = Q, a standard central limit theorem holds (Serfling, 1980, Section 5.5.1):</p><formula xml:id="formula_3">? n( MMD 2 u ? MMD 2 ) d ? N (0, ? 2 H1 ) ? 2 H1 := 4 E[H 12 H 13 ] ? E[H 12 ] 2</formula><p>where H 12 , H 13 refer to H ij above.</p><p>Although it is possible to construct a test based on directly estimating this null distribution <ref type="bibr" target="#b17">(Gretton et al., 2009)</ref>, it is both simpler and, if implemented carefully, faster <ref type="bibr">(Sutherland et al., 2017)</ref> to instead use a permutation test. This general method <ref type="bibr" target="#b11">(Dwass, 1957;</ref><ref type="bibr" target="#b0">Alba Fern?ndez et al., 2008)</ref> observes that under H 0 , the samples from P and Q are interchangeable; we can therefore estimate the null distribution of our test statistic by repeatedly re-computing it with the samples randomly re-assigned to S P or S Q .</p><p>Test power. The main measure of efficacy of a null hypothesis test is its power: the probability that, for a particular P = Q and n, we correctly reject H 0 . Proposition 2 implies, where ? is the standard normal CDF, that</p><formula xml:id="formula_4">Pr H1 n MMD 2 u &gt; r ? ? ? n MMD 2 ? H1 ? r ? n ? H1<label>;</label></formula><p>1 Including k(Xi, Yi) terms in MMDu gives the minimal variance unbiased estimator, and allows m = n. The U -statistic is more convenient for analysis and for efficient permutations; in our settings it behaves similarly to the MVUE and MMD we can find the approximate test power by using the rejection threshold, found via (e.g.) permutation testing, as r. We also know via Proposition 2 that this r will converge to a constant, and MMD, ? H1 are also constants. For reasonably large n, the power is dominated by the first term, and the kernel yielding the most powerful test will approximately maximize <ref type="bibr">(Sutherland et al., 2017)</ref> </p><formula xml:id="formula_5">J(P, Q; k) := MMD 2 (P, Q; k)/? H1 (P, Q; k). (3)</formula><p>Selecting a kernel. The criterion J(P, Q; k) depends on the particular P and Q at hand, and thus we typically will neither be able to choose a kernel a priori, nor exactly evaluate J given samples. We can, however, estimate it wit?</p><formula xml:id="formula_6">J ? (S P , S Q ; k) := MMD 2 u (S P , S Q ; k) ? H1,? (S P , S Q ; k) ,<label>(4)</label></formula><p>where? 2 H1,? is a regularized estimator of ? 2 H1 given by 2</p><formula xml:id="formula_7">4 n 3 n i=1 ? ? n j=1 H ij ? ? 2 ? 4 n 4 ? ? n i=1 n j=1 H ij ? ? 2 + ?. (5)</formula><p>Given S P and S Q , we could construct a test by choosing k to maximize? ? (S P , S Q ; k), then using a test statistic based on MMD(S P , S Q ; k). This sample re-use, however, violates the conditions of Proposition 2, and permutation testing would require repeatedly re-training k with permuted labels.</p><p>Thus we split the data, get k tr ? arg max k?? (S tr P , S tr Q ; k), then compute the test statistic and permutation threshold on S te P , S te Q using k tr . This procedure was proposed for MMD 2 u by <ref type="bibr">Sutherland et al. (2017)</ref>, but the same technique works for a variety of tests <ref type="bibr" target="#b19">(Gretton et al., 2012b;</ref><ref type="bibr" target="#b23">Jitkrittum et al., 2016;</ref><ref type="bibr">Lopez-Paz &amp; Oquab, 2017)</ref>. Our paper adopts this framework (Section 5) and studies it further.</p><p>Relationship to other approaches. One common scheme is to pick a kernel k ? based on some proxy task, such as a related classification problem (e.g. <ref type="bibr">Kirchler et al. 2020</ref> or the KID score of <ref type="bibr" target="#b5">Binkowski et al. 2018)</ref>. Although this approach can work quite well, it depends entirely on features from the proxy task applying well to the differences between P and Q, which can be hard to know in general.</p><p>An alternative is to maximize simply MMD u <ref type="bibr" target="#b17">(Sriperumbudur et al. 2009</ref>; proposed but not evaluated by <ref type="bibr">Kirchler et al.)</ref>. Ignoring ? H1 means that, for instance, this approach would choose to simply scale k ? ?, even though this does not change the test at all. Even when this is not possible, <ref type="bibr">Sutherland et al. (2017)</ref> found this approach notably worse than maximizing (4); we confirm this in our experiments. <ref type="bibr">MMD-GANs (Li et al., 2017;</ref><ref type="bibr" target="#b5">Binkowski et al., 2018)</ref> also simply maximize MMD u to identify the differences between their model Q ? and target P. If Q ? is quite far from P, however, an MMD-GAN requires a "weak" kernel to identify a path for improving Q ? , while our ideal kernel is one which perfectly distinguishes P and Q ? and would likely give no signal for improvement. Our algorithm, theoretical guarantees, and empirical evaluations thus all differ significantly from those for MMD-GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Limits of Simple Kernels</head><p>We can use the criterion? ? of (4) even to select parameters among a simple family, such as the lengthscale of a Gaussian kernel. Doing so on the Blob problem of <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the limitations of using MMD with these kernels.</p><p>In <ref type="figure" target="#fig_3">Figure 2c</ref>, we show how the maximal value of? changes as we see more samples from P and Q, for both a family of Gaussian kernels (green dashed line) and a family (1) of deep kernels (red line). The optimal? is always higher for the deep kernels; as expected, the empirical test power <ref type="figure" target="#fig_3">(Figure 2a</ref>) is also higher for deep kernels.</p><p>Most simple kernels used for MMD tests, whether the Gaussian we use here or Laplace, inverse multiquadric, even automatic relevance determination kernels, are all translation invariant: k(x, y) = k(x?t, y ?t) for any t ? R d . (All kernels used by <ref type="bibr">Sutherland et al. (2017)</ref>, for instance, were of this type.) Hence the kernel behaves the same way across space, as in <ref type="figure" target="#fig_0">Figure 1b</ref>. This means that for distributions whose behavior varies through space, whether because principal directions change (as in <ref type="figure" target="#fig_0">Figure 1</ref>) so the shape should be different, or because some regions are much denser than others and so need a smaller lengthscale (e.g. Wenliang et al., 2019, <ref type="figure" target="#fig_0">Figures 1 and 2)</ref>, any single global choice is suboptimal.</p><p>Kernels which are not translation invariant, such as the deep kernels (1) shown in <ref type="figure" target="#fig_0">Figure 1c</ref>, can adapt to the different shapes necessary in different areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Relationship to Classifier-Based Tests</head><p>Another popular method for conducting two-sample tests is to train a classifier between S tr P and S tr Q , then assess its performance on S te P , S te Q . If P = Q, the classification problem is impossible and performance will be at chance.</p><p>The most common performance metric is the accuracy <ref type="bibr">(Lopez-Paz &amp; Oquab, 2017)</ref>; this scheme is fairly common among practitioners, and <ref type="bibr">Kim et al. (2020)</ref> showed it to be optimal in rate, but suboptimal in constant, in one limited setting (linear discriminant analysis between highdimensional elliptical distributions, e.g. Gaussians, with identical covariances). We will call this approach a Classifier Two-Sample Test based on Sign, C2ST-S. Letting f : X ? R output classification scores, the C2ST-S statistic is acc(S P , S Q ; f ) given by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">2n</head><p>Xi?S</p><formula xml:id="formula_8">P 1(f(X i ) &gt; 0) + 1 2n Yi?S Q 1(f(Y i ) ? 0). Let acc(P, Q; f ) := 1 2 Pr(f (X) &gt; 0) + 1 2 Pr(f (Y ) ? 0)</formula><p>; acc is unbiased for acc and has a simple asymptotically normal null distribution.</p><p>Although it is perhaps not immediately obvious this is the case, C2ST-S is almost a special case of the MMD. Let</p><formula xml:id="formula_9">k (S) f (x, y) = 1 4 1(f(x) &gt; 0) 1(f(y) &gt; 0). (6) A C2ST-S with f is equivalent to an MMD test with k (S) f : Proposition 3. It holds that MMD(P, Q; k (S) f ) = |acc(P, Q; f ) ? 1 2 | MMD b (S P , S Q ; k (S) f ) = | acc(S P , S Q ; f ) ? 1 2 |.</formula><p>Proof. The mean embedding ? P under k (S) f is simply</p><formula xml:id="formula_10">1 2 E 1(f(X) &gt; 0) = 1 2 Pr(f (X) &gt; 0), so the MMD is 1 2 Pr(f (X) &gt; 0)?Pr(f (Y ) &gt; 0) = acc(P, Q; f )? 1 2 .</formula><p>Moreover, acc is acc on empirical distributions. Accessing f only through its sign allows for a simple null distribution, but it ignores f 's measure of confidence: a highly confident output extremely far from the decision boundary is treated the same as a very uncertain one lying in an area of high overlap between P and Q, dramatically increasing the variance of the statistic. A scheme we call C2ST-L instead tests difference in means of f on P and Q <ref type="bibr" target="#b8">(Chen &amp; Cloninger, 2019)</ref>. Let</p><formula xml:id="formula_11">k (L) f (x, y) = f (x)f (y).<label>(7)</label></formula><p>A C2ST-L is equivalent to an MMD test with k  Proposition 4. It holds that</p><formula xml:id="formula_12">MMD(P, Q; k (L) f ) = |E f (X) ? E f (Y )| MMD b (S P , S Q ; k (L) f ) = | 1 n Xi?S P f (X i ) ? 1 n Yi?S Q f (Y i )|.</formula><p>Proof. This kernel's feature map is k</p><formula xml:id="formula_13">(L) f (x, ?) = f (x).</formula><p>Now maximizing accuracy (or a cross-entropy proxy) no longer directly maximizes power. This kernel is differentiable, so we can directly compare the merits of maximizing (4) to maximizing cross-entropy; we will see in Section 7.2 that our more direct approach is empirically superior.</p><p>Compared to using k (L) f , however, Section 7.2 shows that learned MMD tests also obtain better performance using kernels like (1). This is analogous to a similar phenomenon observed in other problems by <ref type="bibr" target="#b5">Binkowski et al. (2018)</ref> and <ref type="bibr">Wenliang et al. (2019)</ref>: C2STs learn a full discriminator function on the training set, and then apply only that function to the test set. Learning a deep kernel like (1) corresponds to learning only a powerful representation on the training set, and then still learning f itself from the test setin a closed form that makes permutation testing simple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning Deep Kernels</head><p>Choice of kernel architecture. Most previous work on deep kernels has used a kernel ? directly on the output of a featurization network ? ? , k ? (x, y) = ?(? ? (x), ? ? (y)). This is certainly also an option for us. Any such k ? , however, is characteristic if and only if ? ? is injective. If we select our kernel well, this is not really a concern. 3 Even so, it would be reassuring to know that, even if the optimization goes awry, the resulting test will still be at least consistent. More importantly, it can be helpful in optimization to add a "safeguard" preventing the learned kernel from considering extremely far-away inputs as too similar. We can achieve these goals with the form (1), repeated here:</p><formula xml:id="formula_14">k ? (x, y) = [(1 ? )?(? ? (x), ? ? (y)) + ] q(x, y).</formula><p>Here ? ? is a deep network (with parameters ?) that extracts features, and ? is a kernel on those features; we use a Gaus-</p><formula xml:id="formula_15">sian with lengthscale ? ? , ?(a, b) = exp ? 1 2? 2 ? a ? b 2 .</formula><p>We choose 0 &lt; &lt; 1 and q a Gaussian with lengthscale ? q .</p><p>Proposition 5. Let k ? be of the form (1) with &gt; 0 and q characteristic. Then k ? is characteristic.</p><p>Learning the deep kernel. The kernel optimization and testing procedure is summarized in Algorithm 1. For larger datasets, or when n = m, we use minibatches in the training procedure; for smaller datasets, we use full batches. We use the Adam optimizer <ref type="bibr" target="#b26">(Kingma &amp; Ba, 2015)</ref>. Note that the parameters , ? ? , and ? q are included in ?, all parameterized in log-space (i.e. we optimize where = exp( )).</p><p>Time complexity. Let E denote the cost of computing an embedding ? ? (x), and K the cost of computing (1) given ? ? (x), ? ? (y). Then each iteration of training in Algorithm 1 costs O mE + m 2 K , where m is the minibatch size; for the moderate m that fit in a GPU-sized minibatch anyway, the mE term typically dominates, matching the complexity of a C2ST. Testing takes time O nE + n 2 K + n 2 n perm , compared to O (nE + n n perm ) for permutation-based C2STs. In either case, the quadratic factors could if necessary be reduced P, Q pair is enough that a perfect optimizer would be able to distinguish the distributions , Proposition 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Testing with a learned deep kernel</head><p>Input: S P , S Q , various hyperparameters used below; ? ? ?0; ? ? 10 ?8 ; Split the data as S P = S tr P ? S te P and S Q = S tr Q ? S te Q ; # Phase 1: train the kernel parameters ? on S tr P and S tr Q for T = 1, 2, . . . , Tmax do X ? minibatch from S tr P ; Y ? minibatch from S te Q ; k? ? kernel function with parameters ?;</p><p># as in (1)</p><formula xml:id="formula_16">M (?) ? MMD 2 u (X, Y ; k?); # using (2) V ? (?) ?? 2 H 1 ,? (X, Y ; k?); # using (5) J ? (?) ? M (?)/ V ? (?); # as in (4) ? ? ? + ??Adam? ? (?); # maximize? ? (?) end for # Phase 2: permutation test with k? on S te P and S te Q est ? MMD 2 u (S te P , S te Q ; k?) for i = 1, 2, . . . , nperm do Shuffle S te P ? S te Q into X and Y perm i ? MMD 2 u (X, Y ; k?) end for Output: k?, est, p-value 1 nperm nperm i=1 1(perm i ? est)</formula><p>with the block estimator approach of <ref type="bibr">Zaremba et al. (2013)</ref>, at the cost of some test power. In our experiments in Section 7, the overall runtime of our methods was scarcely different from the overall runtime of C2STs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Theoretical Analysis</head><p>We now show that optimizing the regularized test power criterion based on a finite number of samples works: as n increases, our estimates converge uniformly over a ball in parameter space, and therefore if there is a unique best kernel, we converge to it. <ref type="bibr">Sutherland et al. (2017)</ref> gave no such guarantees; this result allows us to trust that, at least for reasonably large n and if our optimization process succeeds, we will find a kernel that generalizes nearly optimally rather than just overfitting to S tr .</p><p>We first state a generic result, then show some choices of kernels, particularly deep kernels (1), satisfy the conditions. Theorem 6. Let ? parameterize uniformly bounded ker-</p><formula xml:id="formula_17">nel functions k ? in a Banach space of dimension D, with |k ? (x, y) ? k ? (x, y)| ? L k ? ? ? . Let? s be a set of ? for which ? 2 H1 (P, Q; k ? ) ? s 2 &gt; 0 and ? ? R ? . Take ? = n ?1/3 . Then, with probability at least 1 ? ?, sup ???s |? ? (S P , S Q ; k ? ) ? J(P, Q; k ? )| = O 1 s 2 n 1/3 1 s + D log(R ? n) + log 1 ? + L k .</formula><p>If there is a unique best kernel ? * , the maximizer of? ? converges in probability to ? * as n ? ?.</p><p>A version with explicit constants and more details is given in Appendix A (as Theorem 11 and Corollary 12); the proof is based on uniform convergence of the MMD and variance estimators using an -net argument.</p><p>The following results are shown in Appendix A.4. We first show a result on simple Gaussian bandwidth selection. Proposition 7. Suppose each x ? X has x ? R X , and we choose the bandwidth of a Gaussian kernel among a set whose minimum is at least 1/R ? . Then the conditions of Theorem 6 are met with D = 1 and L k = 2R X / ? e.</p><p>Our results also apply to multiple kernel learning, where in fact the exact maximizer of? ? is efficiently available (Proposition 27).</p><formula xml:id="formula_18">Proposition 8. Let {k i } D i=1 be a fixed set of kernels, with sup x k i (x, x) ? K for all i. Then picking k ? = D i=1 ? i k i among some set of ? with D i=1 ? 2 i ? R 2 ? satisfies the conditions of Theorem 6 with L k = K ? D.</formula><p>We finally establish our results for fully-connected deep kernels; it also applies to convolutional networks with a slightly different R ? (Remark 25). The constants in L k are given in Proposition 23. Proposition 9. Take k ? as in Section 5, with ? ? a fullyconnected network with depth ? and D total parameters, whose activations are 1-Lipschitz with ?(0) = 0 (e.g. ReLU). Suppose the operator norm of each weight matrix and L 2 norm of each bias vector are is at most R ? , and each</p><p>x ? X has x ? R X . Then k ? meets the conditions of Theorem 6 with dimension D and</p><formula xml:id="formula_19">L K = O ?R ??1 ? R X +1 ? ? .</formula><p>The dependence on s in Theorem 6 is somewhat unfortunate, but the ratio structure of J means that otherwise, errors in very small variances can hurt us arbitrarily. Even so, "near-perfect" kernels (with reasonably large MMD and very small variance) will likely still be chosen as the maximizer of the regularized criterion, even if we do not estimate the (extremely large) ratio accurately. Likewise, near-constant kernels (with very small variance but still small J) will generally have their J underestimated, and so are unlikely to be selected when a better kernel is available. The q component in (1) may also help avoid extremely small variances.</p><p>Given N data points, this result also gives insight into how many we should use to train the kernel and how many to test.  ? MMD-D: MMD with a deep kernel; our method described in Section 5.</p><p>? MMD-O: MMD with a Gaussian kernel whose lengthscale is optimized as in Section 5. This gives better results than standard heuristics.</p><p>? Mean embedding (ME): a state-of-the-art test <ref type="bibr" target="#b9">(Chwialkowski et al., 2015;</ref><ref type="bibr" target="#b23">Jitkrittum et al., 2016)</ref> based on differences in Gaussian kernel mean embeddings at a set of optimized points.</p><p>? Smooth characteristic functions (SCF): a state-of-theart test <ref type="bibr" target="#b9">(Chwialkowski et al., 2015;</ref><ref type="bibr" target="#b23">Jitkrittum et al., 2016</ref>) based on differences in Gaussian mean embeddings at a set of optimized frequencies.</p><p>? Classifier two-sample tests, including C2STS-S (Lopez-Paz &amp; Oquab, 2017) and C2ST-L (Chen &amp; Cloninger, 2019) as described in Section 4. We set the test thresholds via permutation for both.</p><p>For synthetic datasets, we take a single sample set for S tr P and S tr Q and learn a kernel/test locations/etc once for each method on that training set. We then evaluate its rejection rate on 100 new sample sets S te P , S te Q from the same distribution. For real datasets, we select a subset of the available data for S tr P and S tr Q and train on that; we then evaluate on 100 random subsets, disjoint from the training set, of the remaining data. We repeat this full process 10 times, and report the mean rejection rate of each test. <ref type="table">Table 5</ref> shows significance tests. Further details are in Appendix B.</p><p>Blob dataset. Blob-D is the dataset shown in <ref type="figure" target="#fig_0">Figure 1</ref>; Blob-S has Q also equal to the distribution shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, so that the null hypothesis holds. Details are given in <ref type="table" target="#tab_7">Table 6</ref> (Appendix B.1).</p><p>Results are shown in <ref type="figure" target="#fig_3">Figure 2</ref>. MMD-D and C2ST-L are the clear winners in power, with MMD-D better in the highersample regime, and MMD-D is more reliable than C2STs. <ref type="figure" target="#fig_3">Figure 2c</ref> shows that J is higher for MMD-D than MMD-O, in addition to the actual test power being better, as discussed in Section 3. All methods have expected Type I error rates.</p><p>High-dimensional Gaussian mixtures. Here we study bimodal Gaussian mixtures in increasing dimension. Each distribution has two Gaussian components; in HDGM-S, P and Q are the same, while in HDGM-D, P and Q differ in the covariance of a single dimension pair but are otherwise the same. Details are in <ref type="table" target="#tab_7">Table 6</ref> (Appendix B.1). We consider both increasing N while keeping d = 10 and increasing d while keeping N = 4 000, with results shown in <ref type="figure" target="#fig_5">Figure 3</ref>. Again, MMD-D has generally the best test power across a range of problem settings, with reasonable type I error.</p><p>Higgs dataset <ref type="bibr" target="#b2">(Baldi et al., 2014)</ref>. We compare the jet ?-momenta distribution (d = 4) of the background process, P, which lacks Higgs bosons, to the corresponding distribution Q for the process that produces Higgs bosons, following <ref type="bibr" target="#b9">Chwialkowski et al. (2015)</ref>. As discussed in these previous works, ?-momenta carry very little discriminating information for recognizing whether Higgs bosons were produced. We consider a series of tests with increased number of samples N .</p><p>We report average test power (comparing P to Q) in <ref type="table">Table 1</ref>, and average type-I error (comparing P to P or Q to Q) in <ref type="table">Table 7</ref> (Appendix B.6). As before, MMD-D generally performs the best; although the improvement over MMD-O here is not dramatic, MMD-D does notably outperform C2ST. All methods maintain reasonable Type I errors.</p><p>MNIST generative model. The MNIST dataset contains 70 000 handwritten digit images <ref type="bibr">(LeCun et al., 1998)</ref>. We compare true MNIST data samples P to samples Q from a pretrained deep convolutional generative adversarial network (DCGAN) <ref type="bibr">(Radford et al., 2016)</ref>. Samples from both distributions are shown in <ref type="figure" target="#fig_10">Figure 4</ref> (in Appendix B.2).</p><p>We consider tests for increasing numbers of samples N , and report average test power (for P to Q) in <ref type="table">Table 2</ref> and , so it is possible that current models themselves are dependent on P. CIFAR-10.1 (Q) is an attempt at an independent sample from this distribution, collected after the models were trained, so that they are truly independent of Q. These models do obtain substantially lower accuracies on Q than on P -but this drop is surprisingly consistent across models, which seems unlikely to be due to the expected overfitting. We train on 1 000 images from each dataset and test on 1 031, so that we use the entirety of CIFAR-10.1 each time, and average over ten repetitions. These tests provide strong  <ref type="table" target="#tab_2">(Table 3</ref>) that images in the CIFAR-10.1 test set are statistically different from the CIFAR-10 test set, with MMD-D again strongest and ME still performing well.</p><p>Our learned kernel also helps provide some ability to interpret the difference between P and Q, particularly if we use it for an ME test. Appendix C explores this.</p><p>Recht et al. (2019) also provide a new ImageNetV2 test set for the ImageNet dataset, with similar properties; we defer this more challenging problem to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Ablation Study</head><p>We now study in more detail the difference between MMD-D and closely related methods. Recall from Section 4 that there are two main differences between MMD-D and C2STs: first, using a "full" kernel (1) rather than the sign-based kernel (6) or the intermediate linear kernel <ref type="formula" target="#formula_11">(7)</ref>. Second, training to maximize? ? (4) rather than a cross-entry surrogate. MMD-D uses a full kernel (1) trained for test power; C2ST-S effectively uses the sign kernel (6) trained for cross entropy.</p><p>In this section, we consider the performance of several intermediate models empirically, demonstrating that both factors help in testing. All are based on the same feature extraction architecture ? ? ; some models add a classification layer with new parameters w and b,  <ref type="table">Table 5</ref>. Paired t-test results (? = 0.05) for the results of Section 7.1. For HDGM, we fix d = 10 (corresponding to <ref type="figure" target="#fig_5">Figure 3a</ref>). indicates MMD-D achieved statistically significantly higher mean test power than the other method, ? that it did not.</p><formula xml:id="formula_20">f ? (x) = w T ? ? (x) + b,</formula><formula xml:id="formula_21">Dataset ME SCF C2ST-S C2ST-L MMD-O Blob ? ? HDGM Higgs ? ? MNIST</formula><p>which is treated as outputting classification logits. The model variants we consider are</p><formula xml:id="formula_22">S A kernel 1(f ? (x) &gt; 0)1(f ? (y) &gt; 0); corresponds to a test statistic of the accuracy of f (Proposition 3).</formula><p>L A kernel f ? (x)f ? (y); corresponds to a test statistic comparing the mean value of f (Proposition 4).</p><formula xml:id="formula_23">G A Gaussian kernel ?(? ? (x), ? ? (y)). D The deep kernel (1) based on ? ? .</formula><p>We combine these model variants with a suffix describing the optimization objective:</p><p>J Choose ?, including possibly w and b, to optimize the approximate test power (4).</p><p>M Choose ?, including possibly w and b, to maximize the value of the empirical MMD between two samples. 5 C Choose ?, including w and b, to optimize cross-entropy using the classifier that specifies the probability of x belonging to P as 1/ (1 + exp(?f ? (x))). 6 <ref type="table" target="#tab_3">Table 4</ref> presents results for all of these methods (except for S+J, which is non-differentiable and hence difficult to optimize). Performance generally improves as we move from S to L to G to D, and from C to J.</p><p>5 If a deep kernel is unbounded, directly maximizing MMD will make optimized parameters of ?? be infinite. Thus, for L+M, we consider a normalized linear deep kernel: tanh(f?(x)/ S F)tanh(f? (y)/ S F), where S = [S P ; S Q ] and ? F is the Frobenius norm. 6 G+C and D+C take the fixed ?? embeddings, then find the optimal lengthscale/etc by optimizing? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Architecture design of deep kernels</head><p>For Blob, HDGM and Higgs, ? ? is a five-layer fullyconnected neural network, with softplus activations. the number of neurons in hidden and output layers of ? ? are set to 50 for Blob, 3d for HDGM and 20 for Higgs, where d is the dimension of samples. in general, we expect similar fully-connected networks, to be reasonable choices for datasets where strong structural assumptions are not known, perhaps with 3d as a baseline width for datasets of at least moderate dimension.</p><p>For MNIST and CIFAR, ? ? is a convolutional neural network (CNN) that contains four convolutional layers and one fully-connected layer. The structure of the CNN follows the structure of the feature extractor in the DCGAN's discriminator (Radford et al., 2016) (see <ref type="figure">Figures 6 and 8</ref> for the structure of ? ? in MMD-D, and <ref type="figure">Figures 7 and 9</ref> for the structure of classifier F in C2ST-S and C2ST-L). In general, we expect GAN discriminator architectures to work well for image datasets, as the problem is closely related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>The test power of MMD is limited by simple kernels (e.g., Gaussian kernel or other translation-invariant kernels) when facing complex-structured distributions, but we can avoid this problem with richer deep kernels, which is no longer translation-invariant. We show that optimizing the parameters of these kernels to maximize the test power, as proposed by <ref type="bibr">Sutherland et al. (2017)</ref>, outperforms state-of-the-art alternatives even when considering large, deep kernels with hundreds of thousands of parameters, rather than the simple shallow kernels they considered. We provide theoretical guarantees that this process is reasonable to conduct on finite samples, and asymptotically selects the most powerful kernel. We also give deeper insight into the relationship between this approach and classifier two-sample tests <ref type="bibr">(Lopez-Paz &amp; Oquab, 2017)</ref>, explaining why this approach outperforms that one.</p><p>We thus recommend practitioners to use optimized deep kernel methods when they wish to check if two distributions are the same, rather than indirectly training a classifier. </p><formula xml:id="formula_24">} n i=1 ? P n , {Y i } n i=1 ? Q n , define the n ? n matrix H (?) ij = k ? (X i , X j ) + k ? (Y i , Y j ) ? k ? (X i , Y j ) ? k ? (X j , Y i )</formula><p>; we will often omit ? when it is clear from context. The U -statistic estimator of the squared MMD (2) i?</p><formula xml:id="formula_25">? ? = 1 n(n ? 1) i =j H ij .</formula><p>The squared MMD is ? ? = E[H 12 ]. The variance of? ? is given by Lemma 10. Lemma 10. For a fixed kernel k ? and random sample sets</p><formula xml:id="formula_26">{X i } n i=1 , {Y i } n i=1 , we have Var[? ? ] = 4(n ? 2) n(n ? 1) ? (?) 1 + 2 n(n ? 1) ? (?) 2 = 4 n ? (?) 1 + 2? (?) 2 ? 4? (?) 1 n(n ? 1) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_27">? (?) 1 = E H (?) 12 H (?) 13 ? E H (?) 12 2 , ? (?) 2 = E H (?) 12 2 ? E H (?) 12 2 . Thus as n ? ?, n Var[? ? ] ? 4? (?) 1 =: ? 2 ? . Proof. Let U denote the pair (X, Y ), and h ? (U, U ) = k ? (X, X ) + k ? (Y, Y ) ? k ? (X, Y ) ? k ? (X , Y ), so that H (?) ij = h ? (U i , U j )</formula><p>. Via Lemma A in Section 5.2.1 of Serfling (1980), we know that (8) holds with</p><formula xml:id="formula_28">? (?) 1 = Var U [E U [h ? (U, U )]] = E U [E U [h ? (U, U )] E U [h ? (U, U )]] ? E U [E U [h ? (U, U )]] 2 = E[H (?) 12 H (?) 13 ] ? E[H (?) 12 ] 2 and ? 2 = Var U,U [h ? (U, U )] = E H (?) 12 2 ? E H (?) 12 2 .</formula><p>We use a V -statistic estimator (5) for ? 2 ? :</p><formula xml:id="formula_29">? 2 ? = 4 ? ? ? 1 n n i=1 ? ? 1 n n j=1 H (?) ij ? ? 2 ? ? ? 1 n 2 n i=1 n j=1 H (?) ij ? ? 2 ? ? ? .</formula><p>As a V -statistic,? 2 ? is biased. In fact, Sutherland et al. <ref type="formula" target="#formula_0">(2017)</ref> and Sutherland (2019) provide an unbiased estimator of Var[? ? ] -including the terms of order 1 n(n?1) . Although this estimator takes the same quadratic time to compute as (5), it contains many more terms, which are cumbersome both for implementation and for analysis. (5) is also marginally more convenient in that it is always at least nonnegative. As we show in Lemma 18, the amount of bias is negligible as n increases. In practice, we expect the difference to be unimportant -or the V -statistic may in fact be beneficial, since underestimating ? 2 harms the estimate of ?/? 2 more than overestimating it does.</p><p>Similarly, although we use the U -statistic estimator (2), it would be very similar to use the biased estimator n ?2 ij H ij , or the minimum variance unbiased estimator n ?1 (n ? 1) ?1</p><formula xml:id="formula_30">i =j (k(X i , X j ) + k(Y i , Y j )) ? 2n ?2 ij k(X i , Y J )</formula><p>. Showing comparable concentration behavior to Proposition 15 is trivially different, and in fact it is also not difficult to show ? 2 ? is the same for all three estimators (up to lower-order terms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Main results</head><p>We will require the following assumptions. These are fairly agnostic as to the kernel form; Appendix A.4.2 shows that these assumptions hold (and gives the constants) for the kernels (1) we use in the paper.</p><p>(A) The kernels k ? are uniformly bounded: sup</p><formula xml:id="formula_31">??? sup x?X k ? (x, x) ? ?.</formula><p>For the kernels we use in practice, ? = 1.</p><p>(B) The possible kernel parameters ? lie in a Banach space of dimension D. Furthermore, the set of possible kernel parameters ? is bounded by</p><formula xml:id="formula_32">R ? , ? ? {? | ? ? R ? }.</formula><p>Appendix A.4.2 builds this space and its norm for the kernels we use in the paper.</p><p>(C) The kernel parameterization is Lipschitz: for all x, y ? X and ?, ? ? ?,</p><formula xml:id="formula_33">|k ? (x, y) ? k ? (x, y)| ? L k ? ? ? .</formula><p>Proposition 23 in Appendix A.4.2 gives an expression for L k for the kernels we use in the paper.</p><p>We will first show the main results under these general assumptions, using uniform convergence results shown in Appendix A.3, then show Assumptions (B) and (C) for particular kernels in Appendix A.4.2.</p><p>Theorem 11. Under Assumptions (A) to (C), let? s ? ? be the set of kernel parameters for which ? 2 ? ? s 2 , and assume ? ? 1. Take ? = n ?1/3 . Then, with probability at least 1 ? ?,</p><formula xml:id="formula_34">sup ???s ? ? ? ?,? ? ? ? ? ? ? 2? s 2 n 1/3 1 s + 2304? 2 ? n + 4s n 1/6 + 1024? L k + 2 log 2 ? + 2D log 4R ? ? n ,</formula><p>and thus, treating ? as a constant,</p><formula xml:id="formula_35">sup ???s |? ? ? ?,? ? ? ? ? ? | =? P 1 s 2 n 1/3 1 s + L k + ? D .</formula><p>Proof. Let ? 2 ?,? := ? 2 ? + ?. Using |? ? | ? 4?, we begin by decomposing</p><formula xml:id="formula_36">sup ???s |? ? ? ?,? ? ? ? ? ? | ? sup ???s |? ? ? ?,? ?? ? ? ?,? | + sup ???s |? ? ? ?,? ?? ? ? ? | + sup ???s |? ? ? ? ? ? ? ? ? | = sup ???s |? ? | 1 ? ?,? 1 ? ?,? |? 2 ?,? ? ? 2 ?,? | ? ?,? + ? ?,? + sup ???s |? ? | 1 ? ?,? 1 ? ? |? 2 ?,? ? ? 2 ? | ? ?,? + ? ? + sup ???s 1 ? ? |? ? ? ? ? | ? sup ???s 4? ? ? s (s + ? ?) |? 2 ? ? ? 2 ? | + 4?? ? s 2 + ? s ? s 2 + ? + s + sup ???s 1 s |? ? ? ? ? | ? 4? s 2 ? ? sup ??? |? 2 ? ? ? 2 ? | + 2? s 3 ? + 1 s sup ??? |? ? ? ? ? |.</formula><p>Propositions 15 and 16 show uniform convergence of? ? and? 2 ? , respectively. Thus, with probability at least 1 ? ?, the error is at most</p><formula xml:id="formula_37">2? s 3 ? + 8? s ? n + 1792? ? ns 2 ? ? 2 log 2 ? + 2D log 4R ? ? n + 8 s ? n + 2048? 2 ? ns 2 ? ? L k + 4608? 3 s 2 n ? ? .</formula><p>Taking ? = n ?1/3 gives 2? s 3 n 1/3 + 8? s ? n + 1792? s 2 n 1/3 2 log 2 ? + 2D log 4R ? ? n + 8 s ? n + 2048? 2 s 2 n 1/3 L k + 4608? 3 s 2 n 5/6 .</p><p>Using 1 ? ?, 1792 &lt; 2048, we can get the slightly simpler upper bound</p><formula xml:id="formula_38">2? s 3 n 1/3 + 8? s ? n + 2048? 2 s 2 n 1/3 L k + 2 log 2 ? + 2D log 4R ? ? n + 4608? 3 s 2 n 5/6 .</formula><p>It is worth noting that, if we are particularly concerned about the s dependence, we can make some slightly different choices in the decomposition to improve the dependence on s while worsening the rate with n.</p><p>Corollary 12. In the setup of Theorem 11, additionally assume that there is a unique population maximizer ? * of J from (3), i.e. for each t &gt; 0 we have sup ???s: ??? * ?t J(P, Q; k ? ) &lt; J(P, Q; k ? * ).</p><p>For each n, let S Q ; k ? ), and tak? ? * n to be a maximizer of? n (?). 7 Then? * n converges in probability to ? * .</p><p>Proof. By Theorem 11, sup ???s |? n (?) ? J(?)| P ? 0. Then the result follows by Theorem 5.7 of <ref type="bibr">Van der Vaart (2000)</ref>.</p><p>Corollary 13. In the setup of Theorem 11, suppose we use n sample points to select a kernel? n ? arg max ???s?? (?) and m sample points to run a test of level ?. Let r (m) ?n denote the rejection threshold for a test with that kernel of size m. Define J * := sup ???s J(?), and constants C, C , C , N 0 depending on ?, L k , D, R ? and s. For any n ? N 0 , with probability at least 1 ? ?, this test procedure has power</p><formula xml:id="formula_39">Pr m?? n &gt; r (m) ?n ? ? ? mJ * ? C ? m n 1 3 log n ? ? C log 1 ? ? C ? m .</formula><p>Proof. Let? n ? arg max ???s?? (?). By Theorem 11, there are some N 0 , C depending on ?, L k , D, R ? , and s such that as long as n ? N 0 , with probability at least 1 ? ? it holds that</p><formula xml:id="formula_40">sup ???s |J ? (?) ? J(?)| ? 1 2 Cn ? 1 3 log n ? =: n .</formula><p>Assume for the remainder of this proof that this event holds. Letting ? * ? arg max J(?), we know because? n maximize? J ? that? ? (? n ) ?? ? (? * ). Using uniform convergence twice,</p><formula xml:id="formula_41">J(? n ) ?? ? (? n ) ? n ?? ? (? * ) ? n ? (J(? * ) ? n ) ? n = J * ? 2 n .</formula><p>Now, although Proposition 2 establishes that r (m) ? ? r ? and it is even known <ref type="bibr">(Korolyuk &amp; Borovskikh, 1988</ref>, Theorem 5) that |r</p><formula xml:id="formula_42">(m) ? ? r ? | is o(1/ ? m)</formula><p>, the constant in that convergence will depend on the choice of ? in an unknown way. It's thus simpler to use the very loose but uniform (McDiarmid-based) bound given by Corollary 11 of <ref type="bibr" target="#b18">Gretton et al. (2012a)</ref>, which implies r (m) ? ? 4? log(? ?1 )m no matter the choice of ?.</p><p>We will now need a more precise characterization of the power than that provided by the central limit theorem of Proposition 2. <ref type="bibr" target="#b6">Callaert &amp; Janssen (1978)</ref> provide such a result, a Berry-Esseen bound on U -statistic convergence: there is some absolute constant C BS = 2 3 4 3 C BS such that</p><formula xml:id="formula_43">sup t |Pr H1 ? m? ? ? ? ? ? 2 ? ? t ? ?(t)| ? C BS E|H 12 | 3 (? ? /2) 3 ? m ? C BS ? 3 ? 3 ? ? m .</formula><p>Letting r (m) ? be the appropriate rejection threshold for k ? with m samples, the power of a test with kernel k ? is</p><formula xml:id="formula_44">Pr m? ? &gt; r (m) ? = Pr ? m? ? ? ? ? ? ? &gt; r (m) ? ? m? ? ? ? m ? ? ? ? ? ? ? mJ(?) ? r (m) ? ? m? ? ? C BS ? 3 ? 3 ? ? m ? ? ? mJ(?) ? r (m) ? s ? m ? C ? m ,</formula><p>using a new constant C := C BS ? 3 /s 3 . Combining the previous results on J(? n ) and r (m) ?n yields the claim.</p><p>Corollary 14. In the setup of Corollary 13, suppose we are given N data points to divide between n training points and m = N ? n testing points, and ? &lt; 0.22 is fixed. Ignoring the Berry-Esseen convergence term outside of ?, the asymptotic power upper bound</p><formula xml:id="formula_45">? ? mJ * ? C ? m n 1 3 log n ? ? C log 1 ?</formula><p>is maximized only when, as other quantities remain constant, we pick n to satisfy</p><formula xml:id="formula_46">lim N ?? n C ? 3J * N ? log N 3 4 = 1.</formula><p>Proof. Because the C term is constant, we wish to choose</p><formula xml:id="formula_47">arg max 0&lt;n&lt;N J * C ? N ? n ? ? N ? n n 1 3 log n ? .</formula><p>Clearly neither endpoint is optimal. Relaxing n to be real-valued, the optimum must be achieved at a stationary point, where</p><formula xml:id="formula_48">?J * 2C ? N ? n + log n ? 2 ? N ? n n 1 3 + 1 3 ? N ? n n ? 4 3 log n ? ? 1 2 ? N ? n n ? 4 3 log n ? ? 1 2 = 0.</formula><p>Multiplying by 2 ? N ? n n 4 3 log n ? and rearranging, we get that a stationary point is achieved exactly when We will show that D ? E ? 0 requires A N ? C/( ? 3J * ), implying the result.</p><p>We first suppose A N = ?(1), further breaking into cases which result in different terms inside D and E becoming dominant: <ref type="figure">N N log N )</ref> .</p><formula xml:id="formula_49">If A N = ?(N ), D = ? A 3 4 N N 3 4 (log N ) 3 8 log A N , E = ? A N N log(N ) log(A N ) . If A N = ? N 1 3 ? log N , A N = o(N ), D = ? A 3 4 N N 3 4 (log N ) 3 8 log N , E = ? (A N N log N ) . If A N = ?(1), A N = o N 1 3 ? log N , D = ? (N log N ) , E = ? (A</formula><p>In each case, E = ?(D) and so D ? E ? ??, contradicting that D = E. Thus a stationary point requires A N = O(1) for a stationary point.</p><p>We now do the same for A N = o(1). First, clearly n ? 1; suppose that in fact n = ?(1), i.e. A N = ? 1/(N ? log N ) . In this case, we would have D = 2 3 N log n ? + ?(1) and E = N + ?(1), so that D = E requires 2 3 log n ? ? 1, i.e. n ? ? exp 3 2 ? 4.5 ?. For ? &lt; 0.22, this contradicts n ? 1. So we know that log n = ?(1). Now, the remaining options for A N all yield D ? E ? ?:</p><formula xml:id="formula_50">If A N = o(1), A N = ? 1 log N , D = ? (N log n) , E = ? (A N N log n) . If A N = o 1 log N , A N = ? 1 N ? log N , D = ? (N log n) , E = ? (N ) .</formula><p>Thus we have established that A N = ?(1). Thus, we obtain that</p><formula xml:id="formula_51">D = 1 2 N log N + O (N ) E = ? 3J * 2C A N N log N + O N log N .</formula><p>Asymptotic equality hence requires A N ? C/( ? 3J * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Uniform convergence results</head><p>These results, on the uniform convergence of? and? 2 , were used in the proof of Theorem 11.</p><p>Proposition 15. Under Assumptions (A) to (C), we have that with probability at least 1 ? ?,</p><formula xml:id="formula_52">sup ? |? ? ? ? ? | ? 8 ? n ? 2 log 2 ? + 2D log 4R ? ? n + L k .</formula><p>Proof. Theorem 7 of <ref type="bibr" target="#b17">Sriperumbudur et al. (2009)</ref> gives a similar bound in terms of Rademacher chaos complexity, but for ease of combination with our bound on convergence of the variance estimator, we use a simple -net argument instead.</p><p>We study the random error function</p><formula xml:id="formula_53">?(?) :=? ? ? ? ? .</formula><p>First, we place T points {? i } T i=1 such that for any point ? ? ?, min i ? ? ? i ? q; Assumption (B) ensures this is possible with at most T = (4R ? /q) D points <ref type="bibr">(Cucker &amp; Smale, 2001, Proposition 5)</ref>. Now, E ? = 0, because? is unbiased. Recall that? = 1 n(n?1) i =j H ij , and via Assumption (A) we know |H ij | ? 4?. This?, and hence ?, satisfies bounded differences: if we replace (X 1 , Y 1 ) with (X 1 , Y 1 ), obtaining? = 1 n(n?1) i =j F ij where F agrees with H except when i or j is 1, then</p><formula xml:id="formula_54">|? ?? | ? 1 n(n ? 1) i =j |H ij ? F ij | = 1 n(n ? 1) i&gt;1 |H i1 ? F i1 | + 1 n(n ? 1) j&gt;1 |H 1j ? F 1j | ? 2 n(n ? 1) i&gt;1 8? = 16? n .</formula><p>Using McDiarmid's inequality for each ?(? i ) and a union bound, we then obtain that with probability at least 1 ? ?,</p><formula xml:id="formula_55">max i?{1,...,T } |?(?)| ? 16? ? 2n log 2T ? ? 8? ? n 2 log 2 ? + 2D log 4R ? q .</formula><p>We also have via Assumption (C), for any two ?, ? ? ?,</p><formula xml:id="formula_56">|? ? ?? ? | ? 1 n(n ? 1) i =j |H (?) ij ? H (? ) ij | ? 1 n(n ? 1) i =j 4L k ? ? ? = 4L k ? ? ? |? ? ? ? ? | = |E H (?) 12 ? E H (? ) 12 | ? E|H (?) 12 ? H (? ) 12 | ? 4L k ? ? ? so that ? L ? 8L k .</formula><p>Combining these two results, we know that with probability at least 1 ? ?</p><formula xml:id="formula_57">sup ? |?(?)| ? max i?{1,...,T } |?(? i )| + 8L k q ? 8? ? n 2 log 2 ? + 2D log 4R ? q + 8L k q;</formula><p>setting q = 1/ ? n yields the desired result.</p><p>Proposition 16. Under Assumptions (A) to (C), with probability at least 1 ? ?,</p><formula xml:id="formula_58">sup ??? ? 2 ? ? ? 2 ? ? 64 ? n 7 2 log 2 ? + 2D log 4R ? ? n + 18? 2 ? n + 8L k ? .</formula><p>Proof. We again use an -net argument on the (random) error function</p><formula xml:id="formula_59">?(?) :=? 2 k? ? ? 2 k? .</formula><p>First, choose T points {? i } T i=1 such that for any point ? ? ?, min i ? ?? i ? q; again, via Assumption (B) and Proposition 5 of <ref type="bibr" target="#b10">Cucker &amp; Smale (2001)</ref> we have T ? (4R ? /q) D . By Lemmas 17 and 18 and a union bound, with probability at least</p><formula xml:id="formula_60">1 ? ?, max i?{1,...,T } |?(?)| ? 448 2 n log 2T ? + 1152? 2 n ? 448 2 n log 2 ? + 2 n D log 4R ? q + 1152? 2 n .</formula><p>Lemma 19 shows that ? L ? 512L k ?, which means that with probability at least 1 ? ?,</p><formula xml:id="formula_61">sup ??? |?(?)| ? 448 2 n log 2 ? + 2 n D log 4R ? q + 1152? 2 n + 512L k ?q.<label>(9)</label></formula><p>Taking q = 1/ ? n gives the desired result.</p><p>Lemma 17. For any kernel k bounded by ? (Assumption (A)), with probability at least 1 ? ?,</p><formula xml:id="formula_62">|? 2 k ? E? 2 k | ? 448 2 n log 2 ? .</formula><p>Proof. We simply apply McDiarmid's inequality to? 2 k . Suppose we change (X 1 , Y 1 ) to (X 1 , Y 1 ), giving a new H matrix F which agrees with H on all but the first row and column. Note that |H ij | ? 4?, and recall</p><formula xml:id="formula_63">? 2 k = 4 ? ? ? 1 n 3 i ? ? j H ij ? ? 2 ? ? ? 1 n 2 ij H ij ? ? 2 ? ? ? .</formula><p>The first term in the parentheses of? 2 k changes by</p><formula xml:id="formula_64">| 1 n 3 i ? ? j H ij ? ? 2 ? 1 n 3 i ? ? j F ij ? ? 2 | ? 1 n 3 ij |H ij H i ? F ij F i |.</formula><p>In this sum, if none of i, j, or are one, the term is zero. The n 2 terms for which i = 1 are each upper-bounded by 32? 2 , simply bounding each H or F by 4?. Of the remainder, there are (n ? 1) terms where j = = 1, each |H 2 i1 ? F 2 i1 | ? 16? 2 . We are left with 2(n ? 1) 2 terms which have exactly one of j or equal to 1; the j = 1 terms are</p><formula xml:id="formula_65">|H i1 H i ? F i1 H i | ? |H i1 ? F i1 ||H i | ? (8?)(4?)</formula><p>, so each of these terms is at most 32? 2 . The total sum is thus at most 1 n 3 n 2 32? 2 + (n ? 1)16? 2 + 2(n ? 1) 2 32? 2 = 6 n ? 7 n 2 + 3 n 3 16? 2 .</p><p>The remainder of the change in? 2 k can be determined by bounding</p><formula xml:id="formula_66">| ij H ij ? ij F ij | ? ij |H ij ? F ij | = j |H 1j ? F 1j | + i&gt;1 |H i1 ? F i1 | ? n(8?) + (n ? 1)(8?) = (8?)(2n ? 1),</formula><p>which then gives us</p><formula xml:id="formula_67">| ? ? 1 n 2 ij H ij ? ? 2 ? ? ? 1 n 2 ij F ij ? ? 2 | = | 1 n 2 ij H ij + 1 n 2 ij F ij || 1 n 2 ij H ij ? 1 n 2 ij F ij | ? (2 ? 4?) 2n ? 1 n 2 (8?) = 64? 2 2 n ? 1 n 2 .</formula><p>Thus</p><formula xml:id="formula_68">|? 2 k ? (? k ) 2 | ? 4 6 n ? 7 n 2 + 3 n 3 16? 2 + 2 n ? 1 n 2 64? 2 = 64? 2 n 3 14n 2 ? 11n + 3 ? 896? 2 n .</formula><p>Because the same holds for changing any of the (X i , Y i ) pairs, the result follows by McDiarmid's inequality.</p><p>Lemma 18. For any kernel k bounded by ? (Assumption (A)), the estimator? 2 k satisfies</p><formula xml:id="formula_69">|E? 2 k ? ? 2 k | ? 1152? 2 n .</formula><p>Proof. We have that</p><formula xml:id="formula_70">E? 2 k = 4 ? ? 1 n 3 ij E [H i H j ] ? 1 n 4 ijab E [H ij H ab ] ? ? .</formula><p>Most terms in these sums have their indices distinct; these are the ones that we care about. (We could evaluate the expectations of the other terms exactly, but it would be tedious.) We can thus break down the first term as </p><formula xml:id="formula_71">n(n ? 1)(n ? 2) n 3 = 1 ? 3 n + 2 n 2 we obtain | 1 n 3 ij E[H i H j ] ? E[H 12 H 13 ]| = 3 n ? 2 n 2 |? E[H 12 H 13 ] + q| ? 3 n ? 2 n 2 32? 2 .<label>(10)</label></formula><p>The second term can be handled similarly: </p><formula xml:id="formula_72">E[H ij H ab ] = n(n ? 1)(n ? 2)(n ? 3) n 4 E[H ij H ab ] + 1 ? n(n ? 1)(n ? 2)(n ? 3) n 4 r,</formula><p>where r is the appropriately-weighted mean of the non-distinct terms, |r| ? 16? 2 . For i, j, a, b all distinct,</p><formula xml:id="formula_73">E[H ij H ab ] = E[H 12 ] 2 . Here n(n ? 1)(n ? 2)(n ? 3) n 4 = (n ? 1)(n 2 ? 5n + 6) n 3 = 1 ? 6 n + 11 n ? 6 n 3 and so | 1 n 4 ijab E[H ij H ab ] ? E[H 12 ] 2 | ? 6 n ? 11 n 2 + 6 n 3 32? 2 .<label>(11)</label></formula><p>Recalling</p><formula xml:id="formula_74">? 2 k = 4(E[H 12 H 13 ] ? E[H 12 ] 2 ), |E? 2 k ? ? 2 k | ? 128? 2 9 n ? 13 n 2 + 6 n 3 ,</formula><p>and since n ? 1, we have 13/n 2 &gt; 6/n 3 , yielding the result.</p><p>Lemma 19. Under Assumptions (A) and (C), we have</p><formula xml:id="formula_75">sup ?,? ?? |? 2 ? ?? 2 ? | ? ? ? ? 256L k ? and sup ?,? ?? |? 2 ? ? ? 2 ? | ? ? ? ? 256L k ?.</formula><p>Proof. We first handle the change in? k :</p><formula xml:id="formula_76">|? 2 k? ?? 2 k ? | = 4| 1 n 3 ij H (?) i H (?) j ? 1 n 3 ij H (? ) i H (? ) j ? 1 n 4 ijab H (?) ij H (?) ab + 1 n 4 ijab H (? ) ij H (? ) ab | ? 4 n 3 ij |H (?) i H (?) j ? H (? ) i H (? ) j | + 4 n 4 ijab |H (?) ij H (?) ab ? H (? ) ij H (? ) ab |.</formula><p>We can handle both terms by bounding</p><formula xml:id="formula_77">|H (?) ij H (?) ab ? H (? ) ij H (? ) ab | ? |H (?) ij H (?) ab ? H (?) ij H (? ) ab | + |H (?) ij H (? ) ab ? H (? ) ij H (? ) ab | = |H (?) ij ||H (?) ab ? H (? ) ab | + |H (?) ij ? H (? ) ij ||H (? ) ab | ? 4? |H (?) ab ? H (? ) ab | + |H (?) ij ? H (? ) ij | .</formula><p>Using Assumption (C) and the definition of H,</p><formula xml:id="formula_78">|H (?) ij ? H (? ) ij | ? 4L k ? ? ? so |H (?) ij H (?) ab ? H (? ) ij H (? ) ab | ? 32?L k ? ? ?<label>(12)</label></formula><p>and hence</p><formula xml:id="formula_79">|? 2 ? ?? 2 ? | ? 256?L k ? ? ? .</formula><p>Again using (12), we also have</p><formula xml:id="formula_80">|? 2 ? ? ? 2 ? | ? 4|E H (?) 12 H (?) 13 ? E H (? ) 12 H (? ) 13 | + 4|E H (?) 12 2 ? E H (? ) 12 2 | ? 4 E|H (?) 12 H (?) 13 ? H (? ) 12 H (? ) 13 | + 4 E|H (?) 12 H (?) 34 ? H (? ) 12 H (? ) 34 | ? 256?L k ? ? ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Constructing appropriate kernels</head><p>We now show Propositions 7 to 9, which each state that Assumption (C) is satisfied by various choices of kernel. The following assumption will be useful for different kernel schemes.</p><p>(I) The domain X is Euclidean and bounded, X ? x ? R d : x ? R X for some constant R X &lt; ?.</p><p>We begin by recalling a well-known property of the Gaussian kernel, useful for both Gaussian bandwidth selection and deep kernels. A proof is in Appendix A.5.</p><formula xml:id="formula_81">Lemma 20. The Gaussian kernel ?(a, b) = exp ? a?b 2 2? 2 satisfies |?(a, b) ? ?(a , b )| ? 1 ? ? e ( a ? b + a ? b ) ? 1 ? ? e ( a ? a + b ? b ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1. GAUSSIAN BANDWIDTH SELECTION (PROPOSITION 7)</head><p>Lemma 20 immediately gives us Assumption (C) when we chose among Gaussian kernels: Proposition 21. Define a one-dimensional Banach space for inverse lengthscales of Gaussian kernels ? &gt; 0, so that k ? (x, y) = ? 1/? (x, y), with standard addition and multiplication and norms defined by the absolute value, and k 0 taken to be the constant 1 function. Let ? be any subset of this space. Under Assumption (I), Assumption (C) holds: for any x, y ? X and ?, ? ? ?,</p><formula xml:id="formula_82">|k ? (x, y) ? k ? (x, y)| ? 2R X ? e |? ? ? |. Proof. |k ? (x, y) ? k ? (x, y)| = |? 1 (?x, ?y) ? ? 1 (? x, ? y)| ? 1 ? e |? x ? y ? ? x ? y | = x ? y ? e |? ? ? |.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2. DEEP KERNELS (PROPOSITION 9)</head><p>To handle the deep kernel case, we will need some more assumptions on the form of the kernel.</p><formula xml:id="formula_83">(II) ? ? (x) = ? (?)</formula><p>? is a feedforward neural network with ? layers given by</p><formula xml:id="formula_84">? (0) ? (x) = x ? ( ) ? (x) = ? ( ) W ( ) ? ? ( ?1) ? (x) + b ( ) ? ,</formula><p>where the network parameter ? consists of all the weight matrices W  <ref type="formula" target="#formula_0">(1)</ref>,</p><formula xml:id="formula_85">k ? (x, y) = [(1 ? )?(? ? (x), ? ? (y)) + ] q(x, y),</formula><p>with 0 ? ? 1, ? a kernel function, and q(x, y) a kernel with sup x q(x, x) ? Q.</p><p>Note that this includes kernels of the form k ? (x, y) = ?(? ? (x), ? ? (y)): take = 0 and q(x, y) = 1.</p><p>(IV) ? in Assumption (III) is a kernel function satisfying</p><formula xml:id="formula_86">|?(a, b) ? ?(a , b )| ? L ? ( a ? a + b ? b ) .</formula><p>This holds for a Gaussian ? via Lemma 20.</p><p>We now turn to proving Assumption (C) for deep kernels. First, we will need some smoothness properties of the network ?. Lemma 22. Under Assumption (II), suppose ?, ? have ? ? R, ? ? R, with R = 1. Then, for any x,</p><formula xml:id="formula_87">? ? (x) ? R ? x + R R ? 1 (R ? ? 1) (13) ? ? (x) ? ? ? (x) ? ?R ??1 x + R R ? 1 ? R ? ? 1 (R ? 1) 2 ? ? ? .<label>(14)</label></formula><p>If R ? 2, we furthermore have</p><formula xml:id="formula_88">? ? (x) ? R ? ( x + 2) (15) ? ? (x) ? ? ? (x) ? ?R ??1 ( x + 2) ? ? ? .<label>(16)</label></formula><p>The proof, by recursion, is given in Appendix A.5. We are now ready to prove Assumption (C) for deep kernels. Proposition 23. Make Assumptions (I) to (IV) and Assumption (B), with R ? ? 2. 8 Then Assumption (C) holds: for any x, y ? X and ?, ? ? ?,</p><formula xml:id="formula_89">|k ? (x, y) ? k ? (x, y)| ? 2Q(1 ? )L ? ?R ??1 ? (R X + 2) ? ? ? .</formula><p>Proof.</p><formula xml:id="formula_90">|k ? (x, y) ? k ? (x, y)| = (1 ? )|?(? ? (x), ? ? (y)) ? ?(? ? (x), ? ? (y))|q(x, y) ? Q(1 ? )L ? (|? ? (x) ? ? ? (x)| + |? ? (y) ? ? ? (y)|) ? Q(1 ? )L ? ?R ??1 ? ( x + y + 4) ? ? ? ? Q(1 ? )L ? ?R ??1 ? (2R X + 4) ? ? ? .</formula><p>Remark 24. For the deep kernels we use in the paper (Assumptions (II) to (IV)) on bounded domains (Assumption (I)), we know L k via Proposition 23; Theorem 6 combines Theorem 11, Corollary 12, and Proposition 23. If we further use a Gaussian kernel q of bandwidth ? ? , the last bracketed term in the error bound of Theorem 11 becomes <ref type="formula" target="#formula_0">(15)</ref>, is approximately the largest that ? ? could make its outputs' norms; ? ? will generally be on a comparable scale to the norm of the actual outputs of the network, so their ratio is something like the "unused capacity" of the network to blow up its inputs. This term is weighted about equally in the convergence bound with the square root of the total number of parameters in the network.</p><formula xml:id="formula_91">2(1 ? ) ? ? ? e ?R ??1 ? (R X + 2) + 2 log 2 ? + 2D log 4R ? ? n . The component R ??1 ? (R X + 2), from</formula><p>Remark 25. We can handle convolutional networks as follows. We define ? in essentially the same way, letting W ( ) ? denote the convolutional kernel (the set of parameters being optimized), but define ? in terms of the operator norm of the linear transform corresponding to the convolution operator. This is given in terms of the operator norm of various discrete Fourier transforms of the kernel matrix by Lemma 2 of <ref type="bibr" target="#b4">Bibi et al. (2019)</ref>; see also <ref type="bibr">Theorem 6 of Sedghi et al. (2019)</ref>. The number of parameters D is then the actual number of parameters optimized in gradient descent, but the radius R ? is computed differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3. MULTIPLE KERNEL LEARNING (PROPOSITION 8)</head><p>Multiple kernel learning <ref type="bibr" target="#b15">(G?nen &amp; Alpayd?n, 2011)</ref> also falls into our setting. A special case of this family of kernels was studied for the (easier to analyze) "streaming" MMD estimator by <ref type="bibr" target="#b19">Gretton et al. (2012b)</ref>.</p><formula xml:id="formula_92">(V) Let {k i } D i=1</formula><p>be a set of base kernels, each satisfying sup x?X k i (x, x) ? K for some finite K. Define k ? as</p><formula xml:id="formula_93">k ? (x, y) = D i=1 ? i k i (x, y).</formula><p>Define the norm of a kernel parameter by the norm of the corresponding vector ? ? R D . Let ? be a set of possible parameters such that for each ? ? ?, k ? is positive semi-definite, and ? ? R ? for some R ? &lt; ?.</p><p>Not only does learning in this setting work (Proposition 26), it is also -unlike the deep setting -efficient to find an exact maximizer of? ? (Proposition 27). Proposition 26. Assumption (V) implies Assumptions (A) to (C). In particular,</p><formula xml:id="formula_94">sup ??? sup x?X k ? (x, x) ? KR ? ? D |k ? (x, y) ? k ? (x, y)| ? K ? D ? ? ? . Proof. Assumption (B) is immediate from Assumption (V), since ? ? R D . Let k(x, y) ? R D denote the vector whose ith entry is k i (x, y), so that k ? (x, y) = ? T k(x, y). As k(x, y) ? ? K, we know k(x, y) ? K ? D.</formula><p>Assumptions (A) and (C) follow by Cauchy-Schwartz.</p><p>Proposition 27. Take Assumption (V), and additionally assume that ? = {? | ?i. ? i ? 0, i ? i = Q} for some Q &lt; ?. A maximizer of? ? (?) can then be found by scaling the solution to a convex quadratic program,</p><formula xml:id="formula_95">? = arg min ??[0,?) D : ? T b=1 ? T (A + ?I)?,? = Q i? i? ? arg max ???? ? (?), where (H ij ) = k (X i , X j ) + k (Y i , Y j ) ? k (X i , Y j ) ? k (X j , Y i ) b = 1 n(n ? 1) i =j H ij ? R D A = 4 n 3 i ? ? j H ij ? ? ? ? j H ij ? ? T ? 4 n 4 ? ? ij H ij ? ? ? ? ij H ij ? ? T ? R D?D ,</formula><p>as long as b has at least one positive entry.</p><p>Proof. The H matrix used by? ? and? ? takes a simple form:</p><formula xml:id="formula_96">H (?) ij = k ? (X i , X j ) + k ? (Y i , Y j ) ? k ? (X i , Y j ) ? k ? (X j , Y i ) = ? T H ij . Thus? ? = ? T ? ? 1 n(n ? 1) i =j H ij ? ? = ? T b ? 2 ? = 4 n 3 i ? ? ? T j H ij ? ? 2 ? 4 n 4 ? ? ? T ij H ij ? ? 2 = ? T ? ? ? 4 n 3 i ? ? j H ij ? ? ? ? j H ij ? ? T ? 4 n 4 ? ? ij H ij ? ? ? ? ij H ij ? ? T ? ? ? ? = ? T A?.</formula><p>Note that because? 2 ? ? 0 for any ?, we have A 0. We have now obtained a problem equivalent to the one in Section 4 of <ref type="bibr" target="#b19">Gretton et al. (2012b)</ref>; the argument proceeds as there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Miscellaneous Proofs</head><p>The following lemma was used for Propositions 21 and 23.</p><p>Lemma 20. <ref type="figure">The Gaussian kernel ?(a, b)</ref> </p><formula xml:id="formula_97">= exp ? a?b 2 2? 2 satisfies |?(a, b) ? ?(a , b )| ? 1 ? ? e ( a ? b + a ? b ) ? 1 ? ? e ( a ? a + b ? b ) .</formula><p>Proof. We have that</p><formula xml:id="formula_98">|?(a, b) ? ?(a , b )| = |exp ? a ? b 2 2? 2 ? exp ? a ? b 2 2? 2 | ? x ? exp ? x 2 2? 2 L | a ? b ? a ? b |.</formula><p>We can bound the Lipschitz constant as its maximal derivative norm,</p><formula xml:id="formula_99">sup x |x| ? 2 exp ? x 2 2? 2 . Noting that d dx log |x| ? 2 exp ? x 2 2? 2 = 1 x ? x ? 2</formula><p>vanishes only at x = ??, the supremum is achieved by using that value, giving</p><formula xml:id="formula_100">x ? exp ? x 2 2? 2 L = 1 ? ? e .</formula><p>The result follows from</p><formula xml:id="formula_101">| a ? b ? a ? b | ? a ? b ? a + b ? a ? a + b ? b .</formula><p>This next lemma was used in Proposition 23. Lemma 22. Under Assumption (II), suppose ?, ? have ? ? R, ? ? R, with R = 1. Then, for any x,</p><formula xml:id="formula_102">? ? (x) ? R ? x + R R ? 1 (R ? ? 1) (13) ? ? (x) ? ? ? (x) ? ?R ??1 x + R R ? 1 ? R ? ? 1 (R ? 1) 2 ? ? ? .<label>(14)</label></formula><p>If R ? 2, we furthermore have</p><formula xml:id="formula_103">? ? (x) ? R ? ( x + 2) (15) ? ? (x) ? ? ? (x) ? ?R ??1 ( x + 2) ? ? ? .<label>(16)</label></formula><p>Proof. First, ?</p><p>? (x) = x , showing (13) when ? = 0. In general,</p><formula xml:id="formula_105">? ( ) ? (x) = ? ( ) W ( ) ? ? ( ?1) ? (x) + b ( ) ? ? W ( ) ? ? ( ?1) ? (x) + b ( ) ? ? W ( ) ? ? ( ?1) ? (x) + b ( ) ? ? R ? ( ?1) ? (x) + R,</formula><p>and expanding this recursion gives</p><formula xml:id="formula_106">? ( ) ? (x) ? R x + m=1 R m = R x + R R ? 1 (R ? 1). Now, we have (14) for ? = 0 because ? (0) ? (x) ? ? (0) ? (x) = 0. For ? 1, we have ? ( ) ? (x) ? ? ( ) ? (x) = ? ( ) W ( ) ? ? ( ?1) ? (x) + b ( ) ? ? ? ( ) W ( ) ? ? ( ?1) ? (x) ? b ( ) ? ? W ( ) ? ? ( ?1) ? (x) ? W ( ) ? ? ( ?1) ? (x) + W ( ) ? ? ( ?1) ? (x) ? W ( ) ? ? ( ?1) ? (x) + b ( ) ? ? b ( ) ? ? W ( ) ? ? W ( ) ? ? ( ?1) ? (x) + W ( ) ? ? ( ?1) ? (x) ? ? ( ?1) ? (x) + ? ? ? ? ? ? ? R ?1 x + R R ? 1 (R ?1 ? 1) + 1 + R ? ( ?1) ? (x) ? ? ( ?1) ? (x) .</formula><p>Expanding the recursion yields</p><formula xml:id="formula_107">? ( ) ? (x) ? ? ( ) ? (x) ? ?1 m=0 R m R ?1?m x + R R ? 1 (R ?m?1 ? 1) + 1 ? ? ? = ?1 m=0 R ?1 x + R R ? 1 ? R m+1 R ? 1 + R m ? ? ? = R ?1 x + R R ? 1 ? R R ? 1 ? 1 ?1 m=0 R m ? ? ? = R ?1 x + R R ? 1 ? 1 R ? 1 R ? 1 R ? 1 ? ? ? .</formula><p>When R ? 2, we have that R/(R ? 1) ? 2 and R &gt; 1, giving (15) and (16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head><p>B.1. Details of synthetic datasets <ref type="table" target="#tab_7">Table 6</ref> shows details of four synthetic datasets. Blob datasets are often used to validate two-sample test methods <ref type="bibr" target="#b19">(Gretton et al., 2012b;</ref><ref type="bibr" target="#b23">Jitkrittum et al., 2016;</ref><ref type="bibr">Sutherland et al., 2017)</ref>, although we rotate each blob to show the benefits of nonhomogeneous kernels. HDGM datasets are first proposed in this paper. HDGM-D can be regarded as high-dimension Blob-D which contains two modes with the same variance and different covariance.  <ref type="figure" target="#fig_0">Figure 1a</ref>). ? h 1 and ? h 2 are set to 0.5 and ?0.5, respectively. <ref type="figure" target="#fig_10">Figure 4</ref> shows images from real-MNIST and "fake"-MNIST, while <ref type="figure">Figure 5</ref> shows samples from CIFAR-10 and CIFAR-10.1.</p><formula xml:id="formula_108">= [0, 0], ? b 2 = [0, 1], ? b 3 = [0, 2], . . . , ? b 8 = [2, 1], ? b 9 = [2, 2] (same with Figure 1a). ? h 1 = 0 d , ? h 2 = 0.5 ? 1 d , I d is an identity matrix with size d. ? b i = ?0.02 ? 0.002 ? (i ? 1) if i &lt; 5 and ? b i = 0.02 + 0.002 ? (i ? 6) if i &gt; 5. if i = 5, ? b i = 0 (same with</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Dataset visualization</head><p>(a) Real-MNIST (b) "Fake"-MNIST </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Configurations</head><p>We implement all methods on Python 3.7 (Pytorch 1.1) with a NIVIDIA Titan V GPU. We run ME and SCF using the official code <ref type="bibr" target="#b23">(Jitkrittum et al., 2016)</ref>, and implement C2ST-S, C2ST-L, MMD-D and MMD-O by ourselves. We use permutation test to compute p-values of C2ST-S and C2ST-L, MMD-D, MMD-O and tests in <ref type="table" target="#tab_3">Table 4</ref>. We set ? = 0.05 for all experiments. Following Lopez-Paz &amp; Oquab (2017), we use a deep neural network F as the classifier in C2ST-S and C2ST-L, and train the F by minimizing cross entropy. To fairly compare MMD-D with C2ST-S and C2ST-L, the network ? ? in MMD-D has the same architecture with feature extractor in F . Namely, F = g ? ? ? , where g is a two-layer fully-connected network. The network g is a simple binary classifier that takes extracted features (through ? ? ) as input. For test methods shown in <ref type="table" target="#tab_3">Table 4</ref>, the network ? ? in them also has the same architecture with that in MMD-D.</p><p>For Blob, HDGM and Higgs, ? ? is a five-layer fully-connected neural network. The number of neurons in hidden and output layers of ? ? are set to 50 for Blob, 3 ? d for HDGM and 20 for Higgs, where d is the dimension of samples. These neurons are with softplus activation function, i.e., log(1 + exp(x)). For MNIST and CIFAR, ? ? is a convolutional neural network (CNN) that contains four convolutional layers and one fully-connected layer. The structure of the CNN follows the structure of the feature extractor in the discriminator of DCGAN (Radford et al., 2016) (see <ref type="table" target="#tab_7">Figures 6</ref> and 8 for the structure of ? ? in MMD-D, and <ref type="figure">Figures 7 and 9</ref> for the structure of classifier F in C2ST-S and C2ST-L). The link of DCGAN code is https://github.com/eriklindernoren/PyTorch-GAN/blob/master/ implementations/dcgan/dcgan.py.</p><p>We use Adam optimizer <ref type="bibr" target="#b26">(Kingma &amp; Ba, 2015)</ref> to optimize 1) parameters of F in C2ST-S and C2ST-L, 2) parameters of ? ? in MMD-D and 3) kernel lengthscale in MMD-O. We set drop-out rate to zero when training C2ST-S, C2ST-L and MMD-D on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Detailed parameters of all test methods</head><p>In this subsection, we demonstrate detailed parameters of all test methods. Except for learning rate of Adam optimizer, we use default parameters of Adam optimizer provided by Pytorch. We use one validation set (with the same size of training set) to roughly search these parameters. Using these parameters, we compute test power of each test method on 100 test sets (with the same size of training set).</p><p>For ME and SCF, we follow <ref type="bibr" target="#b9">Chwialkowski et al. (2015)</ref> and set J = 10 for Higgs. For other datasets, we set J = 5.  <ref type="figure">Figure 6</ref>. The structure of ?? in MMD-D on MNIST. The kernel size of each convolutional layer is 3; stride (S) is set to 2; padding (P) is set to 1. We do not use dropout. Best viewed zoomed in.  <ref type="figure">Figure 7</ref>. The structure of classifier F in C2ST-S and C2ST-L on MNIST. The kernel size of each convolutional layer is 3; stride (S) is set to 2; padding (P) is set to 1. We do not use dropout. In the first layer, we will convert the CIFAR images from 32 ? 32 ? 3 to 64 ? 64 ? 3. Best viewed zoomed in.  <ref type="figure">Figure 8</ref>. The structure of ?? in MMD-D on CIFAR. The kernel size of each convolutional layer is 3; stride (S) is set to 2; padding (P) is set to 1. We do not use dropout in all layers. In the first layer, we will convert the CIFAR images from 32 ? 32 ? 3 to 64 ? 64 ? 3. Best viewed zoomed in.  <ref type="figure">Figure 9</ref>. The structure of classifier F in C2ST-S and C2ST-L on CIFAR. The kernel size of each convolutional layer is 3; stride (S) is set to 2; padding (P) is set to 1. We do not use dropout. Best viewed zoomed in.</p><p>For C2ST-S and C2ST-L, we set batchsize to min{2 ? n b , 128} for Blob, 128 for HDGM and Higgs, and 100 for MNIST and CIFAR. We set the number of epochs to 500 ? 18 ? n b /batchsize for Blob, 1, 000 for HDGM, Higgs and CIFAR, and 2, 000 for MNIST. We set learning rate to 0.001 for Blob, HDGM and Higgs, and 0.0002 for MNIST and CIFAR (following Radford et al. <ref type="formula" target="#formula_0">(2016)</ref>).</p><p>For MMD-O, we use full batch (i.e., all samples) to train MMD-O. we set the number of epochs to 1, 000 for Blob, HDGM, Higgs and CIFAR, and 2, 000 for MNIST. We set learning rate to 0.0005 for Blob, MNIST and CIFAR, and 0.001 for HDGM. <ref type="table">Table 7</ref>. Results on Higgs (? = 0.05). We report average Type I error on Higgs dataset when increasing number of samples (N ). Note that, in Higgs, we have two types of Type I errors: 1) Type I error when two samples drawn from P (no Higgs bosons) and 2) Type I error when two samples drawn from Q (having Higgs bosons). Type I reported here is the average value of 1) and 2). Since Type I error reported here is the average value of two average Type I errors, we do not report standard errors of the average Type I error in this  <ref type="table">Table 8</ref>. Results on MNIST given ? = 0.05. We report average Type I error?standard errors on real-MNIST vs. real-MNIST when increasing number of samples (N ). N ME SCF C2ST-S C2ST-L MMD-O MMD-D 200 0.076?0.011 0.075?0.010 0.035?0.006 0.045?0.005 0.068?0.004 0.056?0.003 400 0.062?0.010 0.056?0.007 0.044?0.006 0.040?0.004 0.053?0.005 0.056?0.005 600 0.051?0.003 0.049?0.009 0.039?0.005 0.054?0.007 0.066?0.008 0.056?0.008 800 0.054?0.006 0.046?0.006 0.043?0.005 0.042?0.007 0.051?0.005 0.054?0.007 1000 0.047?0.006 0.045?0.010 0.038?0.006 0.046?0.005 0.041?0.007 0.062?0.006</p><p>Avg.</p><p>0.058 0.054 0.040 0.045 0.056 0.057</p><p>For MMD-D, we use full batch (i.e., all samples) to train MMD-D with samples from Blob, HDGM and Higgs. We use mini-batch (batchsize is 100) to train MMD-D with samples from MNIST and CIFAR. We set the number of epochs to 1, 000 for Blob, HDGM, Higgs and CIFAR, and 2, 000 for MNIST. We set learning rate to 0.0005 for Blob and Higgs, 10 ?5 for HDGM, 0.001 for MNIST and 0.0002 for and CIFAR (following Radford et al. <ref type="formula" target="#formula_0">(2016)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Links to datasets</head><p>Higgs dataset can be downloaded from UCI Machine Learning Repository. The link is https://archive.ics.uci. edu/ml/datasets/HIGGS.  <ref type="table">Table 7</ref> shows average Type I error on Higgs dataset when increasing number of samples (N ). <ref type="table">Table 8</ref> shows average Type I error on real-MNIST vs. real-MNIST when increasing number of samples (N ).</p><p>C. Interpretability on CIFAR-10 vs CIFAR-10.1</p><p>In Section 7.1, we have shown that images in CIFAR-10 and CIFAR-10.1 are not from the same distribution. Thus, it is interesting to try to understand the major difference between the datasets. Mean Embedding tests <ref type="bibr" target="#b9">(Chwialkowski et al., 2015)</ref> compare the mean embeddings ? P and ? Q at test locations v 1 , . . . , v L , rather than through their overall norm. The test statistic i? ? = nz T n S ?1z n , z i = (k(x i , v j ) ? k(y i , v j )) L j=1 ? R L ,z n = 1 n n i=1 z i , S n = 1 n ? 1 n i=1 (z i ?z n )(z i ?z n ) T ; the asymptotic null distribution of? is ? 2 L , and the estimator is computable in linear time rather than MMD U 's quadratic time. <ref type="bibr" target="#b24">Jitkrittum et al. (2017)</ref> jointly learn the parameters v j and kernel parameters to optimize test power. The best such test locations (L = 1) for a Gaussian kernel (with learned bandwidth) are shown in <ref type="figure" target="#fig_0">Figure 10</ref>. We could also try optimizing a deep kernel (1) and the test locations together; this procedure, however, failed to find a useful test. We can find a better test, though, with a two-stage scheme: first, learn a deep kernel to maximize? ? , then choose v i to maximize? with that kernel fixed. Results are shown in <ref type="figure" target="#fig_0">Figure 11</ref>.</p><p>Although these approaches give nontrivial test power, it is hard to interpret either set of images, as the test locations have moved far outside the set of natural images. We can instead constrain v 1 ? S P ? S Q , simply picking the single point from the dataset which maximizes? (shown in <ref type="figure" target="#fig_0">Figure 12</ref>). This achieves similar test power, but lets us see that the difference might lie in images with smaller objects of interest than the mean for CIFAR-10.</p><p>Learning Deep Kernels for Non-Parametric Two-Sample Tests <ref type="figure" target="#fig_0">Figure 10</ref>. The best test locations (learned by an ME test with L = 1) from 10 experiments on CIFAR-10 vs CIFAR-10.1. Average rejection rate is 0.415. <ref type="figure" target="#fig_0">Figure 11</ref>. The best test locations (learned by an ME test, L = 1, with a deep kernel optimized for an MMD test) from 10 experiments on CIFAR-10 vs CIFAR-10.1. Average rejection rate is 0.637. <ref type="figure" target="#fig_0">Figure 12</ref>. The best test locations (selected among existing images with our learned deep kernel, L = 1) from 10 experiments on CIFAR-10 vs CIFAR-10.1. Average rejection rate is 0.653.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>), arXiv:2002.09116v3 [stat.ML] 14 Jan 2021Learning Deep Kernels for Non-Parametric Two-Sample Tests Samples drawn from P (left) and Q (right). In the Blob dataset, P and Q are each equal mixtures of nine Gaussians with the same modes (a), but each component of P is an isotropic Gaussian whereas the covariance of Q differs in each component. Panels (b) and (c) show the contours of a kernel, k(x, ?i) for each of the nine modes ?i; contour values are 0.7, 0.8 and 0.9. A Gaussian kernel (b) treats points isotropically throughout the space, based only on x ? y . A deep kernel (c) learned by our methods behaves differently in different parts of the space, adapting to the local structure of the data distributions and hence allowing better identification of differences between P and Q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Results on Blob-S and Blob-D given ? = 0.05; see Section 7 for details. n b is the number of samples at each mode, so n b = 100 means drawing 900 samples from each of P and Q. We report, when increasing n b , (a) average test power,(b) standard deviation of test power, (c) the value of? ? , and (d) average type-I error. (a), (b) and (c) are on Blob-D, and (d) is on Blob-S. Shaded regions show standard errors for the mean, and the black line shows ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>With perfect optimization, Corollary 14 shows a bound on the asymptotic power of the test is maximized by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Results on HDGM-S and HDGM-D for ? = 0.05 (black line). Left: average test power (a) and Type I error (b) when increasing the number of samples N , keeping d = 10. Right: average test power (c) and Type I error (d) when increasing the dimension d, keeping N = 4 000. Shaded regions show standard errors for the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The main potential explanation proposed by Recht et al. is dataset shift, but their attempt (in their Appendix C.2.8) at what amounts to a C2ST-S did not reject H 0 . 4 Samples from each distribution are shown in Figure 5 (Appendix B.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Q</head><label></label><figDesc>be sequences of sample sets of size n, let? n (?) denote J ?=n ?1/3 (S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the activation functions ?( )  are each 1-Lipschitz, ? ( ) (x) ? ? ( ) (y) ? x ? y , with ? ( ) (0) = 0 so that ? ( ) (x) ? x .Define a Banach space on ?, with addition and scalar multiplication componentwise, and ? = max ?{1,...,?} max W ( ) ? , b ( ) ? , where the matrix norm denotes operator norm W = sup x W x / x . (For convolutional networks, see Remark 25.) (III) k ? is a kernel of the form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .</head><label>4</label><figDesc>Images from real-MNIST and "fake"-MNIST. "Fake"-MNIST is generated byDCGAN (Radford et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Higgs (? = 0.05): average test power?standard error for N samples. Bold represents the highest mean per row. 120?0.007 0.095?0.022 0.082?0.015 0.097?0.014 0.132?0.005 0.113?0.013 2 000 0.165?0.019 0.130?0.026 0.183?0.032 0.232?0.017 0.291?0.012 0.304?0.035 3 000 0.197?0.012 0.142?0.025 0.257?0.049 0.399?0.058 0.376?0.022 0.403?0.050 5 000 0.410?0.041 0.261?0.044 0.592?0.037 0.447?0.045 0.659?0.018 0.699?0.047 8 000 0.691?0.067 0.467?0.038 0.892?0.029 0.878?0.020 0.923?0.013 0.952?0.024 10 000 0.786?0.041 0.603?0.066 0.974?0.007 0.985?0.005 1.000?0.000 1.000?0.000 MNIST (? = 0.05): average test power?standard error for comparing N real images to N DCGAN samples. 414?0.050 0.107?0.018 0.193?0.037 0.234?0.031 0.188?0.010 0.555?0.044 400 0.921?0.032 0.152?0.021 0.646?0.039 0.706?0.047 0.363?0.017 0.996?0.004 600 1.000?0.000 0.294?0.008 1.000?0.000 0.977?0.012 0.619?0.021 1.000?0.000 800 1.000?0.000 0.317?0.017 1.000?0.000 1.000?0.000 0.797?0.015 1.000?0.000 1 000 1.000?0.000 0.346?0.019 1.000?0.000 1.000?0.000 0.894?0.016 1.000?0.000 the desired Type I error. ME also does well in this case: it is perhaps particularly suited to this problem, since it is capable of identifying either modes dropped by the generative model or spurious modes it inserts.CIFAR-10 vs CIFAR-10.1. CIFAR-10.1(Recht et al.,  2019)  is an attempt to collect a new test set for the very popular CIFAR-10 image classification dataset(Krizhevsky,  2009). Normally, when evaluating a supervised model, we consider the test set an independent sample from the training distribution, ideally never-before-seen by the training algorithm. But modern computer vision model architectures and training procedures have been developed based on repeatedly evaluating on the CIFAR-10 test set (P)</figDesc><table><row><cell>N</cell><cell>ME</cell><cell>SCF</cell><cell>C2ST-S</cell><cell>C2ST-L</cell><cell>MMD-O</cell><cell>MMD-D</cell></row><row><cell>1 000 0.Avg.</cell><cell>0.395</cell><cell>0.283</cell><cell>0.497</cell><cell>0.506</cell><cell>0.564</cell><cell>0.579</cell></row><row><cell>N</cell><cell>ME</cell><cell>SCF</cell><cell>C2ST-S</cell><cell>C2ST-L</cell><cell>MMD-O</cell><cell>MMD-D</cell></row><row><cell>200 0.Avg.</cell><cell>0.867</cell><cell>0.243</cell><cell>0.768</cell><cell>0.783</cell><cell>0.572</cell><cell>0.910</cell></row><row><cell cols="4">average Type I error (P to P) in Table 8 (in Appendix B.6).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MMD-D substantially outperforms its competitors in test</cell><cell></cell><cell></cell><cell></cell></row><row><cell>power, with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>CIFAR-10.1 (? = 0.05): mean rejection rates.</figDesc><table><row><cell>ME</cell><cell>SCF</cell><cell cols="3">C2ST-S C2ST-L MMD-O MMD-D</cell></row><row><cell cols="3">0.588 0.171 0.452</cell><cell>0.529</cell><cell>0.316</cell><cell>0.744</cell></row><row><cell>evidence</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Mean test power on Blob (n b = 40), HDGM (N = 4000, d = 10), Higgs (N = 3000) and MNIST (N = 400) for ? = 0.05. See Section 7.2 for the naming scheme; S+C corresponds to C2ST-S, L+C to C2ST-L, and D+J to MMD-D. L+M is the method proposed byKirchler et al. (2020).Blob 0.835 0.942 0.901 0.900 0.851 0.960 0.906 0.952 0.966 0.985 HDGM 0.472 0.585 0.287 0.302 0.494 0.223 0.539 0.635 0.604 0.659 Higgs 0.257 0.399 0.353 0.384 0.321 0.254 0.379 0.295 0.364 0.403 MNIST 0.646 0.706 0.784 0.803 0.845 0.680 0.760 0.935 0.976 0.996</figDesc><table><row><cell></cell><cell>S+C</cell><cell>L+C</cell><cell>G+C</cell><cell>D+C</cell><cell>L+M</cell><cell>G+M D+M L+J</cell><cell>G+J</cell><cell>D+J</cell></row><row><cell>Avg.</cell><cell cols="8">0.553 0.658 0.581 0.597 0.628 0.529 0.646 0.704 0.727 0.761</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Korolyuk, V. S. and Borovskikh, Y. V. Asymptotic theory of U-statistics. Ukrainian Mathematical Journal, 40(2): 142-154, mar 1988. Krizhevsky, A. Learning multiple layers of features from tiny images, 2009. URL https://www.cs.toronto.edu/?kriz/ learning-features-2009-TR.pdf. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et al. Gradient-based learning applied to document recognition. Stojanov, P., Gong, M., Carbonell, J. G., and Zhang, K. Datadriven approach to multiple-source domain adaptation. In AISTATS, 2019. Sutherland, D. J. Unbiased estimators for the variance of MMD estimators, 2019. arXiv:1906.02104. Sutherland, D. J., Tung, H.-Y., Strathmann, H., De, S., Ramdas, A., Smola, A., and Gretton, A. Generative models and model criticism via optimized maximum mean discrepancy. In ICLR, 2017. Appendix A.2 proves the main results under some assumptions about the kernel parameterization, using intermediate results about uniform convergence of our estimators in Appendix A.3. Appendix A.4 then shows that these assumptions hold for different settings of kernel learning.</figDesc><table><row><cell>Proceedings of the IEEE, 86(11):2278-2324, 1998.</cell><cell>Sz?kely, G. J. and Rizzo, M. L. Energy statistics: A class</cell></row><row><cell>Li, C.-L., Chang, W.-C., Cheng, Y., Yang, Y., and P?czos, B.</cell><cell>of statistics based on distances. Journal of Statistical</cell></row><row><cell>MMD GAN: Towards deeper understanding of moment</cell><cell>Planning and Inference, 143(8):1249-1272, 2013.</cell></row><row><cell>matching network. In NeurIPS, 2017.</cell><cell>Torralba, A., Fergus, R., and Freeman, W. T. 80 million</cell></row><row><cell></cell><cell>tiny images: A large data set for nonparametric object</cell></row><row><cell></cell><cell>and scene recognition. IEEE Transactions on Pattern</cell></row><row><cell></cell><cell>Analysis and Machine Intelligence, 30(11):1958-1970,</cell></row><row><cell></cell><cell>2008.</cell></row><row><cell>Serfling, R. J. Approximation Theorems of Mathematical</cell><cell></cell></row><row><cell>Statistics. John Wiley &amp; Sons, 1980.</cell><cell></cell></row><row><cell>Smola, A. J. and Sch?lkopf, B. Learning with Kernels:</cell><cell></cell></row><row><cell>Support Vector Machines, Regularization, Optimization,</cell><cell></cell></row><row><cell>and Beyond. MIT Press, 2001.</cell><cell></cell></row></table><note>Li, S. and Wang, X. Fully distributed sequential hypothesis testing: Algorithms and asymptotic analyses. IEEE Trans. Information Theory, 64(4):2742-2758, 2018. Lopez-Paz, D. and Oquab, M. Revisiting classifier two- sample tests. In ICLR, 2017. Muandet, K., Fukumizu, K., Sriperumbudur, B., and Sch?lkopf, B. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends? in Ma- chine Learning, 10(1-2):1-141, May 2017. M?ller, A. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29 (2):429-443, 1997. Radford, A., Metz, L., and Chintala, S. Unsupervised rep- resentation learning with deep convolutional generative adversarial networks. In ICLR, 2016. Ramdas, A., Garc?a Trillos, N., and Cuturi, M. On Wasser- stein two-sample testing and related families of nonpara- metric tests. Entropy, 19(2):47, January 2017. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do ImageNet classifiers generalize to ImageNet? In ICML, 2019. Sedghi, H., Gupta, V., and Long, P. M. The singular values of convolutional layers. In ICLR, 2019.Sriperumbudur, B. K., Fukumizu, K., Gretton, A., Lanckriet, G. R., and Sch?lkopf, B. Kernel choice and classifiabil- ity for rkhs embeddings of probability distributions. In NeurIPS, 2009.Van der Vaart, A. W. Asymptotic Statistics. Cambridge University Press, 2000. Wenliang, L., Sutherland, D. J., Strathmann, H., and Gretton, A. Learning deep kernels for exponential family densities. In ICML, 2019. Wilson, A. G., Hu, Z., Salakhutdinov, R., and Xing, E. P. Deep kernel learning. In AISTATS, 2016. Zaremba, W., Gretton, A., and Blaschko, M. B-tests: Low variance kernel two-sample tests. In NeurIPS, 2013.A. Theoretical analysisA.1. Preliminaries Given a kernel k ? and sample sets {X i</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>where q is the appropriately-weighted mean of the various E[H i H j ] terms for which i, j, are not mutually distinct. Since |H ij | ? 4?, E[H i H j ] &lt; 16? 2 and so |q| ? 16? 2 as well. Noting that</figDesc><table><row><cell>1 n 3</cell><cell>ij</cell><cell>E[H i H j ] =</cell><cell>1 n 3</cell><cell>ij :|{i,j, }|=3</cell><cell cols="2">E[H i H j ] +</cell><cell>1 n 3</cell><cell cols="2">ij :|{i,j, }|&lt;3</cell><cell>E[H i H j ]</cell></row><row><cell></cell><cell></cell><cell>=</cell><cell cols="3">n(n ? 1)(n ? 2) n 3</cell><cell cols="3">E[H 12 H 13 ] + 1 ?</cell><cell>n(n ? 1)(n ? 2) n 3</cell><cell>q,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Specifications of P and Q of synthetic datasets. ? b 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Images from CIFAR-10 test set and the new CIFAR-10.1 test set (Recht et al., 2019).</figDesc><table><row><cell cols="5">Learning Deep Kernels for Non-Parametric Two-Sample Tests</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(a) CIFAR-10 test set</cell><cell></cell><cell></cell><cell cols="2">(b) CIFAR-10.1 test set</cell><cell></cell><cell></cell></row><row><cell>Figure 5. conv: 3x3x16; S = 2; P = 1 LeakyReLU(0.2) conv: 3x3x32; S = 2; P = 1 LeakyReLU(0.2)</cell><cell>Batch Norm 32</cell><cell>conv: 3x3x64; S = 2; P = 1 LeakyReLU(0.2)</cell><cell>Batch Norm 64</cell><cell>conv: 3x3x128; S = 2; P = 1 LeakyReLU(0.2)</cell><cell>Batch Norm 128</cell><cell>FC 512 units ReLU</cell><cell>FC 100 units</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>table .</head><label>.</label><figDesc></figDesc><table><row><cell>N</cell><cell>ME</cell><cell>SCF</cell><cell cols="4">C2ST-S C2ST-L MMD-O MMD-D</cell></row><row><cell>1000</cell><cell cols="2">0.048 0.040</cell><cell>0.043</cell><cell>0.048</cell><cell>0.059</cell><cell>0.037</cell></row><row><cell>2000</cell><cell cols="2">0.043 0.032</cell><cell>0.060</cell><cell>0.056</cell><cell>0.055</cell><cell>0.053</cell></row><row><cell>3000</cell><cell cols="2">0.049 0.043</cell><cell>0.046</cell><cell>0.053</cell><cell>0.051</cell><cell>0.069</cell></row><row><cell>5000</cell><cell cols="2">0.056 0.035</cell><cell>0.052</cell><cell>0.065</cell><cell>0.049</cell><cell>0.062</cell></row><row><cell>8000</cell><cell cols="2">0.050 0.034</cell><cell>0.065</cell><cell>0.067</cell><cell>0.056</cell><cell>0.037</cell></row><row><cell cols="3">10000 0.059 0.032</cell><cell>0.057</cell><cell>0.058</cell><cell>0.045</cell><cell>0.048</cell></row><row><cell>Avg.</cell><cell cols="2">0.051 0.036</cell><cell>0.054</cell><cell>0.058</cell><cell>0.050</cell><cell>0.051</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>MNIST dataset can be downloaded via Pytorch. See the code in https://github.com/eriklindernoren/ PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py. CIFAR-10.1 is available from https://github.com/modestyachts/CIFAR-10.1/tree/master/ datasets (we use cifar10.1 v4 data.npy). This new test set contains 2, 031 images from TinyImages(Torralba et al., 2008).B.6. Type I errors on Higgs and MNIST</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This estimator, as a V -statistic, is biased even when ? = 0 (although this bias is only O(1/N ); see Lemma 18). AlthoughSutherland et al. (2017); Sutherland (2019) give a quadratic-time estimator unbiased for ? 2 H 1 , it is much more complicated to implement and analyze, likely has higher variance, and (being unbiased) can be negative, especially e.g. when the kernel is poor.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A characteristic kernel on top of even ??(x) = ? T x with a random ? will be almost surely consistent<ref type="bibr" target="#b21">(Heller &amp; Heller, 2016)</ref>, and in general the existence of even one good ?? for a particular</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">4 points, and testing on the remainder.7. Experimental Results7.1. Comparison on Benchmark DatasetsWe compare the following tests on several datasets:</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Assuming pretrained classifiers are independent of P,Figure 1ofRecht et al. (2019)  indicates that the joint (images, labels) distribution certainly differs between CIFAR-10 and CIFAR-10.1. We test here whether the marginal image distribution differs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">In fact, it suffices for the? * n to only approximately maximize?n, as long as their suboptimality is oP (1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Of course, if we know a bound of R? &lt; 2, the result will still hold using R? = 2. It is also possible to show a tighter result, via(13)and(14)or their analogue for R = 1; the expression is simply less compact.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the Australian Research Council under FL190100149 and DP170101632, and by the Gatsby Charitable Foundation. FL, JL and GZ gratefully acknowledge the support of the NVIDIA Corporation with the donation of two NVIDIA TITAN V GPUs for this work. FL also acknowledges the support from UTS-FEIT and UTS-AAII. DJS would like to thank Aram Ebtekar, Ameya Velingker, and Siddhartha Jain for productive discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A test for the two-sample problem based on empirical characteristic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alba</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jim?nez Gamero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu?oz</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3730" to="3748" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On gradient regularizers for MMD GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Searching for exotic particles in high-energy physics with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">4308</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reproducing Kernel Hilbert Spaces in Probability and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep layers as stochastic solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Berry-Esseen theorem for u-statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Callaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Janssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="417" to="421" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new graph-based twosample test for multivariate and object data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">517</biblScope>
			<biblScope unit="page" from="397" to="409" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Classification logit two-sample testing by neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cloninger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11298</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast two-sample testing with analytic representations of probability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chwialkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the mathematical foundations of learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modified randomization tests for nonparametric hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dwass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="187" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust hypothesis testing using Wasserstein uncertainty sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Practical methods for graph two-sample testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghoshdastidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two-sample tests for large random graphs using network statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghoshdastidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutzeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple kernel learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alpayd?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2211" to="2268" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">and Systems, I. Domain adaptation with conditional transferable components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A fast, consistent kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Testing for homogeneity with kernel Fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multivariate tests of association based on univariate tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised deep kernel learning: Regression with unlabeled data by minimizing predictive variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Interpretable distribution features with maximum testing power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jitkrittum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Szabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chwialkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A linear-time kernel goodness-of-fit test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jitkrittum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Szabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classification accuracy as a proxy for two sample testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02210</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Twosample testing using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lippert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06239</idno>
	</analytic>
	<monogr>
		<title level="m">AISTATS, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
