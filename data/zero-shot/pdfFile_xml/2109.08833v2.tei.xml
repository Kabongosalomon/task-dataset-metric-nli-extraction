<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TVStoryGen: A Dataset for Generating Stories with Character Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
							<email>mchen@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<email>kgimpel@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TVStoryGen: A Dataset for Generating Stories with Character Descriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce TVSTORYGEN, a story generation dataset that requires generating detailed TV show episode recaps from a brief summary and a set of documents describing the characters involved. Unlike other story generation datasets, TVSTORYGEN contains stories that are authored by professional screenwriters and that feature complex interactions among multiple characters. Generating stories in TVSTORYGEN requires drawing relevant information from the lengthy provided documents about characters based on the brief summary. In addition, we propose to train reverse models on our dataset for evaluating the faithfulness of generated stories. We create TVSTORYGEN from fan-contributed websites, which allows us to collect 26k episode recaps with 1868.7 tokens on average. Empirically, we take a hierarchical story generation approach and find that the neural model that uses oracle content selectors for character descriptions demonstrates the best performance on automatic metrics, showing the potential of our dataset to inspire future research on story generation with constraints. Qualitative analysis shows that the best-performing model sometimes generates content that is unfaithful to the short summaries, suggesting promising directions for future work. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Story generation is the task of generating a coherent narrative.</p><p>Due to its open-ended nature, increasing efforts have been devoted to constrained settings to facilitate reliable evaluation of computational models, such as generating stories from short prompts <ref type="bibr">(Fan et al., 2018)</ref> and story <ref type="bibr">continuations (Mostafazadeh et al., 2016)</ref> with various constraints <ref type="bibr" target="#b0">(Akoury et al., 2020)</ref>. In this work, we * Work done while the author was at Toyota Technological Institute at Chicago. <ref type="bibr">1</ref> Data is available at https://github.com/ mingdachen/TVRecap are interested in generating stories that accord with descriptions about the characters involved. The task is akin to writing stories based on true events or historical figures. For example, when writing historical fiction, writers use facts in biographies of historical figures (i.e., character descriptions) <ref type="bibr" target="#b4">(Brown, 1998)</ref>. In a similar vein, cognitive psychologists observed that in order for narrative text to be compelling, it has to base its characters on realworld details such that readers can form emotional attachments to them even if the events occurring in the text are not realistic <ref type="bibr">(Oatley, 1999;</ref><ref type="bibr">Green et al., 2003)</ref>. In either case, computational models for this task can offer assistance in proposing possible stories constrained by relevant documents.</p><p>To this end, we create a story generation dataset TVSTORYGEN that generates detailed TV show episode recaps from a brief summary of the episode and a set of lengthy character descriptions. We construct TVSTORYGEN from fan-contributed websites, which allows us to collect 26k episode recaps covering a variety of genres. An example from TVSTORYGEN is shown in <ref type="figure">Figure 1</ref>. The dataset is challenging in that it requires drawing relevant information from the lengthy character description documents based on the brief summary. Since the detailed episode recaps are constrained by character descriptions, it also can evaluate neural models' ability to maintain consistent traits or goals of particular characters during generation.</p><p>In addition, to evaluate the faithfulness of the generated stories to the brief summaries, we propose a metric that uses the perplexities from reverse models trained on our dataset. The reverse models are trained by considering generating the brief summary from the detailed recap. Human evaluation shows that this automatic metric outperforms competitive baselines in evaluating faithfulness.</p><p>Empirically, we characterize the dataset with several nearest neighbour methods and oracle models, finding that the use of the brief summaries and the character descriptions generally benefits model performance. We find that our non-oracle models are competitive compared to nearest neighbour models, suggesting promising future directions. We also benchmark several large pretrained models on the summarization version of our dataset, finding that they perform worse than an extractive oracle by a large margin despite the fact that the dataset favors abstractive approaches. Human evaluation reveals that without character descriptions, models tend to dwell on each event separately rather than advancing the plot, whereas using character descriptions improves the interestingness of the generated stories. Qualitatively, we show that models are able to generate stories that share similar topics with the summaries, but they may miss events in the summaries, leading to unfaithful generations.</p><p>We summarize our contributions below:</p><p>1. We construct a story generation dataset of 26k instances and show (both qualitatively and quantitatively) that it has several unique challenges.</p><p>2. We show that the reverse models trained on our dataset can be used in evaluation for the original dataset, namely to determine whether generated stories are faithful to their input summaries.</p><p>3. We empirically characterize the story generation dataset and the summarization version of our dataset with several nearest neighbour methods, oracle models, and pretrained models, showing the challenges of these tasks and suggesting future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The TVSTORYGEN Dataset</head><p>In this section, we describe how we construct TVS-TORYGEN and compare it to other story generation datasets. An instance in TVSTORYGEN is comprised of three components: (1) a detailed episode recap, (2) a brief summary of the episode, and (3) character descriptions, i.e., a set of documents describing the characters involved in the episode. The detailed episode recap delineates the events that occurred in the corresponding episode, which is usually written by fans after watching the episode. The documents about the characters contain biographical details and possibly personality traits. The summary either summarizes the whole episode or talks about the setup of the episode (to avoid spoilers). An example instance is shown in <ref type="figure">Figure 1</ref>, which comes from an episode of the TV show "The Simpsons". To demonstrate the level of detail of the character descriptions, we include a relatively complete character description for Selma Bouvier in the appendix, where the description consists of five sections: "Introduction", "Personality", "Physical Appearance", "Relationships", and "Non-canon Appearance". Each section's text is typically hundreds of word tokens long, covering many specifics.</p><p>As there are relevant details mentioned in the character descriptions, generating the detailed recap requires drawing information from the lengthy character descriptions. Moreover, due to the fact that the brief summary sometimes only depicts the setup of the episode, completing the story also necessitates using the information in the character descriptions. That is, the character description information is expected to be helpful for both filling in details that are not present in the brief summary as well as, for some of the instances, generating a plausible ending for the story. While the character descriptions could include some of the details in output stories, it is still challenging for models to retrieve salient information from the lengthy character descriptions and effectively integrate it into the discourse of the stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Construction</head><p>We construct TVSTORYGEN from two fancontributed websites: Fandom 2 (FD) and TVMeg-aSite 3 (TMS). We collect brief summaries and detailed episode recaps for several long-running soap operas from TVMegaSite and other TV shows from Fandom. We collect character descriptions from Fandom. 4 Since the pages on Fandom have hyperlinks pointing to the character pages, we use the hyperlinks to connect episodes to the characters involved. For TVMegaSite, where there are no such hyperlinks, we use string matching to find the characters. To ensure the quality of this dataset, we filter out episodes based on several criteria. See the appendix for more details on the criteria and the string matching algorithm.</p><p>We report detailed statistics about TVSTORY-GEN in <ref type="table" target="#tab_2">Table 2</ref>. As shown in the table, there are systematic differences between FD and TMS in terms of length of detailed episode recaps, summaries, and character descriptions, among others.</p><p>Troy McClure's sagging film career is given a boost when he is seen in public with a woman. To stay in the public eye, Troy must do more than merely date this woman, who happens to be Selma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed episode recap</head><p>Brief summary == Introduction == Selma Bouvier <ref type="bibr">(born February 28, 1952)</ref> is one of Marge's older chain-smoking twin sisters. She works at the DMV and possesses a strong dislike for her brother-in-law, Homer, ? Selma's favorite film actor was reportedly Troy McClure, to whom she was briefly married, before discovering that it was just a publicity stunt by McClure ? == Personality == Selma and Patty tend to be cynical and are noted for their addiction to tobacco smoking?  <ref type="figure">Figure 1</ref>: Excerpts from a TVSTORYGEN instance that corresponds to the episode "A Fish Called Selma" in the TV show "The Simpsons". Colored texts are mentions of characters. Texts surrounded by "==" in the character descriptions are section titles. We underline texts in the character descriptions that are closely related to the detailed recap. The shown part of the story includes complex interactions between Selma Bouvier and Troy McClure , which requires models to use relevant information from the lengthy character descriptions. There are six characters involved in this story but we only show parts of the detailed recap and character descriptions for brevity.   We verify the diversity of topics covered in TVS-TORYGEN, finding that FD covers far more genres than TMS with the most frequent occupying only 15% of episodes. See the appendix for more details. We randomly split the datasets into train/dev/test sets. For TMS, we additionally filter out instances if the overlap ratio of TV show characters appearing in the summary and the detailed recap is lower than 85%. This extra filtering step ensures alignment between the summaries and detailed recaps. The final statistics of the splits is shown in <ref type="table" target="#tab_3">Table 3.</ref> ...Meanwhile, Patty and Selma have received a promotion at the DMV and have more disposable income. As a last resort, Homer asks the two if they will lend him the money. They agree, but he must become their loyal servant. Patty and Selma make Homer's life a living hell... ...The next day, news of Homer's "death" spreads across Springfield, and Marge starts getting condolences from prominent Springfieldians. Patty and Selma offer their condolences in the form of a tombstone celebrating Homer's death... <ref type="table">Table 4</ref>: Two excerpts in detailed recaps from TVSTO-RYGEN that correspond to different episodes in the TV show "The Simpsons". The excerpts involve interactions between Homer and Selma where Selma consistently shows a strong dislike for Homer, matching the character description in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Comparison</head><p>We compare TVSTORYGEN to other story generation datasets in <ref type="table" target="#tab_1">Table 1</ref>. Unlike ROCStories <ref type="bibr">(Mostafazadeh et al., 2016)</ref> and WritingPrompts <ref type="bibr">(Fan et al., 2018)</ref> where the inputs to models are either the first few sentences or short prompts, TVS-TORYGEN has character descriptions as extra constraints, making the task of generating the reference stories from the inputs less open-ended and therefore more feasible.</p><p>Since STORIUM <ref type="bibr" target="#b0">(Akoury et al., 2020)</ref> has character descriptions and other information as constraints, it is the most comparable resource to TVS-TORYGEN. Our dataset differs from STORIUM in that TVSTORYGEN has more stories, more characters, and longer, more diverse, detailed character descriptions. See the appendix for more detailed discussion.</p><p>Moreover, models trained on TVSTORYGEN can potentially complement those from STORIUM by merging all the characters' turns in STORIUM into a coherent narrative. We also quantitatively demonstrate that the source inputs and the output stories are more directly related in our dataset than STORIUM. Details are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset Challenges</head><p>TVSTORYGEN poses several challenges for story generation models. The first challenge stems from the long lengths of the inputs and outputs. Specifically, the average instance in TVSTORYGEN has a story of 1.8k tokens and character descriptions of more than 10k tokens (see <ref type="table" target="#tab_2">Table 2</ref>). In contrast, story generation and multi-document summarization datasets may have lengthy inputs or outputs, but rarely both (see <ref type="table" target="#tab_1">Table 1</ref> and appendix B.4 for detailed statistics). The long inputs and outputs make it challenging to design models that can effectively integrate lengthy character descriptions into the generation process of long and coherent stories.</p><p>The other set of challenges relates to consistency in character modeling. Since the episode recaps are constrained by character descriptions, the dataset provides opportunities to evaluate neural models' ability to maintain consistent personalities or goals of particular characters during generation. The consistency of personalities and goals is related to the notion of "character believability" <ref type="bibr" target="#b1">(Bates et al., 1994;</ref><ref type="bibr" target="#b8">Riedl and Young, 2010)</ref>, which has been deemed important for composing convincing stories. We illustrate this challenge with two excerpts in <ref type="table">Table 4</ref>: the strong dislike that Selma has shown for Homer matches her description and is consistent across episodes. Solving this challenge requires models to first identify related information in the lengthy character descriptions based on the plot and integrate it into the generated narrative. We aim to incorporate this idea into the design of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We follow <ref type="bibr">Fan et al. (2019)</ref> to take a hierarchical story generation approach. The generation process is broken into two steps that use two separately parameterized models: a text-to-plot model and a plot-to-text model. The text-to-plot model first generates detailed plots based on the inputs, and then conditioned on the plots the plot-to-text model generates detailed stories. In this paper, we define the plots as linearized semantic role labeling (SRL) structures. More details on SRL are in the appendix. For example, a plot may be as follows:</p><p>VERB spots ARG0 Mummy Pi ARG1 how messy the car is SEP VERB clean ARG0 they ARG1 the car where SEP is a special token used to separate SRL structures for different sentences.</p><p>Text-to-Plot Model. During training, we use the oracle plots, i.e., the SRL tags extracted from the reference recaps. During test time, we use BM25 <ref type="bibr" target="#b9">(Robertson et al., 1995)</ref> to find the most similar plot in the training set from the same show based on either the summaries or the detailed recaps (as an oracle baseline). <ref type="bibr">6</ref> If a show is not present in the training set, we search over the whole training set.</p><p>Plot-to-Text Model. Our models are based on the sequence-to-sequence transformer architecture <ref type="bibr" target="#b14">(Vaswani et al., 2017)</ref>. Similar to <ref type="bibr" target="#b10">Rothe et al. (2020)</ref> that uses pretrained BERT-like models to initialize sequence-to-sequence models, we use the pretrained RoBERTa-base model <ref type="bibr">(Liu et al., 2019)</ref> as the decoder. 7 For the encoder, we choose to use a one-layer randomly initialized Longformer <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref> due to the lengthy inputs and computational constraints. We randomly initialize other parameters and finetune the whole model during training.</p><p>Given a plot, we use the neural models to generate sentence by sentence as we find this yields better performance than generating the whole detailed recap. When doing so, we concatenate the SRL tags for the adjacent sentence of the target sentence with the SRL tags for the target sentence. This gives similar performance to showing the SRL tags for the entire detailed recap but is much more efficient (due to shorter sequence lengths). Because the character descriptions are lengthy, we use BM25 to retrieve the most salient information from character descriptions (i.e., one sentence) for each sentence in the detailed recap. We note that during test time, when the text-to-plot model retrieves plots from the training set, we also use the corresponding selected character descriptions.</p><p>The pipeline that retrieves relevant character information and then adapts it based on the plot is the first step that we take to simulate a writing system that can dynamically update its belief about particular characters based on the given relevant documents. This differs from prior work on entity representations for story generation (Clark et al., 2018) that does not consider character descriptions as we do.</p><p>The inputs to plot-to-text models contain two sources: plots and character descriptions. Since there could be multiple entries corresponding to different characters in the character descriptions, we include a type embedding to differentiate different entries and sources in the input. Similar approaches have been used to represent table entries in neural models <ref type="bibr">(Dhingra et al., 2019)</ref>. where the subscripts indicate the ID of the type embedding and we always prepend the character names to the corresponding selected character descriptions. The final vector representation of the input is the summation of subword unit embeddings, positional embeddings, and the type embeddings. Conditioned on the input representations, we train the RoBERTa decoder on the reference recaps using a cross-entropy loss.</p><p>Due to computational constraints, for the Longformer encoder, we use the global attention on the SEP tokens, and use the encoded representations for the summary, the SRL tags, and the SEP tokens in character descriptions as the input to decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>For evaluation, we report BLEU (BL) <ref type="bibr">(Papineni et al., 2002)</ref>, ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) <ref type="bibr">(Lin, 2004)</ref> scores. We additionally report perplexities of the summaries given the generated stories using a reverse model. We will refer to this metric as "PL". This metric evaluates the faithfulness of the generated stories to the summaries. Lower PL suggests better faithfulness. When computing PL, we use the Pegasus model <ref type="bibr" target="#b21">(Zhang et al., 2020)</ref> finetuned on our dataset as it has the best test set perplexities. Details on model selection, decoding and hyperparameters are in the appendix.</p><p>When training the reverse model on our dataset, we simply use the detailed episode recap as the source input and the summary as the target output. In the appendix, we benchmark competitive pretrained models on the "inverted" dataset and compare the dataset to other summarization datasets, finding that it is a potentially valuable contribution for the summarization community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We report results for FD and TMS in Tables 5 and 6, respectively. We report several return-input baselines on the test sets to show the benefits of using neural models as plot-to-text models. We report PL on the test sets as an approximated lower bound of  <ref type="table">Table 5</ref>: Results for Fandom in TVSTORYGEN. The results for the return-input baselines and the neural models are indicated by"(Return)" and "(NN)" respectively. The best result in each column for each split (excluding the references) is boldfaced.  <ref type="table">Table 6</ref>: Results for TVMegaSite in TVSTORYGEN. The results for the return-input baselines and the neural models are indicated by "(Return)" and "(NN)" respectively. The best result in each column for each split (excluding the references) is boldfaced. this metric. We do not report PL on return-input baselines as the output detailed recaps involve SRL sequences, which are not natural language, and therefore the results are not comparable to others.</p><formula xml:id="formula_0">BL (?) R1 (?) R2 (?) RL (?) PL (?) Development</formula><formula xml:id="formula_1">BL (?) R1 (?) R2 (?) RL (?) PL (?) Development</formula><p>On the development sets, adding summaries and oracle character descriptions generally improves performance by a significant margin, showing that the extra information aids generation.</p><p>Regarding the test set results, we find that (1) the return-input baselines show that the performances of our neural models are non-trivial; (2) while the oracle nearest neighbour baselines achieve competitive performance to our non-oracle neural models, the non-oracle neural models are consistently better than the non-oracle baselines, showing promising results for future research on our datasets. We note that the return-input baselines involving character descriptions display much worse results than other return-input baselines because they are lengthy,  <ref type="table">Table 7</ref>: Human annotation results analyzing the effect of including different components in the inputs. The percentage is the fraction of annotations that favor the models to include the corresponding component. The numbers in parentheses are the number of positive annotations divided by the total number of annotations. which leads to low precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Human Evaluation</head><p>Effect of Different Components in Our Dataset.</p><p>To measure the impact of including different com-ponents in TVSTORYGEN, we conduct a human evaluation. We show two generated stories from different models along with the corresponding brief summary and ask annotators to choose which story they prefer according to two aspects: (1) which generation is more relevant to the summary; (2) which story is more interesting.</p><p>We make two comparisons: "oracle plot" vs. "oracle plot+summary" for studying the benefits of using summaries ("Prefer summary"), and "oracle plot+summary" vs. "oracle plot+summary+oracle char. desc." for studying the benefits of using character descriptions ("Prefer char. desc."). We sample instances from the FD development set because the average lengths in FD are shorter, and we only show annotators the first 100 tokens of the texts as we expect it to be challenging to annotate lengthy texts. We use Amazon Mechanical Turk (AMT) and collect 20 annotations per question for each comparison with 6 workers involved (shown in Table 7 as "crowdsourced annotations"). <ref type="bibr">8</ref> We (the authors) also annotate 50 instances per comparison using the same interface as AMT (shown in the table as "expert annotations"). While the crowdsourced annotations do not suggest clear benefits of using summaries and character descriptions, the expert annotations show that including the summary helps to improve relevancy but hurts the interestingness of the stories, whereas including character descriptions improves the interestingness despite the marginal benefits of improving the relevancy. Recent work <ref type="bibr">(Karpinska et al., 2021)</ref> also found that compared to experts, AMT workers produce lower quality annotations for tasks like story generation.</p><p>When examining annotations, we find that models without using character descriptions tend to generate sentences that use the word "but" to negate what has been said in the earlier part of the sentence, leaving the sentence dwelling on each event separately rather than advancing the plot (see Sec. 5.2 for examples).</p><p>The PL Metric. To verify the efficacy of our proposed metric PL, we compute accuracies between the PL metric and the human annotations (we use the expert relevancy annotations in human evaluation results). We consider BLEU and BLEURT <ref type="bibr" target="#b11">(Sellam et al., 2020)</ref> as baseline metrics by com-Acc. BLEU 50.0 BLEURT 54.5 PL (our metric) 61.4 <ref type="table">Table 8</ref>: Accuracies when evaluating the automatic metrics against human annotations. When computing the BLEU and BLEURT scores, we compare the generation against the brief summaries. The best performance in each column is in bold.</p><p>puting the generation against the brief summaries. When reporting results, we use the truncated generation to ensure consistency with human annotators. We show the results in <ref type="table">Table 8</ref>. We find that PL outperforms BLEU and BLEURT significantly, showing that PL is a promising metric for evaluating the faithfulness of generated story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generation Examples</head><p>We display the generation examples in <ref type="table">Table 9</ref> where we find that generations from both models generally share similar topics and character names with the summaries and the references. For example, for the first instance, both generations are about a battle that concerns Elfman, Evergreen, and Rustyrose. However, as observed in the human evaluations, the "oracle plot+summary" model suffers from meaningless negation. For example, see the second generation example, where the highlighted texts keep negating the earlier plot development. While the "Oracle plot+summ.+oracle char." model does not have this problem, it is still not faithful to the summary. Specifically, both the summary and the reference mention that Chuck needs Serena's help for his new club opening, but the generation states that "Serena is conducting the club opening" and "Blair asks her to help". This is likely caused by the model's inability to understand the states of each character (possibly due to the fact that our models generate at the sentence level) and to effectively integrate multiple sources of information into a coherent narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Early methods in computational modeling for story generation rely on handwritten rules <ref type="bibr">(Meehan, 1977;</ref><ref type="bibr">Liu and Singh, 2002)</ref> to structure narrative. Recent work has explored different approaches to improve the quality of story generation systems, including commonsense knowledge <ref type="bibr">(Mao et al., 2019;</ref><ref type="bibr">Guan et al., 2020)</ref>, automatically extracted key words <ref type="bibr">(Peng et al., 2018)</ref> and key phrases (Or-Input summary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Oracle plot+summary Oracle plot+summ.+oracle char. Elfman and Evergreen , after much struggle , lose to Rustyrose and his imagination -based Magic . In the meantime , Gray , Lucy , Cana and Loke have been overpowered by Caprico alone . Loke decides to take him on by himself because of his mysterious Magic .</p><p>Elfman and Evergreen encounter a cliff as they run away from Rustyrose 's Belcusas the Thunderclap . Rustyrose appears shortly afterwards and expands on the idea of " The Ultimate World of Magic " , saying that all those who can not use Magic and the trash in the guilds are useless ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elf episode begins with</head><p>Elfman and Evergreen encounter a cliff in the middle of the forest , and the two begin to fight . All , all those those are going to use Magic to defeat them , and they will be able to defeat all of them . Ruth , however , has n't been able to outsmart them , and the two of them begin to fight ... Elfman and Evergreen encounter a cliff as they run away from the danger of the Rustyrose 's attacks . Evergreen explains that " all those who use Magic " will use the power of the Magic of the Seven . However , Rustyrose outsmarts them , stating that their Magic is useless against them , as they have already been defeated .  <ref type="table">Table 9</ref>: Excerpts from generation examples, which come from the TV shows "Fairy Tail" and "Gossip Girl" respectively. The highlighted texts are meaningless negations. We underline texts that describe similar events. The complete generations are in the appendix. bach and Goldberg, 2020; Rashkin et al., 2020), event-based representations <ref type="bibr">(Martin et al., 2018)</ref>, and plot graphs <ref type="bibr">(Li et al., 2013)</ref>.</p><p>As our model involves plot generation and character modeling, it is related to work on plot planning <ref type="bibr" target="#b8">(Riedl and Young, 2010;</ref><ref type="bibr">Li et al., 2013;</ref><ref type="bibr">Martin et al., 2018;</ref><ref type="bibr" target="#b17">Yao et al., 2019;</ref><ref type="bibr">Jhamtani and Berg-Kirkpatrick, 2020)</ref>, character modeling <ref type="bibr">(Clark et al., 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2020)</ref>, and the interplay between the two (Riedl and Young, 2010). Our work is different in that it explicitly requires performing inference on lengthy documents about characters.</p><p>There have been other datasets built from TV shows, such as summarizing TV shows into character descriptions <ref type="bibr" target="#b13">(Shi et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We constructed a story generation dataset of 26k stories where each instance consists of a detailed episode recap, a summary, and several character descriptions. We quantitatively and qualitatively illustrate several unique challenging aspects of this dataset. In addition, we show that the reverse model trained on our dataset, which generates the summary from the detailed recap, is a competitive automatic metric for evaluating faithfulness of stories. We also propose a metric based on the summarization model trained on our dataset for evaluating the faithfulness of the generated stories to the summaries. Empirically, we benchmark several nearest neighbour models and oracle models, showing that the summaries and the character descriptions are helpful in generating better stories, which are verified by human evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>We highlight three limitations. Firstly, TVS-TORYGEN was derived from Fandom and tvmegasite.net. These two websites were contributed by fans to document details for popular TV shows. Due to the inconsistencies between our goal (i.e., story generation) and the goal of these websites, there could be considerable variations across instances regarding how well the data matches our purpose. For example, in Sec. 2.1, we discuss the issue that some summaries do not include story endings.</p><p>The second is that our proposed models are relatively small-scale, with components parameterized by bag-of-words systems (i.e., BM25) due to computational constraints. Future work may explore using more powerful models, e.g., pretrained retrieval models like REALM (Guu et al., 2020) and RETRO <ref type="bibr" target="#b3">(Borgeaud et al., 2021)</ref>.</p><p>The third limitation is human evaluation. In particular, we only have a small number of expert annotations. Future work may explore recruiting more experts from platforms like Upwork. 9 Also, since we only show the beginning of a story to human annotators, it ignores the quality of story endings. Future work may explore a more robust human evaluation process, e.g., splitting stories into multiple parts and having several annotators evaluate different parts of stories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Example of Character Descriptions</head><p>In <ref type="table" target="#tab_1">Table 10</ref>, we show detailed character descriptions for Selma Bouvier from the TV show "The Simpsons".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Construction</head><p>String Matching Algorithm. For example, for the character name "John Doe", valid mentions are itself, "John", "J.D.", and "JD" due to the writing style on TVMegaSite. While this matching algorithm may lead to extra characters aligned to particular episodes, it at least includes all characters that are actually involved in the episode.</p><p>Episode Filtering Criteria. We filter out episodes if (1) an episode contains fewer than 3 characters (to avoid stories that do not involve many character interactions); or (2) the detailed recap has fewer than 200 word tokens (ensuring that stories have enough details); or (3) the brief summary has fewer than 20 word tokens (to ensure that there is sufficient information given as the input).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Detailed Statistics for TVSTORYGEN</head><p>Detailed Statistics for FD and TMS. We show detailed dataset statistics for FD and TMS in <ref type="table" target="#tab_1">Table 11</ref>.</p><p>A.4 Detailed Comparison to STORIUM 1. Our dataset has more stories, more characters, and longer character descriptions.</p><p>2. The elements (e.g., traits of characters and story events) in STORIUM are constrained by cards. These cards have limited variety and are often inherited from other stories. This characteristic makes the involved characters less diverse than those in our dataset.</p><p>3. The stories in STORIUM often have detailed descriptions about environments and character utterances, whereas the stories in TVSTORY-GEN mostly narrate events that happened without these details. While this leads to shorter stories in TVSTORYGEN, it also prevents the task from conflating generating events and generating other kinds of details in story generation.</p><p>4. Due to the fact that the plots in STORIUM are gamified and crafted by amateur writers, 89.8% of stories in STORIUM are unfinished. <ref type="bibr">10</ref> The stories in our dataset are created and refined by professional screenwriters (though the prose is written by fans, who are presumably amateurs).</p><p>5. Stories in STORIUM are turn-based, where each turn is written from the perspective of a particular character and is composed by one player, so the stories often lack direct interactions among characters, unlike TVSTORYGEN.</p><p>6. Unlike other story generation datasets, there is an episodic structure among the stories in TVS-TORYGEN, which can potentially be used to improve the modeling of characters.</p><p>STORIUM lacks direction interactions among characters. We quantify this phenomenon in STORIUM by computing the frequency of occurrences of characters in each turn excluding the character that owns the turn, and the frequency is 0.8 on average with 50.4% of the turns absent such occurrences. <ref type="bibr">11</ref> In contrast, TV shows advance plots by interactions among characters.</p><p>Source Inputs and Output Stories are More Closely Related in TVSTORYGEN than STO-RIUM. To quantitatively illustrate the extent of relatedness between the source inputs and the output stories, we compute the n-gram overlap ratio (i.e., fraction of n-grams in the output stories that also appear in the source inputs) between the inputs and outputs where higher ratios indicate that the two are more directly related. When computing the results for STORIUM, we use the best setting, i.e., the setting that maximizes the automatic and human evaluation scores in the original paper. We report results in <ref type="table" target="#tab_1">Table 12</ref>. From the table, we see that for both FD and TMS, using both character descriptions and summaries leads to an overlap ratio higher than STORIUM, suggesting that the reference stories are more reachable. Also, we observe there are more overlapping n-grams in the character descriptions than the summaries, suggesting that there is useful information that can be extracted from the character descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Genres</head><p>To demonstrate the diversity of topics covered in TVSTORYGEN, we report distributions of genres   <ref type="table" target="#tab_1">Table 12</ref>: Fraction (%) of n-grams in the output stories that also appear in the source inputs. Higher fraction of overlapping n-grams indicates that the two are more directly related. For TVSTORYGEN, we vary different kinds of inputs.</p><p>abilities of repeated trigrams are set to 0 during beam search. We did not find nucleus sampling <ref type="bibr">(Holtzman et al., 2020)</ref> leading to better generation quality (i.e., fluency and faithfulness to the summaries and the recaps) than beam search with n-gram blocking, possibly due to the fact that our   models generate at the sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hyperparamters</head><p>Because our plot-to-text models work at the sentence level, leading to many training instances for both FD and TMS (i.e., 0.5 million and 1.5 million sentences respectively), we train these plot-to-text models for 10 epochs without early stopping. During generation, we set the minimum number of decoding steps to 24 and the maximum number of decoding steps to 48. As for summarization, we benchmark pretrained BART-base, <ref type="bibr">BART-large (Lewis et al., 2020)</ref>, and Pegasus <ref type="bibr" target="#b21">(Zhang et al., 2020)</ref>. As the average length of the detailed recaps is much longer than the de-  fault maximum sequence length of these pretrained models, we extend the maximum sequence length to 4096. When doing so, we randomly initialize new positional embeddings for BART. Since Pegasus uses Sinusoidal positional embeddings, we simply change the default value of maximum sequence length. We train the models for 15 epochs and perform early stopping on the dev set perplexities. During generation, we limit the minimum decoding step to be 50 and 300, and the maximum decoding step to be 100 and 600 for FD and TMS respectively. The minimum decoding steps roughly match the average length of the summaries in FD and TMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Computing Resources and Number of Parameters</head><p>We train our models on a RTX 2080Ti, which takes approximately 8 days to complete training the story generation model that uses plots and character descriptions and summaries as input. It takes approximately 12 hours to train the reverse model. As for numbers of parameters for each models, since we do not modify model architectures, their numbers of parameters should be similar to those of the pretrained checkpoints we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Summarization Dataset</head><p>By considering generating the brief summary from the detailed episode recap, we create an abstractive summarization dataset TVSTORYSUM. In this dataset, we simply use the detailed episode recap as the source input and the summary as the target output and leave the integration of character descriptions to future work. In experiments, we use the abstractive summarization models trained on this dataset for evaluating the faithfulness of generated stories.</p><p>We characterize TVSTORYSUM by comparing it to other abstractive summarization datasets, finding that it favors abstractive approaches. In addition, unlike most other summarization datasets, our dataset focuses on stories. These two characteristics make our dataset a potentially valuable  contribution for the summarization community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Comparison to Other Summarization Datasets</head><p>We briefly compare TVSTORYSUM to three summarization datasets: CNNDM <ref type="bibr">(Hermann et al., 2015)</ref>, <ref type="bibr">XSum (Narayan et al., 2018), and</ref><ref type="bibr">MNews (Fabbri et al., 2019)</ref>. For TVSTORYSUM, we simply use the detailed episode recap as the source input and the summary as the target output and leave the integration of character descriptions to future work. We report n-gram overlap ratio (i.e., fraction of n-grams in the output stories that also appear in the source inputs) and length statistics in <ref type="table" target="#tab_1">Table 16</ref>. The n-gram overlap ratio is usually used as an indicator of the abstractiveness of a summarization dataset. Lower ratio indicates a higher degree of abstraction. CNNDM favors extractive approaches, whereas XSum is known for it is abstractiveness. We also compare to MNews because it shares similar input and output lengths as our dataset. As shown in the table, our dataset tends to be more abstractive. In addition, unlike other summarization datasets, our dataset focuses on stories. These two characteristics make our dataset a potentially valuable contribution for the summarization community. Comparison to other abstractive summarization datasets is in <ref type="table" target="#tab_1">Table 17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Summarization Results</head><p>We report BLEU and ROUGE scores for summarization in <ref type="table" target="#tab_1">Table 18</ref>. We report the performance of an extractive oracle where for each sentence in the reference summary, we pick a sentence in the detailed episode recap that maximizes the average of the three ROUGE scores compared against the summary sentence. While recent pretrained mod-els, such as Pegasus, have outperformed the oracle extractive approaches by a large margin on datasets with a high degree of abstractiveness (e.g., <ref type="bibr">XSum (Narayan et al., 2018)</ref>), the results in the table show that our dataset is still challenging for these pretrained models.</p><p>It is also interesting to see that while Pegasus is best for Fandom, it is not best on TVMegaSite. This may be because TVMegaSite has longer summaries than Fandom. Also, the performance of BART-base is comparable to that of BART-large on Fandom and is better than Pegasus and BART-large on TVMegaSite. This is likely because there is a limited amount of data with similar writing style in pretraining, resulting in little benefit of using larger models for this downstream task. We provide this abstractive summarization task to the community as a challenging dataset for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Details of Semantic Role Labeling</head><p>We use a pretrained model from <ref type="bibr" target="#b12">Shi and Lin (2019)</ref> to generate SRL tags of the detailed episode recaps. We eliminate the SRL tags for sentences that do not contain ARG0 or only contain pronouns to avoid ambiguity. For each sentence, we also only keep the SRL tags that correspond to the first verb that appears in the sentence to avoid the SRL tags being too specific, so there will be a balanced burden between the text-to-plot model and the plot-to-text model. In addition, following <ref type="bibr">Goldfarb-Tarrant et al. (2020)</ref>, we discard SRL tags of generic verbs.</p><p>The list of verbs we discard is as follows: "is", "was", "were", "are", "be", "'s", "'re", "'ll", "can", "could", "must", "may", "have to", "has to", "had to", "will", "would", "has", "have", "had", "do", "does", "did".</p><p>We also eliminate arguments that are longer than 5 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Quantifying Negations in Generated</head><p>Stories.</p><p>To quantify the observation that the models tend to generate sentences that use the word "but" to negate what has been said in the earlier part of the sentence, we compute the frequency of the word "but" per sentence for the reference stories, "oracle plot+summary", and "oracle plot+summary+oracle char. desc.". The results are 0.13, 0.53, and 0.24, respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8 Human Evaluation Instructions</head><p>In <ref type="table" target="#tab_1">Table 19</ref>, we show the instructions presented to the annotators. In particular, we show the annotators beginnings of two stories and a summary and ask them to choose which story is better for the two questions in <ref type="table" target="#tab_1">Table 19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.9 Generation Examples</head><p>We show generation examples in <ref type="table" target="#tab_2">Table 20</ref> and <ref type="table" target="#tab_1">Table 21</ref>.</p><p>Task Description We are going to ask you two questions to evaluate the relevancy and interestingness of beginnings of English stories given a brief summary. Question 1 Please choose which story you think is more relevant to the summary. By relevant, we mean specifically: Do they share similar topic, details or character names? Question 2</p><p>Please choose which story you think is more interesting. By interesting, we mean specifically: Does the story advance the plot smoothly instead of dwelling on each event separately? Does the story have interesting and reasonable details? Please disregard issues of grammaticality and fluency of the presented text. Elfman and Evergreen encounter a cliff as they run away from Rustyrose 's Belcusas the Thunderclap . Rustyrose appears shortly afterwards and expands on the idea of " The Ultimate World of Magic " , saying that all those who can not use Magic and the trash in the guilds are useless . However , Rustyrose outsmarts them despite their teamwork . He proceeds to explain the nature of his Magic , Arc of Embodiment , and then finishes the two off with Tower of Dingir . Meanwhile , Natsu awakes with his scarf and clothes reverted to normal by Wendy . Upon thanking Wendy , he picks up the scent of Zalty , the masked man he met back in Galuna Island . On the Magic Council warship , Doranbolt , formerly known as Mest Gryder , appears before Lahar who is ordering people to repair the damaged ship . Doranbolt reports to Lahar about the events on Tenrou Island Lahar warns him that Gran Doma , the Chairman , is very strict and would likely eliminate those who he believes are wrongdoers without a trial and he might resort to using Etherion once more . In the battle against Caprico , Loke tells the others to leave and look for Caprico 's teammates while he battles Caprico himself . At first , Lucy did n't want to leave . This action makes Caprico slightly recognize her . He discovers that Caprico 's Magic enables him to capture and summon humans and that Caprico has no master .</p><p>Elfman and Evergreen encounter a cliff as they run away from the danger of the Rustyrose 's attacks . Evergreen explains that " all those who use Magic " will use the power of the Magic of the Seven . However , Rustyrose outsmarts them , stating that their Magic is useless against them , as they have already been defeated . He then finishes the two with Tower of Dingling , then uses his Magic on his left arm , freeing Freed . Meanwhile , Natsu and Gray , both reverting back to normal by Wendy 's scarf , prepare to leave for Fairy Tail . After thanking Wendy , he realizes that he is the one who set up the destruction of the island , as he does not want to be a part of his family 's legacy . Meanwhile , Lahar appears from the sky , ordering people to repair the damaged ship , as the Celestial Spirit Mage appears before them . Afterwards , Doranbolt reports to Lahar about the destruction of the Heart Tree from the Onyrium Cascade . However , Lahar warns him that the Celestial Spirit Mage , who is known as Elfman , would have the power to kill him . and he is once more able to resort to using Etherion once more , once more using his Magic , but once again , Etherion is more powerful than ever . Loke then tells the others to leave Gray 's body in the Celestial Spirit Graveyard while he deals with Evergreen . At first , Lucy did n't want to leave , but then she did , and they started to have a friendly relationship . This action makes Capricorn slightly recognize her and makes the Magic Mage lose control of his Magic . . . which makes her lose her Magic . Capricorn 's Magic enables him and humans to capture and summon humans , stating that he has no choice . Lily wants to get rid of Jenny 's old clothes , including one of the dresses she made , but Jenny insists that all of her clothes fit her perfectly and wants to keep them . At The Empire , Chuck tells Blair how the hotel is doing in terms of bookings . Blair says that if he opens the club , it will bring in business and make bookings go up . She suggests an 80s themed party but Chuck shoots down the idea . Blair , suspicious that Chuck is still angry over her lying to him to get the NYU freshman toast ( in Enough About Eve ) , tells him that she apologized and they should move on . He explains that he needs to do things his way and leaves to meet with his accountant . When he 's gone , Blair calls a party planner to plan the opening of the club . At the loft , Nate has brought Dan the Endless Knights trilogy starring Olivia . He tells Dan about how Patrick Roberts , who plays her boyfriend in the movies , was actually her boyfriend in real life . Dan seems unfazed , until he gets a Gossip Girl blast with a picture of Olivia getting some condoms . She 's annoyed because Serena did n't break up Olivia and Dan like she was asked to . KC explains that Olivia needs to be with Patrick , because otherwise he 's headed for a serious career stall and becoming irrelevant . She instructs Serena again to break the two up , or else she 'll be stuck running pointless errands for the rest of her life . On the way to school , Eric and Jenny discuss Halloween . Jenny tells him they have to find a party or else risk handing out candy with Rufus , when they run into her minions . Eric goes off to find Jonathan and Jenny begins bossing her minions around . She tells them to move , not wanting to risk looking weak to her minions , and they do . As they walk off , Jonathan tells Eric that she 's changing . Eric defends Jenny , saying it 's only a mask for school and Jonathan Back at the loft , Nate and Dan are watching the Endless Knights trilogy . Dan watches one of the sex scenes and begins to get uncomfortable . Nate asks if he 's okay , and Dan tries to play it cool when Olivia calls . He pretends to be sick to buy some time to finish the films and think things over . Blair goes to see Serena at KCs office . She tells her that she already hired a party planner , but Serena is confused since Chuck wanted to wait until after the holidays to open the club ... Jenny wants to keep them , and all of her friends get out of the way to get her out of there . At The Empire , Chuck tells Blair how Serena is coming back to town in Los Angeles to see her ex . Blair says that if he opens the club , it 's an opening opportunity , and Chuck says he will . She suggests an 80s themed party , but Chuck is not interested , as she 's taking a break from the party . Afterwards , she tells him that she knows about her lying to him over the phone and that she is n't going to give up . He tells her that he needs to do things his way , and she agrees to do it the next day before he leaves . Back at the VDW 's , Blair calls a party planner to tell everyone that she 's going to the party . At the loft , Nate has brought Dan everything he 's ever wanted to know about Rufus and Jenny . He tells Dan how Serena broke up with him at the bar , and that she 's still in love with him . Dan replies that he gets a Gossip Girl blast every day , and that 's why she 's so obsessed with The Spectator . Serena threatens to break up Olivia and Dan if she does n't break up with him at some point . At the VDW 's , Olivia needs to be with Patrick because she feels like he needs her for something . She instructs Serena to break the two up , and once she does , the two begin to break up . On the way back to school , Eric and Jenny discuss Halloween and agree that it 's the perfect way to celebrate . Jenny tells him they have to figure out a way to get through the party , and Rufus asks where they 're going . Eric goes off to find Jonathan , and he 's surprised to find her at the table with all of her friends . She tells them to move , and they do , and Eric tells her not to do anything stupid and to do what 's right . At The Palace , Jonathan tells Eric that she 's changing , and she tells him that they should move in together . Eric defends Jenny , saying it 's only a matter of time before he 's exiled , and Serena says it is . Back at the loft , Nate and Dan are watching the Endless Knightsnorketomy and discuss Dan 's plans for the night . Dan watches one of the sex scenes and begins to get jealous of Chuck 's lack of sex appeal , but Dan breaks up with him and leaves . Nate calls and asks if he 's okay , and Rufus says it 's fine if he leaves . He pretends to be sick and she goes over to ask if he needs to spend the time with her and Rufus ... <ref type="table" target="#tab_1">Table 21</ref>: Part of the generated text for the second examples. Due to space constraints, we only show the beginnings of the generated stories and the reference story.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For example, for Figure 1 the inputs are SEP 0 Troy McClure's ... 0 SEP 1 Selma Bouvier SEP 1 Selma's favorite film ... 1 SEP 2 ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, constructing knowledge bases(Chu et al., 2021), summarizing TV show screenplays(Chen et al., 2022)  and other textual data<ref type="bibr" target="#b19">(Yu et al., 2016)</ref>, entity tracking (Chen and Choi, 2016; Choi and Chen, 2018), entity linking(Logeswaran et al., 2019), coreference resolution<ref type="bibr" target="#b5">(Chen et al., 2017;</ref><ref type="bibr" target="#b23">Zhou and Choi, 2018)</ref>, question answering(Ma et al., 2018;<ref type="bibr" target="#b16">Yang and Choi, 2019)</ref>, speaker identification(Ma et al., 2017), sarcasm detection(Joshi et al., 2016), emotion detection<ref type="bibr" target="#b20">(Zahiri and Choi, 2017;</ref> Hsu and Ku, 2018), and character relation extraction<ref type="bibr" target="#b18">(Yu et al., 2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>he had met when she gave him an eye test at the Department of Motor Vehicles. This revived his career, leading him to star in Stop the Planet of the Apes, I Want to Get Off!, a musical version of the film Planet of the Apes ? Troy McClureWashed-up actor Troy McClure gets pulled over for erratic driving, due to the fact he's driving without his corrective lenses. When Troy goes to the DMV to get his license changed so that he won't be required to wear glasses anymore, he offers to take Selma Bouvier to dinner if she will let him pass the eye test.</figDesc><table><row><cell>Selma Bouvier</cell></row><row><cell>== Introduction == Troy McClure (born May 8th, 1955) is a cheesy B-movie actor who had fallen on hard times ? == Biography == ? To cover this up, he began a relationship with Selma Bouvier, whom After dinner at the Pimento Grove, photographers notice Troy leaving with a human woman (rumors about a romantic abnormality initially destroyed Troy's career). The next day, Troy's agent calls and says that he can get work again if he continues seeing human women. Troy continues dating Selma to help his career. On his agent 's advice, Troy asks Selma to marry him. Troy gets a part in 'Planet of the Apes' the musical ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc># stories # tokens per story # char. per story # total unique char. # tokens per char. desc. Statistics for story generation datasets with prompts as part of inputs. TVSTORYGEN has moderate numbers of stories, moderate lengths of stories, long character descriptions, and a large number of total unique characters and characters per story.</figDesc><table><row><cell>ROCStories</cell><cell>98.2k</cell><cell></cell><cell>88.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WritingPrompts</cell><cell>303.4k</cell><cell cols="2">735.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STORIUM</cell><cell>5.7k</cell><cell cols="2">19.3k</cell><cell>4.6</cell><cell>26.2k</cell><cell>269.0</cell></row><row><cell>TVSTORYGEN</cell><cell>29.0k</cell><cell cols="2">1868.7</cell><cell>16.7</cell><cell>34.3k</cell><cell>1553.4</cell></row><row><cell></cell><cell></cell><cell>FD</cell><cell>TMS</cell><cell></cell><cell></cell></row><row><cell cols="2">avg. # tokens in summaries</cell><cell>56.7</cell><cell>366.6</cell><cell></cell><cell></cell></row><row><cell cols="2">avg. # tokens in detailed recaps</cell><cell cols="2">1291.7 2375.3</cell><cell></cell><cell></cell></row><row><cell cols="2">avg. # tokens in char. desc. per char.</cell><cell cols="2">702.7 2300.9</cell><cell></cell><cell></cell></row><row><cell cols="4">avg. # tokens in char. desc. per ep. 10891.9 40956.0</cell><cell></cell><cell></cell></row><row><cell>avg. # characters</cell><cell></cell><cell>15.5</cell><cell>17.8</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics for TVSTORYGEN. See the appendix for more details.</figDesc><table><row><cell>Fandom</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell># shows</cell><cell>106</cell><cell>104</cell><cell>106</cell></row><row><cell># episodes</cell><cell cols="3">10833 1320 1430</cell></row><row><cell>TVMegaSite</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell># shows</cell><cell>7</cell><cell>7</cell><cell>9</cell></row><row><cell># episodes</cell><cell cols="3">11586 1452 2392</cell></row><row><cell>We note that the character descriptions for TMS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>also come from FD. Considering the differences,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>we train and evaluate models on the two splits sep-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>arately in experiments. Since the summaries in</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fandom are shorter and likely only depict the se-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tups of the detailed recaps, we conduct a human</cell><cell></cell><cell></cell><cell></cell></row><row><cell>evaluation to check the fraction of setups in the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>summaries, finding that 61.7% of the summaries</cell><cell></cell><cell></cell><cell></cell></row><row><cell>are setups. 5</cell><cell></cell><cell></cell><cell></cell></row></table><note>5 We sample 60 episodes with 2 episodes per show.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Statistics of train/dev/test splits for Fandom and TVMegaSite.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>set results (NN) Nearest neighbour plot + summary + char. desc. Oracle plot + summary + oracle char. desc.</figDesc><table><row><cell></cell><cell>7.1</cell><cell>40.7</cell><cell>11.0</cell><cell>39.6</cell><cell>32.3</cell></row><row><cell>(NN) Oracle plot</cell><cell>21.2</cell><cell>52.8</cell><cell>24.1</cell><cell>51.8</cell><cell>23.1</cell></row><row><cell>(NN) Oracle plot + summary</cell><cell>24.5</cell><cell>54.3</cell><cell>25.6</cell><cell>55.2</cell><cell>20.8</cell></row><row><cell cols="2">(NN) 28.4</cell><cell>63.0</cell><cell>32.8</cell><cell>61.2</cell><cell>17.9</cell></row><row><cell cols="2">Test set results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Return) reference</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>12.9</cell></row><row><cell>(Return) oracle plot</cell><cell>3.6</cell><cell>43.9</cell><cell>19.7</cell><cell>41.9</cell><cell>-</cell></row><row><cell>(Return) oracle plot + summary</cell><cell>5.4</cell><cell>48.5</cell><cell>20.5</cell><cell>46.2</cell><cell>-</cell></row><row><cell>(Return) oracle plot + oracle char. desc.</cell><cell>1.2</cell><cell>11.0</cell><cell>4.6</cell><cell>10.6</cell><cell>-</cell></row><row><cell>(Return) oracle plot + oracle char. desc. + summary</cell><cell>1.2</cell><cell>11.0</cell><cell>4.7</cell><cell>10.6</cell><cell>-</cell></row><row><cell>(Return) Nearest neighbour detailed recap</cell><cell>5.1</cell><cell>41.1</cell><cell>9.3</cell><cell>39.6</cell><cell>31.2</cell></row><row><cell>(Return) Oracle nearest neighbour detailed recap</cell><cell>4.8</cell><cell>41.2</cell><cell>10.8</cell><cell>39.9</cell><cell>28.5</cell></row><row><cell>(NN) Nearest neighbour plot + summary + char. desc.</cell><cell>6.0</cell><cell>41.7</cell><cell>10.7</cell><cell>40.3</cell><cell>28.0</cell></row><row><cell>(NN) Oracle plot + summary + oracle char. desc.</cell><cell>28.4</cell><cell>63.2</cell><cell>32.9</cell><cell>61.5</cell><cell>18.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Blair anywhere near the planning , and he wants her to be at the party for the night ...</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>He then finishes the two with</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Tower of Dingling ..</cell></row><row><cell>Chuck is preparing for his</cell><cell>... Blair is confident her</cell><cell>... Chuck tells Blair that he</cell><cell>... Blair goes to see Serena at</cell></row><row><cell>new club opening and en-</cell><cell>plan will work , until</cell><cell>'s not going to let her go ,</cell><cell>the gallery , and she explains to</cell></row><row><cell>lists Serena 's help , but</cell><cell>Chuck calls and asks for</cell><cell>but that she 's going to be</cell><cell>her that she went to see Chuck</cell></row><row><cell>Blair begins to feel left out</cell><cell>Serena 's help in conduct-</cell><cell>there for him . He he opens</cell><cell>. She tells her that she hired a</cell></row><row><cell>. Jenny , the new Queen</cell><cell>ing the club opening . He</cell><cell>the club and tells her that he</cell><cell>party planner to help her hang</cell></row><row><cell>at Constance , struggles be-</cell><cell>tells her he wants to open</cell><cell>'s going to see Serena at the</cell><cell>out with Chuck , and Serena of-</cell></row><row><cell>tween proving herself and</cell><cell>the next day on Halloween</cell><cell>loft , where she 's staying</cell><cell>fers to help . At the VDW 's</cell></row><row><cell>her friendship with Eric ,</cell><cell>and that he does n't want</cell><cell>. She suggests an 80-year</cell><cell>, Serena is conducting the club</cell></row><row><cell>and Dan feels inferior after</cell><cell>Blair anywhere near the</cell><cell>party party , but she says</cell><cell>opening and Blair asks her to</cell></row><row><cell>watching one of Olivia 's</cell><cell>planning ...</cell><cell>that 's not what she 's look-</cell><cell>help . He tells her that he does</cell></row><row><cell>movies . Meanwhile , Lily</cell><cell></cell><cell>ing for ...</cell><cell>n't want</cell></row><row><cell>tries to respect Rufus ' Hal-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>loween traditions .</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Elizabeth Clark, Yangfeng Ji, and Noah A. Smith. 2018. Neural text generation in stories using entity representations as context. In Proceedings of the 2018 Conference of the North American Chapter of the Jennifer Garst, and Timothy C Brock. 2003. The power of fiction: Determinants and boundaries. In The psychology of entertainment media, pages 169-184. Erlbaum Psych Press. Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A knowledge-enhanced pretraining model for commonsense story generation. Catherine Xiao, and Jinho D. Choi. 2017. Text-based speaker identification on multiparty dialogues using multi-document convolutional neural networks. In Proceedings of ACL 2017, Student Research Workshop, pages 49-55, Vancouver, Canada. Association for Computational Linguistics. Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In International Conference on Learning Representations.</figDesc><table><row><cell>and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Informa-tion Processing Systems, volume 28. Curran Asso-James R Meehan. 1977. Tale-spin, an interactive pro-ciates, Inc. telligence, volume 32. Proceedings of the AAAI Conference on Artificial In-mated story generation with deep neural nets. In Melanie C Green, Transactions of the Association for Computational Linguistics, 8:93-108. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Mark Riedl. 2018. Event representations for auto-William Hancock, Shruti Singh, Brent Harrison, and Lara Martin, Prithviraj Ammanabrolu, Xinyu Wang, chine Learning Research, pages 3929-3938. PMLR. sociation for Computational Linguistics. chine Learning, volume 119 of Proceedings of Ma-IJCNLP), pages 5988-5993, Hong Kong, China. As-ings of the 37th International Conference on Ma-ence on Natural Language Processing (EMNLP-mented language model pre-training. In Proceed-Processing and the 9th International Joint Confer-supat, and Mingwei Chang. 2020. Retrieval aug-ference on Empirical Methods in Natural Language Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-Kaixin Ma, Huanru Henry Mao, Bodhisattwa Prasad Majumder, Julian McAuley, and Garrison Cottrell. 2019. Im-proving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Con-</cell><cell>Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2250-2260, New Orleans, Louisiana. Associ-ation for Computational Linguistics. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Na-zli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long docu-ments. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-nologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computa-tional Linguistics. Ming-Wei Chang, Dipanjan Das, and William Co-hen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Conference on Artificial Intelligence, volume 27. sourced plot graphs. In Proceedings of the AAAI Mark Riedl. 2013. Story generation with crowd-Boyang Li, Stephen Lee-Urban, George Johnston, and Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, for Computational Linguistics. Linguistics, pages 7871-7880, Online. Association nual Meeting of the Association for Computational and comprehension. In Proceedings of the 58th An-training for natural language generation, translation, 2020. BART: Denoising sequence-to-sequence pre-Levy, Veselin Stoyanov, and Luke Zettlemoyer. jan Ghazvininejad, Abdelrahman Mohamed, Omer and Punta Cana, Dominican Republic. Association for Computational Linguistics. Misha Khalman, Yao Zhao, and Mohammad Saleh. 2021. ForumSum: A multi-speaker conversation summarization dataset. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2021, pages 4592-4599, Punta Cana, Dominican Republic. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Mar-Romain Paulus, Nanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. 2018. Towards controllable story generation. In Proceedings of the First Workshop on Storytelling, pages 43-49, New Orleans, Louisiana. Association for Computational Linguistics.</cell></row><row><cell>90-100, Los Angeles. Association for Computational Linguistics. Jinho D. Choi and Henry Y. Chen. 2018. SemEval gram that writes stories. In Ijcai, volume 77, page 9198. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Chao-Chun Hsu and Lun-Wei Ku. 2018. SocialNLP 2018 EmotionX challenge overview: Recognizing emotions in dialogues. In Proceedings of the Sixth International Workshop on Natural Language Pro-cessing for Social Media, pages 27-31, Melbourne, Australia. Association for Computational Linguis-tics. Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguis-tics. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, pages 1419-1436, On-line. Association for Computational Linguistics. Harsh Jhamtani and Taylor Berg-Kirkpatrick. 2020. topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-guage Processing, pages 1797-1807, Brussels, Bel-gium. Association for Computational Linguistics. Narrative text generation with a latent discrete plan. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3637-3650, On-Keith Oatley. 1999. Why fiction may be twice as true as fact: Fiction as cognitive and emotional simula-tion. Review of general psychology, 3(2):101-117. line. Association for Computational Linguistics. Eyal Orbach and Yoav Goldberg. 2020. Facts2Story: Aditya Joshi, Vaibhav Tripathi, Pushpak Bhat-Controlling text generation by key facts. In Proceed-tacharyya, and Mark J. Carman. 2016. Harnessing ings of the 28th International Conference on Com-sequence labeling for sarcasm detection in dialogue putational Linguistics, pages 2329-2345, Barcelona, 2018 task 4: Character identification on multiparty dialogues. In Proceedings of The 12th Interna-tional Workshop on Semantic Evaluation, pages 57-from TV series 'Friends'. In Proceedings of The Spain (Online). International Committee on Compu-ral Language Learning, pages 146-155, Berlin, Ger-20th SIGNLL Conference on Computational Natu-tational Linguistics.</cell><cell>Computational Linguistics, pages 4884-4895, Flo-rence, Italy. Association for Computational Linguis-Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tics. Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstrac-tive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computa-tional Linguistics, pages 1074-1084, Florence, Italy. Association for Computational Linguistics. Danyang Liu, Juntao Li, Meng-Hsuan Yu, Ziming Huang, Gongshen Liu, Dongyan Zhao, and Rui Yan. 2020. A character-centric neural model for auto-mated story generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1725-1732. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-erarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Hugo Liu and Push Singh. 2002. Makebelieve: Us-ing commonsense knowledge to generate stories. In AAAI/IAAI, pages 957-958. pages 889-898, Melbourne, Australia. Association for Computational Linguistics. Angela Fan, Mike Lewis, and Yann Dauphin. 2019. Strategies for structuring story generation. In Pro-ceedings of the 57th Annual Meeting of the Asso-Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap-proach. ciation for Computational Linguistics, pages 2650-2660, Florence, Italy. Association for Computa-tional Linguistics. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-shot entity linking by reading entity de-scriptions. In Proceedings of the 57th Annual Meet-Aleksander Wawer. 2019. SAMSum corpus: A ing of the Association for Computational Linguistics, human-annotated dialogue dataset for abstractive pages 3449-3460, Florence, Italy. Association for summarization. In Proceedings of the 2nd Workshop Computational Linguistics. on New Frontiers in Summarization, pages 70-79, Hong Kong, China. Association for Computational Challenging reading comprehension on daily conver-Linguistics. Kaixin Ma, Tomasz Jurczyk, and Jinho D. Choi. 2018.</cell></row><row><cell>64, New Orleans, Louisiana. Association for Com-many. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-</cell><cell>Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph sation: Passage completion on multiparty dialog. In</cell></row><row><cell>putational Linguistics. Jing Zhu. 2002. Bleu: a method for automatic eval-</cell><cell>Weischedel, and Nanyun Peng. 2020. Content plan-Proceedings of the 2018 Conference of the North</cell></row><row><cell>uation of machine translation. In Proceedings of</cell><cell>ning for neural story generation with aristotelian American Chapter of the Association for Computa-</cell></row><row><cell>the 40th Annual Meeting of the Association for Com-</cell><cell>rescoring. In Proceedings of the 2020 Conference tional Linguistics: Human Language Technologies,</cell></row><row><cell>putational Linguistics, pages 311-318, Philadelphia,</cell><cell>on Empirical Methods in Natural Language Process-Volume 1 (Long Papers), pages 2039-2048, New</cell></row><row><cell>Pennsylvania, USA. Association for Computational</cell><cell>ing (EMNLP), pages 4319-4338, Online. Associa-Orleans, Louisiana. Association for Computational</cell></row><row><cell>Linguistics.</cell><cell>tion for Computational Linguistics. Linguistics.</cell></row></table><note>Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022. SummScreen: A dataset for ab- stractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602-8615, Dublin, Ireland. Association for Computational Linguistics. Yu-Hsin Chen and Jinho D. Choi. 2016. Character identification on multiparty conversation: Identify- ing mentions of characters in TV shows. In Proceed- ings of the 17th Annual Meeting of the Special In- terest Group on Discourse and Dialogue, pagesCuong Xuan Chu, Simon Razniewski, and Gerhard Weikum. 2021. Knowfi: Knowledge extraction from long fictional texts. In 3rd Conference on Automated Knowledge Base Construction.Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text de- generation. In International Conference on Learn- ing Representations.Marzena Karpinska, Nader Akoury, and Mohit Iyyer. 2021. The perils of using Mechanical Turk to eval- uate open-ended text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, pages 1265-1285, Online</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Character Descriptions for Selma Bouvier from the TV show "The Simpsons". We omit part of the text in each section for brevity. The numbers at the end of each section text are the number of omitted word tokens.</figDesc><table><row><cell></cell><cell>FD</cell><cell>TMS</cell></row><row><cell>number of shows</cell><cell>106</cell><cell>9</cell></row><row><cell>number of episodes</cell><cell>13583</cell><cell>15430</cell></row><row><cell>min. # episodes per show</cell><cell>2</cell><cell>17</cell></row><row><cell>max. # episodes per show</cell><cell>574</cell><cell>2665</cell></row><row><cell>median # episodes per show</cell><cell>14.0</cell><cell>300.0</cell></row><row><cell>avg. # episodes per show</cell><cell>43.0</cell><cell>670.9</cell></row><row><cell>avg. # tokens in summaries</cell><cell>56.7</cell><cell>366.6</cell></row><row><cell cols="3">avg. # tokens in detailed recaps 1291.7 2375.3</cell></row><row><cell>avg. # tokens in char. desc.</cell><cell cols="2">702.7 2300.9</cell></row><row><cell>avg. # characters</cell><cell>15.5</cell><cell>17.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Dataset statistics for TVSTORYGEN in Table 13, Table 14, and Table 15. 12 While TVMegaSite has a relatively small number of genres, Fandom covers far more genres with the most frequent occupying only 15% of episodes.</figDesc><table><row><cell></cell><cell>uni.</cell><cell>bi.</cell><cell cols="2">tri. four.</cell></row><row><cell cols="3">TVSTORYGEN (Fandom)</cell><cell></cell><cell></cell></row><row><cell>summ.</cell><cell>34.3</cell><cell>3.4</cell><cell>0.8</cell><cell>0.3</cell></row><row><cell>char. desc.</cell><cell cols="3">88.1 48.3 16.7</cell><cell>6.2</cell></row><row><cell>char. desc. \ summ.</cell><cell cols="3">54.3 45.4 16.3</cell><cell>6.1</cell></row><row><cell>summ. \ char. desc.</cell><cell>0.5</cell><cell>0.6</cell><cell>0.4</cell><cell>0.2</cell></row><row><cell cols="4">char. desc. ? summ. 88.7 48.9 17.1</cell><cell>7.4</cell></row><row><cell cols="4">TVSTORYGEN (TVMegaSite)</cell><cell></cell></row><row><cell>summ.</cell><cell cols="2">61.7 14.7</cell><cell>3.0</cell><cell>1.2</cell></row><row><cell>char. desc.</cell><cell cols="3">93.4 56.9 17.3</cell><cell>3.2</cell></row><row><cell>char. desc. \ summ.</cell><cell cols="3">32.7 44.2 16.1</cell><cell>3.1</cell></row><row><cell>summ. \ char. desc.</cell><cell>0.9</cell><cell>2.0</cell><cell>1.8</cell><cell>1.0</cell></row><row><cell cols="4">char. desc. ? summ. 94.3 58.9 19.1</cell><cell>4.2</cell></row><row><cell>STORIUM</cell><cell cols="2">72.5 24.7</cell><cell>5.4</cell><cell>1.2</cell></row><row><cell>B Decoding</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>For both story generation and summarization, we</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>use a batch size of 200, beam search of size 5 with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>n-gram blocking (Paulus et al., 2018) where prob-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Genres for TV shows in Fandom (left) and TVMegaSite (right).</figDesc><table><row><cell>Genre</cell><cell>Count (Fraction)</cell></row><row><cell>Action</cell><cell>5385 (15.0%)</cell></row><row><cell>Comedy</cell><cell>5214 (14.5%)</cell></row><row><cell>Drama</cell><cell>4793 (13.4%)</cell></row><row><cell>Adventure</cell><cell>4220 (11.8%)</cell></row><row><cell>Children</cell><cell>3280 (9.1%)</cell></row><row><cell>Science-Fiction</cell><cell>2524 (7.0%)</cell></row><row><cell>Anime</cell><cell>2239 (6.2%)</cell></row><row><cell>Fantasy</cell><cell>1949 (5.4%)</cell></row><row><cell>Romance</cell><cell>1408 (3.9%)</cell></row><row><cell>Family</cell><cell>1355 (3.8%)</cell></row><row><cell>Crime</cell><cell>1139 (3.2%)</cell></row><row><cell>Supernatural</cell><cell>729 (2.0%)</cell></row><row><cell>Medical</cell><cell>480 (1.3%)</cell></row><row><cell>Horror</cell><cell>309 (0.9%)</cell></row><row><cell>Mystery</cell><cell>299 (0.8%)</cell></row><row><cell>Thriller</cell><cell>240 (0.7%)</cell></row><row><cell>Music</cell><cell>220 (0.6%)</cell></row><row><cell>History</cell><cell>47 (0.1%)</cell></row><row><cell>Legal</cell><cell>19 (0.1%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Genres in Fandom and their corresponding numbers and percentages of episodes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Genres in TVMegaSite and their corresponding numbers and percentages of episodes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16</head><label>16</label><figDesc></figDesc><table><row><cell>: Fraction (%) of n-grams in the output sum-</cell></row><row><cell>maries that also appear in the inputs, and the average</cell></row><row><cell>numbers of tokens for the inputs and outputs. Datasets</cell></row><row><cell>with smaller fractions of overlapping n-grams tend to</cell></row><row><cell>favor abstractive summarization approaches. Results</cell></row><row><cell>marked by  ? and  ? are from Narayan et al. (2018) and</cell></row><row><cell>Fabbri et al. (2019) respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>Statistics for datasets focusing on abstractive summarization for long-form text or dialogue. The numbers are averaged over instances.</figDesc><table><row><cell></cell><cell>BL</cell><cell>R1</cell><cell>R2</cell><cell>R3</cell></row><row><cell></cell><cell>Fandom</cell><cell></cell><cell></cell></row><row><cell>Extractive oracle</cell><cell cols="4">8.3 37.0 11.3 30.9</cell></row><row><cell>BART-base</cell><cell cols="2">5.2 31.2</cell><cell cols="2">7.3 25.5</cell></row><row><cell>BART-large</cell><cell cols="2">5.4 30.7</cell><cell cols="2">7.6 25.3</cell></row><row><cell>Pegasus</cell><cell cols="2">5.7 31.3</cell><cell cols="2">7.7 25.6</cell></row><row><cell></cell><cell cols="2">TVMegaSite</cell><cell></cell></row><row><cell cols="5">Extractive oracle 16.9 55.8 20.9 53.6</cell></row><row><cell>BART-base</cell><cell cols="4">8.3 43.8 12.6 42.3</cell></row><row><cell>BART-large</cell><cell cols="4">8.1 43.2 12.3 41.8</cell></row><row><cell>Pegasus</cell><cell cols="4">7.7 43.5 12.6 42.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 18 :</head><label>18</label><figDesc>TVSTORYSUM test results for summarizing detailed episode recaps. The best result in each column for each domain (excluding the oracle) is boldfaced.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 19 :</head><label>19</label><figDesc>Instructions to crowd-workers on Amazon Mechanical Turk.</figDesc><table><row><cell>Summary</cell><cell>Reference</cell><cell>Oracle plot + summ. + oracle char. desc.</cell></row><row><cell>Elfman and Evergreen , after much</cell><cell></cell><cell></cell></row><row><cell>struggle , lose to Rustyrose and his</cell><cell></cell><cell></cell></row><row><cell>imagination -based Magic . In</cell><cell></cell><cell></cell></row><row><cell>the meantime , Gray , Lucy , Cana</cell><cell></cell><cell></cell></row><row><cell>and Loke have been overpowered by</cell><cell></cell><cell></cell></row><row><cell>Caprico alone . Loke decides to take</cell><cell></cell><cell></cell></row><row><cell>him on by himself because of his mys-</cell><cell></cell><cell></cell></row><row><cell>terious Magic .</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 20 :</head><label>20</label><figDesc>Complete generation for the first examples. Reference Oracle plot + summ. + oracle char. desc. Chuck is preparing for his new club opening and enlists Serena 's help , but Blair begins to feel left out . Jenny , the new Queen at Constance , struggles between proving herself and her friendship with Eric , and Dan feels inferior after watching one of Olivia 's movies . Meanwhile , Lily tries to respect Rufus ' Halloween traditions .</figDesc><table><row><cell>Summary</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.fandom.com/ 3 http://tvmegasite.net/ 4 Data from Fandom is available under Creative Commons licenses and we have received permission from the owners of tvmegasite.net to publish their data for use with attribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We find the plots generated by neural models to be of lower quality.7  We chose RoBERTa over GPT-2 (Radford et al., 2019) because BERT-style models outperform GPT-2 in the encoderdecoder setting in the results reported byLiu et al. (2019).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">To ensure annotation quality, we hire workers with master qualification and pay them with a target hourly wage of $12. Details on annotation instructions are in the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://www.upwork.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We label a story as unfinished if it has no completion date.11  We use string matching to detect the occurrences of characters as in the way we construct our dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">The category information is from TVMaze (https: //www.tvmaze.com/) where a show may correspond to multiple genres.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank David Bamman and Matthew Sims for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">STO-RIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nader</forename><surname>Akoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The role of emotion in believable agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="122" to="125" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04426</idno>
		<title level="m">Improving language models by retrieving from trillions of tokens</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Historical fiction or fictionalized history? The ALAN Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust coreference resolution and entity linking on dialogues: Character identification on TV show transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PlotMachines: Outlineconditioned generation with dynamic plot state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.349</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4274" to="4295" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Narrative planning: Balancing plot and character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">Michael</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="217" to="268" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Stephen Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging pre-trained checkpoints for sequence generation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00313</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Simple bert models for relation extraction and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DESCGEN: A distantly supervised datasetfor generating entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.35</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="415" to="427" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural network-based abstract generation for opinions and arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FriendsQA: Open-domain question answering on TV show transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5923</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Planand-write: Towards better automatic storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7378" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dialogue-based relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.444</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4927" to="4940" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised text recap extraction for TV series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Emotion detection on TV show transcripts with sequencebased convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Zahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">QMSum: A new benchmark for querybased multi-domain meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutethia</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5905" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">They exist! introducing plural mentions to coreference resolution and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Section Description Introduction Selma Bouvier-Terwilliger-Hutz-McClure-Discoth?que-Simpson-D&apos;Amico (n?e Bouvier) is one of Marge&apos;s older chain-smoking twin sisters. She works at the DMV and possesses a strong dislike for her brother-in-law, Homer, although on extremely rare occasions she shows kindness towards him and seems to tolerate him</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.474</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Patty Bouvier</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5927" to="5934" />
		</imprint>
	</monogr>
	<note>MediaSum: A large-scale media interview dataset for dialogue summarization. She seems to despise Homer slightly less than her twin sister</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">They have a strong, mutual (and reciprocated) dislike for their brother-in-law. Selma and Patty are shown to be older than Homer and Marge, but a birth date has not been given. It is presumed they are in their mid to late 40s since Selma has gone through menopause and the twins are shown as preteens or teenagers when Marge is around Lisa&apos;s age. She enjoys getting foot massages as she is shown getting them constantly. She weighs 168 pounds exact and is considered overweight like Patty, though they are not as fat as Homer. As teenagers and children, they are average weight, while Homer was still fat... (917 tokens) Physical Appearance She is tall and overweight with a similar body type to Patty. She wears a blue sleeveless dress and round earrings. Her hair is long and curly, worn in an m-shape (although she wore different hairstyles when she was younger, it&apos;s always been longer than Patty&apos;s). She is actually a blonde, in contrast to Patty&apos;s red hair, meaning that they are in fact fraternal twins. The gray coloring is from cigarette ashes. On special occasions, Selma will wear earrings in the form of the letter &quot;S&quot;, further distinguishing her from her twin, who wears triangle earrings ... (136 tokens) Relationships Despite being twins, Patty and Selma have very different track records when it comes to finding dates. According to Marge, Patty chose a life of celibacy (possibly because she&apos;s a lesbian), while Selma had celibacy thrust upon her. Selma is a heterosexual. Her standards are extremely low</title>
	</analytic>
	<monogr>
		<title level="m">Selma Bouvier was born two minutes before Patty. Due to a childhood bottle rocket accident, she lost her sense of taste and smell. Selma is 43 years old and is four years older than Marge</title>
		<imprint/>
	</monogr>
	<note>Homer places a ban on smoking, so instead, they smoke electric cigarettes, which they hate. Selma has presumably switched to chewing tobacco after adopting Ling, although the episode &quot;Puffless&quot; disproves this. Single, eh? Well, he passes the Selma Test,&quot; despite him being old enough to be her great-grandfather. Oddly, she was grossed out when Hans Moleman tried to kiss her ... (676 tokens</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selma threatens to stuff Edna Krabappel&apos;s hat down her throat if she catches the bouquet at &quot;Lisa&apos;s Wedding&quot;. This episode is not canon since it aired before Edna&apos;s death and in real time, Edna would not have been at the wedding</title>
	</analytic>
	<monogr>
		<title level="m">Non-canon Appearance At age 55</title>
		<imprint/>
	</monogr>
	<note>Holidays of Future Passed&quot;, at age 72, she has a love-bot, who runs away with Patty&apos;s love bot. much to their annoyance ... (697 tokens</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
