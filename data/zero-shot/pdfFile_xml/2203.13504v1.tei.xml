<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaijing</forename><surname>Li</surname></persName>
							<email>lizaijing@csu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiao</forename><surname>Tang</surname></persName>
							<email>tangfengxiao@csu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhao</surname></persName>
							<email>meanzhao@csu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusen</forename><surname>Zhu</surname></persName>
							<email>zhu_yusen@163.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion recognition in conversation (ERC) aims to analyze the speaker's state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multimodal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multimodal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called Emo-Caps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion recognition in conversation (ERC) is a work that recognizes the speaker's emotion and its influencing factors in the process of conversation. Nowadays, social media such as Facebook and Twitter generate a large amount of dialogue data with various modalities of textual, audio, and video all the time. The study of speaker emotional tendency has huge potential value in the fields of public opinion analysis, shopping, and consumption. Therefore, conversation emotion recognition has attracted more and more attention from researchers and companies.</p><p>In ERC, existing research mainly focuses on the way of contextual information modeling <ref type="bibr" target="#b8">Ghosal et al., 2019)</ref>. However, These models have some shortcomings due to their inability to better extract the grammatical and semantic information of the utterance. Recent studies <ref type="bibr" target="#b18">(Yuzhao Mao et al., 2020</ref>; Weizhou Shen et al., 2020) have introduced the transformer structure into the utterance feature extraction to solve the above problems. <ref type="bibr" target="#b15">Li et al. (2021)</ref> proposed a new expression vector, "emotion vector" for ERC, which is obtained by mapping from sentence vector, but only for textual modality. Meanwhile, existing studies <ref type="bibr" target="#b23">(Song et al., 2004;</ref><ref type="bibr" target="#b4">Dellaert et al., 1996;</ref><ref type="bibr" target="#b0">Amir, 1998)</ref> have shown that only textual information is not enough for emotional presentation, the tone and intonation reflect the speaker's emotions to a certain extent, and the facial expressions also express the inner feelings of the speaker in most cases. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, different modalities contain different information, and all are slightly flawed, so multi-modal based information can better identify the speaker's emotion than a single modality in ERC.</p><p>In order to identify the speaker's emotion in conversation effectively, it is necessary to obtain good utterance features. Also we can't ignore the role of the utterance's emotional tendency. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the emotional tendency of the utterance itself is like an "offset vector", which makes the neutral utterance have an "emotional direction". For single-sentence emotion classification, emotional tendency is consistent with the results of emotion recognition, while in ERC, the influence of the context may cause the emotional tendency to be inconsistent with the result of emotion recognition. However, emotional tendency can provide features for the model so that model can "understand" the reason for emotional reversal.</p><p>So, we propose a new multi-modal emotional tendency extraction method called Emoformer, which is a Transformer-based model but doesn't include the decoder part. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, Emoformer extracts the emotional tendency, i.e., emotion vector, from the modal features through the multi-head self-attention layer and feed-forward layer. More details we will analysis in Section 3.</p><p>Based on the Emoformer, we further propose an end-to-end ERC model to classify the emotion based on multi-modal information, named Emo-Caps. Specifically, we employ the Emoformer structure to extract emotion vectors of textual, audio, and visual features. Then, we merge the emotion vectors of the three modalities with the sentence vector to an emotion capsule. Finally, we employ a context analysis model to get the final result of the emotion classification.</p><p>In general, the contributions of this paper are as follows:</p><p>? We innovatively introduce the concept of emotion vectors to multi-modal emotion recognition and propose a new emotion feature extraction structure, Emoformer, which is used to jointly extract emotion vectors of three modalities and merge them with sentence vector to the emotion capsule.</p><p>? Based on Emoformer, we further propose an end-to-end emotion recognition model named EmoCaps to identify the emotion from multimodal conversation.</p><p>? Our model and the existing state-of-the-art model are tested on MELD and IEMOCAP datasets. The test results show that our model has the best performance both in multimodality and text-modality.</p><p>The rest of the paper is organized as follows: Section 2 discusses related works; Section 3 introduces the proposed EmoCaps model in detail; Section 4 and 5 present the experiment setups on two benchmark datasets and the analysis of experiment results; Finally, Section 6 concludes the paper.  <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber 1997)</ref> in ERC, which builds context information without differentiating among the speakers. ICON <ref type="bibr" target="#b10">(Hazarika et al., 2018b)</ref> is an extension of CMN <ref type="bibr">(Hazarika et al.,2018)</ref>, which contains another GRU structure to connect the output in the CMN model to distinguish the speaker relationship.  use three GRUs to obtain context information and update the speaker status. <ref type="bibr" target="#b8">Ghosal et al. (2019)</ref> construct a conversation into a graph, then use a graph convolutional neural network to convert the emotion classification task of the conversation into a node classification problem of the graph. <ref type="bibr" target="#b7">Ghosal et al. (2020)</ref> use common sense knowledge to learn the interaction of interlocutors. <ref type="bibr" target="#b22">Shen et al. (2021)</ref> design a directed acyclic neural network for encoding the utterances. <ref type="bibr">Hu et al. (2021)</ref> propose the DialogueCRN to fully understand the conversational context from a cognitive perspective.  propose the TFN model, which is a multi-modal method using the tensor outer product. <ref type="bibr" target="#b26">Liang et al. (2018)</ref> propose the model which use a multi-level attention mechanism to extract different modal interaction information. <ref type="bibr" target="#b3">Cai et al. (2019)</ref> propose a hierarchical fusion model to model graphic information for irony recognition. But the above models are not applied in ERC. <ref type="bibr" target="#b10">Hazarika et al. (2018b)</ref> propose the CMN model in ERC, which uses a GRU structure to store multimodal data information and considers the role of contextual information in conversation emotion recognition. Jingwen <ref type="bibr">Hu et al. (2021)</ref> propose the MMGCN model, which is a graph convolutional neural network model based on a multi-modal hybrid approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-modal Emotion Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformer Models</head><p>Inspired by the self-attention mechanism (Bengio et al. 2014), the Transformer is proposed for computing representations and efficiently obtaining long-distance contextual information without using sequence <ref type="bibr">(Vaswani et al. 2017</ref>), which has achieved great success in the field of computer vision and audio processing (Tianyang <ref type="bibr">Lin et al. 2021)</ref>. <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> use the Transformer structure to train a large-scale general-purpose text corpus to obtain a language model with syntactic and semantic information. By employing a transformerbased pretraining model, <ref type="bibr">Hazarika et al. (2020)</ref> transfer the context-level weight of the generated conversation model to the conversation emotion recognition model. Yuzhao <ref type="bibr" target="#b18">Mao et al. (2020)</ref> use Transformer to explore differentiated emotional behaviors from the perspective of within and between models. Weizhou  use the XL-Net model for conversation emotion recognition (DialogXL) to obtain longer-term contextual information. The above-mentioned algorithms use a transformer-based structure but they are not applied for multi-modal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Given a dialogue:u 1 , u 2 , u 3 , . . . , u n , where n is the number of utterances. The purpose of conversation emotion recognition is to input a dialogue and identify the correct emotion classification of each sentence in the dialogue from the emotion label set y:y 1 , y 2 , y 3 , ..., y m , where m is the number of emotional label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unimodal Feature Extraction</head><p>We extract the features of the utterance u, represented as U. In particular, when the input data is multi-modal, features of utterance U can be expressed as:</p><formula xml:id="formula_0">U = [U t , U a , U v ]<label>(1)</label></formula><p>where U t means textual feature, U a means audio feature, and U v means visual feature. Textual Feature Extraction: In order to obtain good utterance representation, we use a pre-trained language model, BERT, to extract text feature vectors. BERT is a large general-purpose pre-trained language model proposed by <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>, which can effectively represent the grammatical and semantic features of the utterance. Specifically, we first split the dialogue into a series of individual utterances, which are used as the input of the BERT-base model. Unlike other downstream tasks, we use the transformer structure to encode the utterances without classifying or decoding; then we get the sentence vector of every utterance with 512 dimensions. It should be noted that using a larger pre-tranined BERT model did not improve the performance, and a smaller BERT model couldn't get good enough performance. Audio Feature Extraction: Identical to <ref type="bibr">Hazarika et al. (2018)</ref>, we use OpenSMILE <ref type="bibr" target="#b6">(Eyben et al. 2010)</ref> for acoustic feature extraction. Specifically, in this work, we use the IS13 ComParE config file, which extracts a total of 6373 features for each utterance video, then we use the fully connected layer to reduce the dimensionality to 512 dimensions. Visual Feature Extraction: We use the 3D-CNN Specifically, we use 3D-CNN with three fully connected layers to get a 512-dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Our Method</head><p>We assume that the emotion of the utterances in the dialogue depends on three factors:</p><p>? The emotional tendency of the utterance itself.</p><p>? Emotional information contained in different modal of utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Context information</head><p>Based on the above three factors, our model Emo-Caps is modeled as follows: We obtain three modal features of dialogue data: textual, audio, and visual; and input them into the Emoformer structure, then get the emotion vector of three modals and fuse them with sentence vector; finally, the context analysis model is used to obtain the emotion recognition result. The framework of the model is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. It is worth noting that our text features, i.e., the sentence vector, are encoded by a transformer-based pre-trained language model, so we no longer use the self-attention mechanism but directly employ a mapping network to extract the emotion vector, then the residual structure concats sentence vector with emotion vector.</p><p>Emoformer Block Existing methods mainly use CNN, TextCNN, GRU, etc., to extract text feature vectors, which extract grammatical information weakly. At the same time, they only take the original feature vectors without emotional tendency as input. Based on this, we propose to use the Emoformer structure to extract the emotion vectors of various modalities. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, Emoformer has an Encoder structure similar to Transformer, but does not include the Decoder structure. A multi-head attention layer is used to get the emotional tendency feature from the original feature, both are connected through the residual structure, then emotion vector is obtained through a mapping network composed of 5 fully connected layers. The self-attention layer can be used to extract features that contain emotional tendencies or emotional factors effectively, and the residual structure ensures the integrity of the original information; finally the mapping network decouples features and reduces feature dimensions. Identical to <ref type="bibr">Vaswani et al. (2017)</ref>, for a given input feature U , we calculate three matrix of query Q ? R T Q ?d Q , key K ? R T K ?d K and value V ? R T V ?d V by linear transformation from U :</p><formula xml:id="formula_1">[Q, K, V ] = U [W Q , W K , W V ]<label>(2)</label></formula><p>where T Q , T K , T V represent the sequence length of the Q, K, V , and d Q , d K , d V represent the dimensions of the Q, K, V , and</p><formula xml:id="formula_2">W Q ? R d Q ?dm , W K ? R d K ?dm , W V ? R d V ?dm .</formula><p>Then we can express the formula of the selfattention layer as:</p><formula xml:id="formula_3">A = sof tmax( QK T ? d k )V (3)</formula><p>where A is the weight of value V , d k is equal to the dimension of u. In this way, multiple selfattention layers are concatenated to get Multi-Head Attention layer:</p><formula xml:id="formula_4">M ultiHead(A) = Concat(A 1 , ..., A h )W (4)</formula><p>where A 1 , ..., A h are the output of self-attention layers, h is the number of layers, and W is the weight parameter. Then a residual connection with normalization layer is used to normalize the output of Multi-Head attention layer, and a Feed Forward layer is employed to get the output of the self-attention parts:</p><formula xml:id="formula_5">N = N orm(A + M ultiHead(A)) (5) F = max(0, N W 1 + b 1 )W 2 + b 2 (6) G = N orm(F + M ultiHead(F ))<label>(7)</label></formula><p>where W 1 , W 2 are the weight parameter, b 1 , b 2 are the bias parameter. Finally, the orignal features U and the output of self-attention parts G are connected through the residual structure, and a mapping network is employed to get the final output E:</p><formula xml:id="formula_6">H = U ? G (8) E = M ap(H)<label>(9)</label></formula><p>where the Map represents the mapping network, which consists of 5 fully connected layers. Combine the above Eq.</p><p>(2) to (9), we can get different modality emotion vectors from different input channels with Emoformer:</p><formula xml:id="formula_7">[E a , E v , E t ] = Emof ormer(U a , U v , U t ) (10)</formula><p>where U a , U v , U t represent the original input of audio,visual and textual features, and E a , E v , E t represent the emotion vectors of modalities. Emotion Capsule For the composition of the emotion capsule, we are based on the following rules: the text feature vector of the utterance contains grammatical and semantic features, emotion vector represents the emotional tendency of the utterance. Both are the main sources of conversation emotion recognition. Textual features most intuitively represent the meaning, emotions, characteristics, etc., of the utterance. However, visual features and audio features contain a few of emotional factors and emotional features, which can provide some emotional clues when the text features do not have sufficient emotional inclination. Therefore, sentence vector concats with three modalities' emotion vector to be an emotion capsule, which just like a "capsule", the emotion vector is "wrapped" by the sentence vector and "absorbed" by the context analysis model to determine the speaker's emotion finally. Our emotion capsule O can be expressed as:</p><formula xml:id="formula_8">O = U t ? E t ? E v ? E a<label>(11)</label></formula><p>Context Modeling Since the same emotion has different expressions, and the same expression can express different emotions in different contexts, it is very difficult to infer the true emotions from a single word <ref type="bibr">(Barrett, 2017)</ref>. According to Grice's theory of implicature <ref type="formula" target="#formula_0">(1957)</ref>, the meaning of a sentence can be canceled, so it is necessary to integrate context to infer the true meaning of a sentence. Therefore, contextual information is an indispensable part of conversation emotion recognition. Context information is divided into two parts: the information obtained from the previous moment is named emotional clue traceability, and the information obtained from the next moment is named emotional reasoning.</p><p>In this paper, we employ a Bi-directional LSTM model as the context analysis model to extract contextual information. In a conversation, we form a batch of emotion capsules of all utterances into the Bi-LSTM model in the order of dialogue, and each LSTM cell corresponds to an emotion capsule. For the time i, in the forward propagation sequence, the contextual information C i at this moment is composed of the hidden state output of the LSTM cells at all previous moments, that is, the emotional clue traceability; in the backpropagation sequence, the contextual information at this moment is composed of the hidden state output of the LSTM cells at all following moments, which is emotional reasoning. The two fed into an MLP with fully connected layers and get the values of the utterance u i under each emotion label:</p><formula xml:id="formula_9">l i = ReLU (W l C i + b l )<label>(12)</label></formula><formula xml:id="formula_10">P i = sof tmax(W smax l i + b smax )<label>(13)</label></formula><p>where W l , W smax are the weight parameter, b l , b smax are the bias parameter.  Finally we choose the max value as the emotion label y for the i-th utterance:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEMOCAP Happy Sad Neutral Angry Excited Frustrated</head><formula xml:id="formula_11">y i = arg max m (P i [m])<label>(14)</label></formula><p>4 Experiment Setting 4.1 Dataset IEMOCAP <ref type="bibr" target="#b2">(Busso et al. 2008)</ref>: The IEMOCAP dataset includes video data of impromptu performances or scripted scenes of about 10 actors. There are in total 7433 utterances and 151 dialogues in IEMOCAP dataset. At the same time, it contains audio and text transcription to meet the needs of multimodal data. In this data set, multiple commentators set the emotional labels of the utterances into six categories: including happy, sad, neutral, angry, excited and frustrated. MELD : The MELD dataset contains 13708 utterances and 1433 conversations, which making up from TV series "Friends". It is also a multi-modal dataset containing video, audio, and text formats. In this dataset, multiple commentators set the emotional labels of the words into seven categories: including neutral, surprise, fear, sadness, joy, disgust, and angry. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation</head><p>For textual data, we use BERT model to obtain the sentence vector then get the textual emotion vector from a mapping network. For audio and visual data, we use Emoformer obtain the audio and visual emotion vector.</p><p>As for the hyperparameter settings, we follow     <ref type="bibr" target="#b15">Li et al. (2021)</ref>. For both of MELD dataset and IEMOCAP dataset, the epochs is set to 80, the learning rate is set to 0.0001, and the dropout rate is set to 0.1. The detailed parameter setting is shown in <ref type="table">Table 1</ref>.</p><formula xml:id="formula_12">- - - - - - 58.23 DialogXL - - - - - - - 62.41 DialogueCRN - - - - - - - 58.39 DAG-ERC - - - - - - - 63.65 MMGCN - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>Our proposed model is compared with other stateof-the-art models on the IEMOCAP dataset and MELD dataset, which are under the same parameter conditions. The experimental results are shown in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_5">Table 3</ref>, our model has the best performance on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Compare with Other Baseline Models</head><p>On the one hand, compared with existed methods, our model encodes sentences through a pre-trained language model to obtain a better utterance representation. On the other hand, our emotion capsule contains the emotional tendency of the utterance itself, combined with contextual information, can more effectively identify the speaker's emotion. The experimental results prove the rationality  of our assumptions about the emotional factors in ERC. <ref type="table" target="#tab_6">Table 4</ref> shows the performance of our model on the MELD dataset and the IEMOCAP dataset under different modality combinations. It is easy to find that the performance of multi-modal input is better than single-modal input. At the same time, among the three modalities of textual, audio, and visual, the textual modal has better performance than the other two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Various Modality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>As shown in <ref type="table" target="#tab_6">Table 4</ref>, the performance of audio and visual modal is not good enough. For audio features, the frequency and amplitude of the sound features can only reflect the intensity of the speaker's emotions, not the specific emotional tendencies. Therefore, when certain emotions have similar frequencies and amplitudes, it is difficult to correctly distinguish the speaker's emotions only through audio data. For example, for two emotions of excited and fear, the frequency and amplitude characteristics in the audio mode are both at high values. Thus it's hard to distinguish the two emotions. For visual features, It is easy for us to judge the speaker's expression by facial features, but when the speaker hides his own expression, the video feature is not enough to judge the speaker's emotion. In addition, for a single video modality, the emotional changes in the context are unexplainable. When the textual modality is added, the performance is significantly improved. In other words, textual modality play a major role in conversation emotion recognition, while audio and visual modalities can help improve the accuracy of recognition, which is consistent with the previous assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact of Speaker Embedding</head><p>In order to analyze the impact of speaker modeling on conversation emotion recognition, we use a variant of DialogueRNN as a context modeling model to test its performance on two benchmark datasets. As shown in <ref type="table" target="#tab_8">Table 5</ref>, the performance of the DialogueRNN-based model on the MELD dataset is better than the LSTM-based model. The reason is that most of the MELD dataset belongs to multi-person dialogue situations, so the speaker modeling model (DialogueRNN-based) is more effective in identifying speaker emotions than the model not using speaker modeling (LSTM-based). However, in the IEMOCAP dataset, which is based on two-person dialogue situations, speaker modeling becomes insignificant.</p><p>Furthermore, compared with the LSTM-based model, using DialogueRNN-based or other models that include speaker modeling structures consumes more computing resources and time. <ref type="figure" target="#fig_6">Figure 5</ref> shows the influence of emotion vector when emotion reversal in a conversation. At the beginning of the conversation, both speakers are in a neutral emotional state, while utterance 4 changes the situation that speakers' emotion turn into surprise and sadness. The sentence vector doesn't "understand" the reason why emotion change, but the emotion vector contains a negative emotion ten-dency which easier to get the correct emotion label. Utterance 7 shows that when the context is in the sad emotion, the emotion vector makes the utterance "biased" to "sad", while the sentence vector is in a neutral emotion. It proves the role of emotion vector in ERC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a new multi-modal feature extraction structure, Emoformer, which is based on the transformer structure. Further, we design a new ERC model, namely EmoCaps. First, we use Emoformer structures to extract the emotion vectors of textual, audio, and visual modalities, then fuse the three modalities emotion vectors and sentence vectors to be an emotion capsule; finally, we employ a context analysis model to get the emotion recognition result. We conduct comparative experiments on two benchmark datasets. The experimental results show that our model performs better than the existing state-of-the-art models. The experimental results also verified the rationality of our hypothesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of the heatmap for an utterance in a conversation, with three modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A map for emotion classification. The dashed arrow represent the offset vector, which is added to the neutral vector to obtain the vector with an emotional direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Schematic diagram of Emoformer. The mapping network consists of 5 fully connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Framework illustration of the EmoCaps based ERC model. model to extract video features, especially the facial expression features of the speaker. The 3D-CNN model can capture changes in the speaker's expression, which is important information in ERC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>BC-LSTM): Bc-LSTM uses Bidirectional LSTM structure to encode contextual semantic information, it does not recognize the speaker relationship. CMN(Hazarika et al. 2018): It takes a multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances of each speaker into memories.DialogueRNN (Majumder et al. 2019): Dia-logueRNN uses different GRU units to obtain contextual information and speaker relationships. It is the first conversation emotion analysis model to distinguish between speakers. DialogueGCN (Ghosal et al. 2019): Dia-logueGCN constructs a conversation into a graph, transforms the speech emotion classification problem into a node classification problem of the graph, and uses the graph convolutional neural network to classify the results. DialogXL (Weizhou Shen et al. 2020): DialogXL use XLNet model for conversation emotion recognition to obtain longer-term contextual information. DialogueCRN (Hu et al. 2021): DialogueCRN introduces the cognitive phase to extract and integrate emotional clues from context retrieved by the perceptive phase for context modeling. DAG-ERC (Weizhou Shen et al. 2021): DAG-ERC is a directed acyclic graph neural network for ERC, which provides a intuitive way to model the information flow between long-distance conversation background and nearby context. MMGCN (Jingwen Hu et al. 2021): MMGCN uses GCN network to obtain contextual information, which can not only make use of multimodal dependencies effectively, but also leverage speaker information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the heatmap in the "Friends" Season 3 Dialogue 2. Speaker C refer to Chandler and Speaker M refer to Monica.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results (F1 score) on the IEMOCAP dataset. Average means weighted average. Some of the models only provide overall average results without results under each emotion category, so some data cells are lacking.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results (F1 score) on the MELD dataset. Average means weighted average. The CMN model only for two-party conversation, but MELD is a multi-party conversation dataset. Some of the models only provide overall average results without results under each emotion category, so some data cells are lacking.</figDesc><table><row><cell>Modality</cell><cell cols="2">Dataset IEMOCAP MELD</cell></row><row><cell>Text</cell><cell>69.49</cell><cell>63.51</cell></row><row><cell>Audio</cell><cell>33.00</cell><cell>31.26</cell></row><row><cell>Video</cell><cell>31.64</cell><cell>31.26</cell></row><row><cell>T+A</cell><cell>71.39</cell><cell>63.73</cell></row><row><cell>T+V</cell><cell>71.30</cell><cell>63.58</cell></row><row><cell>T+V+A</cell><cell>71.77</cell><cell>64.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Performance (F1 score) of EmoCaps under dif-</cell></row><row><cell>ferent multimodal settings. T represent textual modal-</cell></row><row><cell>ity, A represent audio modality, and V represent visual</cell></row><row><cell>modality.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance(F1 score) under the multimodal setting of different models as context modeling model.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards an automatic classification of emotions in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Ron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal sarcasm detection in twitter with hierarchical fusion model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2506" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognizing emotion in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Polzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSLP.1996.608022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP &apos;96</title>
		<meeting>eeding of Fourth International Conference on Spoken Language essing. ICSLP &apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1970" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>W?llmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cosmic: Commonsense knowledge for emotion identification in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Rada Mihalcea, and Soujanya Poria</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Icon: interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics. North American Chapter</title>
		<meeting>the conference. Association for Computational Linguistics. North American Chapter</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">2122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Conversational transfer learning for emotion recognition. Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Huai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Seover: Sentence-level emotion orientation vector based conversation emotion recognition model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaijing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Xipeng Qiu. 2021. A survey of transformers</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dialoguernn: An attentive rnn for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the association for computational linguistics</title>
		<meeting>the 55th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meld: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixian</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Directed acyclic graph network for conversational emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Audio-visual based emotion recognition -a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2004.1315276</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="II" to=" II" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. NIPS2017, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 conference on empirical methods in natural language processing</title>
		<meeting>the 2017 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Prateek Vij, Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
