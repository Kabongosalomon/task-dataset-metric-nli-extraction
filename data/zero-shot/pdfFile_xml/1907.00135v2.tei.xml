<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyuan</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesheng</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RGB-D semantic segmentation methods conventionally use two independent encoders to extract features from the RGB and depth data. However, there lacks an effective fusion mechanism to bridge the encoders, for the purpose of fully exploiting the complementary information from multiple modalities. This paper proposes a novel bottom-up interactive fusion structure to model the interdependencies between the encoders. The structure introduces an interaction stream to interconnect the encoders. The interaction stream not only progressively aggregates modality-specific features from the encoders but also computes complementary features for them. To instantiate this structure, the paper proposes a residual fusion block (RFB) to formulate the interdependences of the encoders. The RFB consists of two residual units and one fusion unit with gate mechanism. It learns complementary features for the modality-specific encoders and extracts modality-specific features as well as cross-modal features. Based on the RFB, the paper presents the deep multimodal networks for RGB-D semantic segmentation called RFBNet. The experiments on two datasets demonstrate the effectiveness of modeling the interdependencies and that the RFBNet achieved state-of-theart performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic scene understanding is one of the fundamental tasks for robotics applications, such as precise agriculture <ref type="bibr" target="#b0">[1]</ref>, autonomous driving <ref type="bibr" target="#b1">[2]</ref>, semantic mapping and modeling <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and localization <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In recent years, this field has achieved huge progress, thanks to the methodology of convolutional neural network (CNN) based semantic segmentation <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. As depth images provide complementary information to the RGB images, increasing research exploits deep multimodal networks to fuse the two modalities <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. This paper investigates the fusion structures of multimodal networks for RGB-D semantic segmentation.</p><p>Nowadays, the RGB-D data can be easily obtained by active sensors, e.g., the Microsoft Kinect, or passive sensors, e.g., stereo cameras. The RGB data contain rich appearance information and textural details. Lots of work has been done in semantic segmentation with fully convolutional encoderdecoder networks by using RGB-only data <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. The depth data provide useful geometric cues which may reduce the uncertainty to segment objects with ambiguous appearance <ref type="bibr" target="#b10">[11]</ref>. It is meaningful and crucial to develop effective models to fuse the two complementary modalities.  Many works have shown improvement in semantic segmentation by fusing depth data with RGB data <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Early fusion approaches (see <ref type="figure" target="#fig_1">Fig. 1(a)</ref>) simply feed the concatenated RGB and depth channels into a conventional unimodal network <ref type="bibr" target="#b9">[10]</ref>. Such methods may not fully exploit the complementary nature of the modalities <ref type="bibr" target="#b19">[20]</ref>. Lots of works turn to the two-stream fusion architecture which processes each modality by a separated and identical encoder and fuses modality-specific features in a single decoder <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. The late fusion approaches <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> (see <ref type="figure" target="#fig_1">Fig. 1(b)</ref>) combine the modality-specific features at the end of the two independent encoders with a combination method, e.g., concatenation and element-wise summation. Instead of fusing at early or late stages, hierarchical fusion approaches involve fusing the features at multiple levels. The approaches usually fuse multi-level features from one modality to another modality in the bottom-up path <ref type="bibr" target="#b10">[11]</ref>(see <ref type="figure" target="#fig_1">Fig. 1</ref>(c)) or fuse multi-level features in the top-down path <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref> (see <ref type="figure" target="#fig_1">Fig. 1(d)</ref>). Although these approaches have achieved encouraging results, they do not fully exploit the interdependencies of the modality-specific encoders. It is essential for the encoders to interact and inform each other for reducing the ambiguity in segmentation. How to construct an effective fusion mechanism for bidirectional interaction remains an open problem.</p><p>In this paper, we propose a bottom-up interactive fusion structure to bridge the modality-specific streams with an interaction stream. The structure should address two aspects. First, it should progressively aggregate the information from the modality-specific streams to the interaction stream and extract the cross-model features. Second, it should compute complementary features and feed them to the modalityspecific streams without destroying the encoders' ability to extract modality-specific features. The proposed structure is illustrated in <ref type="figure" target="#fig_1">Fig.1</ref>(e).</p><p>To instantiate this structure, we propose a residual fusion block (RFB) to formulate the interdependencies of the two encoders. The RFB consists of two modality-specific residual units (RUs) and one gated fusion unit (GFU). The GFU adaptively aggregates features from the RUs and generates complementary features for the RUs. The RFB formulates the complementary feature learning as residual learning, and it can extract modality-specific and cross-modal features. With the RFBs, the modality-specific encoders can interact with each other. We build the deep multimodal networks for RGB-D semantic segmentation based on the RFB, which is called RFBNet. And we conduct experiments on two datasets to verify the effectiveness of modeling the interdependencies for RGB-D semantic segmentation.</p><p>The main contributions of this paper are summarized as follows:</p><p>? We propose the bottom-up interactive fusion structure, which bridges the modality-specific streams, i.e., RGB stream and depth stream, with an interaction stream. ? We propose the residual fusion block (RFB) to formulate the interdependencies of the modality-specific streams and build the RFBNet for RGB-D semantic segmentation. ? We verify the RFBNet on indoor and outdoor datasets including ScanNet <ref type="bibr" target="#b20">[21]</ref> and Cityscapes <ref type="bibr" target="#b21">[22]</ref>. The RFB-Net constantly outperforms the baselines. Particularly, the model achieves 59.2% mIoU on ScanNet test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Semantic Segmentation</head><p>Early semantic segmentation methods largely rely on handcrafted features, and use shallow classifiers such as Random Forest and Boosting to predict the class probabilities; then, usually use probabilistic models known as conditional random fields (CRFs) to refine the results <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>In recent years, great progress has been made in this field along with the advance of deep neural networks due to the emerges of large-scale datasets and high-performance graphics processing unit (GPU). FCNs <ref type="bibr" target="#b6">[7]</ref> successfully improved the accuracy of image semantic segmentation by adapting classification networks into fully convolutional networks. The subsequent works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b24">[25]</ref> are following this line, including ERFNet <ref type="bibr" target="#b12">[13]</ref> and AdapNet++ <ref type="bibr" target="#b11">[12]</ref>. To increase the receptive field and reduce the memory and computational consumption, the encoder-decoder architecture is commonly adopted in these works, in which the encoder gradually reduces the feature maps and captures high-level semantic information, and the decoder recovers the spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGB-D data Fusion for Semantic Segmentation</head><p>The multimodal data fusion has gained long-time attention to exploit the complementary nature of the data of different sources <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Early shallow learning methods mainly consider feature-level (early) fusion and decision-level (late) fusion which respectively fusing low-level features and prediction-level features <ref type="bibr" target="#b25">[26]</ref>. Deep multimodal networks further involve hierarchical fusion or intermediate fusion <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref> due to the ability of CNNs to learn hierarchical features of the data.</p><p>Early fusion can intuitively reuse conventional unimodal semantic segmentation networks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For example, Gouprie et al. <ref type="bibr" target="#b9">[10]</ref> adapted the multi-scale RGB network of Farabet et al. <ref type="bibr" target="#b27">[28]</ref> for RGB-D semantic segmentation by concatenating input RGB and depth channels. Late fusion aims to aggregate the high-level features of two modalities using independent networks <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>. Gupta et al. <ref type="bibr" target="#b28">[29]</ref> concatenated the features extracted by two CNN models from RGB and depth data and classified them with SVM classifier; and they employed a new representation of depth data termed as HHA that encodes horizontal disparity, height above ground and angle with gravity for each pixel. Cheng et al. <ref type="bibr" target="#b17">[18]</ref> devised a gated fusion layer to automatically learn the contributions of high-level modality-specific features for an effective combination. Hierarchical fusion enables to combine multimodal features at different layers <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b31">[32]</ref>. FuseNet <ref type="bibr" target="#b10">[11]</ref> fused multi-level depth features into the RGB encoder in the bottom-up path. RedNet <ref type="bibr" target="#b31">[32]</ref> extended FuseNet by additional fusing multi-level features at top-down path. RDFNet <ref type="bibr" target="#b18">[19]</ref> proposed multi-modal feature block and multi-level feature refinement block to fuse multilevel features at top-down path. SSMA <ref type="bibr" target="#b11">[12]</ref> proposed a selfsupervised model adaptation (SSMA) fusion mechanism to combine modality-specific streams and also fused the multilevel features at the top-down path. It achieved state-of-theart performance on various indoor and outdoor datasets.</p><p>The proposed RFBNet also belongs to hierarchical fusion. As a significant difference with existing methods, our approach explicitly formulates the interdependencies of the modality-specific nets, not just aggregating multi-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gate Mechanism</head><p>Gates are commonly used to regulate the flow of the information <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Hochreiter et al. <ref type="bibr" target="#b32">[33]</ref> used four gates to control the information propagate in and out of the memory cell and Cheng et al. <ref type="bibr" target="#b17">[18]</ref> used weighted gates to combine features from different modalities automatically. Highway networks <ref type="bibr" target="#b33">[34]</ref> used a learned gate mechanism to enable the optimization of very deep networks. We also use four gates in the gated fusion unit to regulate the interaction of useful information between modality-specific streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BOTTOM-UP INTERACTIVE FUSION WITH RESIDUAL FUSION BLOCKS</head><p>A. Architecture</p><p>The architecture of the proposed RFBNet is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. Besides the RGB steam and depth stream, the architecture introduces an additional interaction stream. The three streams are merged by a combination method such as concatenation, summation, and SSMA block <ref type="bibr" target="#b11">[12]</ref>. Finally, a decoder is appended to compute the predictions. The RFBs are employed at high layers to manage the interaction of the three streams. Specifically, the RFBs are employed at layers after three downsampling operations when the spatial size of the feature map is one eighth that of the input data. Moreover, the spatial size of the interaction stream is the same as that of the depth stream, and the channel dimension is half of that of the depth stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shrinking the depth image</head><p>The architectures with two encoders commonly suffer from large computational and memory consumption. RGB data contains rich appearance and textural details to depict the objects, while depth data contains relatively sparse geometric information to depict the shape of objects. We ease the consumption by shrinking the spatial size of the depth stream. We shrink the depth data by a factor of 2 before inputting into the net, which reduces roughly threequarters of computation and memory consumption for the depth stream. The depth stream and the interaction stream are upsampled to the same spatial size as the RGB stream before combining. This strategy makes the proposed net slightly faster than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Residual Fusion Block</head><p>The RFB is the basic module to achieve the idea of bottom-up interactive fusion. The RFB consists of two modality-specific residual units (RUs) and one gated fusion unit (GFU). The RU, as the basic unit of ResNet <ref type="bibr" target="#b34">[35]</ref>, is widely used in unimodal networks to learn unimodal features. The RFB learns the modality-specific features based on the RU. We design the GFU to aggregate features from the modality-specific RUs and compute complementary features for the RUs. The framework of the RFB is illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>Given the input RGB, depth, and cross-modal features</p><formula xml:id="formula_0">x l R ? R C?H?W , x l D ? R C? H 2 ? W 2 , x l RD ? R C 2 ? H 2 ? W 2 and the output features x l+1 R ? R C?H?W , x l+1 D ? R C? H 2 ? W 2 , x l+1 RD ? R C 2 ? H 2 ? W 2</formula><p>, the RFB is formulated as: where x l Rcom and x l Dcom are the complementary features computed by the GFU denoted as G; F R and F D denotes the residual functions of the modality-specific RUs; W l R and W l D are parameters of the RUs. From the <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, we can see the modality-specific RUs have the same parameter form with the standard RU <ref type="bibr" target="#b34">[35]</ref>. The difference is that we add complementary features to the input of the RUs.</p><formula xml:id="formula_1">x l Rcom , x l+1 RD , x l Dcom = G(x l R , x l RD , x l D )<label>(1)</label></formula><formula xml:id="formula_2">x l+1 R = x l R + F R (x l R + x l Rcom , W l R )<label>(2)</label></formula><formula xml:id="formula_3">x l+1 D = x l D + F D (x l D + x l Dcom , W l D )<label>(3)</label></formula><p>The RFB formulates the complementary feature learning as residual learning. The GFU acts as a residual function with respect to an identity mapping as illustrated in <ref type="figure" target="#fig_4">Fig. 4(b)</ref>. Note that we add the complementary features x l Rcom and x l Dcom to the inputs of the modality-specific residual functions (denoted as Point "R") instead of the trunks of the unimodal streams (denoted as Point "T"). The different adding points imply different identity mappings as illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. The complementary feature directly impacts the modality-specific stream when adding to Point "T" (see <ref type="figure" target="#fig_4">Fig. 4(a)</ref>), while it  directly impacts the residual function of the modality-specific RU when adding to Point "R" (see <ref type="figure" target="#fig_4">Fig. 4(b)</ref>). Redundancy, noise, and complementary information exist among different modalities. The GFU explores the underlying complementary relationships in a soft-attention manner via the gate mechanism. The GFU contains two input gates and two output gates. The input gates G Rin , G Din ? R  <ref type="figure" target="#fig_3">Fig. 3</ref>. G(.) consists of two convolutional layers with a ReLU layer in between, and a Sigmoid function ?(.) to squash values to [0, 1] range. Note that we share the first convolutional layer for input gates to reduce the computational cost.</p><p>The useful information regulated by the input gates is concatenated together, following a 1 ? 1 convolutional layer before adding to x l RD (x l RD is zero for the first RFB). Then we adopt a light-weight depthwise separable convolution (denoted as "Sconv" in <ref type="figure" target="#fig_3">Fig. 3)</ref> [36] to process the cross-modal features in the interaction stream. Finally, the GFU compute the complementary features for the modality-specific RUs regulated by the two output gates G Rout and G Dout .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Incooprating with Top-Down Multi-Level Fusion</head><p>The proposed bottom-up interactive fusion structure models the interdependencies for the modality-specific encoders. It is orthogonal to the top-down multi-level fusion structure which fuses the encoders features in the top-down path at the decoder stage. The two structures can be incorporated into a united network. We illustrate the structure in <ref type="figure" target="#fig_5">Fig. 5</ref> to give an intuitive understanding. In the experiments, we employ the proposed bottom-up interactive fusion in the SSMA <ref type="bibr" target="#b11">[12]</ref> which adopts the top-down multi-level fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>Datasets. We choose an indoor dataset, i.e, ScanNett <ref type="bibr" target="#b20">[21]</ref> and an outdoor dataset, i.e, Cityscapes <ref type="bibr" target="#b21">[22]</ref> to evaluate the performance. Each of them provides publicly available training and validation sets as well as an online evaluation server for benchmarking on the test set.</p><p>ScanNet is a large-scale indoor scene understanding dataset. It contains 19, 466 samples for training, 5, 436 for validation, and 2, 537 for testing. The RGB images are captured at a resolution of 1296 ? 968 and depth at 640 ? 480. Cityscapes is a large-scale outdoor RGB-D dataset for urban scene understanding. It contains totally 5, 000 finely annotated samples with a resolution of 2048?1024, of which 2, 975 for training, 500 for validation, and 1, 525 for testing.</p><p>Backbones. We adopt two unimodal backbones, i.e., AdapNet++ <ref type="bibr" target="#b11">[12]</ref> and ERFNet <ref type="bibr" target="#b12">[13]</ref>. AdapNet++ is based on the ResNet-50 model with full pre-activation bottleneck residual units, while ERFNet is a real-time semantic segmentation model based on non-bottleneck factorized residual units. We use the encoder model of the ERFNet (denoted as ERFNetenc) for fast testing and ablation study. A simple bilinear interpolation upsampling layer acts as the decoder in ERFNetenc.</p><p>Criteria. We quantify the performance according to the PASCAL VOC intersection-over-union metric (IoU) <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We first built up two-stream fusion networks with the unimodal backbones. We adopt SSMA <ref type="bibr" target="#b11">[12]</ref>, a state-ofthe-art method, as the base framework which uses SSMA blocks to combine the two modality-specific streams. The SSMA (AdapNet++) is the same as SSMA model proposed in <ref type="bibr" target="#b11">[12]</ref>. For the SSMA (ERFNetEnc) , we use the ERFNetenc to extract two modality-specific features, combine the features with the SSMA block, and use a 1 ? 1 convolutional layer and a bilinear interpolation upsampling layer by a factor of 8 to get the final predictions. To employ our approach, we just replace the corresponding paired RUs with the RFBs for RFBNet <ref type="bibr">(AdapNet++)</ref> and RFBNet <ref type="bibr">(ERFNetenc)</ref> . When employing the RFB for the bottleneck residual units, we feed the output of the first 1 ? 1 layer of the bottleneck RU to the GFU, and add the complementary features to the input of the 3 ? 3 layer of the RU.</p><p>The models were implemented using the Tensorflow 1.13.1 and trained on a single 1080Ti GPU. Adam is used for optimization, and "poly" learning rate scheduling policy is adopted to adjust the learning rate. The weight decay is set to 5e ?4 for the AdapNet++ based models and 1e ?4 for the ERFNetenc based models. The images are resized to a smaller scale for training so that the models can be trained on our 1080Ti GPU. For ScanNet, the images are resized to 640 ? 480; For Cityscapes, the images are resized to 1024 ? 512. We resize the predictions to the full resolution when benchmarking. When training on Cityscapes, we employ a crop of 768 ? 384.  <ref type="bibr" target="#b38">[39]</ref> 49.8 FuseNett <ref type="bibr" target="#b10">[11]</ref> 52.1 SSMA <ref type="bibr" target="#b11">[12]</ref> 57.7 RFBNet 59.2 We first train the unimodal models, then use the trained weights to initialize the encoders of the multimodal models. For the AdapNet++ based models, we follow the training procedure of <ref type="bibr" target="#b11">[12]</ref>. We set a mini-batch of 7 for unimodal models, 6 for multimodal models, and 12 for finetuning. For the ERFNetenc based models, we use an initial learning rate of 5e ?4 . We train 100K iterations with a mini-batch of 12 for the unimodal models, and 25K iterations with a mini-batch of 9 for the multimodal models.</p><p>The raw depth data are usually not perfect and have amounts of noise and missing depth values. We perform depth completion <ref type="bibr" target="#b37">[38]</ref> for the depth images. Moreover, we employ the three-channel HHA encoding <ref type="bibr" target="#b28">[29]</ref> for the depth data. We employed extensive data augmentations for training, including flipping, scaling, rotation, cropping, color jittering, and Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Analysis</head><p>Benchmarking. We report the performance benchmarking results on ScanNet and Cityscapes in <ref type="table" target="#tab_0">Table I and Table II</ref>. Note that the test images of the two datasets are not publicly released, and they are used by the evaluation server for benchmarking. From the tables, we can see that the multimodal models have better performance than the unimodal models as expected.</p><p>In <ref type="table" target="#tab_0">Table I</ref>, we compare against the top performing models on ScanNet test set. The results are taking from the leaderboard. The proposed RFBNet outperforms other methods, e.g., SSMA <ref type="bibr" target="#b11">[12]</ref>, FuseNet <ref type="bibr" target="#b10">[11]</ref> and 3DMV <ref type="bibr" target="#b38">[39]</ref>. Note that the RFBNet and SSMA adopted the same backbone, i.e., Adap-Net++, while the SSMA is trained with a batch size of 16 on multiple GPUs with synchronized batch normalization 1 . Still, the RFBNet achieved 1.5% improvement over the SSMA.  In <ref type="table" target="#tab_0">Table II</ref>, we compare RFBNet with base models of different backbones on Cityscapes test set, which shows that the proposed RFBNet constantly outperforms SSMA with different backbones. Note that the accuracy of AdapNet++ and SSMA (AdapNet++) reported in the Table is lower than the official accuracy. This is reasonable because the official models are trained with crops on full resolution images and a larger batch size on multiple GPUs in <ref type="bibr" target="#b11">[12]</ref>.</p><p>We found that the multimodal models improve less on Cityscapes than on ScanNet. We infer the reason is that the depth values of the outdoor data are much noisier and have poorer accuracy than those of the indoor data.</p><p>Some parsing results on ScanNet and Cityscapes are shown in <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref>.</p><p>Ablation Study. We perform the ablation study on the ScanNet validation set with the ERFNetenc backbone. In Table III, we compare the performance of unimodal models and multimodal models and show how the resolution of the depth data impact the performance. From <ref type="table" target="#tab_0">Table III</ref>, we can see the multimodal models show a large improvement over unimodal models by more than 4%. When shrinking the depth input, the SSMA shows performance decrease by 0.5%. We analyze that shrinking the depth can relatively increase the receptive field of depth encoders, which is beneficial for capturing broader context information, but it may lose some geometric details and reduce the spatial representation accuracy. Thus, the performance of SSMA which adopts independent encoders decreases. As the RFBNet bridges the encoders with an interaction stream, both of the unimodal encoders benefit from broader context information. Although losing some geometric details, RFBNet still shows performance improvement by 0.4%.</p><p>We show how the gates and complementary adding points of the RFB impact the performance in <ref type="table" target="#tab_0">Table IV</ref>. "G" means employing gate mechanism to regulate the features. "T" and "R" means the complementary features are added to <ref type="bibr">Fig. 6</ref>. Qualitative results of RFBNet compared with baseline unimodal and multimodal methods on ScanNet dataset. The last column shows the improvement/error map which denotes the misclassified pixels in red and the pixels that are misclassified by SSMA but correctly predicted by RFBNet in green. <ref type="figure">Fig. 7</ref>. Qualitative results of RFBNet compared with baseline unimodal and multimodal methods on Cityscapes dataset. The last column shows the improvement/error map which denotes the misclassified pixels in red and the pixels that are misclassified by SSMA but correctly predicted by RFBNet in green.</p><p>the trunk and the input of the residual function of RU, respectively. When "T" and "R" are disabled, the interaction stream only aggregates features from unimodal encoders but does not compute complementary features for the encoders. From the table, we can see that the performance improves by 0.4% when employing gates. Moreover, enabling "R" further improves by 0.9% and outperforms enabling "T" by 0.6%, which indicates that it is beneficial for the encoders to interact and inform each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper addresses the RGB-D semantic segmentation by explicitly modeling the interdependencies of the RGB stream and depth stream. We proposed a bottom-up interactive fusion structure to bridge the modality-specific encoders with an interaction stream. Specifically, we proposed the residual fusion block to explicitly formulate the interdependences of the two encoders. Experiments demonstrate that the proposed approach achieved considerable improvements by effectively modeling the interdependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work is supported by the National Natural Science Foundation of China (U1764264/61873165), Shanghai Automotive Industry Science and Technology Development Foundation (1733/1807), International Chair on automated driving of ground vehicle. Ming Yang is the corresponding author. The authors are with Department of Automation, Shanghai Jiao Tong University; Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai, 200240, China (phone: +86-21-34204553; e-mail: MingYang@sjtu.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fusion structures for RGB-D semantic segmentation. The blue color and green color indicate RGB stream and depth stream, respectively. The trapezoids indicate encoder layers and decoder layers. C and + denote the concatenation and summation operations; F denotes a combination method. (a) Early fusion (b) Late fusion (c) Fusing multi-level depth features into RGB steam. (d) Top-down multi-level fusion. (e) The proposed bottom-up interactive fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of the RFBNet. The three bottom-up streams are highlighted by different colors: RGB stream (blue), depth stream (green), and interaction stream (orange). The RFB manages the interaction of the three streams. F denotes the combination method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The framework of the residual fusion block (RFB). The block consists of two RUs and one GFU. It manages three streams: RGB, depth, and interaction streams, and formulates the interdependencies of the modality-specific streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The flows of the residual and identity mappings when adding complementary features at different points. The GFU acts as a residual function with respect to an identity mapping. The red solid arrow indicates the information flow of identity mapping, while the red dashed arrow indicates the information flow of residual mapping. (a) Adding to the trunk (Point "T"). (b) Adding to the input of the residual function of the RU (Point "R").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The illustration of a united structure by incorporating bottom-up interactive fusion with top-down multi-level fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>H 2 ? W 2</head><label>2</label><figDesc>are used to control the unimodal features to flow into the interaction stream, and the output gates G Rout , G Dout ? R 1? H 2 ? W 2 are used to regulate the complementary features. The gates are learned by the same network G(.) as shown in the bottom of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>1 https://github.com/DeepSceneSeg/AdapNet-pp/issues/11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>RESULTS ON THE SCANNET TEST SET.</figDesc><table><row><cell>Network</cell><cell cols="2">Multimodal mIoU</cell></row><row><cell>PSPNet [9]</cell><cell>-</cell><cell>47.5</cell></row><row><cell>AdapNet++ [12]</cell><cell>-</cell><cell>50.3</cell></row><row><cell>3DMV (2d proj)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EVALUATION</head><label>II</label><figDesc>RESULTS ON THE CITYSCAPES DATASET WITH DIFFERENT BACKBONES (INPUT IMAGE DIM: 1024 ? 512).</figDesc><table><row><cell>Method</cell><cell cols="3">Multimodal mIoU@val mIoU@test</cell></row><row><cell>ERFNetEnc</cell><cell>-</cell><cell>69.5</cell><cell>67.1</cell></row><row><cell>SSMA (ERFNetEnc)</cell><cell></cell><cell>70.8</cell><cell>68.9</cell></row><row><cell>RFBNet (ERFNetEnc)</cell><cell></cell><cell>72.0</cell><cell>69.7</cell></row><row><cell>AdapNet++</cell><cell>-</cell><cell>73.4</cell><cell>73.2</cell></row><row><cell>SSMA (AdapNet++)</cell><cell></cell><cell>75.0</cell><cell>74.2</cell></row><row><cell>RFBNet (AdapNet++)</cell><cell></cell><cell>76.2</cell><cell>74.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF ERFNETENC BASED MODELS ON THE SCANNET VALIDATION SET WITH DIFFERENT RESOLUTIONS OF DEPTH DATA.</figDesc><table><row><cell>Method</cell><cell cols="2">Input data</cell><cell cols="2">Shrink depth mIoU</cell></row><row><cell>ERFNetEnc</cell><cell></cell><cell>RGB</cell><cell></cell><cell>-</cell><cell>51.7</cell></row><row><cell>ERFNetEnc</cell><cell></cell><cell>Depth</cell><cell></cell><cell>-</cell><cell>56.7</cell></row><row><cell>SSMA</cell><cell cols="3">Multimodal</cell><cell>?</cell><cell>61.6</cell></row><row><cell>SSMA</cell><cell cols="3">Multimodal</cell><cell>61.1</cell></row><row><cell>RFBNet</cell><cell cols="3">Multimodal</cell><cell>?</cell><cell>62.2</cell></row><row><cell>RFBNet</cell><cell cols="3">Multimodal</cell><cell>62.6</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE IV</cell></row><row><cell cols="5">PERFORMANCE OF RFBNET (ERFNETENC) ON SCANNET VALIDATION SET</cell></row><row><cell cols="5">FOR DIFFERENT SETTINGS OF THE RESIDUAL FUSION BLOCK.</cell></row><row><cell></cell><cell>G</cell><cell>T</cell><cell>R</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lottes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Robot. Autom. IEEE</title>
		<imprint>
			<biblScope unit="page" from="2229" to="2235" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast scene understanding for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02550</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Robot. Autom. IEEE</title>
		<imprint>
			<biblScope unit="page" from="2631" to="2638" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d semantic modelling with label correction for extensive outdoor scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Veh. Symp. IEEE</title>
		<imprint>
			<biblScope unit="page" from="1262" to="1267" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term visual localization using semantically segmented images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Robot. Autom. IEEE</title>
		<imprint>
			<biblScope unit="page" from="6484" to="6490" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic segmentation-based lane-level localization using around view monitoring system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors J</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Restricted deformable convolution based road scene semantic segmentation using surround view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CNN based semantic segmentation for urban traffic scenes using fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intell. Veh. Symp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can we pass beyond the field of view? panoramic annular semantic segmentation for real-world surrounding perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Veh. Symp. IEEE</title>
		<imprint>
			<biblScope unit="page" from="446" to="453" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Symp. Exp. Robot</title>
		<imprint>
			<biblScope unit="page" from="465" to="477" />
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3029" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4980" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="689" to="696" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning depth-sensitive conditional random fields for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Robot. Autom. IEEE</title>
		<imprint>
			<biblScope unit="page" from="6232" to="6237" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="23" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gff: Gated fully fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01803</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning common and specific features for rgb-d semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="664" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth cnns for rgb-d scene recognition: learning from scratch better than transferring from rgbcnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoderdecoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2377" to="2385" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Robot Vis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="452" to="468" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
