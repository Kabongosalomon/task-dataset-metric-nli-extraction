<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Shivanna</surname></persName>
							<email>rakeshshivanna@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Z</forename><surname>Cheng</surname></persName>
							<email>zcheng@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
							<email>sagarj@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lin</surname></persName>
							<email>dongl@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
							<email>lichan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
							<email>edchi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning effective feature crosses is the key behind building recommender systems. However, the sparse and large feature space requires exhaustive search to identify effective crosses. Deep &amp; Cross Network (DCN) was proposed to automatically and efficiently learn bounded-degree predictive feature interactions. Unfortunately, in models that serve web-scale traffic with billions of training examples, DCN showed limited expressiveness in its cross network at learning more predictive feature interactions. Despite significant research progress made, many deep learning models in production still rely on traditional feed-forward neural networks to learn feature crosses inefficiently.</p><p>In light of the pros/cons of DCN and existing feature interaction learning approaches, we propose an improved framework DCN-V2 to make DCN more practical in large-scale industrial settings. In a comprehensive experimental study with extensive hyper-parameter search and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building blocks, and has delivered significant offline accuracy and online business metrics gains across many web-scale learning to rank systems at Google.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning to rank (LTR) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref> has remained to be one of the most important problems in modern-day machine learning and deep learning. It has a wide range of applications in search, recommendation systems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>, and computational advertising <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Among the crucial components of LTR models, learning effective feature crosses continues to attract lots of attention from both academia <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref> and industry <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Effective feature crosses are crucial to the success of many models. They provide additional interaction information beyond individual features. For example, the combination of "country" and "language" is more informative than either one of them. In the era of linear models, ML practitioners rely on manually identifying such feature crosses <ref type="bibr" target="#b42">[43]</ref> to increase model's expressiveness. Unfortunately, this involves a combinatorial search space, which is large and sparse in web-scale applications where the data is mostly categorical. Searching in such setting is exhaustive, often requires domain expertise, and makes the model harder to generalize.</p><p>Later on, embedding techniques have been widely adopted to project features from high-dimensional sparse vectors to much lower-dimensional dense vectors. Factorization Machines (FMs) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> leverage the embedding techniques and construct pairwise feature interactions via the inner-product of two latent vectors. Compared to those traditional feature crosses in linear models, FM brings more generalization capabilities.</p><p>In the last decade, with more computing firepower and huge scale of data, LTR models in industry have gradually migrated from linear models and FM-based models to deep neural networks (DNN). This has significantly improved model performance for search and recommendation systems across the board <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50]</ref>. People generally consider DNNs as universal function approximators, that could potentially learn all kinds of feature interactions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. However, recent studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50]</ref> found that DNNs are inefficient to even approximately model 2nd or 3rd-order feature crosses.</p><p>To capture effective feature crosses more accurately, a common remedy is to further increase model capacity through wider or deeper networks. This naturally crafts a double edged sword that we are improving model performance while making models much slower to serve. In many production settings, these models are handling extremely high QPS, thus have very strict latency requirements for real-time inference. Possibly, the serving systems are already pushed to a stretch that cannot afford even larger models. Furthermore, deeper models often introduce trainability issues, making models harder to train.</p><p>This has shed light on critical needs to design a model that can efficiently and effectively learn predictive feature interactions, especially in a resource-constraint environment that handles realtime traffic from billions of users. Many recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref> tried to tackle this challenge. The common theme is to leverage those implicit high-order crosses learned from DNNs, with explicit and bounded-degree feature crosses which have been found to be effective in linear models. Implicit cross means the interaction is learned through an end-to-end function without any explicit formula modeling such cross. Explicit cross, on the other hand, is modeled by an explicit formula with controllable interaction order. We defer a detailed discussion of these models in Section 2.</p><p>Among these, Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b49">[50]</ref> is effective and elegant, however, productionizing DCN in large-scale industry systems faces many challenges. The expressiveness of its cross network is limited. The polynomial class reproduced by the cross network is only characterized by (input size) parameters, largely limiting its flexibility in modeling random cross patterns. Moreover, the allocated capacity between the cross network and DNN is unbalanced. This gap significantly increases when applying DCN to large-scale production data. An overwhelming portion of the parameters will be used to learn implicit crosses in the DNN.</p><p>In this paper, we propose a new model DCN-V2 that improves the original DCN model. We have already successfully deployed DCN-V2 in quite a few learning to rank systems across Google with significant gains in both offline model accuracy and online business metrics. DCN-V2 first learns explicit feature interactions of the inputs (typically the embedding layer) through cross layers, and then combines with a deep network to learn complementary implicit interactions. The core of DCN-V2 is the cross layers, which inherit the simple structure of the cross network from DCN, however significantly more expressive at learning explicit and bounded-degree cross features. The paper studies datasets with clicks as positive labels, however DCN-V2 is label agnostic and can be applied to any learning to rank systems. The main contributions of the paper are five-fold:</p><p>? We propose a novel model-DCN-V2-to learn effective explicit and implicit feature crosses. Compared to existing methods, our model is more expressive yet remains efficient and simple. ? Observing the low-rank nature of the learned matrix in DCN-V2, we propose to leverage low-rank techniques to approximate feature crosses in a subspace for better performance and latency trade-offs. In addition, we propose a technique based on the Mixture-of-Expert architecture <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45]</ref> to further decompose the matrix into multiple smaller sub-spaces. These sub-spaces are then aggregated through a gating mechanism. ? We conduct and provide an extensive study using synthetic datasets, which demonstrates the inefficiency of traditional ReLUbased neural nets to learn high-order feature crosses. ? Through comprehensive experimental analysis, we demonstrate that our proposed DCN-V2 models significantly outperform SOTA algorithms on Criteo and MovieLen-1M benchmark datasets. ? We provide a case study and share lessons in productionizing DCN-V2 in a large-scale industrial ranking system, which delivered significant offline and online gains.</p><p>The paper is organized as follows. Section 2 summarizes related work. Section 3 describes our proposed model architecture DCN-V2 along with its memory efficient version. Section 4 analyzes DCN-V2. Section 5 raises a few research questions, which are answered through comprehensive experiments on both synthetic datasets in Section 6 and public datasets in Section 7. Section 8 describes the process of productionizing DCN-V2 in a web-scale recommender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The core idea of recent feature interaction learning work is to leverage both explicit and implicit (from DNNs) feature crosses. To model explicit crosses, most recent work introduces multiplicative operations ( 1 ? 2 ) which is inefficient in DNN, and designs a function (x 1 , x 2 ) to efficiently and explicitly model the pairwise interactions between features x 1 and x 2 . We organize the work in terms of how they combine the explicit and implicit components.</p><p>Parallel Structure. One line of work jointly trains two parallel networks inspired from the wide and deep model <ref type="bibr" target="#b5">[6]</ref>, where the wide component takes inputs as crosses of raw features; and the deep component is a DNN model. However, selecting cross features for the wide component falls back to the feature engineering problem for linear models. Nonetheless, the wide and deep model has inspired many works to adopt this parallel architecture and improve upon the wide component.</p><p>DeepFM <ref type="bibr" target="#b12">[13]</ref> automates the feature interaction learning in the wide component by adopting a FM model. DCN <ref type="bibr" target="#b49">[50]</ref> introduces a cross network, which learns explicit and bounded-degree feature interactions automatically and efficiently. xDeepFM <ref type="bibr" target="#b25">[26]</ref> increases the expressiveness of DCN by generating multiple feature maps, each encoding all the pairwise interactions between features at current level and the input level. Besides, it also considers each feature embedding x as a unit instead of each element as a unit. Unfortunately, its computational cost is significantly high (10x of #params), making it impractical for industrial-scale applications. Moreover, both DeepFM and xDeepFM require all the feature embeddings to be of equal size, yet another limitation when applying to industrial data where the vocab sizes (sizes of categorical features) vary from (10) to millions. AutoInt <ref type="bibr" target="#b45">[46]</ref> leverages the multi-head self-attention mechanism with residual connections. InterHAt <ref type="bibr" target="#b24">[25]</ref> further employs Hierarchical Attentions.</p><p>Stacked Structure. Another line of work introduces an interaction layer-which creates explicit feature crosses-in between the embedding layer and a DNN model. This interaction layer captures feature interaction at an early stage, and facilitates the learning of subsequent hidden layers. Product-based neural network (PNN) <ref type="bibr" target="#b34">[35]</ref> introduces inner (IPNN) and outer (OPNN) product layer as the pairwise interaction layers. One downside of OPNN lies in its high computational cost. Neural FM (NFM) <ref type="bibr" target="#b15">[16]</ref> extends FM by replacing the inner-product with a Hadamard product; DLRM <ref type="bibr" target="#b33">[34]</ref> follows FM to compute the feature crosses through inner products; These models can only create up to 2nd-order explicit crosses. AFN <ref type="bibr" target="#b6">[7]</ref> transforms features into a log space and adaptively learns arbitraryorder feature interactions. Similar to DeepFM and xDeepFM, they only accept embeddings of equal sizes.</p><p>Despite many advances made, our comprehensive experiments (Section 7) demonstrate that DCN still remains to be a strong baseline. We attribute this to its simple structure that has facilitated the optimization. However, as discussed, its limited expressiveness has prevented it from learning more effective feature crosses in webscale systems. In the following, we present a new architecture that inherits DCN's simple structure while increasing its expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED ARCHITECTURE: DCN-V2</head><p>This section describes a novel model architecture -DCN-V2 -to learn both explicit and implicit feature interactions. DCN-V2 starts with an embedding layer, followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions. The improvements made in DCN-V2 are critical for putting DCN into practice for highly-optimized production systems. DCN-V2 significantly improves the expressiveness of DCN <ref type="bibr" target="#b49">[50]</ref> in modeling complex explicit cross terms in web-scale production data, while maintaining its elegant formula for easy deployment. The function class modeled by DCN-V2 is a strict superset of that modeled by DCN. The overall model architecture is depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>, with two ways to combine the cross network with the deep network: (1) stacked and (2) parallel. In addition, observing the low-rank nature of the cross layers, we propose to leverage a mixture of low-rank cross layers to achieve healthier trade-off between model performance and efficiency. x l1 </p><formula xml:id="formula_0">x 2 Cross Network ? h 1 h l2 !""#$%"&amp;'()* ? ? (a) Stacked</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>The embedding layer takes input as a combination of categorical (sparse) and dense features, and outputs x 0 ? R . For the -th categorical feature, we project it from a high-dimensional sparse space to a lower-dimensional dense space via x embed, = embed, e , where e ? {0, 1} ;</p><p>? R ? is a learned projection matrix; x embed, ? R is the dense embedded vector; and represents vocab and embedding sizes respectively. For multivalent features, we use the mean of the embedded vectors as the final vector.</p><p>The output is the concatenation of all the embedded vectors and the normalized dense features: x 0 = [x embed,1 ; . . . ; x embed, ; dense ].</p><p>Unlike many related works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref> which requires = ? , , our model accepts arbitrary embedding sizes. This is particularly important for industrial recommenders where the vocab size varies from (10) to (10 5 ). Moreover, our model isn't limited to the above described embedding method; any other embedding techniques such as hashing could be adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross Network</head><p>The core of DCN-V2 lies in the cross layers that create explicit feature crosses. Eq. (1) shows the ( + 1) th cross layer.</p><formula xml:id="formula_1">x +1 = x 0 ? ( x + b ) + x<label>(1)</label></formula><p>where x 0 ? R is the base layer that contains the original features of order 1, and is normally set as the embedding (input) layer.</p><p>x , x +1 ? R , respectively, represents the input and output of the ( + 1)-th cross layer.</p><p>? R ? and b ? R are the learned weight matrix and bias vector. <ref type="figure" target="#fig_2">Figure 2</ref> shows how an individual cross layer functions.</p><p>For an -layered cross network, the highest polynomial order is + 1 and the network contains all the feature crosses up to the highest order. Please see Section 4.1 for a detailed analysis, both from bitwise and feature-wise point of views. When = 1 ? w ? , where 1 represents a vector of ones, DCN-V2 falls back to DCN.</p><p>The cross layers could only reproduce polynomial function classes of bounded degree; any other complex function space could only be approximated 1 . Hence, we introduce a deep network next to complement the modeling of the inherent distribution in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Network</head><p>The th deep layer's formula is given by ? R ? +1 is the weight matrix and b ? R +1 is the bias vector; (?) is an elementwise activation function and we set it to be ReLU; any other activation functions are also suitable.</p><formula xml:id="formula_2">h +1 = ( h + b ), where h ? R , h +1 ? R +1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Deep and Cross Combination</head><p>We seek structures to combine the cross network and deep network. Recent literature adopted two structures: stacked and parallel. In practice, we have found that which architecture works better is data dependent. Hence, we present both structures:</p><p>Stacked Structure ( <ref type="figure" target="#fig_1">Figure 1a</ref>): The input x 0 is fed to the cross network followed by the deep network, and the final layer is given by x final = h , h 0 = x , which models the data as deep ? cross .</p><p>Parallel Structure <ref type="figure" target="#fig_1">(Figure 1b)</ref>: The input x 0 is fed in parallel to both the cross and deep networks; then, the outputs x and h are concatenated to create the final output layer x final = [x ; h ]. This structure models the data as cross + deep .</p><p>In the end, the prediction^is computed as:^= (w ? logit x final ), where w logit is the weight vector for the logit, and ( ) = 1/(1 + exp(? )). For the final loss, we use the Log Loss that is commonly used for learning to rank systems especially with a binary label (e.g., click). Note that DCN-V2 itself is both prediction-task and loss-function agnostic.</p><formula xml:id="formula_3">loss = ? 1 ?? =1 log(^) + (1 ? ) log(1 ?^) + ?? ? ? 2 2 ,</formula><p>where^'s are predictions; 's are the true labels; is the total number of inputs; and is the 2 regularization parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cost-Effective Mixture of Low-Rank DCN</head><p>In real production models, the model capacity is often constrained by limited serving resources and strict latency requirements. It is often the case that we have to seek methods to reduce cost while maintaining the accuracy. Low-rank techniques <ref type="bibr" target="#b11">[12]</ref> are widely used <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> to reduce the computational cost. It approximates a dense matrix ? R ? by two tall and skinny matrices , ? R ? . When ? /2, the cost will be reduced. However, they are most effective when the matrix shows a large gap in singular values or a fast spectrum decay. In many settings, we indeed observe that the learned matrix is numerically low-rank in practice. <ref type="figure" target="#fig_4">Fig. 3a</ref> shows the singular decay pattern of the learned matrix in DCN-V2 (see Eq. <ref type="formula" target="#formula_1">(1)</ref>) from a production model. Compared to the initial matrix, the learned matrix shows a much faster spectrum decay pattern. Let's define the numerical rank with tolerance T to be argmin ( &lt; ? 1 ), where 1 ? 2 ?, . . . , ? are the singular values. Then, means majority of the mass up to tolerance , is preserved in the top singular values. In the field of machine learning and deep learning, a model could still work surprisingly well with a reasonably high tolerance 2 . Hence, it is well-motivated to impose a low-rank structure on . Eq (2) shows the resulting ( + 1)-th low-rank cross layer</p><formula xml:id="formula_4">x +1 = x 0 ? ? x + b + x<label>(2)</label></formula><p>where , ? R ? and ? . Eq (2) has two interpretations: 1) we learn feature crosses in a subspace; 2) we project the input x to lower-dimensional R , and then project it back to R . The two interpretations have inspired the following two model improvements. Interpretation 1 inspires us to adopt the idea from Mixture-of-Experts (MoE) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45]</ref>. MoE-based models consist of two components: experts (typically a small network) and gating (a function of inputs). In our case, instead of relying on one single expert (Eq (2)) to learn feature crosses, we leverage multiple such experts, each learning feature interactions in a different subspaces, and adaptively combine the learned crosses using a gating mechanism that depends on input x. The resulting mixture of low-rank cross layer formulation is shown in Eq. (3) and depicted in <ref type="figure" target="#fig_4">Figure 3b</ref>.</p><formula xml:id="formula_5">x +1 = ?? =1 (x ) (x ) + x (x ) = x 0 ? ? x + b<label>(3)</label></formula><p>where is the number of experts; (?) : R ? ? R is the gating function, common sigmoid or softmax; (?) : R ? ? R is the th expert in learning feature crosses. (?) dynamically weights each expert for input x, and when (?) ? 1, Eq (3) falls back to Eq (2). Interpretation 2 inspires us to leverage the low-dimensional nature of the projected space. Instead of immediately projecting back from dimension ? to ( ? ? ), we further apply nonlinear transformations in the projected space to refine the representation <ref type="bibr" target="#b10">[11]</ref>.</p><formula xml:id="formula_6">(x ) = x 0 ? ? ? ? x + b<label>(4)</label></formula><p>where (?) represents any nonlinear activation function.</p><p>Discussions. This section aims to make effective use of the fixed memory/time budget to learn meaningful feature crosses. From Eqs (1)-(4), each formula represents a strictly larger function class assuming a fixed #params. Different from many model compression techniques where the compression is conducted post-training, our model imposes the structure prior to training and jointly learn the associated parameters with the rest of the parameters. Due to that, the cross layer is an integral part of the nonlinear system (</p><formula xml:id="formula_7">x) = ( ) ? ? ? ? ? 1 ( 1 ) (x), where ( +1 ? )(?) +1 ( (?)</formula><p>). Hence, the training dynamics of the overall system might be affected, and it would be interesting to see how the global statistics, such as Jacobian and Hession matrices of (x), are affected. We leave such investigations to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Complexity Analysis</head><p>Let denote the embedding size, denote the number of cross layers, denote the number of low-rank DCN experts. Further, for simplicity, we assume each expert has the same smaller dimension (upper bound on the rank). The time and space complexity for the cross network is ( 2 ), and for mixture of low-rank DCN (DCN-Mix) it's efficient when ? with (2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL ANALYSIS</head><p>This section analyzes DCN-V2 from polynomial approximation point of view, and makes connections to related work. We adopt the notations from <ref type="bibr" target="#b49">[50]</ref>.</p><p>Notations. Let the embedding vector x = [x 1 ; x 2 ; . . . ; x ] = [ 1 , 2 , . . . , ] ? R be a column vector, where x ? R represents the -th feature embedding, and represents the -th element in</p><formula xml:id="formula_8">x. Let multi-index = [ 1 , ? ? ? , ] ? N and | | = =1 . y ? {1, ? ? ? , } ? &lt; , &gt;</formula><p>. Let 1 be a vector of all 1's, and be an identity matrix. We use capital letters for matrices, bold lower-case letters for vectors, and normal lower-case letters for scalars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Polynomial Approximation</head><p>We analyze DCN-V2 from two perspectives of polynomial approximation -1) Considering each element (bit) as a unit, and analyzes interactions among the elements (Theorem 4.1); and 2) Considering each feature embedding x as a unit, and only analyzes the feature-wise interactions (Theorem 4.2 ) (proofs in Appendix).</p><p>Theorem 4.1 (Bitwise). Assume the input to an -layer cross network be x ? R , the output be (x) = 1 ? x , and the th layer is defined as x = x ? ( ?1) x ?1 + x ?1 . Then, the multivariate polynomial (x) reproduces polynomials in the following class:</p><formula xml:id="formula_9">?? (1) , . . . , ( ) 1 1 2 2 . . . 0 ? | | ? +1, ? N , where = j? | |?1 i? | |?1 =1 ( ) +1 , ( ) is the ( , ) th element of matrix ( ) , and = Permutations (? { , . . . , times | ? 0}).</formula><p>Theorem 4.2 (feature-wise). With the same setting as in Theorem 4.1, we further assume input x = [x 1 ; . . . ; x ] contains feature embeddings and consider each x as a unit. Then, the output x of an -layer cross network creates all the feature interactions up to order + 1. Specifically, for features with their (repeated) indices in , let = ( ), then their order-interaction is given by:</p><formula xml:id="formula_10">?? i? ?? j? ?1 x 1 ? ( 1 ) 1 , 2 x 2 ? . . . ? ( ) , +1 x +1</formula><p>From both bitwise and feature-wise perspectives, the cross network is able to create all the feature interactions up to order + 1 for an -layered cross network. Compared to DCN-V, DCN-V2 characterizes the same polynomial class with more parameters and is more expressive. Moreover, the feature interactions in DCN-V2 is more expressive and can be viewed both bitwise and feature-wise, whereas in DCN it is only bitwise <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Connections to Related Work</head><p>We study the connections between DCN-V2 and other SOTA feature interaction learning methods; we only focus on the feature interaction component of each model and ignore the DNN component. For comparison purposes, we assume the feature embeddings are of equal size .</p><p>DCN. Our proposed model was largely inspired from DCN <ref type="bibr" target="#b49">[50]</ref>. Let's take the efficient projection view of DCN <ref type="bibr" target="#b49">[50]</ref>, i.e., it implicitly generates all the pairwise crosses and then projects it to a lowerdimensional space; DCN-V2 is similar with a different projection structure.</p><formula xml:id="formula_11">x ? DCN = x pairs w 0 ... 0 0 w ... 0 . . . . . . . . . . . . 0 0 ... w , x ? DCN-V2 = x pairs ? ? ? ? ? ? w 1 0 ... 0 0 w 2 ... 0 . . . . . . . . . . . . 0 0 ... w ? ? ? ? ? ? where x pairs = [?] ? , contains all the 2 pairwise interactions between x 0 andx; w ? R is the weight vector in DCN-V; w ? R</formula><p>is the th column of the weight matrix in DCN-V2 (Eq.(1)).</p><p>DLRM and DeepFM. Both are essentially 2nd-order FM without the DNN component (ignoring small differences). Hence, we simplify our analysis and compare with FM which has formula x ? + &lt; ?x , x ?. This is equivalent to 1-layer DCN-V2 (Eq. (1) without residual term) with a structured weight matrix.</p><formula xml:id="formula_12">1 ? x 1 x 2 . . . x ? ? ? ? ? ? ? 0 12 ??? 1 0 0 ??? 2 . . . . . . . . . . . . 0 0 ??? 0 ? ? ? ? ? ? x 1 x 2 .</formula><p>. .</p><formula xml:id="formula_13">x + ? ? ? ? ? ? 1 2 . . . ? ? ? ? ? ? xDeepFM.</formula><p>The ?-th feature map at the -th layer is given by:</p><formula xml:id="formula_14">x ?, * = ?? ?1 =1 ?? =1 ,? (x ?1 , * ? x )</formula><p>The ?-th feature map at the 1st layer is equivalent to 1-layer DCN-V2 (Eq. (1) without residual term).</p><formula xml:id="formula_15">x 1 ?, * = [ , , ? ? ? , ] (x ? ( x)) = ?? =1 x ? ( ,: x)</formula><p>where the ( , )-th block , = ? , and ,:</p><formula xml:id="formula_16">[ ,1 , . . . , , ]</formula><p>. AutoInt. The interaction layer of AutoInt adopted the multihead self-attention mechanism. For simplicity, we assume a single head is used in AutoInt; multi-head case could be compared summarily using concatenated cross layers.</p><p>From a high-level view, the 1st layer of AutoInt outputs</p><formula xml:id="formula_17">x = [ x 1 ; x 2 ; . . . ; x ]</formula><p>, where x encodes all the 2nd-order feature interactions with the i-th feature. Then, x is fed to the 2nd layer to learn higher-order interactions. This is the same as DCN-V2.</p><p>From a low-level view (ignoring the residual terms),</p><formula xml:id="formula_18">x = ?? =1 exp ? q x , k x ? exp ? q x , k x ? ( v x ) = ?? =1 softmax(x ? x ) v x</formula><p>where ??, ?? represents inner (dot) product, and = q k . While in DCN-V2,</p><formula xml:id="formula_19">x = ?? =1 x ? ( , x ) = x ? ( ,: x)<label>(5)</label></formula><p>where , represents the ( , )-th block of . It is clear that the difference lies in how we model the feature interactions. AutoInt claims the non-linearity was from ReLU(?); we consider each summation term to also contribute. Differently, DCN-V2 used x ? , x . PNN. The inner-product version (IPNN) is similar to FM. For the outer-product version (OPNN), it first explicitly creates all the 2 pairwise interactions, and then projects them to a lower dimensional space ? using a ? by 2 dense matrix. Differently, DCN-V2 implicitly creates the interactions using a structured matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESEARCH QUESTIONS</head><p>We are interested to seek answers for these following research questions: Throughout the paper, "CrossNet" or "CN" represents the cross network; suffix "Mix" denotes the mixture of low-rank version.</p><formula xml:id="formula_20">RQ1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EMPIRICAL UNDERSTANDING OF FEATURE CROSSING TECHNIQUES (RQ1)</head><p>Many recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50]</ref> proposed to model explicit feature crosses that couldn't be learned efficiently from traditional neural networks. However, most works only studied public datasets with unknown cross patterns and noisy data; few work has studied in a clean setting with known ground-truth models. Hence, it's important to understand : 1) in which cases would traditional neural nets become inefficient; 2) the role of each component in the cross network of DCN-V2. We use the cross network in DCN models to represent those feature cross methods and compare with ReLUs, which are commonly used in industrial recommender systems. To simplify experiments and ease understanding, we assume each feature is of dimension one, and monomial 1</p><formula xml:id="formula_21">1 2 2 ? ? ?</formula><p>represents a | |-order interaction between features.</p><p>Performance with increasing difficulty. Consider only 2ndorder feature crosses and let the ground-truth model be (</p><formula xml:id="formula_22">x) = | |=2 1 1 2 2 . . .</formula><p>. Then, the difficulty of learning (x) depends on: 1) sparsity ( = 0), the number of crosses, and 2) similarity of the cross patterns (characterized by Var( )), meaning a change in one feature would simultaneously affect most feature crosses by similar amount. We create synthetic datasets with increasing difficulty in Eq. (6).</p><formula xml:id="formula_23">1 (x) = 2 1 + 1 2 + 3 1 + 4 1 2 (x) = 2 1 + 0.1 1 2 + 2 3 + 0.1 2 3 3 (x) = ?? ( , ) ? , x ? R 100 , | | = 100<label>(6)</label></formula><p>where set and weights are randomly assigned, and 's are uniformly sampled from interval [-1, 1].  Role of each component. We also conducted ablation studies on homogeneous polynomials of order 3 and 4, respectively. For each order, we randomly selected 20 cross terms from x ? R 50 . <ref type="figure" target="#fig_5">Figure 4</ref> shows the change in mean RMSE with layer depth. Clearly, x 0 ? ( x ) models order-crosses at layer -1, which is verified by that the best performance for order-3 polynomial is achieved at layer 2 (similar for order-4). At other layers, however, the performance significantly degrades. This is where the bias and residual terms are helpful -they create and maintain all the crosses up to the highest order. This reduces the performance gap between layers, and stabilizes the model when redundant crosses are introduced. This is particularly important for real-world applications with unknown cross patterns. <ref type="figure" target="#fig_5">Fig. 4</ref> also reveals the limited expressiveness of DCN in modeling complicated cross patterns. Performance with increasing layer depth. We now study scenarios closer to real-world settings, where the cross terms are of a combined order.</p><formula xml:id="formula_24">(x) =x ? w + ?? ? 1 1 2 2 ? ? ? + 0.1 sin(2x ? w + 0.1) + 0.01</formula><p>where the randomly chosen set = 2 ? 3 ? 4 , | 2 | = 20, | 3 | = 10, | 4 | = 5, and ? ? , | | = ; sine introduces perturbations and represents Gaussian noises. <ref type="table" target="#tab_3">Table 2</ref> reports the mean RMSE out of 5 runs. With the increase of layer depth, CN-M was able to capture higher-order feature crosses in the data, resulting in improved performance. Thanks to the bias and residual terms, the performance didn't degrade beyond layer 3, where redundant feature interactions were introduced. To summarize, ReLUs are inefficient in capturing explicit feature crosses (multiplicative relations) even with a deeper and larger network. This is well aligned with previous studies <ref type="bibr" target="#b0">[1]</ref>. The accuracy considerably degrades when the cross patterns become more complicated. DCN accurately captures simple cross patterns but fails at more complicated ones. DCN-V2, on the other hand, remains accurate and efficient for complicated cross patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTAL RESULTS (RQ2 -RQ5)</head><p>This section empirically verifies the effectiveness of DCN-V2 in feature interaction learning across 3 datasets and 2 platforms, compared with SOTA. In light of recent concerns about poor reproducibility of published results <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>, we conducted a fair and comprehensive experimental study with extensive hyper-parameter search to properly tune all the baselines and proposed approaches. In addition, for each optimal setup, we train 5 models with different random initialization, and report the mean and standard deviation. Section 7.2 studies the performance of the feature-cross learning components (RQ2) between baselines without integrating with DNN ReLU layers (similar to <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46]</ref>); only sparse features are considered for a clean comparison. Section 7.3 compares DCN-V2 with all the baselines comprehensively (RQ3). Section 7.5 evaluates the influence of hyper-parameters on the performance of DCN-V2 (RQ4). Section 7.6 focuses on model understanding (RQ5) of whether we are indeed discovering meaningful feature crosses with DCN-V2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experiment Setup</head><p>This section describes the experiment setup, including training datasets, baseline approaches, and details of the hyper-parameter search and training process. <ref type="table" target="#tab_4">Table 3</ref> lists the statistics of each dataset: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Datasets.</head><formula xml:id="formula_25">Criteo 3 .</formula><p>The most popular click-through-rate (CTR) prediction benchmark dataset contains user logs over a period of 7 days. We follow <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50]</ref> and use first 6 days for training, and randomly split the last day's data into validation and test set equally. We lognormalize (log( + 4) for feature-2 and log( + 1) for others) the 13 continuous features and embed the remaining 26 categorical features.</p><p>MovieLen-1M 4 . The most popular dataset for recommendation systems research. Each training example includes a ?user-features, movie-features, rating? triplet. Similar to AutoInt <ref type="bibr" target="#b45">[46]</ref>, we formalize the task as a regression problem. All the ratings for 1s and 2s are normalized to be 0s; 4s and 5s to be 1s; and rating 3s are removed. 6 non-multivalent categorical features are used and embedded. The data is randomly split into 80% for training, 10% for validation and 10% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Baselines.</head><p>We compare our proposed approaches with 6 SOTA feature interaction learning algorithms. A brief comparison between the approaches is highlighted in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Implementation Details.</head><p>All the baselines and our approaches are implemented in TensorFlow v1. For a fair comparison, all the implementations were identical across all the models except for the feature interaction component <ref type="bibr" target="#b4">5</ref> .</p><p>Embeddings. All the baselines require each feature's embedding size to be the same except for DNN and DCN. Hence, we fixed it to be Avg vocab 6 ? (vocab cardinality) <ref type="bibr">1 4</ref> (39 for Criteo and 30 for Movielen-1M) for all the models <ref type="bibr" target="#b5">6</ref> . <ref type="table">Table 4</ref>: High-level comparison between models. Assuming the input x 0 = [v 1 ; . . . ; v ] contains feature embeddings that each represented as v . ? denotes concatenation; ? denotes outer-product; ? denotes Hadamard-product. (?) represents implicit feature interactions, i.e., ReLU layers. In the last column, the '+' sign is on the logit level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Explicit Interactions ( ) Final Objective Order (Simplified) Key Formula</p><formula xml:id="formula_26">PNN [35] 2 x = [v ? v | ? , ] (IPNN) ? x = [vec(v ? v ) | ? , ] (OPNN) DeepFM [13] 2 x = [v ? v | ? , ] + DLRM [34] 2 x = [v ? v | ? , ] ? DCN [50] ? 2 x +1 = x 0 ? x w + xDeepFM [26] ? 2 v ? = , ? (v ?1 ? v ) + AutoInt [46] NA v = exp(? v , v ?) v exp (? v , v ?) ) + DCN-V2 (ours) ? 2 x = x 0 ? ( x ) ? +</formula><p>Optimization. We used Adam <ref type="bibr" target="#b21">[22]</ref> with a batch size of 512 (128 for MovieLen). The kernels were initialized with He Normal <ref type="bibr" target="#b14">[15]</ref>, and biases to 0; the gradient clipping norm was 10; an exponential moving average with decay 0.9999 to trained parameters was applied.</p><p>Reproducibility and fair comparisons: hyper-parameters tuning and results reporting. For all the baselines, we conducted a coarse-level (larger-range) grid search over the hyper-parameters, followed by a finer-level (smaller-range) search. To ensure reproducibility and mitigate model variance, for each approach and dataset, we report the mean and stddev out of 5 independent runs for the best configuration. We describe detailed settings below for Criteo; and follow a similar process for MovieLens with different ranges.</p><p>We first describe the hyper-parameters shared across the baselines. The learning rate was tuned from 10 ?4 to 10 ?1 on a log scale and then narrowed down to 10 ?4 to 5 ? 10 ?4 on a linear scale. The training steps were searched over {150k, 160k, 200k, 250k, 300k}. The number of hidden layers ranged in {1, 2, 3, 4} with their layer sizes in {562, 768, 1024}. And the regularization parameter was in {0, 3 ? 10 ?5 , 10 ?4 }.</p><p>We then describe each model's own hyper-parameters, where the search space is designed based on reported setting. For DCN, the number of cross layers ranged from 1 to 4. For AutoInt, the number of attention layers was from 2 to 4; the attention embedding size was in {20, 32, 40}; the number of attention head was from 2 to 3; and the residual connection was either on or off. For xDeepFM, the CIN layer size was in {100, 200}, depth in {2, 3, 4}, activation was identity, computation was either direct or indirect. For DLRM, the bottom MLP layer sizes and numbers was in {(512,256,64), (256,64)}. For PNN, we ran for IPNN, OPNN and PNN*, and for the latter two, the kernel type ranged in {full matrix, vector, number}. For all the models, the total number of parameters was capped at 1024 2 ? 5 to limit the search space and avoid overly expensive computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Performance of Feature Interaction Component Alone (RQ2)</head><p>We consider the feature interaction component alone of each model without their DNN component. Moreover, we only consider the categorical features, as the dense features were processed differently among baselines. <ref type="table" target="#tab_5">Table 5</ref> shows the results on Criteo dataset. Each baseline was tuned similarly as in Section 7.1.3. There are two major observations. 1). Higher-order methods demonstrate a superior performance over 2nd-order methods. This suggests high-order crosses are meaningful in this dataset. 2). Among the high-order methods, cross network achieved the best performance and was on-par or slightly better compared to DNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Performance of Baselines (RQ3)</head><p>This section compares the performance between DCN-V2 approaches and the baselines in an end-to-end fashion. Note that the best setting reported for each model was searched over a wide-ranged model capacity and hyper-parameter space including the baselines. And if two settings performed on-par, we report the lower-cost one. <ref type="table">Table 6</ref> shows the best LogLoss and AUC (Area Under the ROC Curve) on testset for Criteo and MovieLen. For Criteo, a 0.001-level improvement is considered significant (see <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>). We see that DCN-V2 consistently outperformed the baselines (including DNN) and achieved a healthy quality/cost trade-off. It's also worth mentioning that the baselines' performances reported in <ref type="table">Table 6</ref> were improved over the numbers reported by previous papers (see <ref type="table">Table 9</ref> in Appendix); however, when integrated with DNN, their performance gaps are closing up (compared to <ref type="table" target="#tab_5">Table 5</ref>) with their performances on-par and sometimes worse than the ReLU-based DNN with fine-granular model tuning.</p><p>Best Settings. The optimal hyper-parameters are in <ref type="table">Table 6</ref>. For DCN-V2 models, both the 'stacked' and 'parallel' structures outperformed all the baselines, while 'stacked' worked better on Criteo and 'parallel' worked better on Movielen-1M. On Criteo, the setting was gate as constant, hard_tanh activation for DCN-Mix; gate as softmax and identity activation for CrossNet. The best training steps was 150k for all the baselines; learning rate varies for all the models.</p><p>Model Quality -Comparisons among baselines. When integrating the feature cross learning component with a DNN, the advantage of higher-order methods is less pronounced, and the performance gap among all the models are closing up on Criteo (compared to <ref type="table" target="#tab_5">Table 5</ref>). This suggests the importance of implicit feature interactions and the power of a well-tuned DNN model.</p><p>For 2nd-order methods, DLRM performed inferiorly to DeepFM although they are both derived from FM. This might be due to DLRM's omission of the 1st-order sparse features after the dotproduct layer. PNN models 2nd-order crosses more expressively and delivered better performance on MovieLen-1M; however on Criteo, its mean LogLoss was driven up by its high standard deviation. For higher-order methods, xDeepFM, AutoInt and DCN behaved similarly on Criteo, while on MovieLens xDeepFm showed a high variance.</p><p>DCN-V2 achieved the best performance (0.001 considered to be significant on Criteo <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>) by explicitly modeling up to 3rdorder crosses beyond those implicit ones from DNN. DCN-Mix, the mixture of low-rank DCN, efficiently utilized the memory and reduced the cost by 30% while maintaining the accuracy. Interestingly, CrossNet alone outperformed DNN on both datasets; we defer more discussions to Section 7.4.</p><p>Model Quality -Comparisons with DNN. DNNs are universal approximators and are tough-to-beat baselines when highlyoptimized. Hence, we finely tuned DNN along with all the baselines, and used a larger layer size than those used in literature (e.g., 200 -400 in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46]</ref>). To our surprise, DNN performed neck to neck with most baselines and even outperformed certain models.</p><p>Our hypothesis is that those explicit feature crosses from baselines were not modeled in an expressive and easy-to-optimize manner. The former makes its performanc easy to be matched by a DNN with large capacity. The latter would easily lead to trainability issues, making the model unstable, hard to identify a good local optima or to generalize. Hence, when integrated with DNN, the overall performance is dominated by the DNN component. This becomes especially true with a large-capacity DNN, which could already approximate some simple cross patterns.</p><p>In terms of expressiveness, consider the 2nd-order methods. PNN models crosses more expressively than DeepFM and DLRM, which resulted in its superior performance on MovieLen-1M. This also explains the inferior performance of DCN compared to DCN-V2.</p><p>In terms of trainability, certain models might be inherently more difficult to train and resulted in unsatisfying performance. Consider PNN. On MoiveLen-1M, it outperformed DNN, suggesting the effectiveness of those 2nd-order crosses. On Criteo, however, PNN's advantage has diminished and the averaged performance was onpar with DNN. This was caused by the instability of PNN. Although its best run was better than DNN, its high stddev from multiple trials has driven up the mean loss. xDeepFM also suffers from trainability issue (see its high stddev on MovieLens). In xDeepFM, each feature map encodes all the pair-wise crosses while only relies on a single variable to learn the importance of each cross. In practice, a single variable is difficult to be learned when jointly trained with magnitudes more parameters. Then, an improperly learned variable would lead to noises.</p><p>DCN-V2, on the other hand, consistently outperforms DNN. It successfully leveraged both the explicit and implicit feature interactions. We attribute this to the balanced number of parameters between the cross network and the deep network (expressive), as well as the simple structure of cross net which eased the optimization (easy-to-optimize). It's worth noting that the high-level structure of DCN-V2 shares a similar spirit of the self-attention mechanism adopted in AutoInt, where each feature embedding attends to a weighed combination of other features. The difference is that during the attention, higher-order interactions were modeled explicitly in DCN-V2 but implicitly in AutoInt.</p><p>Model Efficiency. <ref type="table">Table 6</ref> also provides details for model size and FLOPS 7 . The reported setting was properly tuned over the hyper-parameters of each model and the DNN component. For most models, the FLOPS is roughly 2x of the #params; for xDeepFM, however, the FLOPS is one magnitude higher, making it impractical in industrial-scale applications (also observed in <ref type="bibr" target="#b45">[46]</ref>). Note that for DeepFM and DLRM, we've also searched over larger-capacity models; however, they didn't deliver better quality. Among all the methods, DCN-V2 delivers the best performance while remaining relatively efficient; DCN-Mix further reduced the cost, achieving a better trade-off between model efficiency and quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Can Cross Layers Replace ReLU layers?</head><p>The solid performance of DCN-V2 approaches has inspired us to further study the efficiency of their cross layers (CrossNet) in learning explicit high-order feature crosses.</p><p>In a realistic setting with resource constraints, we often have to limit model capacity. Hence, we fixed the model capacity (memory / # of parameters) at different levels, and compared the performance between a model with only cross layers (Cross Net), and a ReLU based DNN. <ref type="table" target="#tab_7">Table 7</ref> reports the best test LogLoss for different memory constraints. The memory was controlled by varying the number of cross layers and its rank ({128, 256}), the number of hidden layers and their sizes. The best performance was achieved by the cross network (5-layer), suggesting the ground-truth model could be well-approximated by polynomials. Moreover, the best performance per memory limit was also achieved by the cross network, indicating both solid effectiveness and efficiency.</p><p>It is well known that ReLU layers are the backbone for various Neural Nets models including DNN, Recurrent Neural Net (RNN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref> and Convolutional Neural Net (CNN) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>. It is quite surprising and encouraging to us that we may potentially replace ReLU layers by Cross Layers entirely for certain applications. Obviously we need significant more analysis and experiments to verify the hypothesis. Nonetheless, this is a very interesting preliminary study and sheds light for our future explorations on cross layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">How the Choice of Hyper-parameters Affect DCN-V2 Model Performance (RQ4)</head><p>This section examines the model performance as a function of hyper-parameters that include 1) depth of cross layers; 2) matrix rank of DCN-Mix; 3) number of experts in DCN-Mix. Depth of Cross Layers. By design, the highest feature cross order captured by the cross net increases with layer depth. Hence, we constrain ourselves to the full-rank cross layers, and evaluate the performance change with layer depth <ref type="figure" target="#fig_6">Figure 5a</ref> shows the test LogLoss and AUC while increasing layer depth on the Criteo dataset. We see a steady quality improvement with a deeper cross network, indicating that it's able to capture more <ref type="bibr" target="#b6">7</ref> FLOPS is a close estimation of run time, which is subjective to implementation details. meaningful crosses. The rate of improvement, however, slowed down when more layers were used. This suggests the contribution from that of higher-order crosses is less significant than those from lower-order crosses. We also used a same-sized DNN as a reference. When there were ? 2 layers, DNN outperformed the cross network; when more layers became available, the cross network started to close the performance gap and even outperformed DNN. In the small-layer regime, the cross network could only approximate very low-order crosses (e.g., 1 ? 2); in the large-layer regime, those loworder crosses were characterized with more parameters, and those high-order interactions were started to be captured. Rank of Matrix. The rank of the weight matrix controls the number of parameters as well as the portion of low-frequency signals passing through the cross layers. Hence, we study its influence on model quality. The model is based on a well-performed setting with 3 cross layers followed by 3 hidden layers of size 512. We approximate the dense matrix in each cross layer by ? where , ? R ? , and we vary . We loosely consider the smaller dimension to be the rank. <ref type="figure" target="#fig_6">Figure 5b</ref> shows the test LogLoss and AUC v.s. matrix's rank on Criteo. When was as small as 4, the performance was onpar with other baselines. When was increased from 4 to 64, the LogLoss decreased almost linearly with (i.e., model's improving). When was further increased from 64 to full, the improvement on LogLoss slowed down. We refer to 64 as the threshold rank. The significant slow down from 64 suggests that the important signals characterizing feature crosses could be captured in the top-64 singular values.</p><p>Our hypothesis for the value of this threshold rank is ( ) where represents # features (39 for Criteo). Consider the ( , )-th block of matrix , we can view , = , + , , where , stores the dominant signal (low-frequency) and</p><p>, stores the rest (highfrequency). In the simplest case where , = 11 ? , the entire matrix will be of rank . The effectiveness of this hypothesis remains to be verified across multiple datasets.</p><p>Number of Experts. We study how the number of low-rank experts affects the quality. We've observed that 1) best-performed setting (#expert, gate, matrix activation type) was subjective to datasets and model architectures; 2) the best-performed model of <ref type="table">Table 6</ref>: LogLoss and AUC (test) on Criteo and Movielen-1M. The metrics were averaged over 5 independent runs with their stddev in the parenthesis. In the 'Best Setting' column, the left reports DNN setting and the right reports model-specific setting. denotes layer depth; denotes CIN layer size; ? and , respectively, denotes #heads and att-embed-size; denotes #experts and denotes total rank.  .4422, and 0.4420. The fact that more lower-ranked experts wasn't performing better than a single higher-ranked expert might be caused by the na?ve gating functions and optimizations adopted. We believe more sophisticated gating <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and optimization techniques (e.g., alternative training, special initialization, temperature adjustment) would leverage more from a mixture of experts. This, however, is beyond the scope of this paper and we leave it to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Model Understanding (RQ5)</head><p>One key research question is whether the proposed approaches are indeed learning meaningful feature crosses. A good understanding about the learned feature crosses helps improve model understandability, and is especially crucial to fields like ML fairness and ML for health. Fortunately, the weight matrix in DCN-V2 exactly reveals what feature crosses the model has learned to be important. Specifically, we assume that each input x = [x 1 ; x 2 ; . . . ; x ] contains features with each represented by an embedding x . Then, the block-wise view of the feature crossing component (ignoring the bias) in Eq. <ref type="bibr" target="#b6">(7)</ref> shows that the importance of feature interaction between -th and -th feature is characterized by the ( , )-th block , .</p><p>x ? x =</p><formula xml:id="formula_27">x 1 x 2 .</formula><p>. . </p><formula xml:id="formula_28">x ? ? ? ? ? ? ?</formula><formula xml:id="formula_29">? ? ? ? ? ? x 1 x 2 .</formula><p>. . x (7) <ref type="figure" target="#fig_7">Figure 6</ref> shows the learned weight matrix in the first cross layer. Subplot (a) shows the entire matrix with orange boxes highlighting some notable feature crosses. The off-diagonal block corresponds to crosses that are known to be important, suggesting the effectiveness of DCN-V2. The diagonal block represents selfinteraction ( 2 's). Subplot (b) shows each block's Frobenius norm and indicates some strong interactions learned, e.g., Gender ? UserId, MovieId ? UserId. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">PRODUCTIONIZING DCN-V2 AT GOOGLE</head><p>This section provides a case study to share our experience productionizing DCN-V2 in a large-scale recommender system in Google. We've achieved significant gains through DCN-V2 in both offline model accuracy, and online key business metrics. The Ranking Problem: Given a user and a large set of candidates, our problem is to return the top-items the user is most likely to engage with. Let's denote the training data to be {(x , )} =1 , where x 's represents features of multiple modalities, such as user's interests, an item's metadata and contextual features; 's are labels representing a user's action (e.g., a click). The goal is to learn a function : R ? ? R that predicts the probability ( |x), the user's action given features x.</p><p>Production Data and Model: The production data are sampled user logs consisting of hundreds of billions of training examples. The vocabulary sizes of sparse features vary from 2 to millions. The baseline model is a fully-connected multi-layer perceptron (MLP) with ReLU activations.</p><p>Comparisons with Production Models: When compared with production model, DCN-V2 yielded 0.6% AUCLoss (1 -AUC) improvement. For this particular model, a gain of 0.1% on AUCLoss is considered a significant improvement. We also observed significant online performance gains on key metrics. <ref type="table" target="#tab_9">Table 8</ref> further verifies the amount of gain from DCN-V2 by replacing cross layers with same-sized ReLU layers. Practical Learnings. We share some practical lessons we have learned through productionizing DCN-V2.</p><p>? It's better to insert the cross layers in between the input and the hidden layers of DNN (also observed in <ref type="bibr" target="#b43">[44]</ref>). Our hypothesis is that the physical meaning of feature representations and their interactions becomes weaker as it goes farther away from the input layer. ? We saw consistent accuracy gains by stacking or concatenating 1 -2 cross layers. Beyond 2 cross layers, the gains start to plateau. ? We observed that both stacking cross layers and concatenating cross layers work well. Stacking layers learns higher-order feature interactions, while concatenating layers (similar to multihead mechanism <ref type="bibr" target="#b47">[48]</ref>) captures complimentary interactions. ? We observed that using low-rank DCN with rank (input size)/4 consistently preserved the accuracy of a full-rank DCN-V2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we propose a new model-DCN-V2-to model explicit crosses in an expressive yet simple manner. Observing the low-rank nature of the weight matrix in the cross network, we also propose a mixture of low-rank DCN (DCN-Mix) to achieve a healthier trade-off between model performance and latency. DCN-V2 has been successfully deployed in multiple web-scale learning to rank systems with significant offline model accuracy and online business metric gains. Our experimental results also have demonstrated DCN-V2's effectiveness over SOTA methods. For future work, we are interested in advancing our understanding of 1). the interactions between DCN-V2 and optimization algorithms such as second-order methods; 2). the relation between embedding, DCN-V2 and its rank of matrix. Further, we would like to improve the gating mechanism in DCN-Mix. Moreover, observing that cross layers in DCN-V2 may serve as potential alternatives to ReLU layers in DNNs, we are very interested to verify this observation across more complex model architectures (e.g., RNN, CNN).</p><p>, +1</p><p>x +1</p><p>where weights , represents the ( , )-th block in weight at the -th cross layer, and it serves as two purposes: align the dimensions between features and increase the impressiveness of the feature cross representations. Note that given the order of x 's, the subscripts of matrix 's are uniquely determined. Proposition. We first proof by induction that x has the following formula: . The first 5 equalities are are straightforward. For the 6 th equality, we first interchanged variable ? = + 1 for the first term, and separated the third term into cases of = 2 and &gt; 2. Then, we group the terms into two cases: = 2 and &gt; 2. For the second to the last equality, we combined the summations over . Consider the set of choosing a combination of ?1 indices from +1 integers, it could be separated into two sets, with index + 1 and without.</p><p>Hence,</p><formula xml:id="formula_31">?1 +1 = ?1 ? ( + 1) ? ?2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>Since both the base case and the induction step hold, we conclude that ? ? 1, Eq (9) holds. This completes the proof. In such case, the -th cross layer contains all the feature interactions (feature-wise) of order up to +1. The interactions between different feature set is parameterized differently, specifically, the interactions between features in set (feature's can be repeated) of order is ??</p><formula xml:id="formula_32">i? ? ?? j? ?1 (i, j; x, ) = x 1 ? 1 1 , 2 x 2 ? . . . ? , +1 x +1</formula><p>where ? contains all the permutations of elements in . ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Proofs for Theorem 4.1</head><p>Proof. Instead of treating each feature embedding as a unit, we treat each element in input embedding x = [ 1 , 2 , . . . , ] as a unit. This is a special case of Theorem 4.2 where all the feature <ref type="table">Table 9</ref>: Baseline performance reported in papers. The metrics (Logloss, AUC) are quoted from papers. Each row represents a baseline, each column represents the paper where the metrics are being reported. The best metric for each baseline is marked in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Paper DeepFM <ref type="bibr" target="#b12">[13]</ref> (2017) DCN <ref type="bibr" target="#b49">[50]</ref> (2017) xDeepFM <ref type="bibr" target="#b25">[26]</ref> (2018) DLRM <ref type="bibr" target="#b33">[34]</ref> (2019) AutoInt <ref type="bibr" target="#b45">[46]</ref>  embedding sizes are 1. In such case, all the computations are interchangeable. Hence, we adopt the notations and also the result of <ref type="bibr">Equation 9</ref>, that is, the -th element in the -th layer of cross network x has the following formula:</p><formula xml:id="formula_33">x = +1 ?? =2 ?? ? ?? ? ?1 ( , ; x, ) +<label>(10)</label></formula><p>To ease the proof and simplify the final formula, we assume the final logit for a -layer cross network is 1 ? x , then</p><formula xml:id="formula_34">1 ? x = ?? =1 +1 ?? =2 ?? ? ?? ? ?1 1 ? ( 1 ) 1 2 2 ? . . . ? ( ) +1 +1 + ?? =1 = +1 ?? =2 ?? ? ?? ? ?1 ( 1 ) 1 2 . . . ( ) +1 1 2 . . . +1 + ?? =1 = +1 ?? =2 ?? | |= ?? ? ?1 ?? i? | |?1 =1 ( ) +1 1 1 2 2 ? ? ? + ?? =1 = ?? ?? j? | |?1 ?? i? | |?1 =1 ( ) +1 1 1 2 2 ? ? ? + ?? =1</formula><p>where is the set of all the permutations of ( 1 ? ? ? 1 1 times ? ? ? ? ? ? times ), | |?1 is a set that represents choosing a combination of | | ? 1 indices out of integers {1, ? ? ? , } at a time, specifically,</p><formula xml:id="formula_35">| |?1 y ? [ ] | |?1 ( ? ) ? ( 1 &gt; 2 &gt; . . . &gt; | |?1 ) .</formula><p>The second equality combined the first and the third summations into a single one summing over a new set := [ ] . The third equality re-represented the cross terms (monomials) using multiindex , and modified the index for weights 's accordingly. The last equality combined the first two summations. Thus the proof. ?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of DCN-V2. ? represents the cross operation in Eq. (1), i.e., x +1 = x 0 ? ( x + b ) + x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of a cross layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>respectively, are the input and output of the -th deep layer;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Left: Singular value decay of the learned DCN-V2 weight matrix. The singular values are normalized and 1 = 1 ? 2 ? . . . ? . + represents the randomly initialized truncated normal matrix; ? represents the final learned matrix. Right: Visualization of mixture of low-rank cross layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Homogeneous polynomial fitting of order 3 and 4. -axis represents the number of layers used; -axis represents RMSE (the lower the better). In the legend, the top 3 models are DCN-V2 with different component(s) included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Logloss and AUC (test) v.s. depth &amp; matrix rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of learned weight matrix in DCN-V2. Rows and columns represents real features. For (a), feature names were not shown for proprietary reasons; darker pixel represents larger weight in its absolute value. For (b), each block represents the Frobenius norm of each matrix block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 ( 1 ,(</head><label>11</label><figDesc>, ; x, ) + x<ref type="bibr" target="#b8">(9)</ref> where is a set which represents all the combinations of choosing elements from [ ] with replacement, and with first element fixed to be :=: y ? [ ] 1 = , ? ? , = ( 1 ,. . . , ); and ?1 is a set that represents choosing a combination of ?1 indices out of integers [ ] at a time: ?1 := y ? [ ] ?1 ? &lt; , &gt; . Base case. When = 1, x 1 = x + x . Induction step. Let's assume that when = , , ; x, ) + x where ? denotes adding index + 1 to each element in the set of ?1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>When would feature interaction learning methods become more efficient than ReLU-based DNNs? RQ2 How does the feature-interaction component of each baseline perform without integrating with DNN? RQ3 How does the proposed mDCN approaches compare to the baselines? Could we achieve healthier trade-off between model accuracy and cost through mDCN and the mixture of low-rank DCN? RQ4 How does the settings in mDCN affect model quality? RQ5 Is mDCN capturing important feature crosses? Does the model provide good understandability?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>reports mean RMSE out of 5 runs and the model size. When the cross patterns are simple ( 1 ), both DCN-V2 and DCN are efficient. When the patterns become more complicated ( 3 ), DCN-V2 remains accurate while DCN degrades. DNN's performance remains poor even with a wider and deeper structure (layer sizes [200, 200] for 1 and 2 , [1024, 512, 256] for 3 ). This suggests the inefficiency of DNN in modeling monomial patterns.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>RMSE and Model Size (# Parameters) for Polynomial Fitting of Increasing Difficulty.</figDesc><table><row><cell></cell><cell cols="2">DCN (1Layer)</cell><cell cols="3">DCN-V2 (1Layer) DNN (1Layer)</cell><cell>DNN (large)</cell></row><row><cell></cell><cell>RMSE</cell><cell cols="2">Size RMSE</cell><cell>Size</cell><cell>RMSE Size</cell><cell>RMSE Size</cell></row><row><cell>1</cell><cell>8.9E-13</cell><cell>12</cell><cell cols="2">5.1E-13 24</cell><cell>2.7E-2 24</cell><cell>4.7E-3 41K</cell></row><row><cell>2</cell><cell>1.0E-01</cell><cell>9</cell><cell cols="2">4.5E-15 15</cell><cell>3.0E-2 15</cell><cell>1.4E-3 41K</cell></row><row><cell>3</cell><cell cols="2">2.6E+00 300</cell><cell cols="2">6.7E-07 10K</cell><cell>2.7E-1 10K</cell><cell>7.8E-2 758K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Combined-order (1 -4) Polynomial Fitting.</figDesc><table><row><cell>#Layers</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row></table><note>DCN-V2 1.43E-01 2.89E-02 9.82E-03 9.87E-03 9.92E-03 DNN 1.32E-01 1.03E-01 1.03E-01 1.09E-01 1.05E-01</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Datasets.</figDesc><table><row><cell>Data</cell><cell cols="3"># Examples # Features Vocab Size</cell></row><row><cell>Criteo</cell><cell>45M</cell><cell>39</cell><cell>2.3M</cell></row><row><cell>MovieLen-1M</cell><cell>740k</cell><cell>7</cell><cell>3.5k</cell></row><row><cell>Production</cell><cell>&gt; 100B</cell><cell>NA</cell><cell>NA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>LogLoss (test) of feature interaction component of each model (no DNN). Only categorical features were used. In the 'Setting' column, stands for number of layers.</figDesc><table><row><cell></cell><cell>Model</cell><cell>LogLoss</cell><cell>Best Setting</cell></row><row><cell></cell><cell>PNN [35]</cell><cell cols="2">0.4715 ? 4.430e-04 OPNN, kernel=matrix</cell></row><row><cell>2nd</cell><cell>FM</cell><cell>0.4736 ? 3.04E-04</cell><cell>-</cell></row><row><cell></cell><cell>CIN [26]</cell><cell>0.4719 ? 9.41E-04</cell><cell>l=3, cinLayerSize=100</cell></row><row><cell></cell><cell>AutoInt [46]</cell><cell>0.4711 ? 1.62E-04</cell><cell>l=2, head=3, attEmbed=40</cell></row><row><cell>&gt;2</cell><cell>DNN</cell><cell>0.4704 ? 1.57E-04</cell><cell>l=2, size=1024</cell></row><row><cell></cell><cell>CrossNet</cell><cell>0.4702 ? 3.80E-04</cell><cell>l=2</cell></row><row><cell></cell><cell cols="3">CrossNet-Mix 0.4694 ? 4.35E-04 l=5, expert=4, gate= 1 1+ ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Logloss and AUC (test) with a fixed memory budget.</figDesc><table><row><cell cols="2">#Params</cell><cell cols="4">7.9E+05 1.3E+06 2.1E+06 2.6E+06</cell></row><row><cell>LogLoss</cell><cell cols="2">CrossNet 0.4424</cell><cell>0.4417</cell><cell>0.4416</cell><cell>0.4415</cell></row><row><cell></cell><cell>DNN</cell><cell>0.4427</cell><cell>0.4426</cell><cell>0.4423</cell><cell>0.4423</cell></row><row><cell>AUC</cell><cell cols="2">CrossNet 0.8096</cell><cell>0.8104</cell><cell>0.8105</cell><cell>0.8106</cell></row><row><cell></cell><cell>DNN</cell><cell>0.8091</cell><cell>0.8094</cell><cell>0.8096</cell><cell>0.80961</cell></row><row><cell cols="6">each setting yielded similar results. For example, for a 2-layered</cell></row><row><cell cols="6">cross net with total rank 256 on Criteo, the LogLoss for 1, 4, 8, 16,</cell></row><row><cell cols="6">and 32 experts, respectively, was 0.4418, 0.4416, 0.4416, 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Relative AUCLoss of DCN-V2 v.s. same-sized ReLUs</figDesc><table><row><cell cols="4">1layer ReLU 2layer ReLU 1layer DCN-V2 2layer DCN-V2</cell></row><row><cell>0%</cell><cell>-0.15%</cell><cell>-0.19%</cell><cell>-0.45%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Any function with certain smoothness assumptions can be well-approximated by polynomials. In fact, we've observed in our experiments that cross network alone was able to achieve similar performance as traditional deep networks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is very different from the filed of scientific computing (e.g., solving linear systems), where the approximation accuracy need to be very high. For problems such as CTR prediction, some errors could introduce regularization effect to the model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset 4 https://grouplens.org/datasets/movielens<ref type="bibr" target="#b4">5</ref> We adopted implementation from https://github.com/Leavingseason/xDeepFM, https: //github.com/facebookresearch/dlrm and https://github.com/shenweichen/DeepCTR 6 This formula is a rule-of-thumb number that is widely used<ref type="bibr" target="#b49">[50]</ref>, also see https: //developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank Bin Fu, Gang (Thomas) Fu, and Mingliang Wang for their early contributions of DCN-V2; Tianshuo Deng, Wenjing Ma, Yayang Tian, Shuying Zhang, Jie (Jerry) Zhang, Evan Ettinger, Samuel Ieong and many others for their efforts and supports in productionizing DCN-V2; Ting Chen for his initial idea of mixture of low-rank; and Jiaxi Tang for his valuable comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent cross: Making use of context in recurrent recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: The example of computational advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Qui?onero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elon</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational advertising and recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM conference on Recommender systems</title>
		<meeting>the 2008 ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive mixture of low-rank factorizations for compact neural modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07792</idno>
		<title level="m">Wide &amp; Deep Learning for Recommender Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linpeng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03276</idno>
		<title level="m">Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we really making much progress? A worrying analysis of recent neural recommendation approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Maurizio Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the Nystr?m method for approximating a Gram matrix for improved kernel-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2153" to="2175" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4314</idno>
		<title level="m">Learning factored representations in a deep mixture of experts</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multiscale neural network based on hierarchical nested bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Feliu-Faba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Zepeda-N?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in the Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Matrix Computations Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore and London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Per-Gunnar Martinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating collaborative filtering recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><forename type="middle">G</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="5" to="53" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face recognition: A convolutional neural-network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable Click-Through Rate Prediction through Hierarchical Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="313" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<title level="m">Learning Sparse Neural Networks through _0 Regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Snr: Sub-network routing for flexible parameter sharing in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural networks for optimal approximation of smooth and analytic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hrushikesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mhaskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="164" to="177" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>?ernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08505</idno>
		<title level="m">A metric learning reality check</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jun Michael</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azzolini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00091</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Data Mining. IEEE</title>
		<imprint>
			<biblScope unit="page" from="995" to="1000" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Factorization Machines with libFM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09683</idno>
		<title level="m">Neural Collaborative Filtering vs. Matrix Factorization Revisited</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="56" to="58" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recommender systems in e-commerce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM conference on Electronic commerce</title>
		<meeting>the 1st ACM conference on Electronic commerce</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="158" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature engineering in context-dependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Automatic Speech Recognition &amp; Understanding. IEEE</title>
		<imprint>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Autoint: Automatic feature interaction learning via selfattentive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning polynomials with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep &amp; Cross Network for Ad Click Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD</title>
		<meeting>the ADKDD</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Block Basis Factorization for Scalable Kernel Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1497" to="1526" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On compressing deep models by low rank and sparse decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7370" to="7379" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
