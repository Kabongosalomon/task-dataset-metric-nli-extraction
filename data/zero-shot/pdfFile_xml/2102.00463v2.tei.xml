<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D object detection</term>
					<term>point clouds</term>
					<term>LiDAR</term>
					<term>autonomous driving</term>
					<term>sparse convolution</term>
					<term>Waymo Open Dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose the Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for 3D object detection from point clouds. First, we propose a novel 3D detector, PV-RCNN, which consists of two steps: the voxel-to-keypoint scene encoding and keypoint-to-grid RoI feature abstraction. These two steps deeply integrate the 3D voxel CNN with the PointNet-based set abstraction for extracting discriminative features. Second, we propose an advanced framework, PV-RCNN++, for more efficient and accurate 3D object detection. It consists of two major improvements: the sectorized proposal-centric strategy for efficiently producing more representative keypoints, and the VectorPool aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our PV-RCNN++ is more than 2x faster than PV-RCNN, while also achieving better performance on the large-scale Waymo Open Dataset with 150m ? 150m detection range. Also, our proposed PV-RCNNs achieve state-of-the-art 3D detection performance on both the Waymo Open Dataset and the highly-competitive KITTI benchmark. The source code is available at https://github.com/open-mmlab/OpenPCDet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3 D object detection is indispensable to lots of real-world applications like autonomous driving, intelligent traffic system and robotics. The most commonly-used data representation for 3D object detection is point cloud, which is captured by 3D sensors (e.g., LiDAR sensors) to depict the 3D scene. The sparsity and irregularity of point cloud make it challenging to extend 2D detection methods <ref type="bibr" target="#b6">[1]</ref>, <ref type="bibr" target="#b7">[2]</ref>, <ref type="bibr" target="#b8">[3]</ref>, <ref type="bibr" target="#b9">[4]</ref>, <ref type="bibr" target="#b10">[5]</ref>, <ref type="bibr" target="#b11">[6]</ref>, <ref type="bibr" target="#b12">[7]</ref> to 3D object detection from point clouds.</p><p>To learn discriminative features from the sparse and irregular points, some 3D detection methods <ref type="bibr" target="#b13">[8]</ref>, <ref type="bibr" target="#b14">[9]</ref>, <ref type="bibr" target="#b15">[10]</ref>, <ref type="bibr" target="#b16">[11]</ref>, <ref type="bibr" target="#b17">[12]</ref>, <ref type="bibr" target="#b18">[13]</ref>, <ref type="bibr" target="#b19">[14]</ref>, <ref type="bibr" target="#b20">[15]</ref>, <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b22">[17]</ref> voxelize the points, then process the regular voxels by conventional Convolution Neural Networks (CNNs) and well-studied 2D detection heads <ref type="bibr" target="#b7">[2]</ref>, <ref type="bibr" target="#b8">[3]</ref>. But the voxelization inevitably brings information loss, thus degrading the fine-grained localization accuracy of these voxel-based methods. In contrast, powered by the pioneer PointNet <ref type="bibr" target="#b23">[18]</ref>, <ref type="bibr" target="#b24">[19]</ref>, some other methods directly learn effective features from raw point cloud and predict 3D boxes around the foreground points <ref type="bibr" target="#b25">[20]</ref>, <ref type="bibr" target="#b26">[21]</ref>, <ref type="bibr" target="#b27">[22]</ref>, <ref type="bibr" target="#b28">[23]</ref>. These point-based methods preserve accurate point positions and have flexible receptive field due to the radius-based local feature aggregation operations (e.g., set abstraction <ref type="bibr" target="#b24">[19]</ref>), but are generally computationally intensive.</p><p>We observe that, the voxel representation with dense bird's eye view heads generally produce better 3D box proposals with higher recall rate <ref type="bibr" target="#b29">[24]</ref>, while the point-wise features benefit the proposal refinement with fine-grained and accurate point locations. Motivated by these observations, we propose a unified framework, namely, Point-Voxel Region-based Convolutional Neural Networks (PV-RCNNs), to take the best of both voxel and point representations. The principle lies in the fact that the voxel-based operations can efficiently encode multi-scale features and generate high-quality 3D proposals, while the point-based operation can preserve accurate location information with flexible receptive fields for proposal refinement.</p><formula xml:id="formula_0">? Shaoshuai Shi,</formula><p>In this paper, we first introduce a novel two-stage detector, PV-RCNN, for accurate 3D object detection through a two-step strategy of point-voxel feature aggregation. The first step is voxel-to-keypoint scene encoding, where a voxel CNN with sparse convolutions is adopted for voxel feature learning and proposal generation. The multi-scale voxel features are then summarized into a small set of keypoints by set abstraction <ref type="bibr" target="#b24">[19]</ref>. The keypoints with accurate point locations are sampled by farthest point sampling (FPS) from the raw point cloud. The second step is keypoint-to-grid RoI feature abstraction, where we propose the RoI-grid pooling module to aggregate the above keypoint features to the RoI grids of each proposal. It encodes multi-scale contextual information to form regular grid features for the following proposal refinement.</p><p>We then propose an advanced two-stage detection network, PV-RCNN++, on top of PV-RCNN, for achieving more accurate, efficient and practical 3D object detection. The improvements of PV-RCNN++ over PV-RCNN lie in two aspects. First, we propose a sectorized proposal-centric keypoint sampling strategy, where the limited number of arXiv:2102.00463v2 [cs.CV] 5 Jan 2022 keypoints is concentrated in and around the 3D proposals to encode more effective features for proposal refinement. Meanwhile, the sectorized farthest point sampling is conducted to parallelly sample keypoints in different sectors, which accelerates the keypoint sampling process, while ensuring the uniform distribution of keypoints. Our proposed keypoint sampling strategy is much faster and more effective than the vanilla farthest point sampling that has a quadratic complexity. The efficiency of the whole framework is thus greatly improved, which is particularly important for large-scale 3D scenes with millions of points. Second, we propose a novel local feature aggregation module, VectorPool aggregation, for more effective and efficient local feature encoding on sparse and irregular point cloud. We argue that the relative point locations in a local region are robust, effective and discriminative information for describing local geometry. We propose to split the 3D local space into regular and compact voxels, the features of which are sequentially concatenated to form a hyper feature vector. The voxel features in different locations are encoded with separate kernels to generate position-sensitive local features. In this way, different feature channels of the hyper feature vector are incorporated with different local location information. Compared with previous set abstraction operation for local feature aggregation, our proposed VectorPool aggregation can efficiently handle a very large number of centric points due to the compact local feature representation. Equipped with the VectorPool aggregation in both voxel-based backbone and RoI-grid pooling module, our proposed PV-RCNN++ is more memory-friendly and faster than previous counterparts with comparable or even better performance, which helps in establishing a practical 3D detector for resource-limited devices.</p><p>In summary, our proposed PV-RCNNs have three major contributions. (1) Our proposed PV-RCNN adopts two novel operations, voxel-to-keypoint scene encoding and keypoint-to-grid RoI feature abstraction, to deeply integrate the advantages of both point-based and voxel-based feature learning strategies. (2) Our proposed PV-RCNN++ takes a step in more practical 3D detection system with better performance, less resource consumption and faster running speed. This is enabled by our proposed sectorized proposalcentric keypoint sampling strategy to obtain more representative keypoints with faster speed, and is also powered by our novel VectorPool aggregation for achieving local aggregation on a very large number of central points with less resource consumption and more effective representation. (3) Our proposed 3D detectors surpass all published methods with remarkable margins on multiple challenging 3D detection benchmarks. In particular, our PV-RCNN++ achieves state-of-the-art results on the Waymo Open dataset with 10 FPS inference speed for the 150m ? 150m detection range. The source code is available at https://github.com/ open-mmlab/OpenPCDet <ref type="bibr" target="#b30">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>2D Object Detection with RGB Images. We summarize the 2D object detectors into anchor-based and anchor-free directions. The approaches following the anchor-based paradigm advocate the empirically pre-defined anchor boxes to perform detection, where object detectors are further divided into two-stage <ref type="bibr" target="#b7">[2]</ref>, <ref type="bibr" target="#b31">[26]</ref>, <ref type="bibr" target="#b32">[27]</ref>, <ref type="bibr" target="#b33">[28]</ref>, <ref type="bibr" target="#b34">[29]</ref> and one-stage <ref type="bibr" target="#b8">[3]</ref>, <ref type="bibr" target="#b12">[7]</ref>, <ref type="bibr" target="#b35">[30]</ref>, <ref type="bibr" target="#b36">[31]</ref> categories. Two-stage approaches generally extract the proposal-align features for box refinement, and onestage ones directly perform detection on feature maps. On the other hand, studies of the anchor-free direction mainly fall into keypoint-based <ref type="bibr" target="#b37">[32]</ref>, <ref type="bibr" target="#b38">[33]</ref>, <ref type="bibr" target="#b39">[34]</ref>, <ref type="bibr" target="#b40">[35]</ref> and centerbased <ref type="bibr" target="#b41">[36]</ref>, <ref type="bibr" target="#b42">[37]</ref>, <ref type="bibr" target="#b43">[38]</ref>, <ref type="bibr" target="#b44">[39]</ref> paradigms. The keypoint-based methods represent bounding boxes as keypoints, i.e., corner/extreme points, grid points and a set of bounded points, and the center-based approaches predict the bounding box from foreground points inside objects. Besides, the recently proposed DETR <ref type="bibr" target="#b45">[40]</ref> leverages widely adopted transformers to detect objects with attention mechanism and self-learned object queries, which also gets rid of anchor boxes. 3D Object Detection with RGB images. Image-based 3D object detection aims to estimate 3D bounding boxes from a monocular image or stereo images. Mono3D <ref type="bibr" target="#b46">[41]</ref> generates 3D region proposals with ground-plane assumption, which are scored by exploiting the semantic knowledge from images. The following works <ref type="bibr" target="#b47">[42]</ref>, <ref type="bibr" target="#b48">[43]</ref> incorporate the relations between 2D and 3D boxes as geometric constraint. M3D-RPN <ref type="bibr" target="#b49">[44]</ref> introduces an end-to-end 3D region proposal network with depth-aware convolutions. <ref type="bibr" target="#b50">[45]</ref>, <ref type="bibr" target="#b51">[46]</ref>, <ref type="bibr" target="#b52">[47]</ref>, <ref type="bibr" target="#b53">[48]</ref>, <ref type="bibr" target="#b54">[49]</ref> predict 3D boxes based on a wire-frame template obtained from CAD models. RTM3D <ref type="bibr" target="#b55">[50]</ref> performs coarse keypoints detection to localize 3d objects in real-time. On the stereo side, Stereo R-CNN <ref type="bibr" target="#b56">[51]</ref>, <ref type="bibr" target="#b57">[52]</ref> capitalizes on a stereo RPN to associate proposals from left and right images. DSGN <ref type="bibr" target="#b58">[53]</ref> introduces the differentiable 3D geometric volume to simultaneously learn depth information and semantic cues in an end-to-end optimized pipelines. Pseudo-LiDARs <ref type="bibr" target="#b57">[52]</ref>, <ref type="bibr" target="#b59">[54]</ref>, <ref type="bibr" target="#b60">[55]</ref> propose to covert the image pixels to artificial point clouds, where the LiDAR-based detectors can operate on them for 3D box estimation. These imagebased 3D detection methods suffer from inaccurate depth estimation and can only generate coarse 3D bounding boxes. Representation Learning on Point Clouds. Recently representation learning on point clouds has drawn lots of attention for improving the performance of point cloud classification and segmentation <ref type="bibr" target="#b15">[10]</ref>, <ref type="bibr" target="#b23">[18]</ref>, <ref type="bibr" target="#b24">[19]</ref>, <ref type="bibr" target="#b61">[56]</ref>, <ref type="bibr" target="#b62">[57]</ref>, <ref type="bibr" target="#b63">[58]</ref>, <ref type="bibr" target="#b64">[59]</ref>, <ref type="bibr" target="#b65">[60]</ref>, <ref type="bibr" target="#b66">[61]</ref>, <ref type="bibr" target="#b67">[62]</ref>, <ref type="bibr" target="#b68">[63]</ref>, <ref type="bibr" target="#b69">[64]</ref>, <ref type="bibr" target="#b70">[65]</ref>, <ref type="bibr" target="#b71">[66]</ref>, <ref type="bibr" target="#b72">[67]</ref>. In terms of 3D detection, previous methods generally project the point clouds to regular bird view grids <ref type="bibr" target="#b14">[9]</ref>, <ref type="bibr" target="#b17">[12]</ref> or 3D voxels [?], <ref type="bibr" target="#b15">[10]</ref> for processing point clouds with 2D/3D CNN. 3D sparse convolution <ref type="bibr" target="#b73">[68]</ref>, <ref type="bibr" target="#b74">[69]</ref> are adopted in <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b29">[24]</ref> to effectively learn sparse voxel-wise features from the point clouds. Qi et al. <ref type="bibr" target="#b23">[18]</ref>, <ref type="bibr" target="#b24">[19]</ref> proposes the PointNet to directly learn point-wise features from the raw points, where set abstraction operation enables flexible receptive fields by setting different search radii. <ref type="bibr" target="#b75">[70]</ref> combines both voxelbased CNN and point-based multi-layer percetron (MLP) network for efficient point feature learning. In comparison, our PV-RCNNs take advantages from both the voxel-based (i.e., 3D sparse convolution) and PointNet-based (i.e., set abstraction operation) strategies to enable both high-quality 3D proposal generation and flexible receptive fields for improving the 3D detection performance. 3D Object Detection with Grid-based Representation on Point Clouds. To tackle the irregular data format of point clouds, most existing works project the point clouds to regular grids. The pioneer work MV3D <ref type="bibr" target="#b14">[9]</ref> projects the point clouds to 2D bird view grids and places lots of predefined 3D anchors for generating 3D boxes, and the following works <ref type="bibr" target="#b16">[11]</ref>, <ref type="bibr" target="#b18">[13]</ref>, <ref type="bibr" target="#b22">[17]</ref>, <ref type="bibr" target="#b76">[71]</ref>, <ref type="bibr" target="#b77">[72]</ref>, <ref type="bibr" target="#b78">[73]</ref> develop better strategies for multi-sensor fusion. <ref type="bibr" target="#b17">[12]</ref>, <ref type="bibr" target="#b19">[14]</ref>, <ref type="bibr" target="#b20">[15]</ref> introduce more efficient frameworks with bird-eye view representation while <ref type="bibr" target="#b79">[74]</ref> proposes to fuse grid features of multiple scales. MVF <ref type="bibr" target="#b80">[75]</ref> integrates 2D features from bird-eye view and perspective view before projecting points into pillar representations <ref type="bibr" target="#b20">[15]</ref>. Some other works <ref type="bibr" target="#b13">[8]</ref>, <ref type="bibr" target="#b15">[10]</ref> divide the points into 3D voxels to be processed by 3D CNN. 3D sparse convolution <ref type="bibr" target="#b74">[69]</ref> is introduced by <ref type="bibr" target="#b21">[16]</ref> for efficient 3D voxel processing. <ref type="bibr" target="#b81">[76]</ref>, <ref type="bibr" target="#b82">[77]</ref> utilize multiple detection heads for detecting 3D objects with different scales. In addition, <ref type="bibr" target="#b83">[78]</ref>, <ref type="bibr" target="#b84">[79]</ref> predicts bounding box parameters following the anchor-free paradigm. These grid-based methods are generally efficient for accurate 3D proposal generation but the receptive fields are constraint by the kernel size of 2D/3D convolutions. 3D Object Detection with Point-based Representation on Point Clouds. F-PointNet <ref type="bibr" target="#b25">[20]</ref> first proposes to apply PointNet <ref type="bibr" target="#b23">[18]</ref>, <ref type="bibr" target="#b24">[19]</ref> for 3D detection from the cropped points based on the 2D image boxes. PointRCNN <ref type="bibr" target="#b26">[21]</ref> generates 3D proposals directly from 3D raw points for 3D detection with point clouds only. <ref type="bibr" target="#b85">[80]</ref> proposes the hough voting strategy for better object feature grouping. 3DSSD <ref type="bibr" target="#b86">[81]</ref> introduces F-FPS as a complement of D-FPS and develops a one stage object detector operating on raw points. These point-based methods are mostly based on the PointNet series, especially the set abstraction operation <ref type="bibr" target="#b24">[19]</ref>, which enables flexible receptive fields for point cloud feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Object Detection with Hybrid Representation on Point</head><p>Clouds. There are also some works that utilize both the point-based and grid-based point representations. STD <ref type="bibr" target="#b28">[23]</ref> proposes the PointsPool to transform point-wise features to voxel features for refining the proposals. Fast Point R-CNN <ref type="bibr" target="#b87">[82]</ref> fuses the deep voxel features with raw point data for 3D detection. Part-A2-Net <ref type="bibr" target="#b29">[24]</ref> aggregates the pointwise part locations by the voxel-based RoI-aware pooling to improve the 3D detection performance. However, these methods do not fuse the deeper features from both two representations and also not fully explore the advantages of these representations. In contrast, our proposed PV-RCNN frameworks explore on how to deeply aggregate the features by learning with both point-based (i.e., set abstraction) and voxel-based (i.e., sparse convolution) feature learning operations to boost the performance of 3D detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>State-of-the-art 3D object detectors mostly adopt two-stage frameworks that generally achieve higher performance by splitting the complex detection problem into two stages, including the region proposal generation and per-proposal refinement. In this section, we briefly introduce our chosen strategy for the fundamental feature extraction and proposal generation stage, and then discuss the challenges of straightforward methods for the 3D proposal refinement. 3D Voxel CNN and Proposal Generation. Voxel CNN with 3D sparse convolution <ref type="bibr" target="#b73">[68]</ref>, <ref type="bibr" target="#b74">[69]</ref> is a popular choice of stateof-the-art 3D detectors <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b29">[24]</ref>, <ref type="bibr" target="#b88">[83]</ref> thanks to its efficiency of converting irregular points into 3D sparse feature volumes. The input points P are first divided into small voxels with spatial resolution of L?W ?H, where non-empty voxel features are directly calculated by averaging the features of inside points ( e.g., 3D coordinates and reflectance intensities). The network utilizes a series of 3D sparse convolution to gradually convert the points into feature volumes with 1?, 2?, 4?, 8? downsampled sizes. Such sparse feature volumes at each level can be viewed as a set of sparse voxel-wise feature vectors. The Voxel CNN backbone can be naturally combined with the 2D detection heads <ref type="bibr" target="#b7">[2]</ref>, <ref type="bibr" target="#b8">[3]</ref>, <ref type="bibr" target="#b43">[38]</ref> by converting the encoded 8? downsampled 3D feature volumes into 2D bird-view feature maps. Specifically, we follow <ref type="bibr" target="#b21">[16]</ref> to stack the 3D feature volumes along Z axis to obtain the L 8 ? W 8 bird-view feature maps, which can be combined with both the anchor-based head <ref type="bibr" target="#b8">[3]</ref> and the center head <ref type="bibr" target="#b89">[84]</ref> for high quality 3D proposal generation. Discussions on 3D Proposal Refinement. In the proposal refinement stage, the proposal-specific features are required to be extracted from the 3D feature volumes or 2D maps. Intuitively, the feature extraction should be conducted in the 3D space instead of the 2D feature maps to learn more fine-grained features. However, these 3D feature volumes from the 3D voxel CNN have major limitations in the following aspects. (i) These feature volumes are generally of low spatial resolution as they are downsampled by up to 8 times, which hinders accurate localization of objects. (ii) Even if one can upsample to obtain feature volumes/maps with larger spatial sizes, they are generally still quite sparse. The commonly used trilinear or bilinear interpolation in the RoIPool/RoIAlign <ref type="bibr" target="#b11">[6]</ref> operations can only extract features from very small neighborhoods (i.e., 4 and 8 for bilinear and trilinear interpolation respectively), which would therefore obtain features with mostly zeros and waste much computations and memory for proposal refinement.</p><p>On the other hand, the point-based local feature aggregation methods <ref type="bibr" target="#b24">[19]</ref> have shown strong capability of encoding sparse features from local neighborhoods with arbitrary scales. We therefore propose to incorporate a 3D voxel CNN with the point-based local feature aggregation operation for conducting accurate and robust proposal refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PV-RCNN: POINT-VOXEL FEATURE SET AB-STRACTION FOR 3D OBJECT DETECTION</head><p>To learn effective features from sparse points, state-of-theart 3D detectors are based on either 3D voxel CNNs with sparse convolution or PointNet-based operators. Generally, the 3D voxel CNNs with sparse convolutional layers are more efficient and are able to generate high-quality 3D proposals <ref type="bibr" target="#b29">[24]</ref>, <ref type="bibr" target="#b89">[84]</ref>, while the PointNet-based operators naturally preserve accurate point locations and can capture rich context information with flexible receptive fields <ref type="bibr" target="#b24">[19]</ref>.</p><p>We propose a novel two-stage 3D detection framework, PV-RCNN, to deeply integrate the advantages of two types of operators for 3D object detection from point clouds. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, PV-RCNN consists of a 3D voxel CNN with sparse convolution as the backbone for efficient feature encoding and proposal generation. Given each 3D proposal, we propose to generate the proposal-aligned features in two novel steps for proposal refinement, which consists of the voxel-to-keypoint scene encoding and keypoint-to-grid RoI feature abstraction. They are introduced in Sec. 4.1 and Sec. 4.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Voxel-to-Keypoint Scene Encoding</head><p>Our proposed PV-RCNN first aggregates the voxel-wise scene features at multiple neural layers of 3D voxel CNN into a small number of keypoints, which bridge the 3D voxel CNN feature encoder and the proposal refinement network. Keypoints Sampling. We simply adopt the farthest point sampling (FPS) algorithm to sample a small number of keypoints K = {p 1 , ? ? ? , p n } from the point clouds P, where n is a hyper-parameter (n=4,096 for Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref> and n=2,048 for KITTI dataset <ref type="bibr" target="#b91">[86]</ref>). It encourages that the keypoints are uniformly distributed around non-empty voxels and can be representative to the overall scene. Voxel Set Abstraction Module. We propose the Voxel Set Abstraction (VSA) module to encode multi-scale semantic features from 3D feature volumes to the keypoints. The set abstraction <ref type="bibr" target="#b24">[19]</ref> is adopted for aggregating voxel-wise feature volumes. Different with the original set abstraction, the surrounding local points are now regular voxel-wise semantic features from 3D voxel CNN, instead of the neighboring raw points with features learned by PointNet <ref type="bibr" target="#b23">[18]</ref>.</p><formula xml:id="formula_1">Specifically, denote F (l k ) = {f (l k ) 1 , . . . , f (l k ) N k } as the set of voxel-wise features in the k-th level of 3D voxel CNN, V (l k ) = {v (l k ) 1 , . . . , v (l k )</formula><p>N k } as their corresponding 3D coordinates in the uniform 3D metric space, where N k is the number of non-empty voxels in the k-th level. For each keypoint p i , to retrieve the set of neighboring voxel-wise feature vectors, we first identify its neighboring non-empty voxels at the k-th level within a radius r k as</p><formula xml:id="formula_2">S (l k ) i = ? ? ? ? ? ? ? f (l k ) j , v (l k ) j ? p i v (l k ) j ? p i &lt; r k , ?v (l k ) j ? V (l k ) , ?f (l k ) j ? F (l k ) ? ? ? ? ? ? ? ,<label>(1)</label></formula><p>where the local relative position v</p><formula xml:id="formula_3">(l k ) j ?p i is concatenated to indicate the relative location of f (l k ) j . The features within neighboring set S (l k ) i</formula><p>are then aggregated by a PointNetblock <ref type="bibr" target="#b23">[18]</ref> to generate the keypoint feature as</p><formula xml:id="formula_4">f (pv k ) i = max G M S (l k ) i ,<label>(2)</label></formula><p>where M(?) denotes randomly sampling at most T k voxels from the neighboring set S . Here multiple radii are utilized to capture richer contextual information.</p><p>The above voxel feature aggregation is performed at the outputs of different scales of the 3D voxel CNN, and the aggregated features from different scales are concatenated to obtain the multi-scale semantic feature for keypoint p i as</p><formula xml:id="formula_5">f (pv) i = f (pv1) i , f (pv2) i , f (pv3) i , f (pv4) i , for i = 1, . . . , n,<label>(3)</label></formula><p>where the generated feature f (pv) i incorporates both the voxel-wise feature f (l k ) j from 3D CNN and the PointNetbased features from Eq. (2). Moreover, the 3D coordinates of p i also naturally preserves accurate location information. Further Enriching Keypoint Features. We further enrich the keypoint features with the raw points P and with the 8? downsampled 2D bird-view feature maps, where the raw points can partially make up the quantization loss of the point voxelization while the 2D bird-view maps have larger receptive fields along the Z axis. Specifically, the raw point feature f (raw) i is also aggregated as that in Eq. (2), while the bird-view features f (bev) i are obtained by performing bilinear interpolation with projected 2D keypoints on the 2D feature maps. Hence, the keypoint features for p i is further enriched by concatenating all its associated features as</p><formula xml:id="formula_6">f (p) i = f (pv) i , f (raw) i , f (bev) i , for i = 1, . . . , n,<label>(4)</label></formula><p>which have the strong capacity of preserving 3D structural information of the entire scene for the following fine-grained proposal refinement step. Predicted Keypoint Weighting. As mentioned before, the keypoints are chosen by farthest point sampling and some of them might only represent the background regions. Intuitively, keypoints belonging to the foreground objects should contribute more to the proposal refinement, while the ones from the background regions should contribute less. Hence, we propose a Predicted Keypoint Weighting (PKW) module to re-weight the keypoint features with extra supervisions from point segmentation. The segmentation labels are free-of-charge and can be generated from the 3D box annotations, i.e., by checking whether each keypoint is inside or outside of a ground-truth 3D box, since the 3D objects in autonomous driving scenes are naturally separated in the 3D space. This module can be formulated as</p><formula xml:id="formula_7">f (p) i = A(f (p) i ) ? f (p) i ,<label>(5)</label></formula><p>where A(?) is a three-layer MLP network with a sigmoid function to predict foreground confidence. It is trained with focal loss <ref type="bibr" target="#b12">[7]</ref> (default hyper-parameters) to handle the imbalanced foreground/background points of the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Keypoint-to-Grid RoI Feature Abstraction</head><p>After summarizing the multi-scale semantic features into a small number of keypoints, in this step, we propose keypoint-to-grid RoI feature abstraction to generate accurate proposal-aligned features from the keypoint features</p><formula xml:id="formula_8">F = {f (p) 1 , ? ? ? ,f (p)</formula><p>n } for fine-grained proposal refinement. RoI-grid Pooling via Set Abstraction. Given each 3D proposal, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we propose the RoI-grid pooling module to aggregate the keypoint features to the RoI-grid points with multiple receptive fields. We uniformly sample 6?6?6 grid points within each 3D proposal, which are then flattened and denoted as G = {g 1 , ? ? ? , g 216 }. We utilize set abstraction to obtain features of grid points via aggregating the keypoint features. Specifically, we firstly identify the neighboring keypoints of grid point g i as</p><formula xml:id="formula_9">? = f (p) j , p j ? g i p j ? g i &lt;r, ?p j ? K, ?f (p) j ?F ,<label>(6)</label></formula><p>where p j ? g i is appended to indicate the local relative location within the ball of radiusr. A PointNet-block <ref type="bibr" target="#b23">[18]</ref> is then adopted to aggregate the neighboring keypoint feature set? to obtain the feature for grid point g i as</p><formula xml:id="formula_10">f (g) i = max G M ? ,<label>(7)</label></formula><p>where M(?) and G(?) are defined in the same way as Eq. <ref type="formula" target="#formula_4">(2)</ref>. We set multiple radiir and aggregate keypoint features with different receptive fields, which are concatenated together for capturing richer multi-scale contextual information. Next, all RoI-grid features of the same RoI can be vectorized and transformed by a two-layer MLP with 256 feature dimensions to represent the overall features of proposal.</p><p>Our proposed RoI-grid pooling operation can aggregate much richer contextual information than the previous RoIpooling/RoI-align operation <ref type="bibr" target="#b26">[21]</ref>, <ref type="bibr" target="#b28">[23]</ref>, <ref type="bibr" target="#b29">[24]</ref>. It is because a single keypoint can contribute to multiple RoI-grid points due to the overlapped neighboring balls of RoI-grid points, and their receptive fields are even beyond the RoI boundaries by capturing the contextual keypoint features outside the 3D RoI. In contrast, the previous state-of-the-art methods either simply average all point-wise features within the proposal as the RoI feature <ref type="bibr" target="#b26">[21]</ref>, or pool many uninformative zeros as the RoI features because of the very sparse pointwise features <ref type="bibr" target="#b28">[23]</ref>, <ref type="bibr" target="#b29">[24]</ref>. Proposal Refinement and Confidence Prediction. Given the RoI feature extracted by the above RoI-grid pooling module, the refinement network learns to predict the size and location (i.e. center, size and orientation) residuals relative to the 3D proposal box. Two sibling sub-networks are employed for confidence prediction and proposal refinement. Each sub-network consists of a two-layer MLP and a linear prediction layer. We follow <ref type="bibr" target="#b29">[24]</ref> to conduct the IoU-based confidence prediction. The binary cross-entropy loss is adopted to optimize the IoU branch while the box residuals are optimized with smooth-L1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Losses</head><p>The proposed PV-RCNN framework is trained end-to-end with the region proposal loss L rpn , keypoint segmentation loss L seg and the proposal refinement loss L rcnn . (i) We adopt the same region proposal loss L rpn as that in <ref type="bibr" target="#b21">[16]</ref>,</p><formula xml:id="formula_11">L rpn = L cls + ? r?O L smooth-L1 ( ?r a , ?r a ) + ?L dir ,<label>(8)</label></formula><p>where O = {x, y, z, l, h, w, ?}, the anchor classification loss L cls is calculated with the focal loss <ref type="bibr" target="#b12">[7]</ref>, L dir is a binary classification loss for orientation to eliminate the ambiguity of ?? a as in <ref type="bibr" target="#b21">[16]</ref>, and smooth-L1 loss is for anchor box regression with the predicted residual ?r a and regression target ?r a . Loss weights are set as ? = 0.2 and ? = 2.0 in the training process. (ii) The keypoint segmentation loss L seg is also conducted with the focal loss as mentioned in Sec. 4.1. (iii) The proposal refinement loss L rcnn includes the IoU-guided confidence prediction loss L iou and the box refinement loss as</p><formula xml:id="formula_12">L rcnn = L iou + r?O L smooth-L1 ( ?r p , ?r p ),<label>(9)</label></formula><p>where ?r p is the predicted box residual and ?r p is the proposal regression target that is encoded same with ?r a . The overall training loss are then the sum of these three losses with equal loss weights.  <ref type="figure">Fig. 3</ref>. The overall architecture of our proposed PV-RCNN++. We propose the sectorized proposal-centric keypoint sampling module to concentrate keypoints to the neighborhoods of 3D proposals while it can also accelerate the process with sectorized farthest point sampling. Moreover, our proposed VectorPool module is utilized in both the voxel set abstraction module and the RoI-grid pooling module to improve the local feature aggregation and save memory/computation resources</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BEV Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PV-RCNN++: FASTER AND BETTER 3D DE-TECTION WITH PV-RCNN FRAMEWORK</head><p>To make our PV-RCNN framework more practical for realworld applications, we propose an improved version of PV-RCNN, i.e., PV-RCNN++ framework, for more accurate and efficient 3D object detection with less resource consumption. As shown in <ref type="figure">Fig. 3</ref>, we present two novel modules to improve both the accuracy and efficiency of the PV-RCNN framework. One is the sectorized proposal-centric strategy for much faster and better keypoint sampling, and the other one is the VectorPool aggregation module for more effective and efficient local feature aggregation from large-scale point clouds. These two novel modules are adopted to replace their counterparts in the PV-RCNN framework, which are introduced in Sec. 5.1 and Sec. 5.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sectorized Proposal-Centric Sampling for Efficient and Representative Keypoint Sampling</head><p>The keypoint sampling is critical for our PV-RCNN framework since keypoints bridge the point-voxel representations and influence the performance of proposal refinement. However, the previous keypoint sampling algorithm has two main drawbacks. (i) Farthest point sampling is timeconsuming due to its quadratic complexity, which hinders the training and inference speed of PV-RCNN, especially for keypoint sampling of large-scale points. (ii) Previous algorithm would generate a large number of background keypoints that are generally useless to the proposal refinement, since only the keypoints around the proposals can be retrieved by the RoI-grid pooling module. To mitigate these drawbacks, we propose a more efficient and effective keypoint sampling algorithm for 3D object detection. Sectorized Proposal-Centric (SPC) Keypoint Sampling. As discussed above, the number of keypoints is limited and vanilla farthest point sampling algorithm would generate wasteful keypoints in the background regions, which decrease the capability of keypoints to well represent objects for proposal refinement. Hence, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we propose the Sectorized Proposal-Centric (SPC) keypoint sampling to uniformly sample keypoints from more concentrated neighboring regions of proposals, while also being much faster than the vanilla farthest point sampling algorithm, Specifically, we denote the raw point clouds as P, and denote the centers and sizes of 3D proposal as C and D, respectively. To better generate the set of restricted keypoints, we first restrict the keypoint candidates P to the neighboring point sets of all proposals as  where r (s) is a hyperparameter indicating the maximum extended radius of the proposals, and dx j , dy j , dz j are the sizes of the 3D proposal. Through this proposal-centric filtering process, the number of candidate keypoints for sampling is greatly reduced from |P| to |P |, which not only reduces the time complexity of the follow-up keypoint sampling, but also concentrates the limited number of keypoints to better encode the neighboring regions of proposals.</p><formula xml:id="formula_13">P = ? ? ? p i p i ? c j &lt; max(dxj ,dyj ,dzj ) 2 + r (s) , (dx j , dy j , dz j ) ? D ? R 3 , p i ? P ? R 3 , c j ? C ? R 3 , ? ? ? ,<label>(10)</label></formula><p>To further parallelize the keypoint sampling process for acceleration, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we divide the proposalcentric point set P into s sectors centered at the scene center, and the point set of the k-th sector can be represented as</p><formula xml:id="formula_14">S k = p i (arctan(py i , px i ) + ?) ? s 2? = k ? 1, p i = (px i , py i , pz i ) ? P ? R 3 ,<label>(11)</label></formula><p>where k ? {1, . . . , s}, and arctan(py i , px i ) ? (??, ?] is to indicate the angle between the positive X axis and the ray ended with (px i , py i ). Through this process, we divide the task of sampling n keypoints into s subtasks of sampling local keypoints, where the k-th sector samples |S k | |P | ? n keypoints from S k . These subtasks are eligible to be executed in parallel on GPUs, while the scale of keypoint sampling (i.e., the time complexity) is further reduced from |P | to max k?{1,...,s} |S k |. Note that here we adopt the farthest point sampling algorithm in each subtask since we find that farthest point sampling can generate more uniformly distributed keypoints to better cover the whole regions, which is critical for the final detection performance.</p><p>Therefore, our proposed sectorized proposal-centric keypoint sampling greatly reduces the scale of keypoint sampling from |P| to the much smaller max k?{1,...,s} |S k |, which not only effectively accelerates the keypoint sampling process, but also increases the capability of keypoint feature representation by concentrating the keypoints to the more important neighboring regions of 3D proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Local Vector Representation for Structure-Preserved Local Feature Learning from Point Clouds</head><p>How to aggregate informative features from local point clouds is critical in our proposed point-voxel-based object detection system. As discussed in Sec. 4, PV-RCNN adopts the set abstraction for local feature aggregation in both voxel set abstraction module and RoI grid-pooling. However, we observe that the set abstraction operation can be extremely time-and resource-consuming on large-scale point clouds. Hence, in this section, we first analyze the limitations of set abstraction, and then propose the VectorPool aggregation module for local feature aggregation on the large-scale point clouds, which is integrated into our PV-RCNN++ framework for more accurate and efficient 3D object detection. Limitations of Set Abstraction. As shown in Eqs. (2) and <ref type="bibr" target="#b12">(7)</ref>, the set abstraction operation samples T k point-wise features from each local neighborhood, which are encoded separately by several shared-parameter MLPs. Suppose that there are a total number of N local point-cloud neighborhoods and the feature dimensions of each point is C in , then N ? T k point-wise features with C in channels should be encoded by the shared-parameter MLP G(?). It generates point-wise features of size N ?T k ?C out (generally multiple MLPs are utilized and each MLP will generate such a tensor). Both the space complexity and computations would be significant when either the number of local neighborhoods or the number of MLPs or T k are quite large.</p><p>For instance, in our RoI-grid pooling module, the number of RoI-grid points can be very large (N = 100 ? 6 ? 6 ? 6 = 21, 600) with 100 proposals and grid size 6. This module is therefore slow and also consumes much GPU memory in PV-RCNN, which restricts its capability to be run on lightweight devices with limited computation and memory resources. Moreover, the max-pooling operation in set abstraction abandons the spatial distribution information of local points and harms the representation capability of locally aggregated features from point clouds. VectorPool Aggregation for Structure-Preserved Local Feature Learning. To extract more informative features from local point-cloud neighborhoods, we propose a novel local feature aggregation operation, VectorPool aggregation, which preserves spatial point distributions of local neighborhoods and also costs less memory/computation resources than the commonly used set abstraction. We propose to generate position-sensitive features in different local neighborhoods by encoding them with separate kernel weights and separate feature channels, which are then concatenated to explicitly represent the spatial structures of local point features.</p><p>Specifically, denote the input point coordinates and fea-</p><formula xml:id="formula_15">tures as P = {(f i , p i ) | f i ? R Cin , p i ? R 3 , i ? {1, .</formula><p>. . , M }}, and the centers of N local neighborhoods as Q = {q i | q i ? R 3 , i ? {1, . . . , N }}. We are going to extract N local pointwise features with C out channels for each point in Q.</p><p>To reduce the parameter size, computational and memory consumption, motivated by <ref type="bibr" target="#b92">[87]</ref>, we first summarize the point-wise features f i ? R Cin to more compact representationsf i ? R Cr 1 with a parameter-free scheme a?</p><formula xml:id="formula_16">f i (k) = nr?1 j=0 f i (jC r1 + k), k ? {0, . . . , C r1 ? 1},<label>(12)</label></formula><p>where C r1 is the number of output feature channels and C in = n r ? C r1 . Eq. (12) sums up every n r input feature channels into one output feature channel to reduce the feature channels by n r times, which can effectively reduce the resource consumptions of the follow-up processing.</p><p>To generate position-sensitive features for a local cubic neighborhood centered at q k , we split its neighboring cubic space (denote the half length of this cubic space as l) into n x ? n y ? n z small local voxels. The features of each local voxel are generated by aggregating the above point-wise features from its neighborhood. Specifically, this feature aggregation process depends on the set of neighboring points of each local neighborhood q k , hence we first identify these neighboring points as follows:</p><formula xml:id="formula_17">Y k = ? ? ? f j , p j max(|r x |, |r y |, |r z |) &lt; 2 ? l, (r x , r y , r z ) = (p j ? q k ) ? R 3 , ?(p j , f j ) ? P ? ? ? .<label>(13)</label></formula><p>Note that we double the half length (e.g., 2?l) of the original cubic space to contain more neighboring points for feature aggregation of the small local voxels. For calculating the features of local voxel with index (i x , i y , i z ), inspired by <ref type="bibr" target="#b24">[19]</ref>, we utilize the inverse distance weighted strategy to interpolate its features based on 3 nearest neighbors from Y k :</p><formula xml:id="formula_18">f (v) ix,iy,iz = 3 j=1 w ?(j)f?(j) 3 j=1 w ?(j) , f ?(j) , p ?(j) ? Y k ,<label>(14)</label></formula><p>where w ?(j) = (||p ?(j) ? v ix,iy,iz ||) ?1 with v ix,iy,iz as the center position of this local voxel, and ?(j) finds the index of its three nearest neighbor in Y k . Here we have</p><formula xml:id="formula_19">i x ? {1, . . . , n x }, i y ? {1, . . . , n y }, i z ? {1, . . . , n z }. The resulted features f (v)</formula><p>ix,iy,iz encodes the local features of the specific local voxel (i x , i y , i z ) in this local cubic.</p><p>There are also two other alternative strategies to aggregate the features of local voxels by simply averaging the features within each local voxel or by randomly choosing one point within each local voxel. Both of them generate lots of empty features in the empty local voxels, which may degrade the performance. In contrast, our interpolation based strategy can generate more effective features even on empty local voxels.</p><p>Those features in different local voxels may represent very different local features. Hence, instead of encoding the local features with a shared-parameter MLP as in <ref type="bibr" target="#b24">[19]</ref>, we propose to encode different local voxels with separate local kernel weights for capturing position-sensitive features a?</p><formula xml:id="formula_20">U (i x , i y , i z ) = E r ix,iy,iz , f (v) ix,iy,iz ? W v ,<label>(15)</label></formula><p>where? (i x , i y , i z ) ? R Cr 2 ,r ix,iy,iz ? R (3?3) indicating the relative positions of its three nearest neighbors, and E(?) is an operation fusing the relative position and features (by default, we use concatenation). W v ? R (9+Cr 1 )?Cr 2 is the learnable kernel weights for encoding the specific features of local voxel (i x , i y , i z ) with channel C r2 , and different positions have different kernel weights for encoding positionsensitive local features. Finally, we directly sort the local voxel feature? U (i x , i y , i z ) according to their spatial order (i x , i y , i z ), and their features are sequentially concatenated to generate the final local vector representation as U = <ref type="figure" target="#fig_0">[? (1, 1, 1)</ref>, . . . ,? (i x , i y , i z ), . . . ,? (n x , n y , n z )], <ref type="bibr" target="#b21">(16)</ref> where U ? R nx?ny?nz?Cr 2 encodes the structure-preserved local features by simply assigning the features of different locations to their corresponding feature channels, which naturally preserves the spatial structures of local features in the neighboring space centered at q k , This local vector representation would be finally processed with another MLPs to encode the local features to C out feature channels for follow-up processing.</p><p>Note that compared with set abstraction, our proposed VectorPool aggregation can greatly reduce the needed computations and memory resources by adopting channel summation and utilizing our local vector representation before MLPs. Moreover, instead of conducting max-pooling on local point-wise features as in the set abstraction, our proposed spatial-structure-preserved local vector representation can encode the position-sensitive local features with different feature channels, to provide more effective representation for local feature learning. PV-RCNN++ with VectorPool Aggregation for Local Feature Aggregation. Our proposed VectorPool aggregation is integrated in our PV-RCNN++ detection framework, to replace the set abstraction operation in both the voxel set abstraction layer and the RoI-grid pooling module. Thanks to our VectorPool aggregation operation, compared with PV-RCNN framework, our PV-RCNN++ not only consumes much less memory and computation resources, but also achieves better 3D detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we evaluate our proposed framework in the large-scale Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref> and the highlycompetitive KITTI dataset <ref type="bibr" target="#b91">[86]</ref>. In Sec. 6.1, we first introduce our experimental setup and implementation details. In Sec. 6.2, we conduct extensive ablation experiments and analysis to investigate the individual components of both our PV-RCNN and PV-RCNN++ frameworks. In Sec. 6.3, we present the main results of our PV-RCNN/PV-RCNN++ frameworks and compare with state-of-the-art methods on both the Waymo dataset and the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Datasets and Evaluation Metrics. Our methods are evaluated on the following two datasets.</p><p>Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref> is currently the largest dataset with LiDAR point clouds for autonomous driving. There are totally 798 training sequences with around 160k LiDAR samples, 202 validation sequences with 40k LiDAR samples and 150 testing sequences with 30k LiDAR samples. It annotated the objects in the full 360 ? field instead of annotating the front view only as in KITTI dataset. The evaluation metrics are calculated by the official evaluation tools, where the mean average precision (mAP) and the mean average precision weighted by heading (mAPH) are used for evaluation. The 3D IoU threshold is set as 0.7 for vehicle detection and 0.5 for pedestrian/cyclist detection. We present the comparison in terms of two ways. The first way is based on objects' different distances to the sensor: 0 ? 30m, 30 ? 50m and &gt; 50m. The second way is to split the data into two difficulty levels, where the LEVEL 1 denotes the ground-truth objects with at least 5 inside points while the LEVEL 2 denotes the ground-truth objects with at least 1 inside points. As utilized by the official Waymo evaluation server, the mAPH of LEVEL 2 difficulty is the most important evaluate metric for all experiments.</p><p>KITTI Dataset <ref type="bibr" target="#b91">[86]</ref> is one of the most popular 3D detection datasets for autonomous driving. There are 7, 481 training samples and 7, 518 test samples. We compare PV-RCNNs with state-of-the-art methods on this highlycompetitive 3D detection learderboard <ref type="bibr" target="#b93">[88]</ref>. The evaluation metrics are calculated by the official evaluation tools, where the mean average precision (mAP) is calculated with 40 recall positions on three difficulty levels.</p><p>Network Architecture. For the PV-RCNN framework, the 3D voxel CNN has four levels (see <ref type="figure" target="#fig_0">Fig. 1</ref>) with feature dimensions 16, 32, 64, 64, respectively. Their two neighboring radii r k of each level in the voxel set abstraction module are set as (0.4m, 0.8m), (0.8m, 1.2m), (1.2m, 2.4m), (2.4m, 4.8m), and the neighborhood radii of set abstraction for raw points are (0.4m, 0.8m). For the proposed RoI-grid pooling operation, we uniformly sample 6?6?6 grid points in each 3D proposal and the two neighboring radiir of each grid point are (0.8m, 1.6m).</p><p>For the PV-RCNN++ framework, we set the maximum extended radius r (s) = 1.6m for the proposal-centric filtering, and each scene is split into 6 sectors for parallel keypoint sampling. Two VectorPool aggregation operations are adopted to the 4? and 8? feature volumes of the voxel set abstraction module with the half length l = (1.2m, 2.4m) and l = (2.4m, 4.8m) respectively, and both of them have local voxels n x = n y = n z = 3 and channel reduced factor n r = 2. The VectorPool aggregation operation on the raw points is set with local voxels n x = n y = n z = 2 and without channel reduction. All VectorPool aggregation utilize the concatenation operation as E(?) for encoding relative positions and point-wise features. For RoI-grid pooling, we adopt the same number of RoI-grid points (6 ? 6 ? 6) with PV-RCNN, and the utilized VectorPool aggregation has local voxels n x = n y = n z = 3, channel reduced factor n r = 3 and half length l = (0.8m, 1.6m).</p><p>Training and Inference Details. Both PV-RCNN and PV-RCNN++ frameworks are trained from scratch in an endto-end manner with ADAM optimizer, learning rate 0.01 and cosine annealing learning rate decay strategy. To train the proposal refinement stage, we randomly sample 128 proposals with 1:1 ratio for positive and negative proposals, where a proposal is considered as a positive sample if it has at least 0.55 3D IoU with the ground-truth boxes, otherwise it is treated as a negative sample.</p><p>During training, we adopt the widely used data augmentation strategies for 3D object detection, including the random scene flipping, global scaling with a random scaling factor sampled from [0.95, 1.05], global rotation around Z axis with a random angle sampled from [? ? 4 , ? 4 ], and the ground-truth sampling augmentation <ref type="bibr" target="#b21">[16]</ref> to randomly "paste" some new objects from other scenes to current training scene for simulating objects in various environments. For the inference speed, our final PV-RCNN++ framework can achieve state-of-the-art performance with 10 FPS for 150m ? 150m detection range on the Waymo Open Dataset (three times faster than PV-RCNN framework), while also achieving state-of-the-art performance with 16 FPS for 70m ? 80m detection region on the KITTI dataset. Both of them are profiling on a single TITAN RTX GPU card. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Feature</head><note type="other">Extraction RoI Pooling Module LEVEL</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1</head><p>Effects of voxel set abstraction (VSA) and RoI-grid pooling modules. We adopt same UNet-decoder with <ref type="bibr" target="#b29">[24]</ref>. All experiments are based on our PV-RCNN framework with an anchor-based head for proposal generation, and only the above two modules are changed during the ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Studies for PV-RCNN Framework</head><p>We investigate the individual components of our proposed PV-RCNN/PV-RCNN++ frameworks with extensive ablation experiments. We conduct all experiments on the large Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref> with detection range 150m ? 150m for more comprehensive evaluation. For efficiently conducting the ablation experiments, we generate a small representative training set by uniformly sampling 20% frames (about 32k frames) from the training set as in <ref type="bibr" target="#b30">[25]</ref>, and all results are evaluated on the full validation set (about 40k frames) with the official evaluation tool. All models are trained with 30 epochs and batch size 2 on each GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">The Component Analysis of PV-RCNN</head><p>In this section, all models are equipped with the anchorbased RPN head as in <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b29">[24]</ref>, and all experiments are conducted on the vehicle category of Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref>. Effects of Voxel-to-Keypoint Scene Encoding. In Sec. 4.1, we propose the voxel-to-keypoint scene encoding strategy to encode the global scene features to a small set of keypoints, which serves as a bridge between the backbone network and the proposal refinement network. As shown in the 2 nd and 4 th rows of <ref type="table">Table 1</ref>, our proposed voxelto-keypoint encoding strategy achieves comparable performance with the UNet-based decoder by summarizing the scene features to much less point-wise features than the UNet-based decoder. For instance, our voxel set abstraction module encodes the whole scene to around 4k keypoints for feeding into the RoI-grid pooling module, while the UNet-based decoder network needs to summarize the scene features to around 80k point-wise features, which validates the effectiveness of our proposed voxel-to-keypoint scene encoding strategy. We consider that it might benefit from the fact that the keypoint features are aggregated from multi-scale feature volumes and raw point clouds with large receptive fields, while also keeping the accurate point locations. Besides that, the feature dimension of UNet-based decoder is generally smaller than the feature dimensions of our keypoints since it is limited to its large memory consumption on large-scale point clouds, which may degrade its performance. We also notice that our voxel set abstraction module achieves worse performance (the 1 st and 3 rd rows of Table 1) than the UNet-decoder when it is combined with RoIaware pooling <ref type="bibr" target="#b29">[24]</ref>. This is to be expected since RoI-aware pooling module generates lots of empty voxels in each proposal by taking only 4k keypoints, which may degrade the performance. In contrast, our voxel set abstraction module can be ideally combined with RoI-grid pooling module and VSA Input Feature LEVEL 1 (3D) LEVEL 2 (3D) f    can slightly improve the performance and the best performance is achieved by taking all the feature components as the keypoint features. Effects of Predicted Keypoint Weighting Module. We propose the predicted keypoint weighting (PKW) module in Sec. 4.1 to re-weight the point-wise features of keypoints with extra keypoint segmentation supervision. As shown in <ref type="table" target="#tab_7">Table 3</ref>, the experiments show that the performance slightly drops after removing this module, which demonstrates that the predicted keypoint weighting module enables better multi-scale feature aggregation by focusing more on the foreground keypoints, since they are more important for the succeeding proposal refinement network. Effects of RoI-grid pooling module. RoI-grid pooling module is proposed in Sec. 4.2 for aggregating RoI features from very sparse keypoints. Here we investigate the effects of RoI-grid pooling module by replacing it with the RoIaware pooling <ref type="bibr" target="#b29">[24]</ref> and keeping other modules consistent. As shown in <ref type="table">Table 1</ref>, the experiments show that the performance drops significantly when replacing RoI-grid pooling module. It validates that our proposed RoI-grid pooling module can aggregate much richer contextual information to generate more discriminative RoI features.</p><formula xml:id="formula_21">(pv 1 ),(pv 2 ) i f (pv 3 ),(pv 4 ) i f (bev) i f (</formula><p>Compared with the previous RoI-aware pooling module, our proposed RoI-grid pooling module <ref type="bibr" target="#b29">[24]</ref>    denser grid-wise feature representation by supporting different overlapped ball areas among different grid points, while RoI-aware pooling module may generate lots of zeros due to the sparse inside points of RoIs. That means our proposed RoI-grid pooling module is especially effective for aggregating local features from very sparse point-wise features, such as in our PV-RCNN framework to aggregate features from a very small number of keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">The Component Analysis of PV-RCNN++</head><p>In this section, the experiments in <ref type="table">Table 4</ref> adopt the same setting with the experiments in Sec. 6.2.1. Except for that, all other models are equipped with the center-based RPN head as in <ref type="bibr" target="#b89">[84]</ref>, and all experiments are jointly conducted on three categories (vehicle, pedestrian and cyclist) of Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref>, where the mAPH of LEVEL 2 difficulty is adopted as the evaluation metric as used by the official Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Proposal-Centric Filtering for Keypoint Sampling:</head><p>In the 1 st and 2 nd rows of <ref type="table">Table 4</ref>, we investigate the effectiveness of our proposed proposal-centric keypoint filtering (see Sec. 5.1), where we can see that compared with the strong baseline PV-RCNN, our proposal-centric keypoint filtering can further improve the detection performance by about 1.0 mAP/mAPH in both LEVEL 1 and LEVEL 2 difficulties. It validates our argument that our proposed proposal-centric keypoint sampling strategy can generate more representative keypoints by concentrating the small number of keypoints to the more informative neighboring regions of proposals. Moreover, improved by our proposal-centric keypoint filtering, our keypoint sampling algorithm is about five times (133ms vs. 27ms) faster than the baseline farthest point sampling algorithm by reducing the number of candidate keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Different Strategies for Local Keypoint</head><p>Sampling. In Sec. 5.1, we propose the sectorized farthest point sampling algorithm to further speed up the local keypoint sampling after the above proposal-centric filtering. Besides our proposed algorithm, we further explore four alternative strategies for accelerating the keypoint sampling strategy, which are as follows: (i) Random Sampling: the keypoints are randomly chosen from raw points. (ii) Voxelized-FPS-Voxel: the raw points are firstly voxelized to reduce the number of points (i.e., voxels), then the farthest point sampling is applied to sample keypoints from voxels by taking the voxel centers. (iii) Voxelized-FPS-Point: unlike Voxelized-FPS-Voxel, here a raw point is randomly selected within the selected voxels as keypoints.</p><p>(iv) RandomParallel-FPS: the raw points are randomly split into several groups, then farthest point sampling is utilized to these groups in parallel for faster keypoint sampling. As shown in <ref type="table">Table 4</ref>, compared with the vanilla farthest point sampling (2 nd row) algorithm, the detection performances of all four alternative strategies drop a lot. In contrast, the performance of our proposed sectorized farthest point sampling algorithm is on par with the vanilla farthest point sampling algorithm while being three times (27ms vs. 9ms) faster than the vanilla farthest point sampling algorithm. We argue that the uniformly distributed keypoints are important for the following proposal refinement, where a better keypoint distribution should cover more input points to benefit the scene feature aggregation. Hence, to better demonstrate the quality of different keypoint distributions in terms of statistics, we propose an evaluation metric, coverage rate, which is defined as the ratio of input points that are within the coverage region of any keypoints. Specifically, for a set of input points P = {p 1 , p 2 , ? ? ? , p m } and a set of sampled keypoints K = {p 1 , p 2 , ..., p n }, the coverage rate C can be formulated as follows:</p><formula xml:id="formula_22">C = m i=1 min 1.0, n j=1 1 ||p i ? p j || &lt; R c m ,<label>(17)</label></formula><p>where R c is a scalar that denotes the coverage radius of each keypoint, and 1 (?) is the indicator function to indicate whether p i is covered by p j . As shown in <ref type="table">Table 4</ref>, we evaluate the coverage rate of different keypoint sampling algorithms in terms of multiple coverage radii. Our proposed sectorized farthest point sampling achieves similar average coverage rate (84.76%) with the vanilla farthest point sampling algorithm (84.78%), which is much better than other sampling algorithms. The higher average coverage rate demonstrates that our proposed sectorized farthest point sampling can sample more uniformly distributed keypoints to better cover the input points, which is consistent with the qualitative visualization of different keypoint sampling strategies as in <ref type="figure">Fig. 6</ref>.</p><p>Besides that, the coverage rate <ref type="table">(Table 4</ref>) and qualitative visualization <ref type="figure">(Fig. 6</ref>) also demonstrate some properties of different sampling algorithms: (i) The random sampling can achieve higher coverage rate with small radius (51.44% with 0.1m radius) since it can probably sample more keypoints on the clustered region, but it can not guarantee the coverage rate with large radius (94.74% with 0.5m radius) and may lose some important points such as the distant points. (ii) Compared with the Voxelized-FPS-Voxel, the Voxelized-FPS-Point achieves much better coverage rate with small</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Sampling</head><p>Voxelized-FPS-Voxel Voxelized-FPS-Point RandomParallel-FPS Sectorized-FPS (Ours) <ref type="figure">Fig. 6</ref>. Illustration of the keypoint distributions from different keypoint sampling strategies. Some dashed circles are utilized to highlight the missing parts and the clustered keypoints after using these keypoint sampling strategies. We can see that our proposed Sectorized-FPS generates better uniformly distributed keypoints that cover more input points to better encode the scene features for proposal refinement, while other strategies may miss some important regions or generate some clustered keypoints.  <ref type="table">TABLE 5</ref> Effects of different components in our proposed PV-RCNN++ frameworks. All models are trained with 20% frames from the training set and are evaluated on the full validation set of the Waymo Open Dataset, and the evaluation metric is the mAPH in terms of LEVEL 1 (L1) and LEVEL 2 (L2) difficulties as used in <ref type="bibr" target="#b90">[85]</ref>. "FPS" denotes farthest point sampling, "SPC-FPS" denotes our proposed sectorized proposal-centric keypoint sampling strategy, "VSA" denotes the voxel set abstraction module, "SA" denotes the set abstraction operation and "VP" denotes our proposed VectorPool aggregation. All models adopt the center-based head for proposal generation.</p><p>radii and also achieves better performance by taking raw points as keypoints. However, it is still about twice slower than our method and its performance is also lower than our method. (iii) The RandomParallel-FPS generates small clustered keypoints since the nearby raw points can be divided into different groups, and all of them can be sampled as keypoints from different groups. In short, our sectorized farthest point sampling can generate uniformly distributed keypoints to better cover the input points, by splitting points into different groups based on the radial distribution of LiDAR points. Although there may still exist a very small number of clustered keypoints in the margins of different groups, the experiments show that they have negligible effect on the performance. We consider the reason may be that the clustered keypoints are mostly in the regions around the scene centers, where the objects are generally easier to detect since the raw points around scene centers are much denser than distant regions. Effects of VectorPool Aggregation. In Sec. 5.2, to tackle the resource-consuming problem of set abstraction in our PV-RCNN framework, we propose the VectorPool aggregation module to effectively and efficiently summarize the structure-preserved local features from point clouds. As shown in Tabel 5, by adopting our VectorPool aggregation in both the voxel set abstraction module and the RoI-grid pooling module, our PV-RCNN++ framework consumes much less computations (i.e., a reduction of 4.679 GFLOPS) and GPU memory (from 10.62GB to 7.58GB) than the original PV-RCNN framework, while the performance is also consistently increased from 65.92% to 66.87% in terms of average mAPH (LEVEL 2) of three categories. Note that the reduction of memory consumption / calculations can be more significant with larger batch size.</p><p>The significant reduction of memory consumption and calculations demonstrates the effectiveness of our Vector-Pool aggregation for feature learning from large-scale point clouds, which makes our PV-RCNN++ framework a more practical 3D detector to be used on resource-limited devices. Moreover, the final PV-RCNN++ framework also benefits from the structure-preserved spatial features from our Vec-torPool aggregation, which is critical for the following finegrained proposal refinement.</p><p>We further analyze the effects of VectorPool aggregation by separately investigating the effects of channel reduction <ref type="bibr" target="#b92">[87]</ref> (see Eq. 12). As shown in <ref type="table" target="#tab_12">Table 6</ref>, our VectorPool aggregation is more effective in reducing the memory consumption no matter whether the channel reduction is incorporated (by comparing the 1 st / 3 rd rows or the 2 nd / 4 th rows), since the activations in our VectorPool aggregation modules consume much less memory than those in the set abstraction, by adopting a single local vector representation before multi-layer percetron networks. Meanwhile, <ref type="table" target="#tab_12">Table 6</ref> Strategy   <ref type="bibr" target="#b92">[87]</ref>. For the local feature extraction strategies, "SA" denotes the set abstraction operation and "VP" denotes our proposed VectorPool aggregation module. All experiments are based on our PV-RCNN++ framework with a center-based head for proposal generation, and only the local feature extraction modules are changed during the ablation experiments.  also demonstrates that our proposed VectorPool aggregation can achieve better performance than set abstraction <ref type="bibr" target="#b24">[19]</ref> in both two cases (with or without channel reduction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Different Feature Aggregation Strategies for</head><p>Local Voxels. As mentioned in Sec. 5.2, in addition to our adopted interpolation-based method, there are two alternative strategies (average pooling and random selection) for aggregating features of local voxels. We investigate the effects of these three strategies in <ref type="table" target="#tab_14">Table 7</ref>, where we notice that the our interpolation based feature aggregation achieves much better performance than the other two strategies, especially for the small objects like pedestrian and cyclist. We consider that our strategy can generate more effective features by interpolating from three nearest neighbors (even beyond this local voxel), while both of the other two methods might generate lots of zero features on the empty local voxels, which may degrade the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Separate Local Kernel Weights in VectorPool</head><p>Aggregation. We have demonstrated the fact in Eq. (15) that our proposed VectorPool aggregation generates positionsensitive features by encoding relative position features with separate local kernel weights. The 1 st and 2 nd rows of    <ref type="table" target="#tab_17">Table 9</ref>, we investigate the effects of the number of keypoints for encoding the scene features. <ref type="table" target="#tab_17">Table 9</ref> shows that larger number of keypoints achieves better performance, and similar performance is observed when using more than 4,096 keypoints. Hence, to balance the performance and computation cost, we empirically choose to encode the whole scene to 4,096 keypoints for the Waymo dataset (2,048 keypoints for the KITTI dataset since it only needs to detect the frontal-view areas). The above experiments show that our method can effectively encode the whole scene to a small number of keypoints while keeping similar performance with a large number of keypoints, which demonstrates the effectiveness of the keypoint feature encoding strategy of our proposed PV-RCNN detection framework. Effects of the Grid Size in RoI-grid Pooling. <ref type="table" target="#tab_18">Table 10</ref> shows the performance of adopting different RoI-grid sizes for RoI-grid pooling module. We can see that the performance increases along with the RoI-grid sizes from 3 ? 3 ? 3 to 6 ? 6 ? 6, and the settings with larger RoI-grid sizes than 6?6?6 achieve similar performance. Hence we finally adopt RoI-grid size 6 ? 6 ? 6 for the RoI-grid pooling module.   <ref type="bibr" target="#b80">[75]</ref>. ?: re-implemented by ourselves with their open source code. ? : use the 3D voxel CNN with residual connections as the backbone network, while other settings of our PV-RCNN/PV-RCNN++ frameworks utilize a commonly used 3D voxel CNN without residual connections <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b29">[24]</ref>. We highlight the top two results for each evaluation metric.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 13</head><p>Performance comparison on the Waymo Open Dataset with 202 validation sequences for the vehicle detection. * : re-implemented by <ref type="bibr" target="#b80">[75]</ref>. ?: re-implemented by ourselves with their open source code.</p><p>Moreover, from <ref type="table" target="#tab_18">Table 10</ref> and <ref type="table">Table 5</ref>, we also notice that PV-RCNN++ with a much smaller RoI-grid size 4 ? 4 ? 4 (66.45% in terms of mAPH@L2) can also outperform PV-RCNN with larger RoI-grid size 6 ? 6 ? 6 (65.24% in terms of mAPH@L2), which further validates the effectiveness of our proposed sectorized proposal-centric sampling strategy and the VectorPool aggregation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Main Results of PV-RCNN Framework and Comparison with State-of-the-Art Methods</head><p>In this section, we demonstrate the main results of our proposed PV-RCNN/PV-RCNN++ frameworks, and make the comparison with state-of-the-art methods on both the largescale Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref> and the highly-competitive KITTI dataset <ref type="bibr" target="#b91">[86]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">3D Detection on the Waymo Open Dataset</head><p>To evaluate our methods on the Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref>, we adopt two settings of both PV-RCNN and PV-RCNN++ frameworks by equipping with different RPN heads for  proposal generation, which are the anchor-based RPN head as in <ref type="bibr" target="#b29">[24]</ref> and the center-based RPN head as in <ref type="bibr" target="#b89">[84]</ref>.</p><p>Comparison with State-of-the-Art Methods. As shown in <ref type="table" target="#tab_19">Table 11</ref>, by equipping with the commonly used 3D voxel CNN as in <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b29">[24]</ref>, our PV-RCNN++ with anchorbased RPN head achieves state-of-the-art performance on vehicle category, which surpasses the previous state-of-theart work CenterPoint <ref type="bibr" target="#b89">[84]</ref> with +1.65% mAPH of LEVEL 2 difficulty. Our PV-RCNN++ with center-based RPN head outperforms previous state-of-the-art works <ref type="bibr" target="#b29">[24]</ref>, <ref type="bibr" target="#b89">[84]</ref> on all three categories with remarkable performance gains (+1.61% for vehicle, +1.0% for pedestrian and +4.69% for cyclist in terms of mAPH of LEVEL 2 difficulty). Moreover, by improving the backbone network with residual connections as used in CenterPoint <ref type="bibr" target="#b89">[84]</ref>, the performance of PV-RCNN++ with center-based RPN head can be further boosted, which outperforms CenterPoint significantly with a +1.88% mAPH gain for vehicle detection and +2.70% mAPH gain for pedestrian detection in terms of LEVEL 2 difficulty. We also present the performance of center-based PV-RCNN/PV-RCNN++ at different distance ranges (see <ref type="table" target="#tab_7">Table 13</ref>, <ref type="table" target="#tab_22">Table 14 and Table 15</ref>) for reference, where we follow <ref type="bibr" target="#b80">[75]</ref>, <ref type="bibr" target="#b94">[89]</ref> to evaluate the models on the LEVEL 1 difficulty </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 16</head><p>Performance comparison on the KITTI test set. The results are evaluated by the mean Average Precision with 40 recall positions by submitting to the official KITTI evaluation server. "L" indicates the LiDAR-only methods while "R+L" indicates that multi-modality methods with both RGB images and LiDAR sensors.</p><p>for comparing with previous methods. Comparison between PV-RCNN and PV-RCNN++. <ref type="table" target="#tab_19">Table 11</ref> demonstrates that no matter which type of RPN head is adopted, our PV-RCNN++ framework consistently outperforms previous PV-RCNN framework on all three categories of all difficulty levels. Specifically, for the anchorbased setting, PV-RCNN++ surpasses PV-RCNN with a performance gain of +1.54% for vehicle, +3.33% for pedestrian and 4.24% for cyclist in terms of LEVEL 2 difficulty. By taking the center-based head, PV-RCNN++ also outperforms PV-RCNN with a +0.93% mAPH gain for vehicle, a +1.58% mAPH gain for pedestrian and a +1.83% mAPH gain for cyclist in terms of LEVEL 2 difficulty. Meanwhile, we also evaluate our PV-RCNN/PV-RCNN++ frameworks with center-based head on the test set by submitting to the official test server of Waymo Open Dataset <ref type="bibr" target="#b90">[85]</ref>. As shown in <ref type="table" target="#tab_6">Table 12</ref>, our PV-RCNN++ outperforms previous PV-RCNN with remarkable margins on all three categories. The stable and consistent improvements on both the validation and test set of Waymo Open Dataset prove the effectiveness of our proposed sectorized proposal-centric sampling algorithm and the VectorPool aggregation module. Moreover, our PV-RCNN++ consumes much less calculations and GPU memory than PV-RCNN framework, while also increasing the processing speed from 3.3 FPS to 10 FPS for the 3D detection of 150m?150m such a large area, which further validates the efficiency and the effectiveness of our proposed PV-RCNN++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">3D detection on the KITTI dataset</head><p>To evaluate our PV-RCNN frameworks on the highlycompetitive 3D detection learderboard of KITTI dataset, we train our models with 80% of train + val data and the remaining 20% data is used for validation. Both PV-RCNN and PV-RCNN++ frameworks are equipped with an anchorbased RPN head to generate 3D proposals. All results are evaluated by submitting to the official evaluation server. Comparison with State-of-the-Art Methods. As shown in <ref type="table" target="#tab_12">Table 16</ref>, our PV-RCNN and PV-RCNN++ outperform all published methods with remarkable margins on the most important moderate difficulty level. Specifically, compared with previous LiDAR-only state-of-the-art methods on the 3D detection benchmark of car category, our PV-RCNN++ increases the mAP by +1.78%, +2.17%, +2.06% on easy, moderate and hard difficulty levels, respectively. For the 3D detection and bird-view detection of both pedestrian and cyclist, our methods outperforms all previous methods with large margins on the moderate and hard difficulty level.</p><p>Compared with our preliminary work PV-RCNN, our PV-RCNN++ achieves better performance on the moderate and hard levels of 3D detection over all three categories, while also greatly reducing the GPU-memory consumption and increasing the running speed from 10 FPS to 16 FPS in the KITTI dataset. The significant improvements on both the performance and the efficiency manifest the effectiveness of our PV-RCNN++ framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we present two novel frameworks, named PV-RCNN and PV-RCNN++, for accurate 3D object detection from point clouds. Our PV-RCNN framework adopts a novel voxel set abstraction module to deeply integrates both the multi-scale 3D voxel CNN features and the PointNetbased features to a small set of keypoints, and the learned discriminative keypoint features are then aggregated to the RoI-grid points through our proposed RoI-grid pooling module to capture much richer contextual information for proposal refinement. Our PV-RCNN++ further improves the PV-RCNN framework by efficiently generating more representative keypoints with our novel sectorized proposalcentric keypoint sampling strategy, and also by equipping with our proposed VectorPool aggregation module to learn structure-preserved local features in both the voxel set abstraction module and RoI-grid pooling module. Thus, our PV-RCNN++ finally achieves better performance with much faster running speed than the PV-RCNN.</p><p>Both of our proposed two PV-RCNN frameworks significantly outperform previous 3D detection methods and achieve new state-of-the-art performance on both the Waymo Open Dataset and the KITTI 3D detection benchmark, and extensive experiments are designed and conducted to deeply investigate the individual components of our proposed frameworks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall architecture of our proposed PV-RCNN. The raw point clouds are first voxelized to feed into the 3D sparse convolution based encoder to learn multi-scale semantic features and generate 3D object proposals. Then the learned voxel-wise feature volumes at multiple neural layers are summarized into a small set of key points via the novel voxel set abstraction module. Finally the keypoint features are aggregated to the RoI-grid points to learn proposal specific features for fine-grained proposal refinement and confidence prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(l k ) i</head><label></label><figDesc>for saving computations, G(?) denotes a multi-layer perceptron (MLP) network to encode voxel-wise features and relative locations. The operation max(?) maps diverse number of neighboring voxel features to a single keypoint feature f (pv k ) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of RoI-grid pooling module. Rich context information of each 3D RoI is aggregated by the set abstraction operation with multiple receptive fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of Sectorized Proposal-Centric (SPC) keypoint sampling. It contains two steps, where the first proposal filter step concentrates the limited number of keypoints to the neighborhoods of proposals, and the following sectorized-FPS step divides the whole scene into several sectors for accelerating the keypoint sampling process while also keeping the keypoints uniformly distributed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of VectorPool aggregation for local feature aggregation from point clouds. The local space around a center point is divided into dense voxels, where the inside point-wise features are generated by interpolating from three nearest neighbors. The features of each volume are encoded with position-specific kernels to generate position-sensitive features, that are sequentially concatenated to generate the local vector representation to explicitly encode the spatial structure information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>neither 2Dsemantic-only nor point-only are enough for the proposal refinement. (ii) As shown in 5 th row of Table 2, f (pv3) i and f (pv4) i contain both 3D structure information and high level semantic features, which can improve the performance a lot by combining with the bird-view semantic features f (bev) i and the raw point locations f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4</head><label>4</label><figDesc>Effects of different keypoint sampling algorithms. The running time is the average running time of keypoint sampling process on the validation set of the Waymo Open Dataset. The coverage rate is calculated by averaging the coverage rate of each scene on the validation set of the Waymo Open Dataset. "FPS" indicates the farthest point sampling and "PC-Filter" indicates our proposal-centric filtering strategy. All experiments are conducted by adding different keypoint sampling algorithms to our PV-RCNN framework with an anchor-based head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>For the Waymo Open dataset, the detection range is set as [?75.2, 75.2]m for both the X and Y axes, and [?2, 4]m for the Z axis, while the voxel size is set as (0.1m, 0.1m, 0.15m). For the KITTI dataset, the detection range is set as [0, 70.4]m for X axis, [?40, 40]m for Y axis and [?3, 1]m for the Z axis, which is voxelized with voxel size (0.05m, 0.05m, 0.1m) in each axis.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>RoI-aware Pooling [24] 71.82 71.29 64.33 63.82 UNet-decoder RoI-grid Pooling 73.84 73.18 64.76 64.15 VSA RoI-aware Pooling [24] 69.89 69.25 61.03 60.46 VSA RoI-grid Pooling 74.06 73.38 64.99 64.38</figDesc><table><row><cell>1 (3D) LEVEL 2 (3D)</cell></row><row><cell>mAP mAPH mAP mAPH</cell></row><row><cell>UNet-decoder</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>raw) i mAP mAPH mAP mAPH 71.55 70.94 64.04 63.46 71.97 71.36 64.44 63.86 72.15 71.54 64.66 64.07 73.77 73.09 64.72 64.11 74.06 73.38 64.99 64.38 74.10 73.42 65.04 64.43</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2</head><label>2</label><figDesc>Effects of different feature components for voxel set abstraction module. All experiments are based on our PV-RCNN framework with an anchor-based head for proposal generation.</figDesc><table><row><cell>Use PKW</cell><cell cols="2">LEVEL 1 (3D) mAP mAPH</cell><cell cols="2">LEVEL 2 (3D) mAP mAPH</cell></row><row><cell></cell><cell>73.90</cell><cell>73.29</cell><cell>64.75</cell><cell>64.23</cell></row><row><cell></cell><cell>74.06</cell><cell>73.38</cell><cell>64.99</cell><cell>64.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3</head><label>3</label><figDesc>Effects of predicted keypoint weighting (PKW) module. The experiments are based on our PV-RCNN framework with an anchor-based head for proposal generation.</figDesc><table><row><cell>(bev) i</cell><cell>) or accurate point locations (f</cell></row></table><note>they can benefit each other by taking a small number of keypoints as the intermediate connection. Effects of Different Features for Voxel Set Abstraction. Our proposed voxel set abstraction module incorporates multiple feature components (see Sec. 4.1 ), and their effects are explored in Table 2. We can summarize the observations as follows: (i) The performance drops a lot if we only ag- gregate features from high level bird-view semantic features (f</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6</head><label>6</label><figDesc>Effects of VectorPool aggregation with and without channel reduction</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 7</head><label>7</label><figDesc></figDesc><table><row><cell>Effects of the feature aggregation strategies in the feature aggregation</cell></row><row><cell>process of local voxels of VectorPool aggregation. All experiments are</cell></row><row><cell>based on our PV-RCNN++ framework with a center-based head for</cell></row><row><cell>proposal generation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8</head><label>8</label><figDesc>We investigate the number of dense voxels n x ?n y ?n z in VectorPool aggregation for voxel set abstraction module and RoI-grid pooling module, where we can see that Vec-</figDesc><table><row><cell>show that the performance drops a bit if we remove</cell></row><row><cell>the separate local kernel weights and adopt shared kernel</cell></row><row><cell>weights for relative position encoding. It validates that</cell></row><row><cell>the separate local kernel weights are better than previous</cell></row><row><cell>shared-parameter MLP based local feature encoding, and it</cell></row><row><cell>is important in our VectorPool aggregation operation.</cell></row><row><cell>Effects of Dense Voxel Numbers in VectorPool Aggrega-</cell></row><row><cell>tion.</cell></row></table><note>torPool aggregation with 3 ? 3 ? 3 and 4 ? 4 ? 4 achieve similar performance while the performance of 2 ? 2 ? 2 setting drops a lot. We consider that our interpolation-based VectorPool aggregation can generate effective voxel-wise features even with large dense voxels, hence the setting with 4 ? 4 ? 4 achieves slightly better performance than</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 8</head><label>8</label><figDesc>Effects of separate local kernel weights and the number of dense voxels in our proposed VectorPool aggregation module. All experiments are based on our PV-RCNN++ framework with a center-based head for proposal generation.</figDesc><table><row><cell>Number of Keypoints</cell><cell cols="3">Vehicle Pedestrian Cyclist</cell><cell>Average</cell></row><row><cell>8192</cell><cell>68.85</cell><cell>64.11</cell><cell>67.88</cell><cell>66.95</cell></row><row><cell>4096</cell><cell>68.62</cell><cell>63.74</cell><cell>68.26</cell><cell>66.87</cell></row><row><cell>2048</cell><cell>67.99</cell><cell>62.14</cell><cell>67.41</cell><cell>65.85</cell></row><row><cell>1024</cell><cell>66.67</cell><cell>59.21</cell><cell>65.07</cell><cell>63.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table><row><cell>Effects of the number of keypoints for encoding the global scene. All</cell></row><row><cell>experiments are based on our PV-RCNN++ framework with a</cell></row><row><cell>center-based head for proposal generation.</cell></row><row><cell>the setting with 3 ? 3 ? 3. However, since the setting with</cell></row><row><cell>4 ? 4 ? 4 greatly improves the calculations and memory</cell></row><row><cell>consumption, we finally choose the setting of 3?3?3 dense</cell></row><row><cell>voxel representation in both voxel set abstraction module</cell></row><row><cell>(except the raw point layer) and RoI-grid pooling module</cell></row><row><cell>of our PV-RCNN++ framework.</cell></row></table><note>Effects of the Number of Keypoints. In</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 10</head><label>10</label><figDesc>Effects of the grid size in RoI-grid pooling module. All experiments are based on our PV-RCNN++ framework with a center-based head for proposal generation.</figDesc><table><row><cell>RoI-grid Size</cell><cell cols="3">Vehicle Pedestrian Cyclist</cell><cell>Average</cell></row><row><cell>8 ? 8 ? 8</cell><cell>68.88</cell><cell>63.74</cell><cell>67.84</cell><cell>66.82</cell></row><row><cell>7 ? 7 ? 7</cell><cell>68.76</cell><cell>63.81</cell><cell>68.00</cell><cell>66.85</cell></row><row><cell>6 ? 6 ? 6</cell><cell>68.62</cell><cell>63.74</cell><cell>68.26</cell><cell>66.87</cell></row><row><cell>5 ? 5 ? 5</cell><cell>68.28</cell><cell>63.54</cell><cell>67.69</cell><cell>66.50</cell></row><row><cell>4 ? 4 ? 4</cell><cell>68.21</cell><cell>63.58</cell><cell>67.56</cell><cell>66.45</cell></row><row><cell>3 ? 3 ? 3</cell><cell>67.33</cell><cell>62.93</cell><cell>67.22</cell><cell>65.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 11</head><label>11</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences.</figDesc><table /><note>* : re-implemented by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>RCNN 72.81 72.39 71.81 66.05 69.13 67.80</figDesc><table><row><cell>Difficulty</cell><cell>Method</cell><cell>Vehicle mAP mAPH mAP mAPH mAP mAPH Pedestrian Cyclist</cell></row><row><cell>LEVEL 1</cell><cell cols="2">PV-RCNN 80.60 80.15 78.16 72.01 71.80 70.42 PV-RCNN++ 81.62 81.20 80.41 74.99 71.93 70.76</cell></row><row><cell>LEVEL 2</cell><cell>PV-</cell><cell></cell></row></table><note>PV-RCNN++ 73.86 73.47 74.12 69.00 69.28 68.15</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 12 79.10 93.34 78.08 57.19 91.57 98.13 90.95 81.54</head><label>12</label><figDesc>Performance comparison on the test set of Waymo Open Dataset by submitting to the official test evaluation server. Both the PV-RCNN and PV-RCNN++ frameworks are equipped with the center-based RPN head for 3D proposal generation. 84.90 53.11 23.92 71.57 92.94 74.92 48.87 ? SECOND [16] 72.26 90.66 70.03 47.55 89.18 96.84 88.39 78.37 PointPillar [15] 56.62 81.01 51.75 27.94 75.57 92.10 74.06 55.47 MVF [75] 62.93 86.30 60.02 36.02 80.40 93.59 79.21 63.09 Pillar-based [78] 69.80 88.53 66.50 42.93 87.11 95.78 84.74 72.12 ? Part-A2-Net [24] 77.05 92.35 75.91 54.06 90.81 97.52 90.35 80.12 PV-RCNN (center) 78.00 92.96 76.47 55.96 90.99 97.76 90.20 80.69 PV-RCNN++ (center)</figDesc><table><row><cell>Method</cell><cell>Vehicle 3D mAP (IoU=0.7) Overall 0-30m 30-50m 50m-Inf Overall 0-30m 30-50m 50m-Inf Vehicle BEV mAP (IoU=0.7)</cell></row><row><cell>LaserNet [90]</cell><cell>55.10</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 14 73.49 83.65 68.90 51.41 75.94 84.06 72.89 57.10</head><label>14</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences for the pedestrian detection. * : re-implemented by [75]. ?: re-implemented by ourselves with their open source code. SECOND [16] 60.61 73.33 55.51 41.98 63.55 74.58 59.31 46.75 ? Part-A2-Net [24] 68.60 80.87 62.57 45.04 71.00 81.96 66.38 48.15 PV-RCNN (center) 71.46 81.10 65.65 52.58 74.00 82.07 69.43 57.61 PV-RCNN++ (center)</figDesc><table><row><cell>Method</cell><cell>Cyclist 3D mAP (IoU=0.7) Overall 0-30m 30-50m 50m-Inf Overall 0-30m 30-50m 50m-Inf Cyclist BEV mAP (IoU=0.7)</cell></row><row><cell></cell><cell></cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE 15</head><label>15</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences for the cyclist detection. ?: re-implemented by ourselves with their open source code.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>86 63.84</head><label></label><figDesc>Car) BEV Detection (Car) 3D Detection (Ped.) BEV Detection (Ped.) 3D Detection (Cyc.) BEV Detection (Cyc.) Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard MV3D [9] R+L 74.97 63.63 54.00 86.62 78.93 69.FPN [11] R+L 83.07 71.76 65.73 90.99 84.82 79.62 50.46 42.27 39.04 58.49 50.32 46.98 63.76 50.55 44.93 69.39 57.12 51.09 F-PointNet [20] R+L 82.19 69.79 60.59 91.17 84.67 74.77 50.53 42.15 38.08 57.13 49.57 45.48 72.27 56.12 49.01 77.26 61.37 53.78 UberATG-MMF [17] R+L 88.40 77.43 70.22 93.67 88.21 81.99 CVF at SPA [72] R+L 89.20 80.05 73.11 93.52 89.56 82.45 72.55 65.82 89.39 83.77 78.59 ------71.33 52.08 45.83 76.50 56.05 49.45 PointPillars [15] L 82.58 74.31 68.99 90.07 86.56 82.81 51.45 41.92 38.89 57.60 48.64 45.78 77.10 58.65 51.92 79.90 62.73 55.58 PointRCNN [21] L 86.96 75.64 70.70 92.13 87.39 82.72 47.98 39.37 36.01 54.77 46.13 42.84 74.96 58.82 52.53 82.56 67.24 60.28 3D IoU Loss [92] L 86.16 76.50 71.39 91.36 86.22 81.79.71 75.09 94.74 89.19 86.42 53.29 42.47 38.35 60.02 48.72 44.55 78.69 61.59 55.30 81.36 67.23 59.35 Part-A2-Net [24] L 87.81 78.49 73.51 91.70 87.79 84.61 53.10 43.35 40.06 59.04 49.81 45.92 79.17 63.52 56.93 83.43 68.73 61.85 3DSSD [81] L 88.36 79.57 74.55 92.66 89.02 85.86 54.64 44.27 40.23 60.54 49.94 45.73 82.48 64.10 56.90 85.04 67.62 61.14 Point-GNN [93] L 88.33 79.47 72.29 93.11 89.17 83.90 51.92 43.77 40.14 55.36 47.07 44.61 78.60 63.48 57.08 81.17 67.28 59.67 PV-RCNN (Ours) L 90.25 81.43 76.82 94.98 90.65 86.14 52.17 43.29 40.29 59.86 50.57 46.74 78.60 63.71 57.65 82.49 68.89 62.41 PV-RCNN++ (Ours) L 90.14 81.88 77.15 92.66 88.74 85.97 54.29 47.19 43.49 59.73 52.43 48.73 82.22 67.33 60.04 84.60 71.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell>3D Detection (80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ContFuse [13]</cell><cell>R+L</cell><cell>83.68 68.78 61.67 94.07 85.35 75.88</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">AVOD--</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">3D--</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLOCs [91]</cell><cell>R+L</cell><cell>88.94 80.67 77.15 93.05 89.80 86.57</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SECOND [16]</cell><cell>L</cell><cell>83.34 20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STD [23]</cell><cell>L</cell><cell>87.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Method Pedestrian 3D mAP (IoU=0.7) Pedestrian BEV mAP (IoU=0.7) Overall 0-30m 30-50m 50m-Inf Overall 0-30m 30-50m</title>
		<idno>50m-Inf LaserNet [90] 63.40 73.47 61.55 42.69 70.01 78.24 69.47 52.68</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Second</surname></persName>
		</author>
		<idno>16] 68.70 74.39 67.24 56.71 76.26 79.92 75.50 67.92</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Pointpillar</surname></persName>
		</author>
		<idno>15] 59.25 67.99 57.01 41.29 68.57 75.02 67.11 53.86</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Part</surname></persName>
		</author>
		<idno>A2-Net [24] 75.24 81.87 73.65 62.34 80.25 84.49 79.22 70.34</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pv-Pcnn</surname></persName>
		</author>
		<idno>center) 79.21 83.33 78.53 69.36 84.23 87.20 83.87 76.74</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pv-Rcnn++</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">80</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">STD: sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and partaggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Openpcdet: An open-source toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Libra rcnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Are cars just 3d boxes?-jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A coarse-to-fine model for 3d pose estimation and sub-category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06310</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Workshops</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">441</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Submanifold sparse convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ECCV</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Epnet: Enhancing point features with image semantics for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Hvnet: Hybrid voxel network for lidar based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Voxel-fpn: Multi-scale voxel feature aggregation for 3d object detection from lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">704</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Class-balanced grouping and sampling for point cloud 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09492</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Pillar-based object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10323</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">3dssd: Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Fishnet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
				<ptr target="http://www.cvlibs.net/datasets/kitti/evalobject.php?objbenchmark=3d" />
		<title level="m">KITTI leader board of 3D object detection benchmark</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Starnet: Targeted computation for object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11069</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Clocs: Camera-lidar object candidates fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
