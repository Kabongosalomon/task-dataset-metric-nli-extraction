<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransFG: A Transformer Architecture for Fine-Grained Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Neng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TransFG: A Transformer Architecture for Fine-Grained Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The selfattention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Fine-grained visual classification aims at classifying subclasses of a given object category, e.g., subcategories of birds <ref type="bibr" target="#b25">(Wah et al. 2011;</ref><ref type="bibr" target="#b22">Van Horn et al. 2015)</ref>, cars <ref type="bibr" target="#b17">(Krause et al. 2013)</ref>, aircrafts <ref type="bibr" target="#b20">(Maji et al. 2013</ref>). It has long been considered as a very challenging task due to the small interclass variations and large intra-class variations along with the deficiency of annotated data, especially for the longtailed classes. Benefiting from the progress of deep neural networks <ref type="bibr" target="#b18">(Krizhevsky, Sutskever, and Hinton 2012;</ref><ref type="bibr" target="#b21">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b14">He et al. 2016)</ref>, the performance of FGVC has obtained a steady progress in recent years. To avoid labor-intensive part annotation, the community currently focuses on weakly-supervised FGVC with Copyright ? 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="figure">Figure 1</ref>: An overview of performance comparison of ViT and TransFG with state-of-the-art methods CNNs on five datasets. We achieve state-of-the-art performance on most datasets while performing a little bit worse on Stanford Cars possibly due to the more regular and simpler car shapes.</p><p>only image-level labels. Methods now can be roughly classified into two categories, i.e., localization methods and feature-encoding methods. Compared to feature-encoding methods, the localization methods have the advantage that they explicitly capture the subtle differences among subclasses which is more interpretable and yields better results.</p><p>Early works in localization methods rely on the annotations of parts to locate discriminative regions while recent works <ref type="bibr" target="#b11">(Ge, Lin, and Yu 2019a;</ref><ref type="bibr" target="#b19">Liu et al. 2020;</ref><ref type="bibr" target="#b7">Ding et al. 2019</ref>) mainly adopt region proposal networks (RPN) to propose bounding boxes which contain the discriminative regions. After obtaining the selected image regions, they are resized into a predefined size and forwarded through the backbone network again to acquire informative local features. A typical strategy is to use these local features for classification individually and adopt a rank loss <ref type="bibr" target="#b4">(Chen et al. 2009</ref>) to maintain consistency between the quality of bounding boxes and their final probability output. However, this mechanism ignores the relation between selected regions and thus inevitably encourages the RPN to propose large bounding boxes that contain most parts of the objects which fails to locate the really important regions. Sometimes these bounding boxes can even contain large areas of background and lead to confusion. Additionally, the RPN module with different optimizing goals compared to the backbone network makes the network harder to train and the re-use of backbone complicates the overall pipeline.</p><p>Recently, the vision transformer <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020</ref>) achieved huge success in the classification task which shows that applying a pure transformer directly to a sequence of image patches with its innate attention mechanism can capture the important regions in images. A series of extended works on downstream tasks such as object detection <ref type="bibr" target="#b2">(Carion et al. 2020</ref>) and semantic segmentation <ref type="bibr" target="#b32">(Zheng et al. 2021;</ref><ref type="bibr" target="#b28">Xie et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2021)</ref> confirmed the strong ability for it to capture both global and local features.</p><p>These abilities of the Transformer make it innately suitable for the FGVC task as the early long-range "receptive field" <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020</ref>) of the Transformer enables it to locate subtle differences and their spatial relation in the earlier processing layers. In contrast, CNNs mainly exploit the locality property of image and only capture weak longrange relation in very high layers. Besides, the subtle differences between fine-grained classes only exist in certain places thus it is unreasonable to convolve a filter which captures the subtle differences to all places of the image.</p><p>Motivated by this opinion, in the paper, we present the first study which explores the potential of vision transformers in the context of fine-grained visual classification. We find that directly applying ViT on FGVC already produces satisfactory results while a lot of adaptations according to the characteristics of FGVC can be applied to further boost the performance. To be specific, we propose Part Selection Module which can find the discriminative regions and remove redundant information. A contrastive loss is introduced to make the model more discriminative. We name this novel yet simple transformer-based framework TransFG, and evaluate it extensively on five popular fine-grained visual classification benchmarks <ref type="bibr">(CUB-200-2011</ref>). An overview of the performance comparison can be seen in <ref type="figure">Fig 1 where</ref> our TransFG outperforms existing SOTA CNN methods with different backbones on most datasets. In summary, we make several important contributions in this work: 1. To the best of our knowledge, we are the first to verify the effectiveness of vision transformer on fine-grained visual classification which offers an alternative to the dominating CNN backbone with RPN model design. 2. We introduce TransFG, a novel neural architecture for fine-grained visual classification that naturally focuses on the most discriminative regions of the objects and achieve SOTA performance on several benchmarks. 3. Visualization results are presented which illustrate the ability of our TransFG to accurately capture discriminative image regions and help us to better understand how it makes correct predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we briefly review existing works on finegrained visual classification and transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Grained Visual Classification</head><p>Many works have been done to tackle the problem of finegrained visual classification and they can roughly be classified into two categories: localization methods <ref type="bibr" target="#b11">(Ge, Lin, and Yu 2019a;</ref><ref type="bibr" target="#b19">Liu et al. 2020;</ref><ref type="bibr" target="#b29">Yang et al. 2021</ref>) and featureencoding methods <ref type="bibr" target="#b30">(Yu et al. 2018;</ref><ref type="bibr" target="#b31">Zheng et al. 2019;</ref><ref type="bibr" target="#b10">Gao et al. 2020</ref>). The former focuses on training a detection network to localize discriminative part regions and reuse them to perform classification. The latter targets at learning more informative features by either computing higher-order information or finding the relationships among contrastive pairs.</p><p>Localization FGVC methods Previously, some works <ref type="bibr" target="#b1">(Branson et al. 2014;</ref><ref type="bibr" target="#b27">Wei, Xie, and Wu 2016)</ref> tried to exploit the part annotations to supervise the learning procedure of the localization process. However, since such annotations are expensive and usually unavailable, weakly-supervised parts proposal with only image-level labels draw more attentions nowadays. Ge et al. <ref type="bibr" target="#b11">(Ge, Lin, and Yu 2019a)</ref> exploited Mask R-CNN and CRF-based segmentation alternatively to extract object instances and discriminative regions. <ref type="bibr" target="#b29">Yang et al. (Yang et al. 2021)</ref> proposed a re-ranking strategy to rerank the global classification results based on the database constructed with region features. However, these methods all need a special designed module to propose potential regions and these selected regions need to be forwarded through the backbone again for final classification which is not required in our model and thus keeps the simplicity of our pipeline.</p><p>Feature-encoding methods The other branch of methods focus on enriching the feature representations to obtain better classification results. Yu et al. <ref type="bibr" target="#b30">(Yu et al. 2018)</ref> proposed a hierarchical framework to do cross-layer bilinear pooling. Zheng et al. <ref type="bibr" target="#b31">(Zheng et al. 2019</ref>) adopted the idea of group convolution to first split channels into different groups by their semantic meanings and then do the bilinear pooling within each group without changing the dimension thus it can be integrated into any existed backbones directly. However, these methods are usually not interpretable such one does not know what makes the model distinguish subcategories with subtle differences while our model drops unimportant image patches and only keeps those that contain most information for the fine-grained recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>Transformer and self-attention models have greatly facilitated research in natural language processing and machine translation <ref type="bibr" target="#b5">(Dai et al. 2019;</ref><ref type="bibr" target="#b6">Devlin et al. 2018;</ref><ref type="bibr" target="#b24">Vaswani et al. 2017)</ref>. Inspired by this, many recent studies try to apply transformers in computer vision area. Initially, transformer is used to handle sequential features extracted by CNN backbone for the videos <ref type="bibr" target="#b13">(Girdhar et al. 2019)</ref>. Later, transformer models are further extended to other popular computer vision tasks such as object detection <ref type="bibr" target="#b2">(Carion et al. 2020;</ref><ref type="bibr" target="#b33">Zhu et al. 2020)</ref>, segmentation <ref type="bibr" target="#b28">(Xie et al. 2021;</ref><ref type="bibr" target="#b26">Wang et al. 2021)</ref>, object tracking <ref type="bibr" target="#b22">(Sun et al. 2020)</ref>. Most recently, pure transformer models are becoming more and more popular.</p><p>ViT <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020</ref>) is the first work to show that applying a pure transformer directly to a sequence of image patches can yield state-of-the-art performance on image classification. Based on that, Zheng et al. <ref type="bibr" target="#b32">(Zheng et al. 2021)</ref> proposed SETR to exploit ViT as the encoder for segmentation. He et al. <ref type="bibr" target="#b15">(He et al. 2021)</ref> proposed TransReID which embedded side information into transformer along with the JPM to boost the performance on object re-identification. In this work, we extend ViT to fine-grained visual classification and show its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We first briefly review the framework of vision transformer and show how to do some preprocessing steps to extend it into fine-grained recognition. Then, the overall framework of TransFG will be elaborated.</p><p>Vision transformer as feature extractor Image Sequentialization. Following ViT, we first preprocess the input image into a sequence of flattened patches x p . However, the original split method cut the images into non-overlapping patches, which harms the local neighboring structures especially when discriminative regions are split.</p><p>To alleviate this problem, we propose to generate overlapping patches with sliding window. To be specific, we denote the input image with resolution H * W , the size of image patch as P and the step size of sliding window as S. Thus the input images will be split into N patches where</p><formula xml:id="formula_0">N = N H * N W = H ? P + S S * W ? P + S S<label>(1)</label></formula><p>In this way, two adjacent patches share an overlapping area of size (P ? S) * P which helps to preserve better local region information. Typically speaking, the smaller the step S is, the better the performance will be. But decreasing S will at the same time requires more computational cost, so a trade-off needs to be made here. Patch Embedding. We map the vectorized patches x p into a latent D-dimensional embedding space using a trainable linear projection. A learnable position embedding is added to the patch embeddings to retain positional information as follows:</p><formula xml:id="formula_1">z 0 = [x 1 p E, x 2 p E, ? ? ? , x N p E] + E pos (2)</formula><p>where N is the number of image patches, E ? R (P 2 ?C) * D is the patch embedding projection, and E pos ? R N * D denotes the position embedding. The Transformer encoder <ref type="bibr" target="#b24">(Vaswani et al. 2017</ref>) contains L layers of multi-head self-attention (MSA) and multi-layer perceptron (MLP) blocks. Thus the output of the l-th layer can be written as follows:</p><formula xml:id="formula_2">z l = M SA(LN (z l?1 )) + z l?1 l ? 1, 2, ? ? ? , L (3) z l = M LP (LN (z l )) + z l l ? 1, 2, ? ? ? , L (4) where LN (?)</formula><p>denotes the layer normalization operation and z l is the encoded image representation. ViT exploits the first token of the last encoder layer z 0 L as the representation of the global feature and forward it to a classifier head to obtain the final classification results without considering the potential information stored in the rest of the tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TransFG Architecture</head><p>While our experiments show that the pure Vision Transformer can be directly applied into fine-grained visual classification and achieve impressive results, it does not well capture the local information required for FGVC. To this end, <ref type="figure">Figure 3</ref>: A confusing pair of instances from the CUB-200-2011 dataset. Model needs to has the ability to capture the subtle differences in order to classify them correctly. The second column shows the overall attention maps and two selected tokens of our TransFG method. Best viewed in color.</p><p>we propose the Part Selection Module (PSM) and apply contrastive feature learning to enlarge the distance of representations between confusing sub-categories. The framework of our proposed TransFG is illustrated in <ref type="figure" target="#fig_0">Fig 2.</ref> Part Selection Module One of the most important problems in fine-grained visual classification is to accurately locate the discriminative regions that account for subtle differences between similar sub-categories. For example, <ref type="figure">Fig 3</ref> shows a confusing pair of images from the CUB-200-2011 (citation) dataset. The model needs to have the ability to capture the very small differences, i.e., the color of eyes and throat in order to distinguish these two bird species. Region proposal networks and weakly-supervised segmentation strategies are widely introduced to tackle this problem in the traditional CNN-based methods.</p><p>Vision Transformer model is perfectly suited here with its innate multi-head attention mechanism. To fully exploit the attention information, we change the input to the last Transformer Layer. Suppose the model has K self-attention heads and the hidden features input to the last layer are denoted as</p><formula xml:id="formula_3">z L?1 = [z 0 L?1 ; z 1 L?1 , z 2 L?1 , ? ? ? , z N L?1 ].</formula><p>The attention weights of the previous layers can be written as follows:</p><formula xml:id="formula_4">a l = [a 0 l , a 1 l , a 2 l , ? ? ? , a K l ] l ? 1, 2, ? ? ? , L ? 1 (5) a i l = [a i0 l ; a i1 l , a i2 l , ? ? ? , a i N l ] i ? 0, 1, ? ? ? , K ? 1 (6)</formula><p>Previous works <ref type="bibr" target="#b21">(Serrano and Smith 2019;</ref><ref type="bibr" target="#b0">Abnar and Zuidema 2020)</ref> suggested that the raw attention weights do not necessarily correspond to the relative importance of input tokens especially for higher layers of a model, due to lack of token identifiability of the embeddings. To this end, we propose to integrate attention weights of all previous layers. To be specific, we recursively apply a matrix multiplica-tion to the raw attention weights in all the layers as</p><formula xml:id="formula_5">a f inal = L?1 l=0 a l<label>(7)</label></formula><p>As a f inal captures how information propagates from the input layer to the embeddings in higher layers, it serves as a better choice for selecting discriminative regions compared to the single layer raw attention weights a L?1 . We then choose the index of the maximum value A 1 , A 2 , ? ? ? , A K with respect to the K different attention heads in a f inal . These positions are used as index for our model to extract the corresponding tokens in z L?1 . Finally, we concatenate the selected tokens along with the classification token as the input sequence which is denoted as:</p><formula xml:id="formula_6">z local = [z 0 L?1 ; z A1 L?1 , z A2 L?1 , ? ? ? , z A K L?1 ]<label>(8)</label></formula><p>By replacing the original entire input sequence with tokens corresponding to informative regions and concatenate the classification token as input to the last Transformer Layer, we not only keep the global information but also force the last Transformer Layer to focus on the subtle differences between different sub-categories while abandoning less discriminative regions such as background or common features among a super class.</p><p>Contrastive feature learning Following ViT, we still adopt the first token z i of the PSM module for classification. A simple cross-entropy loss is not enough to fully supervise the learning of features since the differences between sub-categories might be very small. To this end, we adopt contrastive loss L con which minimizes the similarity of classification tokens corresponding to different labels and maximizes the similarity of classification tokens of samples with the same label y. To prevent the loss being dominated by easy negatives (different class samples with little similarity), a constant margin ? is introduced that only negative pairs with similarity larger than ? contribute to the loss L con . Formally, the contrastive loss over a batch of size B is denoted as:</p><formula xml:id="formula_7">L con = 1 B 2 B i [ B j:yi=yj (1 ? Sim(z i , z j )+ B j:yi =yj max((Sim(z i , z j ) ? ?), 0)]<label>(9)</label></formula><p>where z i and z j are pre-processed with l2 normalization and Sim(z i , z j ) is thus the dot product of z i and z j .</p><p>In summary, our model is trained with the sum of crossentropy loss L cross and contrastive L con together which can be expressed as: L = L cross (y, y ) + L con (z) (10) where L cross (y, y ) is the cross-entropy loss between the predicted label y and the ground-truth label y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we first introduce the detailed setup including datasets and training hyper-parameters. Quantitative analysis is then given followed by ablation studies. We further give qualitative analysis and visualization results to show the interpretability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Setup</head><p>Datasets. We evaluate our proposed TransFG on five widely used fine-grained benchmarks, i.e., <ref type="bibr">CUB-200-2011</ref><ref type="bibr" target="#b25">(Wah et al. 2011</ref>, Stanford Cars <ref type="bibr" target="#b17">(Krause et al. 2013)</ref>, Stanford Dogs <ref type="bibr" target="#b16">(Khosla et al. 2011</ref><ref type="bibr">), NABirds (Van Horn et al. 2015</ref> and iNat2017 <ref type="bibr" target="#b23">(Van Horn et al. 2018)</ref>. We also exploit its usage in large-scale challenging fine-grained competitions. Implementation details. Unless stated otherwise, we implement TransFG as follows. First, we resize input images to 448 * 448 except 304 * 304 on iNat2017 for fair comparison (random cropping for training and center cropping for testing). We split image to patches of size 16 and the step size of sliding window is set to be 12. Thus the H, W, P, S in Eq 1 are 448, 448, 16, 12 respectively. The margin ? in Eq 9 is set to be 0.4. We load intermediate weights from official ViT-B 16 model pretrained on ImageNet21k. The batch size is set to 16. SGD optimizer is employed with a momentum of 0.9. The learning rate is initialized as 0.03 except 0.003 for Stanford Dogs dataset and 0.01 for iNat2017 dataset. We adopt cosine annealing as the scheduler of optimizer.</p><p>All the experiments are performed with four Nvidia Tesla V100 GPUs using the PyTorch toolbox and APEX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Analysis</head><p>We compare our proposed method TransFG with state-ofthe-art works on above mentioned fine-grained datasets. The experiment results on CUB-200-2011 and Stanford Cars are shown in <ref type="table" target="#tab_0">Table 1</ref>. From the results, we find that our method outperforms all previous methods on CUB dataset and achieve competitive performance on Stanford Cars.</p><p>To be specific, the third column of <ref type="table" target="#tab_0">Table 1</ref> shows the comparison results on CUB-200-2011. Compared to the best result StackedLSTM <ref type="bibr" target="#b12">(Ge, Lin, and Yu 2019b)</ref> up to now, our TransFG achieves a 1.3% improvement on Top-1 Accuracy metric and 1.4% improvement compared to our base framework ViT <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020)</ref>. Multiple ResNet-50 are adopted as multiple branches in <ref type="bibr" target="#b7">(Ding et al. 2019)</ref> which greatly increases the complexity. It is also worth noting that StackLSTM is a very messy multi-stage training  model which hampers the availability in practical use, while our TransFG maintains the simplicity. The fourth column of <ref type="table" target="#tab_0">Table 1</ref> shows the results on Stanford Cars. Our method outperforms most existing methods while performs worse than PMG <ref type="bibr" target="#b9">(Du et al. 2020</ref>) and API-Net <ref type="bibr" target="#b34">(Zhuang, Wang, and Qiao 2020)</ref> with small margin. We argue that the reason might be the much more regular and simpler shape of cars. However, even with this property, our TransFG consistently gets 1.1% improvement compared to the standard ViT model.</p><p>The results of experiments on Stanford Dogs are shown in <ref type="table" target="#tab_1">Table 2</ref>. Stanford Dogs is a more challenging dataset compared to Stanford Cars with its the more subtle differences between certain species and the large variances of samples from the same category. Only a few methods have tested on this dataset and our TransFG outperforms all of them. While ViT <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020</ref>) outperforms other methods by a large margin, our TransFG achieves 92.3% accuracy which outperforms SOTA by 2.0% with its discriminative part selection and contrastive loss supervision.</p><p>NABirds is a much larger birds dataset not only from the side of images numbers but also with 355 more categories which significantly makes the fine-grained visual classification task more challenging. We show our results on it in Table 3. We observe that most methods achieve good results by either exploiting multiple backbones for different branches or adopting quite deep CNN structures to extract better features. While the pure ViT <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020</ref>) can directly achieve 89.9% accuracy, our TransFG constantly gets 0.9% performance gain compared to ViT and reaches 90.8% accuracy which outperforms SOTA by 1.6%.</p><p>iNat2017 is a large-scale dataset for fine-grained species recognition. Most previous methods do not report results on iNat2017 because of the computational complexity of the multi-crop, multi-scale and multi-stage optimization. With the simplicity of our model pipeline, we are able to scale TransFG well to big datasets and evaluate the performance which is shown in <ref type="table" target="#tab_3">Table 4</ref>. This dataset is very challenging for mining meaningful object parts and the background  is very complicated as well. We find that Vision Transformer structure outperforms ResNet structure a lot in these large challenging datasets. ViT outperformes ResNet152 by nearly 10% and similar phenomenon can also be observed in iNat2018 and iNat2019. Our TransFG is the only method to achieve above 70% accuracy with input size of 304 and outperforms SOTA with a large margin of 3.5%.</p><p>For the just ended iNat2021 competition which contains 10,000 species, 2.7M training images, our TransFG achieves very high single model accuracy of 91.3%. (The final performance was obtained by ensembling many different models along with multi-modality processing) As far as we know, at least two of the Top5 teams in the final leaderboard adopted TransFG as one of their ensemble models. This clear proves that our model can be further extended to large-scale challenging scenarios besides academy datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We conduct ablation studies on our TransFG pipeline to analyze how its variants affect the fine-grained visual classification result. All ablation studies are done on CUB-200-2011 dataset while the same phenomenon can be observed on other datasets as well. Influence of image patch split method. We investigate the influence of our overlapping patch split method through experiments with standard non-overlapping patch split. As shown in <ref type="table" target="#tab_4">Table 5</ref>, both on the pure Vision Transformer and our improved TransFG framework, the overlapping split method bring consistently improvement, i.e., 0.2% for both frameworks. The additional computational cost introduced by this is also affordable as shown in the fourth column. Influence of Part Selection Module. As shown in <ref type="table" target="#tab_5">Table 6</ref>, by applying the Part Selection Module (PSM) to select dis-  criminative part tokens as the input for the last Transformer layer, the performance of the model improves from 90.3% to 91.0%. We argue that this is because in this way, we sample the most discriminative tokens as input which explicitly throws away some useless tokens and force the network to learn from the important parts.</p><p>Influence of contrastive loss. The comparisons of the performance with and without contrastive loss for both ViT and TransFG frameworks are shown in <ref type="table" target="#tab_6">Table 7</ref> to verify the effectiveness of it. We observe that with contrastive loss, the model obtains a big performance gain. Quantitatively, it increases the accuracy from 90.3% to 90.7% for ViT and 91.0% to 91.5% for TransFG. We argue that this is because contrastive loss can effectively enlarge the distance of representations between similar sub-categories and decrease that between the same categories which can be clearly seen in the comparison of confusion matrix in  Influence of margin ?. The results of different setting of the margin ? in Eq 9 is shown in <ref type="table" target="#tab_7">Table 8</ref>. We find that a small value of ? will lead the training signals dominated by easy negatives thus decrease the performance while a high value of ? hinder the model to learn sufficient information for increasing the distances of hard negatives. Empirically, we find 0.4 to be the best value of ? in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>We show the visualization results of proposed TransFG on the four benchmarks in <ref type="figure" target="#fig_3">Fig 5.</ref> We randomly sample three images from each dataset. Two kinds of visualizations are presented. The first and the third row of <ref type="figure" target="#fig_3">Fig 5 illustrated</ref> the selected tokens positions. For better visualization results, we only draw the Top-4 image patches (ranked by the attention score) and enlarge the square of the patches by two times while keeping the center positions unchanged. The second and fourth rows show the overall attention map of the whole image where we use the same attention integration method as described above to first integrate the attention weights of all layers followed by averaging the weights of all heads to obtain a single attention map. The lighter a region is, the more important it is. From the figure, we can see that our TransFG successfully captures the most important regions for an object, i.e., head, wings, tail for birds; ears, eyes, legs for dogs; lights, doors for cars. At the same time, our overall attention map maps the entire object precisely even in complex backgrounds and it can even serves as a segmentation mask in some simple scenarios. These visualization results clearly prove the interpretability of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose a novel fine-grained recognition framework TransFG and achieve state-of-the-art results on four common fine-grained benchmarks. We exploit selfattention mechanism to capture the most discriminative regions. Compared to bounding boxes produced by other methods, our selected image patches are much smaller thus becoming more meaningful by showing what regions really contribute to the fine-grained classification. The effectiveness of such small image patches also comes from the Transformer Layer to handle the inner relationships between these regions instead of relying on each of them to produce results separately. Contrastive loss is introduced to increase the discriminative ability of the classification tokens. Experiments are conducted on both traditional academy datasets and large-scale competition datasets to prove the effectiveness of our model in multiple scenarios. Qualitative visualizations further show the interpretability of our method.</p><p>With the promising results achieved by TransFG, we believe that the transformer-based models have great potential on fine-grained tasks and our TransFG could be a starting point for future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The framework of our proposed TransFG. Images are split into small patches (a non-overlapping split is shown here) and projected into the embedding space. The input to the Transformer Encoder consists of patch embeddings along with learnable position embeddings. Before the last Transformer Layer, a Part Selection Module (PSM) is applied to select tokens that corresponds to the discriminative image patches and only use these selected tokens as input. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of contrastive loss. Confusion matrices without and with contrastive loss of a batch with four classes where each contains four samples are shown. The metric of confusion matrix is cosine similarity. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization results of TransFG on CUB-200-2011, Stanford Dogs, Stanford Cars and NABirds datasets. Two kinds of visualization are given, where the first and the third row show the selected Top-4 token positions while the second and fourth rows show the overall global attention maps. See examples from NABirds dataset where birds are sitting on twigs. The bird parts are lighted while the occluded twigs are ignored. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different methods on CUB-200-2011, Stanford Cars.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">CUB Cars</cell></row><row><cell>ResNet-50</cell><cell>ResNet-50</cell><cell>84.5</cell><cell>-</cell></row><row><cell>NTS-Net</cell><cell>ResNet-50</cell><cell>87.5</cell><cell>93.9</cell></row><row><cell>Cross-X</cell><cell>ResNet-50</cell><cell>87.7</cell><cell>94.6</cell></row><row><cell>DBTNet</cell><cell>ResNet-101</cell><cell>88.1</cell><cell>94.5</cell></row><row><cell>FDL</cell><cell cols="2">DenseNet-161 89.1</cell><cell>94.2</cell></row><row><cell>PMG</cell><cell>ResNet-50</cell><cell>89.6</cell><cell>95.1</cell></row><row><cell>API-Net</cell><cell cols="2">DenseNet-161 90.0</cell><cell>95.3</cell></row><row><cell>StackedLSTM</cell><cell>GoogleNet</cell><cell>90.4</cell><cell>-</cell></row><row><cell>DeiT</cell><cell>DeiT-B</cell><cell>90.0</cell><cell>93.9</cell></row><row><cell>ViT</cell><cell>ViT-B 16</cell><cell>90.3</cell><cell>93.7</cell></row><row><cell>TransFG</cell><cell>ViT-B 16</cell><cell>91.7</cell><cell>94.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different methods on Stanford Dogs.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Dogs</cell></row><row><cell cols="2">MaxEnt DenseNet-161</cell><cell>83.6</cell></row><row><cell>FDL</cell><cell>DenseNet-161</cell><cell>84.9</cell></row><row><cell>Cross-X</cell><cell>ResNet-50</cell><cell>88.9</cell></row><row><cell>API-Net</cell><cell>ResNet-101</cell><cell>90.3</cell></row><row><cell>ViT</cell><cell>ViT-B 16</cell><cell>91.7</cell></row><row><cell>TransFG</cell><cell>ViT-B 16</cell><cell>92.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different methods on NABirds.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>NABirds</cell></row><row><cell>Cross-X</cell><cell>ResNet-50</cell><cell>86.4</cell></row><row><cell>API-Net</cell><cell>DenseNet-161</cell><cell>88.1</cell></row><row><cell>CS-Parts</cell><cell>ResNet-50</cell><cell>88.5</cell></row><row><cell>FixSENet-154</cell><cell>SENet-154</cell><cell>89.2</cell></row><row><cell>ViT</cell><cell>ViT-B 16</cell><cell>89.9</cell></row><row><cell>TransFG</cell><cell>ViT-B 16</cell><cell>90.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different methods on iNat2017.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>iNat2017</cell></row><row><cell>ResNet152</cell><cell>ResNet152</cell><cell>59.0</cell></row><row><cell cols="2">IncResNetV2 IncResNetV2</cell><cell>67.3</cell></row><row><cell>TASN</cell><cell>ResNet101</cell><cell>68.2</cell></row><row><cell>ViT</cell><cell>ViT-B 16</cell><cell>68.7</cell></row><row><cell>TransFG</cell><cell>ViT-B 16</cell><cell>71.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on split way of image patches on CUB-200-2011 dataset.</figDesc><table><row><cell>Method</cell><cell>Patch Split</cell><cell cols="2">Accuracy (%) Training Time (h)</cell></row><row><cell>ViT</cell><cell>Non-Overlap</cell><cell>90.3</cell><cell>1.30</cell></row><row><cell>ViT</cell><cell>Overlap</cell><cell>90.5</cell><cell>3.38</cell></row><row><cell cols="2">TransFG Non-Overlap</cell><cell>91.5</cell><cell>1.98</cell></row><row><cell>TransFG</cell><cell>Overlap</cell><cell>91.7</cell><cell>5.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on Part Selection Module (PSM) on CUB-200-2011 dataset.</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>ViT</cell><cell>90.3</cell></row><row><cell>TransFG</cell><cell>91.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on contrastive loss on CUB-200-2011 dataset.</figDesc><table><row><cell>Method</cell><cell>Contrastive Loss Acc (%)</cell></row><row><cell>ViT</cell><cell>90.3</cell></row><row><cell>ViT</cell><cell>90.7</cell></row><row><cell>TransFG</cell><cell>91.0</cell></row><row><cell>TransFG</cell><cell>91.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on value of margin ? on CUB-200-2011 dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Value of ? Accuracy (%)</cell></row><row><cell>TransFG</cell><cell>0</cell><cell>91.1</cell></row><row><cell>TransFG</cell><cell>0.2</cell><cell>91.4</cell></row><row><cell>TransFG</cell><cell>0.4</cell><cell>91.7</cell></row><row><cell>TransFG</cell><cell>0.6</cell><cell>91.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00928</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<title level="m">Bird species categorization using pose normalized deep convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ranking measures and loss functions in learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Channel Interaction Networks for Fine-Grained Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10818" to="10825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification From the Bottom Up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04378</idno>
		<title level="m">Transreid: Transformer-based object re-identification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Novel Dataset for Fine-Grained Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ima-geNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Pereira, F.</editor>
		<editor>Burges, C. J. C.</editor>
		<editor>Bottou, L.</editor>
		<editor>and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03731</idno>
		<idno>arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Is attention interpretable</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Transtrack: Multiple-object tracking with transformer</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5463" to="5474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06878</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans2Seg: Transparent Object Segmentation with Transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Re-rank Coarse Classification with Local Region Enhanced Features for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09875</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03621</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13130" to="13137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
