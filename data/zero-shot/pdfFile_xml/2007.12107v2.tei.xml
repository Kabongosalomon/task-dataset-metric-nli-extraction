<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Object Detection and Viewpoint Estimation for Objects in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
						</author>
						<title level="a" type="main">Few-shot Object Detection and Viewpoint Estimation for Objects in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Few-shot learning</term>
					<term>Meta learning</term>
					<term>Object detection</term>
					<term>Viewpoint estimation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting objects and estimating their viewpoints in images are key tasks of 3D scene understanding. Recent approaches have achieved excellent results on very large benchmarks for object detection and viewpoint estimation. However, performances are still lagging behind for novel object categories with few samples. In this paper, we tackle the problems of few-shot object detection and few-shot viewpoint estimation. We demonstrate on both tasks the benefits of guiding the network prediction with class-representative features extracted from data in different modalities: image patches for object detection, and aligned 3D models for viewpoint estimation. Despite its simplicity, our method outperforms state-of-the-art methods by a large margin on a range of datasets, including PASCAL and COCO for few-shot object detection, and Pascal3D+ and ObjectNet3D for few-shot viewpoint estimation. Furthermore, when the 3D model is not available, we introduce a simple category-agnostic viewpoint estimation method by exploiting geometrical similarities and consistent pose labeling across different classes. While it moderately reduces performance, this approach still obtains better results than previous methods in this setting. Last, for the first time, we tackle the combination of both few-shot tasks, on three challenging benchmarks for viewpoint estimation in the wild, ObjectNet3D, Pascal3D+ and Pix3D, showing very promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D ETECTING objects in 2D images and estimating their 3D pose, as shown in <ref type="figure">Figure 1</ref>, is extremely useful for applications such as 3D scene understanding, augmented reality and robot manipulation. With the emergence of large databases annotated with object bounding boxes and viewpoints, deep-learning-based methods have achieved very good results on both tasks. However these methods, because they rely on rich labeled data, usually fail to generalize to novel object categories when only a few annotated samples are available. Additionally, creating 3D annotations is tedious and requires a large amount of expert effort, which slows down the applications of these methods to new objects. Fewshot learning, i.e., being able to transfer the knowledge learned from large base categories with abundant annotated images to novel categories with scarce annotated samples is therefore highly desirable in this context.</p><p>To address the few-shot learning of object detection, some approaches simultaneously tackle few-shot classification and few-shot localization by disentangling the learning of category-agnostic and category-specific network parameters <ref type="bibr" target="#b0">[1]</ref>. Others extract a class-informative feature vector for each class and use these vectors to reweight full-image features <ref type="bibr" target="#b1">[2]</ref> or region-of-interest (RoI) features <ref type="bibr" target="#b2">[3]</ref>. This reweighting module computes a feature similarity between query images and support classes, which has also been demonstrated to be useful in few-shot instance segmentation <ref type="bibr" target="#b3">[4]</ref> and few-shot image classification <ref type="bibr" target="#b4">[5]</ref>. However, this reweighting can easily be affected by noisy class-informative features, especially in the few-shot setting where only a few labeled samples are provided for novel categories. Instead, we propose to rely on a slightly more complex combination of query-image features and class-informative features. We show that this more general aggregation module can provide better few-shot object detection performances with smaller variations when experimented with different choices of support images. Besides, it can also be used to exploit class-exemplar 3D models for few-shot viewpoint estimation. Furthermore, we explore the usage of a cosine-similaritybased classifier <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and find that it slightly improves the detection results.</p><p>In parallel to the endeavours made in few-shot object detection, recent work proposes to perform category-agnostic viewpoint estimation that can be directly applied to novel object categories without retraining <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, these methods either require the testing categories to be similar to the training ones <ref type="bibr" target="#b7">[8]</ref>, or assume the exact CAD model to be provided for each object during inference <ref type="bibr" target="#b8">[9]</ref>. Differently, the meta-learning-based method MetaView <ref type="bibr" target="#b9">[10]</ref> introduces the category-level few-shot viewpoint estimation problem and addresses it by learning to estimate category-specific keypoints, requiring extra annotations.</p><p>While MetaView <ref type="bibr" target="#b9">[10]</ref> has achieved significantly improved performance on novel categories for few-shot viewpoint estimation, there are two main disadvantages: 1) specific keypoints have to be designed for different object categories, which requires some expertise and can be difficult to annotate and estimate for tiny and occluded objects; 2) the number of class-specific keypoint estimation branches increases linearly with the number of object classes.</p><p>Instead, we rely on a category-agnostic viewpoint estimation network, that directly predicts three Euler angles from an image embedding, without explicit class knowledge. To that end, we exploit the fact that similar classes, e.g., sofa and chair, often have a consistent canonical pose, with aligned similarities. The reason probably is that many objects are <ref type="figure">Fig. 1</ref>: Few-shot object detection and viewpoint estimation. Starting with images labeled with bounding boxes and viewpoints of objects from base classes, and given only a few similarly labeled images for new categories (top), we predict in a query image the 2D location of objects of new categories, as well as their 3D poses, optionally leveraging just a few arbitrary 3D class models (bottom). To the best of our knowledge, we are the first to conduct this joint task of object detection and viewpoint estimation in the few-shot regime.</p><p>Moreover, we propose to optionally use 3D models, which we call "exemplar 3D models", as additional input of the viewpoint estimation network and to condition the final viewpoint prediction on both the image embeddings and the 3D model embeddings through a feature aggregation module. These 3D models are easy to obtain for many categories <ref type="bibr" target="#b12">[13]</ref>. They do not need to correspond exactly to the objects present in the input image-in fact we use the same exemplar 3D model for all the objects of a same category. Their purpose is only to help the viewpoint estimation network generalize better to new classes. The use of these exemplar 3D models for viewpoint estimation is similar to exploiting images annotated with bounding boxes for object detection, from which we extract the task-aware class-specific information. Using this information, we obtain an embedding for each class and condition the network prediction on both the classinformative embeddings and instance-wise query image embeddings through a feature aggregation module. This exploitation of 3D models leads to a clear performance improvement of viewpoint estimation on novel classes under the few-shot learning regime.</p><p>Finally, by combining our few-shot object detection with our few-shot viewpoint estimation, we address the joint problem of learning to detect objects in images and to estimate their viewpoints from only a few shots. This corresponds to the real world in contrast with other few-shot viewpoint estimation methods, that only evaluate in the ideal case with ground-truth classes and ground-truth bounding boxes. We demonstrate that our few-shot viewpoint estimation method can achieve very good results even based on the predicted classes and bounding boxes.</p><p>To summarize, our contributions are three-fold. First, we define a simple yet effective unifying framework that addresses both few-shot object detection and few-shot viewpoint estimation in images, and achieves state-of-theart performances across various benchmarks. Second, we show how the performance of our category-agnostic few-shot viewpoint estimation method is boosted by the additional knowledge at training time of one or a few exemplar 3D models per class, requiring only viewpoint supervision (as opposed to extra annotations such as keypoints), which is a realistic scenario. Third, we propose an evaluation of the new few-shot learning task of jointly detecting objects and estimating their viewpoint, for which we provide promising results. Our data and code are available at http://imagine.enpc.fr/ ? xiaoy/FSDetView/. This paper is an extended version of our previous work <ref type="bibr" target="#b13">[14]</ref>, with several improvements:</p><p>? introducing a category-agnostic few-shot viewpoint estimation method that predicts viewpoint directly from image embeddings, without relying on any 3D models during training and testing. ? providing a more in-depth explanation of implementation details and a thorough analysis of different components of the method. ? extended evaluation of joint few-shot object detection and viewpoint estimation on Pascal3D+ and Pix3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Since there is a vast amount of literature on both object detection and viewpoint estimation, we focus here on recent work that targets these tasks in the case of limited labels.</p><p>Few-shot learning. Few-shot learning has been defined for the purpose of transferring the knowledge learned from large base categories with abundant annotated samples to novel categories with only a few annotated samples. Li et al. <ref type="bibr" target="#b14">[15]</ref> employ Bayesian inference to generalize knowledge from a pre-trained model to perform one-shot learning. While some methods propose to hallucinate additional training examples for the data-starved novel classes <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, recent work is more focused on meta-learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> , which we detail below. Such meta-learning-based methods can be roughly divided into three categories. 1) Metric-learning-based approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> aim to learn an embedding space that is efficiently transferable for scarcely annotated training samples. MatchingNet <ref type="bibr" target="#b4">[5]</ref> uses the cosine similarity to find the most similar class for the query image among a small set of labeled images. ProtoNet <ref type="bibr" target="#b21">[22]</ref> replaces the weighted nearest neighbor classifier in <ref type="bibr" target="#b4">[5]</ref> by a linear classifier where the squared Euclidean distance is used. Rela-tionNet <ref type="bibr" target="#b22">[23]</ref> proposes to learn the relation between support data and query data through a neural network, which is similar to CAN <ref type="bibr" target="#b28">[29]</ref> and LGM-Net <ref type="bibr" target="#b29">[30]</ref>. 2) Optimizationbased fast adaptation approaches <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> intend to adjust the optimization algorithm such that the model can quickly converge on the few annotated samples. Ravi and Larochelle <ref type="bibr" target="#b23">[24]</ref> train a LSTM-based meta-learner to learn a classifier in new few-shot tasks. Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b30">[31]</ref> explicitly optimizes the parameters of the model such that a small number of gradient descents on the novel task will produce good generalization performance. Sun et al. <ref type="bibr" target="#b32">[33]</ref> propose to adapt a model for few-shot learning tasks by learning scaling and shifting functions of model weights for multiple tasks. 3) Parameter-prediction-based approaches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> attempt to generate network parameters for new tasks. Bertinetto et al. <ref type="bibr" target="#b19">[20]</ref> learn the parameters of factorized weight layers based on a single example of each class. Gidaris and Komodakis <ref type="bibr" target="#b33">[34]</ref> introduce an attention-based few-shot classification weight generator.</p><p>Besides the standard few-shot learning setting, there is also other work focused on different settings. In transductive few-shot learning <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b35">[36]</ref>, the unlabeled query set is assumed to be accessible for training and testing. This is highly related to the semi-supervised few-shot learning <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, where an extra unlabeled training set is allowed. These approaches only tackle the problem of few-shot image classification, while we seek to study the more challenging and under-explored problem of few-shot object detection and viewpoint estimation.</p><p>Object detection with limited annotations. The general deep-learning models for object detection can be divided into two groups: proposal-based methods and direct methods without proposals. While the R-CNN series <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> and FPN <ref type="bibr" target="#b41">[42]</ref> fall into the former line of work, the YOLO series <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> and SSD <ref type="bibr" target="#b44">[45]</ref> belong to the latter. All these methods mainly focus on learning from abundant data to improve detection regarding accuracy and speed. Yet, there are also some attempts to solve the problem with limited labeled data.</p><p>Chen et al. <ref type="bibr" target="#b45">[46]</ref> propose an approach based on transfer learning to train a network to detect objects of novel classes from just a few annotated images in the target domain. Recent work <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> proposes to integrate a reweighting module in existing detection models such as YOLO or Faster R-CNN, which enables the network to learn generalizable features and automatically adjust them for novel class detection through a set of class-specific coefficient vectors produced from the support samples. Similar to the parameterprediction-based few-shot learning methods, Wang et al. <ref type="bibr" target="#b0">[1]</ref> propose to disentangle the learning of category-agnostic and category-specific components in the detection model and learn a weight-generation module to predict categoryspecific parameters for novel classes. More recently, Wang et al. <ref type="bibr" target="#b6">[7]</ref> find that a simple fine-tuning detection model can achieve impressive results on novel classes using a categoryagnostic box regressor and a cosine-similarity-based box classifier. They also analyze the variance of the detection results obtained with different support samples and show the importance of averaging evaluation results over multiple experimental runs, which has been sometimes disregarded in previous work.</p><p>In contrast, we replace the feature reweighting module in <ref type="bibr" target="#b2">[3]</ref> by a feature aggregation module that achieves a better detection performance under the few-shot regime. Following <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, we also conduct multiple experiments with randomly selected support samples and report average results to prevent biases in evaluation.</p><p>There is also prior work focusing on object detection with limited annotations in different settings. Weakly-supervised detection <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> considers the problem of training a detection model with only image-level labels, but without bounding box annotations that are more difficult to acquire. Semi-supervised detection <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> makes use of a small amount of labeled images per class to generate pseudo labels on a large amount of unlabeled images for training. Zeroshot detection <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> considers there is no available annotations for the novel categories and relies on external information such as inter-class relation or word embeddings for novel class detection. Since these settings differs from the few-shot object detection setting, they are out of our scope in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Viewpoint estimation with limited annotations.</head><p>Deeplearning methods for viewpoint estimation follow roughly three different paths: direct estimation of Euler angles <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, template-based matching <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> that encodes images in latent spaces and compares them against a dictionary of pre-defined viewpoints, and keypoint detection relying on 3D bounding box corners <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref> or semantic keypoints <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b64">[65]</ref>. Training a viewpoint estimation network requires a large amount of images manually labeled with aligned 3D CAD models or 2D keypoints, which are expensive to obtain in terms of time and human labor. To overcome this limitation, recent works propose to conduct unsupervised viewpoint estimation <ref type="bibr" target="#b65">[66]</ref> or predict generic 3D keypoints for all object classes <ref type="bibr" target="#b7">[8]</ref>. Alternatively, along with the improvement of image quality and processing speed in rendering methods, abundant synthetic images can be automatically generated for network training <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>. Some work also focuses on training the viewpoint estimation network on a collection of unlabeled images by self-supervised learning <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>.</p><p>Most of the existing viewpoint estimation methods are designed for known object categories or instances; very little work reports performance on unseen objects <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>. Zhou et al. <ref type="bibr" target="#b7">[8]</ref> propose a category-agnostic method to learn general keypoints for both seen and unseen objects, while Xiao et al. <ref type="bibr" target="#b8">[9]</ref> show that better results can be obtained when exact 3D models of the objects are additionally provided. Park et al. <ref type="bibr" target="#b72">[73]</ref> propose a novel framework for 6D pose estimation of unseen objects by learning a latent 3D representation from a set of reference views for each target object during inference. In contrast to these categoryagnostic methods, Tseng et al. <ref type="bibr" target="#b9">[10]</ref> specifically address the few-shot scenario by training a category-specific viewpoint estimation network for novel classes with limited samples. More recently, Wang et al. <ref type="bibr" target="#b73">[74]</ref> study the problem of learning to estimate the 3D object pose from a few labeled examples and a collection of unlabeled data, and show promising results in particular on vehicle categories.</p><p>Instead of using exact 3D object models as <ref type="bibr" target="#b8">[9]</ref>, we propose a meta-learning approach to extract a class-informative canonical shape feature vector for each novel class from a few labeled samples, with random object models. Besides, our network can be applied to both base and novel classes without changing the network architecture, while <ref type="bibr" target="#b9">[10]</ref> requires a separate meta-training procedure for each class and needs keypoint annotations in addition to the viewpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we first introduce the setup for few-shot object detection and few-shot viewpoint estimation (Section 3.1). Then, we present our network architecture for these two tasks with class data (Section 3.2) and a fine-tuning categoryagnostic viewpoint estimation method (Section 3.3). Finally, we describe the learning procedure adopted in both few-shot learning tasks (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Few-shot Learning Setup</head><p>For both the object detection and viewpoint estimation tasks, we assume we have training samples (x, y) ? (X , Y). A few 3D shapes may also be available for viewpoint estimation.</p><p>? In the case of object detection, x is an image, y = {(cls i , box i ) | i ? Obj x } indicates the class label cls i and bounding box box i of each object i in the image. ? In the case of viewpoint estimation, x = (cls, box, img)</p><p>represents an object of class cls(x) pictured in bounding box box(x) of an image img(x); y = ang = (azi, ele, inp) is the 3D pose (viewpoint) of the object, given by Euler angles (azimuth, elevation, in-plane rotation). For each class c ? C = {cls i | x ? X , i ? Obj x }, we consider a set Z c of class data (see <ref type="figure" target="#fig_0">Figure 2</ref>) to learn from using metalearning:</p><formula xml:id="formula_0">? For object detection, Z c = {(x, mask i ) | x ? X , i ? Obj x }</formula><p>is made of images x plus an extra channel with a binary mask for bounding box box i of object i ? Obj x . ? For viewpoint estimation, Z c is an optional, additional set of 3D models of class c, which is not used in the purely image-based category-agnostic variant. At each training iteration, class data z c is randomly sampled in Z c for each c ? C.</p><p>In the few-shot setting, we have a partition of the classes C = C base ? C novel , with many samples for base classes in C base and only a few samples (possibly also including a few (a) few-shot object detection.</p><p>(b) few-shot viewpoint estimation. shapes) for novel classes in C novel . The goal is to transfer the knowledge learned on base classes with abundant samples to little-represented novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Few-shot Learning with Class Data</head><p>Our general approach has three steps illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. First, query data x and class-informative data z c pass respectively through the query encoder F qry and the class encoder F cls to generate corresponding feature vectors , for each each region of interest (RoI) and each class respectively. Next, a feature aggregation module A combines a query feature (for a given RoI) with a class feature. Finally, the output of the network is obtained by passing each aggregated feature through a task-specific predictor P:</p><p>? For object detection, the predictor estimates a classification score and an object location (i.e.., bounding box) for each region of interest (RoI) and each class. ? For viewpoint estimation, the predictor selects quantized angles by classification, that are refined using regressed angular offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Few-shot Object Detection.</head><p>We adopt the popular Faster R-CNN <ref type="bibr" target="#b39">[40]</ref> approach in our few-shot object detection network (see <ref type="figure" target="#fig_1">Figure 3</ref>(a)). The query encoder F qry includes the backbone, the region proposal network (RPN) and the proposal-level feature alignment module. In parallel, the class encoder F cls is the backbone sharing the same weights as F qry except for the first convolutional layer, that has an additional fourth channel for extracting class features from RGB images with binary masks of the object bounding boxes <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Each extracted vector of query features is aggregated with each extracted vector of class features before being processed for class classification and bounding box regression:</p><formula xml:id="formula_1">(cls i,c , box i,c ) = P A f qry i , f cls c for f qry i ? F qry (x), f cls c = F cls (z c ), c ? C train<label>(1)</label></formula><p>where C train is the set of all training classes, and where cls i,c and box i,c are the predicted classification scores and object locations for the i th RoI in query image x and for class c. The prediction branch P is implemented as two fully-connected layers of size 4096 without activation that output respectively N train = |C train | classification scores and Cosine similarity for box classifier. Inspired by Wang et al. <ref type="bibr" target="#b6">[7]</ref>, we use a cosine-similarity-based classifier in the bounding box predictor. We note the weight matrix of the box classifier as W = [w 1 , w 2 , . . . , w c ], where w c ? R d is the class-wise weight vector and d is the dimension of the aggregated features. Thus, the classification score for the i th RoI and class c can be written as:</p><formula xml:id="formula_2">cls i,c = ?A f qry i , f cls c w c A f qry i , f cls c w c ,<label>(2)</label></formula><p>where ? is a scaling factor, set to 20 in all experiments. The instance-level feature normalization used in this cosinesimilarity-based classifier was found empirically to be helpful in reducing the intra-class variance and improving the detection accuracy of novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Few-shot Viewpoint Estimation.</head><p>For few-shot viewpoint estimation, we rely on the recently proposed PoseFromShape <ref type="bibr" target="#b8">[9]</ref> architecture to implement our network. To create class data z c , we transform the 3D models in the dataset into point clouds by uniformly sampling points on the surface, with coordinates in a normalized, canonical object space. The query encoder F qry and class encoder F cls (cf. <ref type="figure" target="#fig_1">Figure 3</ref>(b)) correspond respectively to the image encoder ResNet-18 <ref type="bibr" target="#b74">[75]</ref> and shape encoder PointNet <ref type="bibr" target="#b75">[76]</ref> in PoseFromShape. By aggregating the query features and class features, we estimate the three Euler angles via the predictor P, which is implemented as a three-layer fully-connected network of sizes 800, 400, 200, each layer being followed by a batch normalization and ReLU activation:</p><formula xml:id="formula_3">(azi, ele, inp) = P A f qry , f cls with f qry = F qry (crop(img(x), box(x))), and f cls = F cls (z c ), c = cls(x)<label>(3)</label></formula><p>where crop(img(x), box(x)) indicates that the query features are extracted from the object-centred crops. Unlike the object detection making a prediction for each class, here we only make the prediction for the object class cls(x) by passing the corresponding class data through the network. We also use the mixed classification-and-regression viewpoint estimator of <ref type="bibr" target="#b8">[9]</ref>: the output consists of angular bin classification scores and within-bin offsets for three Euler angles: azimuth (azi), elevation (ele), and in-plane rotation (inp).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Feature Aggregation.</head><p>In recent few-shot object detection methods such as FSRW <ref type="bibr" target="#b1">[2]</ref> and Meta R-CNN <ref type="bibr" target="#b2">[3]</ref>, features are aggregated by reweighting the query features f qry according to the output f cls of the class encoder F cls :</p><formula xml:id="formula_4">A(f qry , f cls ) = f qry f cls ,<label>(4)</label></formula><p>where represents element-wise multiplication (Hadamard product) and f qry has the same number of channels as f cls . By jointly training the query encoder F qry and the class encoder F cls with this reweighting module, it is possible to learn to generate meaningful reweighting vectors f cls . F qry and F cls actually share their weights, except the first layer <ref type="bibr" target="#b2">[3]</ref>. We choose to rely on a slightly more complex aggregation scheme. The fact is that feature subtraction is a different but also effective way to measure similarity between image features <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>. The image embedding f qry itself, without any reweighting, contains relevant information too. Our aggregation thus concatenates the three forms:</p><formula xml:id="formula_5">A(f qry , f cls ) = [f qry f cls , f qry ? f cls , f qry ] ,<label>(5)</label></formula><p>where [?, ?, ?] represents channel-wise concatenation. The last part of the aggregated features in Eq. <ref type="formula" target="#formula_5">(5)</ref> is independent of the class data. As observed experimentally in <ref type="table">Table 3</ref>, this partial disentanglement does not only improve few-shot detection performance, it also reduces the variation introduced by the randomness of support samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Category-agnostic Viewpoint Estimation</head><p>We also consider the case where no 3D model is provided.</p><p>In this case, we bypass the requirement of task-aware class data as mentioned in the previous section and we estimate viewpoints only from the image embeddings. Given a query object x pictured in image img(x) and its bounding box box(x), the query encoder generates an image embedding f qry . Then, given such an embedding, the viewpoint prediction component estimates the three Euler angles:</p><formula xml:id="formula_6">(azi, ele, inp) = P f qry with f qry = F qry (crop(img(x), box(x))) .<label>(6)</label></formula><p>The feature extraction module is category-agnostic and all object classes share the same prediction module. And the viewpoint predictor P is implemented in the same way as in Section 3.2.2. Therefore, the network can fully leverage the geometrical similarities between related categories such as bicycle and motorbike. As illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, we first train on a large base-class dataset and then fine-tune on a balanced dataset consisting of base and novel classes. While not exactly following the general framework of <ref type="figure" target="#fig_1">Figure 3</ref>, it follows a related pattern, where the 3D branch is removed, as well as, consequently, the aggregation module. This simple yet effective approach outperforms previous methods on fewshot viewpoint estimation (see Section 4.2).</p><p>Following previous few-shot approaches, we fine-tune the network on both base and novel categories for "learning without forgetting", which prevents the network to only focus on increasing its performance on novel categories ignoring possible dramatic drops on base categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Procedure</head><p>Our learning procedure consists of two phases: base-class training on many samples from base classes (C train = C base ), followed by few-shot fine-tuning on a balanced small set of samples from both base and novel classes (C train = C base ? C novel ). More precisely, in the K-shot fine-tuning stage where only K labeled samples are available for each novel class, we randomly select K samples for each base class to balance the training iterations between base and novel classes. In both phases, we optimize the network using the same loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Loss Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection loss function.</head><p>We optimize our few-shot object detection network using the same loss function as Meta R-CNN <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_7">L = L rpn + L cls + L loc + L meta ,<label>(7)</label></formula><p>where L rpn is applied to the output of the RPN to distinguish foreground from background and refine the proposals, L cls is a cross-entropy loss for box classification, L loc is a Huber loss for box regression, and L meta is a cross-entropy loss encouraging class features to be diverse for different classes <ref type="bibr" target="#b2">[3]</ref>.</p><p>Viewpoint loss function. For the task of estimating viewpoints, we discretize each Euler angle with a bin size of 15 degrees and use the same loss function as PoseFromShape <ref type="bibr" target="#b8">[9]</ref> to train the network:</p><formula xml:id="formula_8">L = ??{azi,ele,inp} L ? cls + L ? reg ,<label>(8)</label></formula><p>where L ? cls is a cross-entropy loss for angle bin classification of Euler angle ?, and L ? reg is a Huber loss for the regression of offsets relatively to bin centers. Here we remove the meta loss L meta used in object detection since we want the network to learn useful inter-class similarities for viewpoint estimation, instead of the inter-class differences for box classification in object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Class Data Construction</head><p>For viewpoint estimation, unless otherwise stated, we make use of all the 3D models available for each class (typically less than 10) during both training stages. In contrast, the class data used in object detection requires the information of object class and location, which is limited for novel classes by the number of annotated samples. Therefore, we use a large number of class data for base classes in the base training stage (typically |Z c | = 200, as in Meta R-CNN <ref type="bibr" target="#b2">[3]</ref>) and limit the size of Z c to the number of shots for both base and novel classes in the K-shot fine-tuning stage (|Z c | = K).</p><p>For inference, instead of randomly sampling class data from the dataset as done during training, we construct class features once and for all after learning is finished: for each class c, we average all class features used in the few-shot fine-tuning stage:</p><formula xml:id="formula_9">f cls c = 1 |Z c | zc?Zc F cls (z c ) .<label>(9)</label></formula><p>This corresponds to the offline computation of all orange feature vectors in <ref type="figure" target="#fig_1">Figure 3</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first evaluate on few-shot object detection (Section 4.1) and few-shot viewpoint estimation benchmarks (Section 4.2) to empirically assess the effectiveness of our method. For a fair comparison, we use the same splits between base and novel classes as used in previous work <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref> and report the performance averaged over multiple runs with different groups of few-shot training examples to obtain a sensible accuracy estimation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Furthermore, we conduct an evaluation of the joint task of few-shot object detection and viewpoint estimation on three datasets to demonstrate the generalization capacity of our method for both tasks in the few-shot regime (Section 4.3). We conclude this empirical study with limitations of our approach (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Few-shot Object Detection</head><p>We adopt a well-established evaluation protocol for fewshot object detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and report performance on PASCAL VOC <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref> (reported in <ref type="table">Table 1</ref>) and MS-COCO <ref type="bibr" target="#b80">[81]</ref> (reported in <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experimental Setup</head><p>Datasets. PASCAL VOC <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref> is a small-scale object detection dataset containing 20 object categories. Following the common protocol <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b43">[44]</ref>, we use the test set of VOC 2007 <ref type="bibr" target="#b78">[79]</ref> for testing and the train-val set of VOC 07-12 <ref type="bibr" target="#b79">[80]</ref> for training, which results in 16,551 training images and 4,952 testing images. Among the 20 object categories, <ref type="bibr" target="#b1">[2]</ref> introduces three few-shot splits by randomly selecting 5 classes as the novel ones while keeping the remaining 15 ones as the base: (bird, bus, cow, motorbike, sofa / rest); (aeroplane, bottle, cow, horse, sofa / rest); (boat, cat, motorbike, sheep, sofa / rest). We evaluate on these 3 different base/novel splits assuming that only K annotated bounding boxes are provided for each novel class during training, where K equals 1, 2, 3, 5 or 10. MS-COCO <ref type="bibr" target="#b80">[81]</ref> is a large-scale object detection dataset containing 80 object categories. We follow <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> to use 5,000 images from the mini-val set for testing and use the remaining 118,287 images in train-val set for training. Among the 80 object categories, we select the 20 classes common to PASCAL VOC as novel classes and consider the remaining 60 classes as base classes. For this dataset, the evaluation TABLE 1: Few-shot object detection evaluation on PASCAL VOC. We report the Average Precision with a single IoU threshold at 0.5 (AP 0.5 ) under 3 different splits for 5 novel classes <ref type="bibr" target="#b1">[2]</ref> with a small number of shots. *Results computed over single experimental run with a fix set of support images.  protocol used in previous work is to test on K = 10 or 30 annotated bounding boxes for each novel class.</p><p>Evaluation metrics. We measure the Average Precision (AP) of detections as the area under a precision-recall curve. For few-shot object detection on PASCAL VOC, we classically report AP 0.5 , that computes AP with a single Intersection over Union (IoU) threshold at 0.5. For evaluation on MS-COCO, we use the standard MS-COCO evaluation metrics <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b43">[44]</ref>: mAP, AP 0.5 , AP 0.75 , AP S , AP M , AP L , AR 1 , AR 10 , AR 100 , AR S , AR M , AR L . While AP 0.5 and AP 0.75 represent respectively the AP with a single IoU threshold at 0.5 and 0.75, mAP is the averaged AP over multiple IoU thresholds from 0.5 to 0.95 with a step of 0.05. Average Recall (AR) computed with the N most confident predictions per image is noted as AR N , where N equals 1, 10 or 100. Moreover, we report the detection performance across different object scales: S (small: area &lt; 32 2 square pixels), M (medium: 32 2 ? area &lt; 96 2 ) and L (large: 96 2 ? area).</p><p>Training details. We employ the same learning scheme as <ref type="bibr" target="#b2">[3]</ref>, which uses the SGD optimizer with an initial learning rate of 10 ?3 and a batch size of 4. Weight decay and momentum are set to 0.0005 and 0.9, respectively. In the first training stage, we train for 20 epochs and divide the learning rate by 10 after each 5 epochs. In the second stage, we train for 5 epochs with a learning rate of 10 ?3 and another 4 epochs with a learning rate of 10 ?4 . For anchor scales, we use three scales (128 2 , 256 2 , 512 2 ) for PASCAL VOC and add a fourth scale of 64 2 for MS-COCO. The three aspect ratios of anchors are set to 1:2, 1:1, 2:1. We augment the data with horizontal flipping. Training on a single Titan-X GPU takes around one day for PASCAL VOC and ten days for MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Few-shot Detection Results</head><p>Cosine similarity vs. dot product. We first compare the cosine-similarity-based box classifier (Ours w/cos) with the normal FC-based classifier (Ours w/fc) that uses a simple dot product between feature representations and weight vectors to compute the classification scores. Indeed, in fewshot learning tasks, features learned with a cosine-similaritybased classifier have been found empirically to generalize better to novel categories compared to features learned with FC-based classifier <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b33">[34]</ref>. As observed in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>, even though the improvement is not systematic on novel classes of PASCAL VOC, cosine similarity does bring a consistent performance boost on novel classes of COCO, compared to FC-based classifier with direct dot product.</p><p>Different feature aggregations. We analyze the impact of different feature aggregation schemes. For this purpose, we evaluate K-shot object detection on PASCAL VOC with K = 3 or 10. Here, we compare results obtained by models with an FC-based classifier. The results are reported in <ref type="table">Table 3</ref>. We can see that our feature aggregation scheme  <ref type="table">3</ref>: Ablation study on the feature aggregation scheme. Using the same class splits of PASCAL VOC as in <ref type="table">Table 1</ref>, we measure the performance of few-shot object detection on the novel classes for 3 shots and 10 shots. We report the average and standard deviation of the AP50 metric over ten runs. f qry is the query features and f cls is the class features. [f qry f cls , f qry ? f cls , f qry ] yields the best precision. In particular, although the difference f qry ? f cls could in theory be learned from the individual feature vectors [f qry , f cls ], the network performs better when explicitly provided with their subtraction. Moreover, our aggregation scheme significantly reduces the variance introduced by the random sampling of few-shot support data, which is a major issues in few-shot learning (although sometimes neglected).</p><p>Comparison with the state of the art. <ref type="table" target="#tab_1">Tables 1 and 2</ref> show the comparison with previous few-shot object detection methods. On the PASCAL VOC dataset, our method achieves the best performance in most cases, in particular when the number of shots tends to be large. This indicates that our method can better leverage the task-relevant information from novel classes when more labeled examples are provided. Moreover, it significantly improves results on MS-COCO for all evaluation metrics, which validates again the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Few-shot Viewpoint Estimation</head><p>Following the few-shot viewpoint estimation protocol proposed in <ref type="bibr" target="#b9">[10]</ref>, we evaluate our method in two settings: intradataset on ObjectNet3D <ref type="bibr" target="#b83">[84]</ref> (cf. <ref type="table" target="#tab_4">Table 4</ref>) and inter-dataset between ObjectNet3D and Pascal3D+ <ref type="bibr" target="#b84">[85]</ref> (cf. <ref type="table" target="#tab_5">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experimental Setup</head><p>Datasets. Pascal3D+ <ref type="bibr" target="#b84">[85]</ref> is a standard evaluation benchmark used in 3D pose estimation. Unlike 6D pose estimation datasets <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref> that usually focus on dozens of objects with limited environment variations, Pascal3D+ contains 12 man-made object categories with 2k to 4k images per category,allowing the benchmarking of object pose estimation in the wild. ObjectNet3D <ref type="bibr" target="#b83">[84]</ref>, extended from Pascal3D+, features 100 object categories, with 90,127 images and 201,888 objects in total. In both datasets, only a small number of roughly-aligned 3D models are provided for each category.</p><p>Evaluation metrics. We use the most common metrics for evaluation: Acc30, which is the percentage of estimations with a rotational error smaller than 30 ? , and Med-Err, which is the median rotational error measured in degrees. We compute the rotational error as ?(R pred , R gt ) =</p><formula xml:id="formula_10">log(R pred ,Rgt) F ? 2</formula><p>, where ? F is the Frobenius norm. Following previous work <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we only use the non-occluded and non-truncated objects for evaluation, and assume in this subsection, for all methods, that the ground-truth classes and ground-truth bounding boxes are provided at test time.</p><p>Training details. We resize the object image crops into 224 ? 224 pixels as the input for our viewpoint estimation networks, with (Ours w/ 3D) or without (Ours w/o 3D) using exemplar 3D models. Both networks are trained using the Adam optimizer with a batch size of 16. Weight decay is set to 0.0005. During the base-class training stage, we train for 150 epochs with a learning rate of 10 ?4 . For few-shot fine-tuning, we train for 50 epochs with learning rate of 10 ?4 and another 50 epochs with a learning rate of 10 ?5 . Standard data augmentation is applied during training, such as random rotation, random flipping and color jittering. The training is done in about one day on a single Titan-X GPU.</p><p>Compared methods. For few-shot viewpoint estimation, we compare our method to MetaView <ref type="bibr" target="#b9">[10]</ref> and to two adaptations of StarMap <ref type="bibr" target="#b7">[8]</ref>. More precisely, the authors of MetaView <ref type="bibr" target="#b9">[10]</ref> re-implemented StarMap with one stage of ResNet-18 as the backbone, and trained the network with MAML <ref type="bibr" target="#b30">[31]</ref> for a fair comparison in the few-shot regime (StarMap+M). They also provided StarMap results by just fine-tuning it on the novel classes using the scarce labeled data (StarMap+F). We consider the two variants of our method, with (Ours w/ 3D) or without 3D data (Ours w/o 3D) at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Few-shot Viewpoint Estimation Results</head><p>Intra-dataset evaluation. We follow the protocol of <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> to split the 100 categories of ObjectNet3D into 80 base classes and 20 novel classes. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our model outperforms the recently proposed meta-learningbased method MetaView <ref type="bibr" target="#b9">[10]</ref> by a very large margin in overall performance: +16 points in Acc30 and half MedErr (from 31.5 ? down to 15.6 ? ). Besides, keypoint annotations are not available for some object categories such as door, pen and shoe in ObjectNet3D. This lack of annotations limits the generalization of keypoint-based approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> as they require a set of manually labeled keypoints for network training. In contrast, our model can be trained and evaluated on all object classes of ObjectNet3D as we only rely on the viewpoint annotations. More importantly, our model can be directly deployed on different classes using the same architecture, while MetaView learns a set of separate category-specific semantic keypoint detectors for each class. This flexibility suggests that our approach is likely to exploit the similarities between different categories (e.g., bicycle and motorbike) and has more potentials for applications to robotics and augmented reality.</p><p>Inter-dataset evaluation. To further evaluate our method in a more practical scenario, we use a source dataset for base  classes and another target dataset for novel (disjoint) classes. Following the same split as MetaView <ref type="bibr" target="#b9">[10]</ref>, we use all 12 categories of Pascal3D+ as novel categories and the remaining 88 categories of ObjectNet3D as base categories. Distinct from the previous intra-dataset experiment that focuses more on the cross-category generalization capacity, this inter-dataset setup also reveals the cross-domain generalization ability. As shown in <ref type="table" target="#tab_5">Table 5</ref>, our approach again significantly outperforms StarMap and MetaView. Our overall improvement in inter-dataset evaluation is even larger than in intra-dataset evaluation: we gain +19 points in Acc30 and again divide MedErr by about 2 (from 51.3 ? down to 28.3 ? ). This indicates that our approach, by leveraging viewpoint-relevant 3D information, not only helps the network generalize to novel classes from the same domain, but also addresses the domain shift issues when trained and evaluated on different datasets.</p><p>Visual results. We illustrate on <ref type="figure">Figure 5</ref> viewpoint estimation for novel objects in ObjectNet3D and Pascal3D+. We show both success (green boxes) and failure cases (red boxes) to help analyze possible error types. We visualize categories giving large rotational errors: iron, knife, rifle and slipper for ObjectNet3D, aeroplane, bicycle, boat and chair for Pascal3D+. The most common failure cases come from objects with similar appearances in different poses, e.g., iron and knife in ObjectNet3D, aeroplane and boat in Pascal3D+. It seems that more complex methods based on keypoints <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> perform a bit better on this kind of objects, although being nevertheless grossly wrong too. Other failure cases include heavy clutter cases (bicycle) and large shape variations between training objects and testing objects (chair).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Ablation Study</head><p>Different 3D model representations, if any. In <ref type="table" target="#tab_6">Table 6</ref>, we analyze the impact of different 3D model representations in our few-shot viewpoint estimation approach using exemplar 3D models. Besides using a point cloud (Point Cloud), we can also represent 3D shapes using a group of depth images (Depth) or non-textured rendered images (Rendering) captured in a set of camera locations defined on the upper hemisphere. We also use the normalized, canonical object space <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref> to represent the 3D models by transforming the 3D coordinates into RGB values (Object Coord.). For these variants that consider 2D inputs rather than a 3D point cloud, we implement the class encoder F cls using a ResNet-18 to extract features from images.</p><p>We find that using point clouds (with PointNet encoding) provides the best overall performance compared to training with the other 3D representations. This demonstrates the effectiveness of 3D model embedding with point clouds for viewpoint estimation. By comparing the performance gap between our methods using 3D models, regardless of the choice of 3D representation, and our method without using 3D models (first row in <ref type="table" target="#tab_6">Table 6</ref>), we note again that the 3D models can indeed help improve the viewpoint estimation <ref type="figure">Fig. 5</ref>: Qualitative results of few-shot viewpoint estimation using ground-truth 2D bounding boxes (and classes). We visualize results on ObjectNet3D and Pascal3D+. For each category, we show three success cases (first six columns) and one failure case (last two columns). CAD models are shown here only for the purpose of illustrating the estimated viewpoint. Failure cases usually result from appearance ambiguities of a same object in different poses, or from heavily cluttered scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-dataset (base)</head><p>Intra-dataset (novel) Inter-dataset (base) Inter-dataset (novel)</p><p>No 3D Exemplar Single 3D Exemplar Multiple 3D Exemplars <ref type="figure">Fig. 6</ref>: Few-shot viewpoint estimation evaluation using different number of shots. For each metric, we report the average and standard deviation computed over 10 random experiments. accuracy on novel classes and reduce the variance introduced by different support training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of exemplars.</head><p>We show detailed evaluation of fewshot viewpoint estimation with different number of shots in <ref type="figure">Figure 6</ref>. For both the intra-dataset and inter-dataset evaluations, we compute the accuracies and median errors on base and novel classes. We report the average results and the standard deviations computed over 10 experimental runs with different support training samples.</p><p>We first note that all variants of our viewpoint estimation approach can achieve better results when more annotated samples are provided. Secondly, we find that our approach using only one 3D exemplar model per class clearly improves the performance on both base and novel classes compared to results without using 3D models. Moreover, adding 3D information also reduces the variance on novel classes, which can clearly be seen in the inter-dataset evaluation. This shows that our method without 3D models, which relies on geometrical similarities and consistent labeling between different categories, can already learn a good image embedding space for few-shot viewpoint estimation. Yet, adding 3D models can certainly provide a more direct guidance for a better generalization towards novel categories.</p><p>On the other hand, we note that the performance gap between our approach using a single 3D exemplar per class or using multiple 3D exemplars per class is negligible compared to the gap between using or not 3D models. It demonstrates that even a single 3D model is sufficient to obtain a good 3D-aware class embedding for viewpoint estimation. It is possible that extra 3D exemplars could prove useful to generate more informative class embeddings, but it would probably require a more sophisticated feature combination than just feature averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Detection and Viewpoint Estimation</head><p>To further demonstrate the generality of our approach in realworld scenarios, we consider the joint problem of detecting objects of novel classes in images and estimating their viewpoints. The fact is that evaluating a viewpoint estimator on ground-truth classes and ground-truth bounding boxes is a toy setting <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, that is not representative of actual needs. On the contrary, estimating viewpoints based on predicted detection is much more realistic and challenging. Note that our object detection model and our viewpoint estimation model were trained separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experimental Setup</head><p>Datasets. As introduced in Section 4.2, Pascal3D+ <ref type="bibr" target="#b84">[85]</ref> and ObjectNet3D <ref type="bibr" target="#b83">[84]</ref> are two common viewpoint estimation benchmarks that have already been used in a number of previous publications. Apart from these two datasets, we also evaluate our method on a more recent benchmark: Pix3D <ref type="bibr" target="#b91">[92]</ref>. This is a large-scale dataset of 10,069 image-shape pairs with accurate 2D-3D alignment. It contains 395 3D shapes of 9 object categories. Each shape is associated with a set of images capturing the exact object in various environments.</p><p>Evaluation metric. As we are considering the joint evaluation of object detection and viewpoint estimation in this section, the metric should reflect the performance of both tasks. We thus compute the percentage of objects for which the intersection over union between the ground-truth bounding box and the predicted bounding box (with the right class) is larger than 0.5 and the rotational error between the ground-truth viewpoint and the predicted viewpoint is smaller than 30 ? . This metric corresponds to the Acc R ? 6 proposed in <ref type="bibr" target="#b92">[93]</ref>, which is used to evaluate a joint focal length and 3D pose estimation approach.</p><p>Compared methods. We compare our approach to the other viewpoint estimation methods, namely MetaView <ref type="bibr" target="#b9">[10]</ref> and StarMap+M, which is the best performing adaptation of StarMap <ref type="bibr" target="#b7">[8]</ref> (cf. . However, these methods are only evaluated on perfect detections, i.e., ground-truth classes and ground-truth bounding boxes, and no code is available to rerun them on other inputs. Regarding our approach, we consider the case of imperfect detections, where classes and bounding boxes are predicted by our object detector. Note that the object class is only useful for our viewpoint estimation variant that exploits exemplar 3D models (Ours w/ 3D), as the method variant without 3D information (Ours w/o 3D) is category-agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>Intra-dataset evaluation on ObjectNet3D. To experiment with this scenario, we split ObjectNet3D into 80 base classes and 20 novel classes as done in Section 4.2, and train the object detector and viewpoint estimator using the abundant annotated samples of base classes and scarce labeled samples of novel classes. In this setting, both training and testing samples are from the same dataset, i.e. ObjectNet3D.</p><p>As recalled in the left part of <ref type="table">Table 7</ref>, our few-shot viewpoint estimation outperforms other methods by a large margin when evaluated using ground-truth classes and ground-truth bounding boxes in the 10-shot setting. When using predicted classes and predicted bounding boxes, accuracy drops for most categories. One explanation is that viewpoint estimation becomes difficult when the objects are truncated by imperfect predicted bounding boxes, especially 7: Evaluation of joint few-shot detection and viewpoint estimation. We first recall viewpoint estimation results assuming perfect detection, i.e., using the ground-truth classes and ground-truth bounding boxes (cf. Tables 4-5). Then we use as input predicted classes and estimated bounding boxes given an object detector. As no code is available to evaluate StarMap+M and MetaView in this setting, we can only evaluate our viewpoint estimation method, for which we used our own detections as input. (Ours w/o 3D actually does not need to know the class as it is category-agnostic.) We report the percentage of objects that are correctly detected (right class) with IoU threshold at 0.5, and a rotational error less than 30 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-dataset evaluation on ObjectNet3D</head><p>Inter-dataset evaluation on <ref type="table">Pascal3D+   Method  bed  bshelf  calc  cphone  comp  door  fcabin  guit  iron  knife  micro  pen  pot  rifle  shoe  slipper  stove  toilet  tub  wchair  All  aero  bike  boat  bottle  bus  car  chair  table  mbike</ref>  for tiny objects (shoes) and ambiguous objects with similar appearances in different poses (knives, rifles). Yet, by comparing the performance gap between, on the one hand, our method when tested using predicted classes and predicted boxes, and, on the other hand, MetaView when tested using ground-truth classes and ground-truth boxes, we find that our approach is able to reach a better accuracy: 50% against 48%. This improvement is a strongly encouraging achievement since we free the viewpoint estimation approach from requiring the perfect ground-truth bounding boxes (and classes) without degrading the performance.</p><p>Inter-dataset evaluation on Pascal3D+. Here, we consider all 12 object categories of Pascal3D+ as novel classes, while the base classes are a set of disjoint object categories from ObjectNet3D and COCO for viewpoint estimation and object detection, respectively. We use the same split as in the interdataset few-shot viewpoint estimation (Section 4.2), that divides the 100 ObjectNet3D categories into 12 novel ones that intersect with Pascal3D+ and 88 remaining base classes. Besides, the 12 classes of Pascal3D+ are completely included in the 20 PASCAL VOC object categories, which are set to be the novel classes in the few-shot object detection on MS-COCO (Section 4.1). Therefore, we first use the 10-shot object detection network trained on MS-COCO to detect the novel objects on Pascal3D+, and then, using the predicted 2D bounding boxes, the 10-shot viewpoint estimation network trained on ObjectNet3D. Unlike the intra-dataset evaluation on ObjectNet3D, our networks are trained and tested on different datasets in this part. We report the results in the right part of <ref type="table">Table 7</ref>. Again, our few-shot viewpoint estimation network outperforms other methods by a large margin when evaluated using ground-truth classes and ground-truth bounding boxes in the 10-shot setting. Even though a performance drop appears when replacing the ground-truth bounding boxes by the predicted ones, our method using exemplar 3D models still outperforms other methods: 35% against 33%. This improvement is especially impressive considering the fact that our object detection and viewpoint estimation networks are both tested on a new dataset that is different from the training datasets, which is a big step towards realistic scenarios and industrial applications.</p><p>Visual results. We provide in <ref type="figure">Figure 7</ref> some qualitative results of few-shot object detection and viewpoint estimation of novel objects on ObjectNet3D and Pascal3D+. For each sample we show the predicted bounding boxes on the left and the estimated viewpoints on the right (visualized by the projected CAD models). Besides the appearance ambiguities causing major viewpoint estimation errors, we note that the principal failure cases result from the target objects being missed by our object detector (iron and knife) or the objects being wrongly classified (car and motorbike). Another error is that only one bounding box is predicted for multiple objects of the same class, which usually occurs in cluttered scenes (pen). These detection errors contribute considerably to the performance drop between evaluating using ground-truth bounding boxes and evaluating using predicted bounding boxes, especially for categories mainly containing tiny objects such as knife in ObjectNet3D and bottle in Pascal3D+.</p><p>Additional results on Pix3D. To further demonstrate the effectiveness of our few-shot object detection network and few-shot viewpoint estimation network, we follow GP2C <ref type="bibr" target="#b92">[93]</ref> and conduct evaluation on four object categories of Pix3D: bed, chair, sofa and table. As these four classes are completely included in the 12 Pascal3D+ object categories that are considered as novel categories in the inter-dataset evaluation described before, we use the same object detector trained on MS-COCO and viewpoint estimator trained on ObjectNet3D to perform an inter-dataset evaluation on Pix3D. <ref type="figure">Fig. 7</ref>: Qualitative results of joint few-shot object detection and viewpoint estimation using the predicted 2D bounding boxes given by our object detection model. We visualize results on ObjectNet3D and Pascal3D+. For each category, we show three success cases (the first six columns) and one failure case (the last two columns). For each testing image, we project the CAD model of the corresponding class into the predicted 2D bounding box and rotate it according to the estimated viewpoint. Error cases include: missing target objects (iron, knife, boat); failed classification (motorbike, car); cluttered objects being detected as one (pen); successful detection but failed viewpoint estimation (shoe and airplane).</p><p>We first report our results evaluated using the 2D bounding boxes predicted by GP2C in the middle of <ref type="table" target="#tab_9">Table 8</ref>. Even though the performance drops from 91% to 72%, this result is very encouraging since our viewpoint estimation network has only trained on 10 annotated samples for each testing category while previous methods has trained on thousands of annotated samples. Besides using only a small number of annotated training samples of the target classes, our viewpoint estimation network is trained on ObjectNet3D images and directly tested on Pix3D images, while Finegrained <ref type="bibr" target="#b93">[94]</ref> and GP2C <ref type="bibr" target="#b92">[93]</ref> use images from the same dataset for training and testing. Therefore, our setting is much harder compared to <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref>. We then report our results evaluated using predicted bounding boxes given by our few-shot object detector at the bottom of <ref type="table" target="#tab_9">Table 8</ref>. The overall performance drops around 20% points compared to the evaluation using bounding boxes predicted by GP2C, where the detection network is pre-trained on all 80 object categories of MS-COCO and fine-tuned on the 4 categories of Pix3D. In both cases, our viewpoint estimation method using 3D models performs better than our method without 3D models. This consistent improvement demonstrates again the benefits of adding 3D information in viewpoint estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Limitations</head><p>Our work shares a common limitation with other work on viewpoint estimation in that it does not handle very well small objects, which have less visible cues, and objects that are nearly symmetrical, such as knives. In the latter case, a wrong prediction of the front-back orientation can result in a very large prediction error, although the rendered views can be very similar to the actual images. It is even more so in the few-shot setting, where only a few labeled samples are provided for the novel categories. Preventing such failure cases could require a specific treatment of almostsymmetries.</p><p>Also, as discussed in the introduction regarding the case where we do not use 3D model information for viewpoint estimation but a class-agnostic approach instead, we rely on the fact that objects of different but related classes often are consistently oriented, with aligned similarities. While it is the case for all datasets we know of, this fact is not guaranteed.</p><p>Yet, in case of orientation discrepancies between classes, a dataset can somehow be "normalized" before training by applying systematic rotations of ground-truth viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND PERSPECTIVES</head><p>In this work, we presented an approach to few-shot object detection and viewpoint estimation that can tackle both tasks in a coherent and efficient framework. We demonstrated the benefits of this approach in terms of accuracy, and significantly improved the state of the art on several standard benchmarks for few-shot object detection and few-shot viewpoint estimation. Moreover, we showed that our few-shot viewpoint estimation model can achieve promising results on the novel objects detected by our few-shot detection model, compared in an adversarial setting to other existing methods tested on perfect detection, i.e., ground-truth classes and ground-truth bounding boxes. This is of particular interest for scene understanding in weakly-controlled environments, such as robotic manipulation with various objects in the wild. In future work, we are interested in developing category-agnostic models that can detect arbitrary objects and estimate their poses without seeing them during training. We will also expand our approach to perform 3D model retrieval and estimation refinement by selecting the 3D candidate that best agrees with the measured visual evidence, which might include RGB images, depth maps, and deep features extracted by a neural network. The exploitation of multiple views and additional inputs such as depth maps could also be considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of class data for object detection (left) &amp; viewpoint estimation (right). While the images with box masks capture the characteristic appearances and the common context for different classes, the point clouds in a canonical object space capture the geometric information such as the principal axis of symmetry and the position of the main object parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Method overview. (a) For object detection, we sample for each class c one image x in the training set containing an object j of class c, to which we add an extra channel for the binary mask mask j of the ground-truth bounding box box j of object j. Each corresponding vector of class features f cls c (red) is then combined with each vector of query features f qry i (blue) associated to one of the region of interest i in the query image, via an aggregation module. Finally, the aggregated features f agg i,c pass through a predictor that estimates a class probability cls i,c and regresses a bounding box box i,c . (b) For few-shot viewpoint estimation, we represent the 3D pose using three Euler angles. We estimate them either directly from the query features extracted from the image or, optionally, indirectly from aggregated features made of both query features and class information extracted from a few point clouds with coordinates in a normalized, canonical object space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of our category-agnostic viewpoint estimation approach without using 3D models. The network is first trained on abundant labeled images of base classes (left), then fine-tuned on a balanced set of images containing both base and novel classes (right).N train box regressions for each RoI. The final predictions are obtained by concatenating all the class-wise network outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>We report the standard MS-COCO evaluation metrics on the 20 novel classes of COCO. *Results computed over single experimental run with a fix set of support images.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average Recall</cell><cell></cell><cell></cell></row><row><cell>Shots</cell><cell>Method</cell><cell>0.5:0.95</cell><cell>0.5</cell><cell>0.75</cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>S</cell><cell>M</cell><cell>L</cell></row><row><cell></cell><cell>LSTD [46]*</cell><cell>3.2</cell><cell>8.1</cell><cell>2.1</cell><cell>0.9</cell><cell>2.0</cell><cell>6.5</cell><cell>7.8</cell><cell>10.4</cell><cell>10.4</cell><cell>1.1</cell><cell>5.6</cell><cell>19.6</cell></row><row><cell></cell><cell>FSRW [2]*</cell><cell>5.6</cell><cell>12.3</cell><cell>4.6</cell><cell>0.9</cell><cell>3.5</cell><cell>10.5</cell><cell>10.1</cell><cell>14.3</cell><cell>14.4</cell><cell>1.5</cell><cell>8.4</cell><cell>28.2</cell></row><row><cell></cell><cell>MetaDet [1]</cell><cell>7.1</cell><cell>14.6</cell><cell>6.1</cell><cell>1.0</cell><cell>4.1</cell><cell>12.2</cell><cell>11.9</cell><cell>15.1</cell><cell>15.5</cell><cell>1.7</cell><cell>9.7</cell><cell>30.1</cell></row><row><cell></cell><cell>Meta R-CNN [3]</cell><cell>8.7</cell><cell>19.1</cell><cell>6.6</cell><cell>2.3</cell><cell>7.7</cell><cell>14.0</cell><cell>12.6</cell><cell>17.8</cell><cell>17.9</cell><cell>7.8</cell><cell>15.6</cell><cell>27.2</cell></row><row><cell>10</cell><cell>FSOD [82]* MPSR [83]*</cell><cell>11.1 9.8</cell><cell>20.4 17.9</cell><cell>10.6 9.7</cell><cell>-3.3</cell><cell>-9.2</cell><cell>-16.1</cell><cell>-15.7</cell><cell>-21.2</cell><cell>-21.2</cell><cell>-4.6</cell><cell>-19.6</cell><cell>-34.3</cell></row><row><cell></cell><cell>TFA w/fc [7]</cell><cell>9.1</cell><cell>17.3</cell><cell>8.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TFA w/cos [7]</cell><cell>9.1</cell><cell>17.1</cell><cell>8.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Ours w/fc</cell><cell>12.5</cell><cell>27.3</cell><cell>9.8</cell><cell>2.5</cell><cell>13.8</cell><cell>19.9</cell><cell>20.0</cell><cell>25.5</cell><cell>25.7</cell><cell>7.5</cell><cell>27.6</cell><cell>38.9</cell></row><row><cell></cell><cell>Ours w/cos</cell><cell>13.6</cell><cell>28.6</cell><cell>11.3</cell><cell>2.6</cell><cell>14.6</cell><cell>22.1</cell><cell>20.6</cell><cell>26.8</cell><cell>27.0</cell><cell>7.9</cell><cell>28.8</cell><cell>41.3</cell></row><row><cell></cell><cell>LSTD [46]*</cell><cell>6.7</cell><cell>15.8</cell><cell>5.1</cell><cell>0.4</cell><cell>2.9</cell><cell>12.3</cell><cell>10.9</cell><cell>14.3</cell><cell>14.3</cell><cell>0.9</cell><cell>7.1</cell><cell>27.0</cell></row><row><cell></cell><cell>FSRW [2]*</cell><cell>9.1</cell><cell>19.0</cell><cell>7.6</cell><cell>0.8</cell><cell>4.9</cell><cell>16.8</cell><cell>13.2</cell><cell>17.7</cell><cell>17.8</cell><cell>1.5</cell><cell>10.4</cell><cell>33.5</cell></row><row><cell></cell><cell>MetaDet [1]</cell><cell>11.3</cell><cell>21.7</cell><cell>8.1</cell><cell>1.1</cell><cell>6.2</cell><cell>17.3</cell><cell>14.5</cell><cell>18.9</cell><cell>19.2</cell><cell>1.8</cell><cell>11.1</cell><cell>34.4</cell></row><row><cell></cell><cell>Meta R-CNN [3]</cell><cell>12.4</cell><cell>25.3</cell><cell>10.8</cell><cell>2.8</cell><cell>11.6</cell><cell>19.0</cell><cell>15.0</cell><cell>21.4</cell><cell>21.7</cell><cell>8.6</cell><cell>20.0</cell><cell>32.1</cell></row><row><cell>30</cell><cell>MPSR [83]*</cell><cell>14.1</cell><cell>25.4</cell><cell>14.2</cell><cell>4.0</cell><cell>12.9</cell><cell>23.0</cell><cell>17.7</cell><cell>24.2</cell><cell>24.3</cell><cell>5.5</cell><cell>21.0</cell><cell>39.3</cell></row><row><cell></cell><cell>TFA w/fc [7]</cell><cell>12.0</cell><cell>22.2</cell><cell>11.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TFA w/cos [7]</cell><cell>12.1</cell><cell>22.0</cell><cell>12.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Ours w/fc</cell><cell>14.7</cell><cell>30.6</cell><cell>12.2</cell><cell>3.2</cell><cell>15.2</cell><cell>23.8</cell><cell>22.0</cell><cell>28.2</cell><cell>28.4</cell><cell>8.3</cell><cell>30.3</cell><cell>42.1</cell></row><row><cell></cell><cell>Ours w/cos</cell><cell>16.4</cell><cell>32.6</cell><cell>14.7</cell><cell>3.5</cell><cell>17.1</cell><cell>26.2</cell><cell>23.3</cell><cell>29.7</cell><cell>29.9</cell><cell>8.8</cell><cell>31.9</cell><cell>44.7</cell></row></table><note>Few-shot object detection evaluation on MS-COCO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>qry f cls , f qry ] 36.6 ? 7.1 49.6 ? 4.3 27.5 ? 5.7 41.6 ? 3.7 28.7 ? 5.9 44.0 ? 2.7 [f qry f cls , f qry , f cls ] 37.6 ? 7.2 54.2 ? 4.9 30.0 ? 2.9 41.0 ? 5.3 33.6 ? 5.0 47.5 ? 2.3 [f qry f cls , f qry ? f cls ] 39.2 ? 4.5 55.5 ? 3.9 31.7 ? 6.2 45.2 ? 3.3 35.6 ? 5.6 48.9 ? 3.3 [f qry f cls , f qry ? f cls , f qry ]</figDesc><table><row><cell></cell><cell cols="2">Novel Set 1</cell><cell cols="2">Novel Set 2</cell><cell cols="2">Novel Set 3</cell></row><row><cell>Method \ Shots</cell><cell>3</cell><cell>10</cell><cell>3</cell><cell>10</cell><cell>3</cell><cell>10</cell></row><row><cell>[f qry f cls ]</cell><cell>35.0 ? 3.6</cell><cell>51.5 ? 5.8</cell><cell>29.6 ? 3.5</cell><cell>45.4 ? 5.5</cell><cell>27.5 ? 5.2</cell><cell>48.1 ? 5.9</cell></row><row><cell cols="2">[f 42.2 ? 2.1</cell><cell>57.4 ? 2.7</cell><cell>31.9 ? 2.7</cell><cell>45.7 ? 1.8</cell><cell>37.2 ? 3.5</cell><cell>49.6 ? 2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Intra-dataset 10-shot viewpoint estimation evaluation. We report Acc30(?) / MedErr(?) on the same 20 novel classes of ObjectNet3D for each method, while 80 are used as base classes. All models are trained and tested on ObjectNet3D.</figDesc><table><row><cell>Method</cell><cell>bed</cell><cell>bookshelf</cell><cell>calculator</cell><cell>cellphone</cell><cell>computer</cell><cell>door</cell><cell>f cabinet</cell></row><row><cell>StarMap+F [8]</cell><cell>0.32 / 47.2</cell><cell>0.61 / 21.0</cell><cell>0.26 / 50.6</cell><cell>0.56 / 26.8</cell><cell>0.59 / 24.4</cell><cell>-/ -</cell><cell>0.76 / 17.1</cell></row><row><cell>StarMap+M [8]</cell><cell>0.32 / 42.2</cell><cell>0.76 / 15.7</cell><cell>0.58 / 26.8</cell><cell>0.59 / 22.2</cell><cell>0.69 / 19.2</cell><cell>-/ -</cell><cell>0.76 / 15.5</cell></row><row><cell>MetaView [10]</cell><cell>0.36 / 37.5</cell><cell>0.76 / 17.2</cell><cell>0.92 / 12.3</cell><cell>0.58 / 25.1</cell><cell>0.70 / 22.2</cell><cell>-/ -</cell><cell>0.66 / 22.9</cell></row><row><cell>Ours w/o 3D</cell><cell>0.53 / 26.8</cell><cell>0.82 / 9.4</cell><cell>0.76 / 11.6</cell><cell>0.54 / 24.0</cell><cell>0.82 / 11.8</cell><cell>0.86 / 3.1</cell><cell>0.83 / 11.1</cell></row><row><cell>Ours w/ 3D</cell><cell>0.64 / 14.8</cell><cell>0.90 / 7.8</cell><cell>0.90 / 8.2</cell><cell>0.61 / 13.2</cell><cell>0.86 / 10.3</cell><cell>0.90 / 0.8</cell><cell>0.86 / 10.2</cell></row><row><cell>Method</cell><cell>guitar</cell><cell>iron</cell><cell>knife</cell><cell>microwave</cell><cell>pen</cell><cell>pot</cell><cell>rifle</cell></row><row><cell>StarMap+F [8]</cell><cell>0.54 / 27.9</cell><cell>0.00 / 128</cell><cell>0.05 / 120</cell><cell>0.82 / 19.0</cell><cell>-/ -</cell><cell>0.51 / 29.9</cell><cell>0.02 / 100</cell></row><row><cell>StarMap+M [8]</cell><cell>0.59 / 21.5</cell><cell>0.00 / 136</cell><cell>0.08 / 117</cell><cell>0.82 / 17.3</cell><cell>-/ -</cell><cell>0.51 / 28.2</cell><cell>0.01 / 100</cell></row><row><cell>MetaView [10]</cell><cell>0.63 / 24.0</cell><cell>0.20 / 77</cell><cell>0.05 / 98</cell><cell>0.77 / 17.9</cell><cell>-/ -</cell><cell>0.49 / 31.6</cell><cell>0.21 / 81</cell></row><row><cell>Ours w/o 3D</cell><cell>0.60 / 21.5</cell><cell>0.08 / 118</cell><cell>0.21 / 137</cell><cell>0.91 / 8.9</cell><cell>0.39 / 63.2</cell><cell>0.64 / 17.5</cell><cell>0.15 / 91</cell></row><row><cell>Ours w/ 3D</cell><cell>0.68 / 19.4</cell><cell>0.34 / 60</cell><cell>0.27 / 137</cell><cell>0.93 / 7.4</cell><cell>0.47 / 36.4</cell><cell>0.76 / 11.8</cell><cell>0.28 / 87</cell></row><row><cell>Method</cell><cell>shoe</cell><cell>slipper</cell><cell>stove</cell><cell>toilet</cell><cell>tub</cell><cell>wheelchair</cell><cell>All</cell></row><row><cell>StarMap+F [8]</cell><cell>-/ -</cell><cell>0.08 / 128</cell><cell>0.80 / 16.1</cell><cell>0.38 / 36.8</cell><cell>0.35 / 39.8</cell><cell>0.18 / 80.4</cell><cell>0.41 / 41.0</cell></row><row><cell>StarMap+M [8]</cell><cell>-/ -</cell><cell>0.15 / 128</cell><cell>0.83 / 15.6</cell><cell>0.39 / 35.5</cell><cell>0.41 / 38.5</cell><cell>0.24 / 71.5</cell><cell>0.46 / 33.9</cell></row><row><cell>MetaView [10]</cell><cell>-/ -</cell><cell>0.07 / 115</cell><cell>0.74 / 21.7</cell><cell>0.50 / 32.0</cell><cell>0.29 / 46.5</cell><cell>0.27 / 55.8</cell><cell>0.48 / 31.5</cell></row><row><cell>Ours w/o 3D</cell><cell>0.35 / 47.2</cell><cell>0.19 / 125</cell><cell>0.86 / 11.3</cell><cell>0.49 / 30.2</cell><cell>0.50 / 32.0</cell><cell>0.36 / 57.8</cell><cell>0.56 / 22.0</cell></row><row><cell>Ours w/ 3D</cell><cell>0.49 / 30.6</cell><cell>0.28 / 93</cell><cell>0.91 / 9.5</cell><cell>0.69 / 17.8</cell><cell>0.65 / 16.4</cell><cell>0.35 / 61.2</cell><cell>0.65 / 15.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Inter-dataset 10-shot viewpoint estimation evaluation. We report Acc30(?) / MedErr(?) on the 12 novel classes of Pascal3D+, while the 88 base classes are in ObjectNet3D. All models are trained on ObjectNet3D and tested on Pascal3D+.</figDesc><table><row><cell>Method</cell><cell>aero</cell><cell>bike</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>chair</cell></row><row><cell>StarMap+F [8]</cell><cell>0.03 / 102</cell><cell>0.05 / 98.8</cell><cell>0.07 / 99</cell><cell>0.48 / 31.9</cell><cell>0.46 / 33.0</cell><cell>0.18 / 80.8</cell><cell>0.22 / 74.6</cell></row><row><cell>StarMap+M [8]</cell><cell>0.03 / 99</cell><cell>0.08 / 88.4</cell><cell>0.11 / 92</cell><cell>0.55 / 28.0</cell><cell>0.49 / 31.0</cell><cell>0.21 / 81.4</cell><cell>0.21 / 80.2</cell></row><row><cell>MetaView [10]</cell><cell>0.12 / 104</cell><cell>0.08 / 91.3</cell><cell>0.09 / 108</cell><cell>0.71 / 24.0</cell><cell>0.64 / 22.8</cell><cell>0.22 / 73.3</cell><cell>0.20 / 89.1</cell></row><row><cell>Ours w/o 3D</cell><cell>0.14 / 88</cell><cell>0.30 / 67.8</cell><cell>0.20 / 83</cell><cell>0.81 / 12.1</cell><cell>0.73 / 9.6</cell><cell>0.43 / 53.8</cell><cell>0.30 / 78.8</cell></row><row><cell>Ours w/ 3D</cell><cell>0.21 / 73</cell><cell>0.33 / 64.7</cell><cell>0.25 / 78</cell><cell>0.91 / 11.6</cell><cell>0.74 / 9.0</cell><cell>0.49 / 32.8</cell><cell>0.32 / 79.1</cell></row><row><cell>Method</cell><cell>table</cell><cell>mbike</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>All</cell><cell></cell></row><row><cell>StarMap+F [8]</cell><cell>0.46 / 31.4</cell><cell>0.09 / 91.6</cell><cell>0.32 / 44.7</cell><cell>0.36 / 41.7</cell><cell>0.52 / 29.1</cell><cell cols="2">0.25 / 64.7</cell></row><row><cell>StarMap+M [8]</cell><cell>0.29 / 36.8</cell><cell>0.11 / 83.5</cell><cell>0.44 / 42.9</cell><cell>0.42 / 33.9</cell><cell>0.64 / 25.3</cell><cell cols="2">0.28 / 60.5</cell></row><row><cell>MetaView [10]</cell><cell>0.39 / 36.0</cell><cell>0.14 / 74.7</cell><cell>0.29 / 46.2</cell><cell>0.61 / 23.8</cell><cell>0.58 / 26.3</cell><cell cols="2">0.33 / 51.3</cell></row><row><cell>Ours w/o 3D</cell><cell>0.51 / 31.2</cell><cell>0.36 / 49.8</cell><cell>0.49 / 34.6</cell><cell>0.62 / 16.1</cell><cell>0.77 / 18.7</cell><cell cols="2">0.46 / 38.3</cell></row><row><cell>Ours w/ 3D</cell><cell>0.59 / 20.9</cell><cell>0.44 / 37.2</cell><cell>0.58 / 23.9</cell><cell>0.72 / 12.1</cell><cell>0.79 / 19.0</cell><cell cols="2">0.51 / 29.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Efficacy of different 3D representations, if any. We show few-shot viewpoint estimation results on the 20 novel classes of ObjectNet3D. The first row represents our approach without using any form of 3D information, while other rows correspond to our method using exemplar 3D models with different representations. We also plot the four different 3D representations of an example CAD model on the bottom.</figDesc><table><row><cell></cell><cell cols="2">Acc30(?) / MedErr(?)</cell></row><row><cell>3D exemplar</cell><cell>Base</cell><cell>Novel</cell></row><row><cell>None</cell><cell>0.58 ? 0.01 / 21.3 ? 0.31</cell><cell>0.56 ? 0.01 / 22.1 ? 0.80</cell></row><row><cell>Depth</cell><cell>0.61 ? 0.01 / 22.0 ? 0.97</cell><cell>0.57 ? 0.02 / 24.3 ? 1.52</cell></row><row><cell>Object Coord.</cell><cell>0.61 ? 0.01 / 22.0 ? 0.54</cell><cell>0.59 ? 0.02 / 23.7 ? 1.09</cell></row><row><cell>Rendering</cell><cell>0.61 ? 0.01 / 21.7 ? 0.92</cell><cell>0.60 ? 0.01 / 22.9 ? 0.77</cell></row><row><cell>Point Cloud</cell><cell cols="2">0.64 ? 0.01 / 17.5 ? 0.18 0.65 ? 0.01 / 15.6 ? 0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Inter-dataset few-shot detection and viewpoint estimation evaluation on Pix3D. ? Detection network from<ref type="bibr" target="#b92">[93]</ref>. Our few-shot object detection and viewpoint estimation networks trained and tested on different datasets.</figDesc><table><row><cell>Method</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>table</cell><cell>Mean</cell></row><row><cell cols="5">Detection + Viewpoint Estimation</cell><cell></cell></row><row><cell>Fine-grained [94]</cell><cell>95</cell><cell>88</cell><cell>95</cell><cell>73</cell><cell>88</cell></row><row><cell>GP2C [93]</cell><cell>98</cell><cell>91</cell><cell>97</cell><cell>77</cell><cell>91</cell></row><row><cell cols="5">Detection  ? + Few-shot Viewpoint Estimation  ?</cell><cell></cell></row><row><cell>Ours w/o 3D</cell><cell>81</cell><cell>47</cell><cell>88</cell><cell>53</cell><cell>67</cell></row><row><cell>Ours w/ 3D</cell><cell>86</cell><cell>51</cell><cell>92</cell><cell>58</cell><cell>72</cell></row><row><cell cols="6">Few-shot Detection  ? + Few-shot Viewpoint Estimation  ?</cell></row><row><cell>Ours w/o 3D</cell><cell>68</cell><cell>34</cell><cell>81</cell><cell>13</cell><cell>49</cell></row><row><cell>Ours w/ 3D</cell><cell>71</cell><cell>36</cell><cell>87</cell><cell>14</cell><cell>52</cell></row></table><note>?</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vincent Lepetit is a research director at ENPC ParisTech, France. Before that, he was a full professor at the Institute for Computer Graphics and Vision, Graz University of Technology, and before that, a senior researcher at CVLab, EPFL, Switzerland. He currently focuses on 3D scene understanding from images, with application to 3D hand and object tracking, 3D reconstruction, and camera localization. With his co-authors, he received the Koenderick 'test of time' award for the BRIEF local descriptor in 2020.</p><p>Renaud Marlet is a Senior Researcher at?cole des Ponts ParisTech (ENPC) and a Principal Scientist at valeo.ai, France. He has held positions both in academia (researcher at Inria) and in the software industry (expert at Simulog, deputy CTO of Trusted Logic). He was the head of the IMAGINE group at LIGM/ENPC (2010-2019). He is currently interested in scene understanding and semantized 3D reconstruction, with applications to robotics, autonomous driving and civil engineering.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meta-learning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fewshot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta R-CNN : Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fgn: Fully guided network for few-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">StarMap for categoryagnostic keypoint and viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pose from shape: Deep pose estimation for arbitrary 3D objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CANet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Class-agnostic few-shot object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">F-vaegan-d2: A feature generating framework for any-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Empirical Bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lgm-net: Learning to generate matching networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Meta-learning for semisupervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LSTD: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guoyou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Watch and learn: Semi-supervised learning of object detectors from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Model recommendation: Generating object detectors from few samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zero shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3D model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Posecontrast: Class-agnostic object viewpoint estimation in the wild with pose-aware contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Implicit 3D orientation learning for 6D object detection from RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-path learning for object pose estimation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Puang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>M?rton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">3D pose estimation and 3D model retrieval for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">CorNet: Generic 3D corners for 6D pose estimation of new objects without retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pitteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVw)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">6-DoF object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Feature mapping for learning fast and accurate 3d pose inference from synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cosypose: Consistent multi-view multi-object 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Self-supervised 6d object pose estimation for robot manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bretl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Self-supervised viewpoint learning from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Self6d: Self-supervised monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Pose induction for novel object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Latentfusion: End-toend differentiable reconstruction and rendering for unseen object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Neural view synthesis and matching for semi-supervised few-shot learning of 3d pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">PointNet: deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Target driven instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ammirato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04610</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">ShapeMask: Learning to segment novel objects by refining shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-RPN and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">ObjectNet3D: A large scale database for 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Beyond PASCAL: A benchmark for 3D object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoi?er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">T-less: An rgb-d dataset for 6d pose estimation of texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Obdrz?lek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I A</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">PoseCNN: A convolutional neural network for 6D object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6D object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Location field descriptor: Single image 3D model retrieval in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Gp2c: Geometric projection parameter consensus for joint 3d pose and focal length estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">3d pose estimation for fine-grained object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop (ECCVw)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
