<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label Decoupling Framework for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
							<email>jun.wei@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
							<email>wangshuhui@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
							<email>zhe.wu@vipl.ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
							<email>suchi@kingsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Kingsoft Cloud</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff4">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label Decoupling Framework for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To get more accurate saliency maps, recent methods mainly focus on aggregating multi-level features from fully convolutional network (FCN) and introducing edge information as auxiliary supervision. Though remarkable progress has been achieved, we observe that the closer the pixel is to the edge, the more difficult it is to be predicted, because edge pixels have a very imbalance distribution. To address this problem, we propose a label decoupling framework (LDF) which consists of a label decoupling (LD) procedure and a feature interaction network (FIN). LD explicitly decomposes the original saliency map into body map and detail map, where body map concentrates on center areas of objects and detail map focuses on regions around edges. Detail map works better because it involves much more pixels than traditional edge supervision. Different from saliency map, body map discards edge pixels and only pays attention to center areas. This successfully avoids the distraction from edge pixels during training. Therefore, we employ two branches in FIN to deal with body map and detail map respectively. Feature interaction (FI) is designed to fuse the two complementary branches to predict the saliency map, which is then used to refine the two branches again. This iterative refinement is helpful for learning better representations and more precise saliency maps. Comprehensive experiments on six benchmark datasets demonstrate that LDF outperforms state-of-the-art approaches on different evaluation metrics. Codes can be found at https: //github.com/weijun88/LDF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection (SOD) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> aims at identifying the most visually attractive objects or parts in an image or video, which is widely applied as a pre-processing <ref type="table">Table 1</ref>. Mean absolute error of the predicted saliency maps (MAE global ) and edge areas (MAE edge ) of two state-of-the-art methods over three datasets. MAE edge is much larger than MAE global , demonstrating that edge prediction is more difficult.</p><p>EGNet <ref type="bibr" target="#b40">[41]</ref> SCRN <ref type="bibr" target="#b33">[34]</ref>   <ref type="figure">Figure 1</ref>. Distribution of prediction error with respect to distance from pixel to its nearest edge. Horizontal coordinate represents the distance, which has been normalized to [0,1] and vertical coordinate is the prediction error. As can be seen, the closer the pixel is to the edge, the more difficult it is to be predicted. procedure in downstream computer vision tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>. During the past decades, researchers have proposed hundreds of SOD methods based on hand-crafted features (e.g., color, texture and brightness) <ref type="bibr" target="#b28">[29]</ref>. However, these features can not capture high-level semantic information, which restricts their applications in complex scenes. Recently, convolutional neural networks (CNNs) have demonstrated powerful capability of feature representation and greatly promoted the development of SOD. Many CNNsbased methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref> have achieved remarkable performance by designing different decoders to aggregate multi-level CNN features. To get better feature representations, these methods focus on mining more context information and devising more effective feature fusion strategies. Besides, introducing the boundary information is another key point in SOD. Existing methods attempt to take edges as supervision to train SOD models, which significantly improves the accuracy of saliency maps <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref>. However, the imbalance between edge pixels and nonedge ones makes it hard to get good edge predictions. Therefore, directly taking edges as supervision may lead to suboptimal solutions. To better elaborate this statement, we calculate the mean absolute error (MAE) of two stateof-the-art methods (i.e., EGNet <ref type="bibr" target="#b40">[41]</ref> and SCRN <ref type="bibr" target="#b33">[34]</ref>) over three SOD datasets (i.e., ECSSD <ref type="bibr" target="#b35">[36]</ref>, DUTS <ref type="bibr" target="#b24">[25]</ref> and DUT-O <ref type="bibr" target="#b36">[37]</ref>) in Tab. 1. Though two methods get low error in global saliency prediction, they perform much worse in edge prediction, which shows that edge pixels are more difficult to predict than others. To further explore the prediction difficulties of pixels, we analyse the distribution of prediction error about the distance to the nearest edge of EGNet and SCRN in <ref type="figure">Fig. 1</ref>.</p><p>In <ref type="figure">Fig. 1</ref>, the prediction error curves gradually increases from far away to close to the edge (i.e., the right axis to the left axis). When the distance is larger than 0.4, these curves rise slowly. However, when the distance gets smaller than 0.4, these curves begin to go upwards quickly. Based on this observation, we can divide each of the curves into two parts according to pixel distance from their nearest edges. Pixels near the edges correspond to much larger prediction errors than far-away pixels. These pixels with high prediction errors consists of both edge pixels and many other pixels close to edges that are ignored by recent edge-aware methods. Most of the hard pixels that can greatly improve the performance of SOD are not fully used, while using only edge pixels will lead to difficulties because of the imbalance distribution between edge pixels and background ones. In contrast, pixels far away from edges have relatively low prediction errors, which are much easier to be classified. However, traditional saliency labels treat all pixels inside salient object equally, which may cause pixels with low prediction errors to suffer distractive effects from those near edges.</p><p>We propose label decoupling framework to address the above problems. LDF mainly consists of a label decoupling procedure and a feature interaction network. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, a saliency label is decomposed into a body map and a detail map by LD. Different from the pure edge map, the detail map consists of both edges as well as nearby pixels, which makes full use of pixels near edge and thus has a more balanced pixel distribution. The body map mainly concentrates on pixels far away from edges. Without the disturbance of pixels near edges, the body map can supervise the model to learn better representations. Accordingly, FIN is designed with two branches to adapt to body map and detail map respectively. The two complementary branches in FIN are fused to predict the saliency map, which is then used to refine the two branches again. This iterative refine-ment procedure is helpful for obtaining gradually accurate saliency maps prediction.</p><p>We conduct experiments on six popular SOD datasets and demonstrate the superior performance of LDF. In summary, our contributions are as follows:</p><p>? We analyse the shortcomings of edge-based SOD methods and propose a label decoupling procedure to decompose a saliency label into body map and detail map to supervise the model, respectively.</p><p>? We design a feature interaction network to make full use of the complementary information between branches. Both branches will be enhanced by iteratively exchanging information to produce more precise saliency maps.</p><p>? Extensive experiments on six SOD datasets show that our model outperforms state-of-the-art models by a large margin. In particularly, we demonstrate the good performance of LDF in different challenging scenes in the SOC dataset <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>During the past decades, a huge body of traditional methods have been developed for SOD. These methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref> mainly rely on intrinsic cues (e.g., color and texture) to extract features. However, these features cannot capture highlevel semantic information and are not robust to variations, which limits their applications in complex scenarios. Recently, deep learning based models have achieved remarkable performance, which can be divided into aggregationbased models and edge-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Aggregation-based Models</head><p>Most of the aggregation-based models adopt the encoder-decoder framework, where the encoder is used to extract multi-scale features and the decoder is used to integrate the features to leverage context information of different levels. Hou et al. <ref type="bibr" target="#b14">[15]</ref> constructed shortcut connections on fully convolutional networks <ref type="bibr" target="#b21">[22]</ref> and integrated features of different layers to output more accurate maps. Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a reverse attention network, which erased the current predicted salient regions to expect the network to mine out the missing parts. Deng et al. <ref type="bibr" target="#b6">[7]</ref> designed an iterative strategy to learn the residual map between the prediction and ground truth by combining features from both deep and shallow layers. Wu et al. <ref type="bibr" target="#b32">[33]</ref> found that features of shallow layers greatly increased the computation cost, but only brought little improvement in final results. Liu et al. <ref type="bibr" target="#b19">[20]</ref> utilized simple pooling and a feature aggregation module to build fast and accurate model. Zhao et al. <ref type="bibr" target="#b41">[42]</ref> introduced the channel-wise attention and spatial attention to extract valuable features and suppress background noise.  <ref type="figure">Figure 2</ref>. An overview of our proposed label decoupling framework (LDF). LDF is based on ResNet-50 <ref type="bibr" target="#b13">[14]</ref> with supervision from body map, detail map and saliency map. LDF consists of two encoders and two decoders, i.e., a backbone encoder for feature extraction, an interaction encoder for exchanging information, a body decoder and a detail decoder to generate body map and detail map respectively. The interaction encoder is not involved until body decoder and detail decoder output features.</p><p>Wang et al. <ref type="bibr" target="#b29">[30]</ref> designed a top-down and bottom-up workflow to infer the salient object regions with multiple iterations. Liu et al. <ref type="bibr" target="#b20">[21]</ref> proposed a pixel-wise contextual attention network to learn the context of each pixel, and combined the global context and local context for saliency prediction. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> designed a bi-directional message passing model for better feature selection and integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Edge-based Models</head><p>In addition to saliency masks, edge label is also introduced to SOD in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> to assist the generation of saliency maps. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> and Zhao et al. <ref type="bibr" target="#b41">[42]</ref> directly built the edge loss with binary crossentropy to emphasize the importance of boundaries. Qin et al. <ref type="bibr" target="#b22">[23]</ref> designed a hybrid loss to supervise the training process of SOD on pixel-level, patch-level and maplevel. Liu et al. <ref type="bibr" target="#b19">[20]</ref> used additional edge dataset for joint training of both edge detection and SOD models. Feng et al. <ref type="bibr" target="#b12">[13]</ref> applied a boundary-enhanced loss to generate sharp boundaries and distinguish the narrow background margins between two foreground areas. Li et al. <ref type="bibr" target="#b17">[18]</ref> used a twobranch network to simultaneously predict the contours and saliency maps, which can automatically convert the trained contour detection model to SOD model. Wu et al. <ref type="bibr" target="#b33">[34]</ref> investigated the logical inter-relations between segmentation and edge maps, which are then promoted to bidirectionally refine multi-level features of the two tasks. Although these methods take into account the relationship between edges and saliency maps, edge prediction is a hard task because of imbalanced pixel distribution. In this paper, we explicitly decouple the saliency label into body map and detail map, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. Detail map helps model learn better edge features and body map decreases the distraction from pixels near edges to center ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first introduce the label decoupling method and give the specific steps to decompose the saliency map into body map and detail map. Then, to take advantage of the complementarity between features, we introduce FIN which facilitates the iterative information exchange between branches. The overview of the proposed model is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Label Decoupling</head><p>As described in Sec. 1, the prediction difficulty of a pixel is closely related to its position. Because of the cluttered background, pixels near the edge are more prone to be mispredicted. In comparison, central pixels have higher prediction accuracy due to the internal consistency of the salient target. Instead of treating these pixels equally, it will be more reasonable to deal with them according to their respective characteristics. Accordingly, we propose to decouple the original label into body label and detail label, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. To achieve this goal, we introduce Distance Transformation (DT) to decouple the original label, which is a traditional image processing algorithm. DT can convert the binary image into a new image where each foreground pixel has a value corresponding to the minimum distance from the background by a distance function.</p><p>Specifically, the input of DT is a binary image I, which can be divided into two groups (i.e., foreground I f g and background I bg . For each pixel p, I(p) is its corresponding value. If p ? I f g , I(p) equals 1, and 0 if p ? I bg . To get the DT result of image I, we define the metric function f (p, q) = (p x ? q x ) 2 + (p y ? q y ) 2 to measure the distance between pixels. If pixel p belongs to the foreground, DT will first look up its nearest pixel q in the background and then use f (p, q) to calculate the distance between pixel p and q. If pixel p belongs to the background, their minimum distance is set to zero. We use f (p, q) as the pixels of a newly generated image, and the distance transformation can be expressed as</p><formula xml:id="formula_0">I (p) = ? ? ? min q?I bg f (p, q), p ? I f g 0, p ? I bg<label>(1)</label></formula><p>After the distance transformation, the original image I has been transformed into I where pixel value I (p) no longer equals to 0 or 1. We normalize the pixel values in . Compared with the original image I which treats all pixels equally, pixel value of I not only depends on whether it belongs to foreground or background, but also is related to its relative position. Pixels located in the center of object have the largest values and those far away from the center or in background have the smallest values. So I represents the body part of the original image, which mainly focuses on the central pixels that are relatively easy. We use it as the body label in the following experiments. Correspondingly, by removing the body image I from the original image I, we can get the detail image, which is regarded as the detail label in consequent experiments and mainly concentrates on pixels far away from the main regions. In addition, we multiply the newly generated labels with the original binary image I to remove the background interference as</p><formula xml:id="formula_1">Label ? BL = I * I DL = I * (1 ? I )<label>(2)</label></formula><p>where BL means the body label and DL represents the detail label. Now the original label has been decoupled into two different kinds of supervision to assist the network to learn both the body and detail features with different characteristics respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction</head><p>As suggested by <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref>, we use ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as our backbone network. Specifically, we remove the fully  connected layer and retain all convolutional blocks. Given an input image with shape H ? W , this backbone will generate five scales of features with decreasing spatial resolution by stride 2 due to downsampling. We denote these features as F = {F i |i = 1, 2, 3, 4, 5}. The size of the i-th feature is</p><formula xml:id="formula_2">W 2 i ? H 2 i ? C i , where C i is the channel of the i-th feature.</formula><p>It has been shown that low-level features greatly increase computation cost, but bring limited performance improvement <ref type="bibr" target="#b32">[33]</ref>. So we only utilize features from {F i |i = 2, 3, 4, 5}, as shown in <ref type="figure">Fig. 2</ref>. Two convolution layers are applied to these features to adapt them seperately to the body prediction task and detail prediction task. Then we get two groups of features B = {B i |i = 2, 3, 4, 5} and D = {D i |i = 2, 3, 4, 5}, which all have been squeezed to 64 channels and sent to the decoder network for saliency map generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Interaction Network</head><p>Feature interaction network is built to adapt to the label decoupling, as shown in <ref type="figure">Fig. 2</ref>. With label decoupling, the saliency label has been transformed into the body map and the detail map, both of which are taken as supervision for model learning. FIN is designed as a two-branch structure, each of which is responsible for one label kind. Since both the body map and detail map are derived from the same saliency label, there exists a certain level of similarity and complementarity between the features from two branches. We introduce feature interaction between the complementary branches for information exchanging.</p><p>On the whole, the proposed framework is made up of one backbone encoder network, one interaction encoder network, one body decoder network and one detail decoder network. As discussed in Sec. 3.2, ResNet-50 <ref type="bibr" target="#b13">[14]</ref> is used as the backbone network to extract multi-level features B = {B i |i = 2, 3, 4, 5} and D = {D i |i = 2, 3, 4, 5}. For features B, a body decoder network is applied to gen-erate body maps. Similarly, for features D, a detail decoder network is applied to generate detail maps. After getting the output features of these two branches, the simplest way to deal with them is to concatenate these features and apply a convolutional layer to get final saliency maps. However, this way ignores the relationship between branches. To explicitly promote the information exchange between branches, an interaction encoder network is introduced.</p><p>More specifically, interaction decoder takes the concatenated features of the body decoder and detail decoder as input. It stacks multiple convolutions to extract multi-level features. Then these multi-level features will be applied with 3x3 convolution layers to make them appropriate for body decoder and detail decoder respectively. Direct addition is used to fuse the interaction features with features from backbone encoder to produce more accurate saliency maps. On the surface, the whole network is unusual since the latter branch outputs are used in the former decoder. But in fact, feature interaction consists of multiple iterations. At the first iteration, two branches output features without exchanging information. From the second iteration, interaction is involved between branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>Our training loss is defined as the summation of the outputs of all iterations as,</p><formula xml:id="formula_3">L = K k=1 ? k (k) ,<label>(3)</label></formula><p>where (k) is the loss of the k-th iteration, K denotes the total number of iterations and ? k is the weight of each iteration. To simplify the problem, we set ? k = 1 to treat all iterations equally. For each iteration, we will get three outputs (i.e., body, detail and segmentation) and each of them corresponds to one loss. So (k) can be defined as the combination of three losses as follows:</p><formula xml:id="formula_4">(k) = (k) body + (k) detail + (k) segm ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">(k) body ,<label>(k)</label></formula><p>detail and (k) segm denote body loss, detail loss and segmentation loss, respectively. We directly utilize binary cross entropy (BCE) to calculate both [g(x,y)log(p(x,y))+(1?g(x,y))log(1?p(x,y))], <ref type="bibr" target="#b4">(5)</ref> where g(x, y) ? [0, 1] is the ground truth label of the pixel (x, y) and p(x, y) ? [0, 1] is the predicted probability of being salient object. However, BCE calculates the loss for each pixel independently and ignores the global structure of the image. To remedy this problem, as suggested by <ref type="bibr" target="#b22">[23]</ref> we utilize the IoU loss to calculate (k) segmentation , which can measure the similarity of two images on the whole rather than a single pixel. It is defined as:</p><formula xml:id="formula_6">iou = 1 ? (x,y) [g(x, y) * p(x, y)] (x,y) [g(x, y) + p(x, y) ? g(x, y) * p(x, y)] ,<label>(6)</label></formula><p>where the notations are the same as Eq. 5. We do not apply IoU loss on detail , because IoU loss requires the ground truth to be binary or it will result in wrong predictions, while body label and detail label do not satisfy this requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>To evaluate the proposed method, six popular benchmark datasets are adopted, including ECSSD <ref type="bibr" target="#b35">[36]</ref> with 1000 images, PASCAL-S <ref type="bibr" target="#b18">[19]</ref> with 850 images, HKU-IS <ref type="bibr" target="#b16">[17]</ref> with 4447 images, DUT-OMRON <ref type="bibr" target="#b36">[37]</ref> with 5168 images, DUTS <ref type="bibr" target="#b24">[25]</ref> with 15572 images and THUR15K <ref type="bibr" target="#b4">[5]</ref> with 6232 images. Among them, DUTS is the largest saliency detection benchmark, which contains 10,553 training images (DUTS-TR) and 5,019 testing images (DUTS-TE). DUTS-TR is used to train the model, other datasets for evaluation. In addition, we also measure the model performance on the challenging SOC dataset <ref type="bibr" target="#b7">[8]</ref> of different attributes. Five metrics are used to evaluate the performance of our model and existing state-of-the-art methods. The first metric is the mean absolute error (MAE), as shown in Eq. 7, which is widely adopted in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. Mean F -measure (mF ), E-measure (E ? ) <ref type="bibr" target="#b8">[9]</ref>, weighted F -measure (F ? ? ) and S-measure (S ? ) are also widely used to evaluate saliency maps. In addition, precision-recall (PR) and F -measure curves are drawn to show the overall performance.</p><formula xml:id="formula_7">M AE = 1 H ? W H i=1 W j=1 |P (i, j) ? G(i, j)| (7)</formula><p>where P is the predicted map and G is the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The proposed model is trained on DUTS-TR and tested on the above mentioned six datasets. For data augmentation, we use horizontal flip, random crop and multi-scale input images. ResNet-50, pretrained on ImageNet, is used to initialize the backbone (i.e., block1 to block5) and other parameters are randomly initialized. We set the maximum learning rate to 0.005 for ResNet-50 backbone and 0.05 for other parts. Warm-up and linear decay strategies are used. The whole network is trained end-to-end by stochastic gradient descent (SGD). Momentum and weight decay are set to 0.9 and 0.0005, respectively. Batchsize is set to 32 and maximum epoch is set to 48. During testing, each image is <ref type="table">Table 2</ref>. Performance comparison with state-of-the-art methods on six datasets. MAE (smaller is better), mean F -measure (mF , larger is better) and E-measure (E ? , larger is better) are used to measure the model performance. '-' means the author has not provided corresponding saliency maps. The best and the second best results are highlighted in red and blue respectively.    simply resized to 352 x 352 and then fed into the network to get prediction without any post-processing. It is worth noting that the output saliency maps are used as the predictions rather than the addition of predicted body and detail maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUT-OMRON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Number of Feature Interaction. Tab. 4 shows the performance with different numbers of feature interaction. Compared with the baseline which has no feature inter-action (Number=0), model with one feature interaction achieves better results. When the number is larger, the performance becomes worse. Because repeated feature interaction makes the network to grow too deeper and harder to optimize. So in all the following experiments, we set the number to 1 to balance the model optmization and performance.</p><p>Different Combinations of Supervision. Tab. 5 shows the performance with different combinations of supervision.  <ref type="figure">Figure 5</ref>. Visual comparison of different algorithms. Each row represents one image and corresponding saliency maps. Each column represents the predictions of one method. Apparently, our method is good at dealing with cluttered background and producing more accurate and clear saliency maps.  <ref type="bibr" target="#b3">[4]</ref>, PiCA-R <ref type="bibr" target="#b20">[21]</ref>, AFNet <ref type="bibr" target="#b12">[13]</ref>, BASNet <ref type="bibr" target="#b22">[23]</ref>, CPD-R <ref type="bibr" target="#b32">[33]</ref>, EGNet-R <ref type="bibr" target="#b40">[41]</ref>, PAGE <ref type="bibr" target="#b30">[31]</ref>, TDBU <ref type="bibr" target="#b29">[30]</ref>, SCRN <ref type="bibr" target="#b33">[34]</ref>, SIBA <ref type="bibr" target="#b23">[24]</ref> and PoolNet <ref type="bibr" target="#b19">[20]</ref>. For fair comparison, we evaluate all the saliency maps provided by the authors with the same evaluation codes. We compare the proposed method with others in terms of MAE, mF and E ? , which are shown in Tab. 2. The best results are highlighted with red color. Obviously, compared with other counterparts, our method outperforms previous state-of-the-art methods by a large margin. Besides, <ref type="figure" target="#fig_8">Fig. 4</ref>   <ref type="figure">Figure 6</ref>. Error-Distance distribution of different methods. The proposed method has the smallest error along the distance. Especially around edge areas, the proposed method performs much better. presents the precision-recall curves and F -measure curves on five datasets. As can be seen, the curves of the proposed method consistently lie above others. In addition, we calculate the Error-Distance distribution of different methods in <ref type="figure">Fig. 6</ref>, where predictions produced by the proposed method have the minimum error along distance, especially around the edge areas.</p><p>Visual Comparison. Some prediction examples of the proposed method and other state-of-the-art approaches have been shown in <ref type="figure">Fig. 5</ref>. We observe that the proposed method not only highlights the correct salient object regions clearly, but also well suppresses the background noises. It is robust in dealing with various challenging scenarios, including cluttered background, manufactured structure and low contrast foreground. Compared with other counterparts, the saliency maps produced by the proposed method are clearer and more accurate.</p><p>Performance on SOC of Different Attributes. SOC [8] is a challenging dataset with multiple attributes. Images with the same attribute have certain similarity and reflect the common challenge in real world. We utilize this dataset to test the robustness of model under different scenes. Specifically, we evaluate the mean F -measure score of our model as well as 11 state-of-the-art methods. Each model will get nine scores under nine attributes. In addition, an overall score is calculated to measure the whole performance under all scenes. Tab. 3 shows the scores. We can see the proposed model achieves the best results among most of attributes except "BO", which indicates the good generalization of the proposed method. It can be applied in different challenging scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose the label decoupling framework for salient object detection. By empirically showing that edge prediction is a challenging task in saliency prediction, we propose to decouple the saliency label into body map and detail map. Detail map helps model learn better edge features and body map avoids the distraction from pixels near edges. Supervised by these two kinds of maps, the proposed method achieves better performance than direct supervison with saliency maps. Besides, feature interaction network is introduced to make full use of the complementarity between body and detail maps. Experiments on six datasets demonstrate that the proposed method outperforms state-of-the-art methods under different evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ECSSD DUTS DUT-O ECSSD DUTS DUT-O M AE global 0.037 0.039 0.053 0.037 0.040 0.056 M AE edge 0.289 0.292 0.298 0.299 0.297 0.302</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>I</head><label></label><figDesc>using a simple linear function I = I ?min(I ) max(I )?min(I ) to map the original value to [0, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Some examples of label decoupling. (c) represents the body label of the ground truth, where pixels close to the center of the target have larger values. (d) means the detail label of the ground truth, where pixels near the boundary of the target have larger values. The sum of (c) and (d) is equal to (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>is a widely used loss in binary classification and segmentation, which is defined as:bce = ? (x,y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Performance comparison with state-of-the-art methods on five datasets. The first row shows precision-recall curves. The second row shows F -measure curves with different thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MAE mF E ? MAE mF E ? MAE mF E ? MAE mF E ? MAE mF E ? BMPM [38] .044 .894 .914 .073 .803 .838 .049 .762 .859 .039 .875 .937 .063 .698 .839 .079 .704 .803 DGRL [28] .043 .903 .917 .074 .807 .836 .051 .764 .863 .037 .881 .941 .063 .709 .843 .077 .716 .811 R 3 Net [7] .051 .883 .914 .101 .775 .824 .067 .716 .827 .047 .853 .921 .073 .690 .814 .078 .693 .803 RAS [4] .055 .890 .916 .102 .782 .832 .060 .750 .861 .045 .874 .931 .063 .711 .843 .075 .707 .821 PiCA-R [21] .046 .867 .913 .075 .776 .833 .051 .754 .862 .043 .840 .936 .065 .695 .841 .081 .690 .803 AFNet [13] .042 .908 .918 .070 .821 .846 .046 .792 .879 .036 .888 .942 .057 .738 .853 .072 .730 .820 BASNet [23] .037 .880 .921 .076 .775 .847 .048 .791 .884 .032 .895 .946 .056 .756 .869 .073 .733 .821 CPD-R [33] .037 .917 .925 .072 .824 .849 .043 .805 .886 .034 .891 .944 .056 .747 .866 .068 .738 .829 EGNet-R [41] .037 .920 .927 .074 .823 .849 .039 .815 .891 .032 .898 .948 .053 .755 .867 .067 ..926 .064 .832 .857 .040 .808 .888 .034 .896 .949 .056 .746 .863 .066 .741 .833 SIBA [24] .035 .923 .928 .070 .830 .855 .040 .815 .892 .032 .900 .950 .059 .746 .860 .068 .741 .832 PoolNet [20] .039 .915 .924 .074 .822 .850 .040 .809 .889 .032 .899 .949 .056 .747 .863 .070 .732 .822 LDF(ours) .034 .930 .925 .060 .848 .865 .034 .855 .910 .027 .914 .954 .051 .773 .873 .064 .764 .842</figDesc><table><row><cell></cell><cell>ECSSD</cell><cell>PASCAL-S</cell><cell>DUTS-TE</cell><cell>HKU-IS</cell><cell>DUT-OMRON</cell><cell>THUR15K</cell></row><row><cell></cell><cell>1,000 images</cell><cell>850 images</cell><cell>5,019 images</cell><cell>4,447 images</cell><cell>5,168 images</cell><cell>6,232 images</cell></row><row><cell></cell><cell cols="6">MAE mF E ? 741 .829</cell></row><row><cell>PAGE [31]</cell><cell cols="6">.042 .906 .920 .077 .810 .841 .052 .777 .869 .037 .882 .940 .062 .736 .853 -</cell><cell>-</cell><cell>-</cell></row><row><cell>TDBU [30]</cell><cell cols="6">.041 .880 .922 .071 .779 .852 .048 .767 .879 .038 .878 .942 .061 .739 .854 -</cell><cell>-</cell><cell>-</cell></row><row><cell>SCRN [34]</cell><cell>.037 .918</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance on SOC<ref type="bibr" target="#b7">[8]</ref> of different attributes. Each row represents one attribute and we report the mean F -measure scores of LDF and state-of-the-art methods. The last row shows the whole performance on the SOC dataset. The best and the second best results are highlighted in red and blue respectively. To demonstrate the effectiveness of the proposed method, 14 state-of-the-art SOD methods are introduced to compare, including BMPM<ref type="bibr" target="#b37">[38]</ref>, DGRL<ref type="bibr" target="#b27">[28]</ref>, R 3 Net<ref type="bibr" target="#b6">[7]</ref>, RAS</figDesc><table><row><cell cols="7">Attr PiCA-R BMPM R 3 Net DGRL RAS AFNet BASNet PoolNet CPD-R EGNet-R SCRN Ours</cell></row><row><cell>AC 0.721</cell><cell>0.727 0.659 0.744 0.664 0.763</cell><cell>0.773</cell><cell>0.746</cell><cell>0.765</cell><cell>0.739</cell><cell>0.770 0.774</cell></row><row><cell>BO 0.706</cell><cell>0.802 0.637 0.847 0.654 0.824</cell><cell>0.780</cell><cell>0.677</cell><cell>0.821</cell><cell>0.743</cell><cell>0.743 0.803</cell></row><row><cell>CL 0.703</cell><cell>0.708 0.667 0.735 0.616 0.740</cell><cell>0.721</cell><cell>0.723</cell><cell>0.741</cell><cell>0.707</cell><cell>0.751 0.772</cell></row><row><cell>HO 0.727</cell><cell>0.738 0.683 0.773 0.682 0.778</cell><cell>0.769</cell><cell>0.768</cell><cell>0.766</cell><cell>0.747</cell><cell>0.775 0.807</cell></row><row><cell>MB 0.779</cell><cell>0.757 0.669 0.809 0.687 0.794</cell><cell>0.791</cell><cell>0.784</cell><cell>0.810</cell><cell>0.741</cell><cell>0.815 0.840</cell></row><row><cell>OC 0.692</cell><cell>0.711 0.625 0.724 0.608 0.730</cell><cell>0.721</cell><cell>0.713</cell><cell>0.741</cell><cell>0.699</cell><cell>0.732 0.756</cell></row><row><cell>OV 0.778</cell><cell>0.783 0.677 0.797 0.666 0.805</cell><cell>0.802</cell><cell>0.774</cell><cell>0.799</cell><cell>0.768</cell><cell>0.801 0.820</cell></row><row><cell>SC 0.678</cell><cell>0.702 0.626 0.725 0.645 0.711</cell><cell>0.713</cell><cell>0.723</cell><cell>0.726</cell><cell>0.708</cell><cell>0.738 0.774</cell></row><row><cell>SO 0.569</cell><cell>0.588 0.546 0.618 0.560 0.615</cell><cell>0.619</cell><cell>0.631</cell><cell>0.635</cell><cell>0.605</cell><cell>0.639 0.676</cell></row><row><cell>Avg 0.662</cell><cell>0.673 0.611 0.698 0.608 0.700</cell><cell>0.697</cell><cell>0.694</cell><cell>0.709</cell><cell>0.680</cell><cell>0.710 0.739</cell></row><row><cell cols="2">From this table, combinations including detail label perform</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">better than those including edge label, which demonstrates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">the effectiveness of detail label than edge label. In addi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">tion, combinations including body label perform better than</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">those including saliency label (Sal). It confirms that with-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">out the interference of edges, center pixels can learn better</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">feature representations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4.4. Comparison with State-of-the-arts Quantitative Comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Performance with different numbers of feature interaction. Number=0 means two branches have no feature interaction. Comparison on different combinations of supervision. Body, detail, saliency and edge maps are used, respectively. MAE mF E ? Body + Detail 0.064 0.764 0.842 0.034 0.855 0.910 Body + Edge 0.066 0.758 0.836 0.036 0.850 0.904 Sal + Detail 0.066 0.756 0.835 0.037 0.848 0.901 Sal + Edge 0.070 0.752 0.827 0.039 0.844 0.895</figDesc><table><row><cell>Number</cell><cell>THUR15K MAE mF</cell><cell cols="2">DUTS-TE E ? MAE mF</cell><cell>E ?</cell></row><row><cell>0</cell><cell cols="4">0.069 0.751 0.834 0.038 0.839 0.897</cell></row><row><cell>1</cell><cell cols="4">0.064 0.764 0.842 0.034 0.855 0.910</cell></row><row><cell>2</cell><cell cols="4">0.066 0.756 0.837 0.035 0.849 0.903</cell></row><row><cell>3</cell><cell cols="4">0.068 0.753 0.834 0.037 0.842 0.897</cell></row><row><cell>Label</cell><cell cols="2">THUR15K MAE mF E ?</cell><cell cols="2">DUTS-TE</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang Andn Huaizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="268" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting local and global patch rarities for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="page" from="236" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Salientshape: group saliency in image collections. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="443" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (15)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="196" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), September</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrently aggregating deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6943" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contour knowledge transfer for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (15)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for realtime salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>J?gersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selectivity or invariance: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3796" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (4)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4039" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An iterative and cooperative top-down and bottom-up inference network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5968" to="5977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A mutual learning method for salient object detection with intertwined multi-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reverse densely connected feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="530" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You He, and Gang Wang. A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
