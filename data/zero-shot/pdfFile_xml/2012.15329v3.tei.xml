<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Schumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Computational Linguistics Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
							<email>rschuman|riezler@cl.uni-heidelberg.de</email>
							<affiliation key="aff1">
								<orgName type="department">Computational Linguistics &amp;</orgName>
								<orgName type="institution">IWR Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Car-focused navigation services are based on turns and distances of named streets, whereas navigation instructions naturally used by humans are centered around physical objects called landmarks. We present a neural model that takes OpenStreetMap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions. Routes on the map are encoded in a location-and rotation-invariant graph representation that is decoded into natural language instructions. Our work is based on a novel dataset of 7,672 crowd-sourced instances that have been verified by human navigation in Street View. Our evaluation shows that the navigation instructions generated by our system have similar properties as human-generated instructions, and lead to successful human navigation in Street View. 1 www.openstreetmap.org 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current navigation services provided by the automotive industry or by Google Maps generate route instructions based on turns and distances of named streets. In contrast, humans naturally use an efficient mode of navigation based on visible and salient physical objects called landmarks. As shown by <ref type="bibr" target="#b20">Tom and Denis (2004)</ref>, route instructions based on landmarks are easier processed and memorized by humans. <ref type="bibr" target="#b14">May et al. (2003)</ref> recommend that in pedestrian navigation systems, "landmarks should be used as the primary means of providing directions". Another navigation scenario where landmarks are useful is if GPS tracking is poor or not available, and if information is inexact regarding distances (e.g., in human estimates) or street names (e.g., for users riding a bicycle). We present a neural model that takes a real-world map repre-sentation from OpenStreetMap 1 as input and generates navigation instructions that contain salient landmarks, learned directly from human natural language instructions.</p><p>In our framework, routes on the map are learned by discretizing the street layout, connecting street segments with adjacent points of interest, thus encoding visibility of landmarks, and encoding the route and surrounding landmarks in a location-and rotation-invariant graph. Based on crowd-sourced natural language instructions for such map representations, a graph-to-text mapping is learned that decodes graph representations into natural language route instructions that contain salient landmarks. Our work is accompanied by a dataset of 7,672 instances of routes in OpenStreetMap and corresponding crowd-sourced natural language instructions. The navigation instructions were generated by workers on the basis of maps including all points of interest, but no street names. They were verified by different workers who followed the navigation instructions on Google Street View 2 .</p><p>Experimental results on randomly sampled test routes show that our graph-to-text model produces landmarks with the same frequency found in human reference instructions. Furthermore, the timenormalized success rate of human workers finding the correct goal location on Street View is 0.664. Since these routes can have a partial overlap with routes in the training set, we further performed an evaluation on completely unseen routes. The rate of produced landmarks drops slightly compared to human references, and the time-normalized success rate also drops slightly to 0.629. While there is still room for improvement, our results showcase a promising direction of research, with a wide potential of applications in various existing map <ref type="figure">Figure 1</ref>: The data collection is split into two tasks. In the navigation instructions task (top) annotators see a rendered map and write instructions to follow the route. The navigation run task (bottom) is used to validate navigation instructions. A different annotator tries to find the goal location in Street View.</p><p>applications and navigation systems.</p><p>The main contributions of this paper are: ? We collect and publish a large scale dataset of natural language landmark navigation instructions that are validated by human navigation runs in Street View. ? We present a method to represent geospatial routes as a graph and propose an appropriate graph-to-text architecture that learns to generate navigation instructions from real-world data.</p><p>2 Related Work and Datasets <ref type="bibr" target="#b15">Mirowski et al. (2018)</ref> published a subset of Street View covering parts of New York City and Pittsburgh. Street View is a navigable environment that is build from connected real-world 360 ? panoramas. This data is used by <ref type="bibr" target="#b8">Hermann et al. (2020)</ref> to train a visual agent to follow turn-by-turn instructions generated by Google Maps API. <ref type="bibr" target="#b3">Chen et al. (2019)</ref> published a Street View dataset 3 with more recent and higher resolution panorama images that covers the lower half of Manhattan. They further introduce the Touchdown task that has the goal to navigate 3 www.streetlearn.cc Street View in order to find a hidden teddy bear. The data for that task is obtained from annotation workers that follow a predefined route in Street View and write down navigation instructions along the way. A central difference between Touchdown and our dataset is the annotation modality: Touchdown annotators use panorama images along the route, while our instruction writers only see the rendered route on a map. See Section 4.3 for a more detailed discussion.</p><p>Our work puts the task of natural language navigation upside down by learning to generate humanlike navigation instructions from real-world map data instead of training an agent to follow human generated instructions. Prior work in this area has used rule-based systems to identify landmarks <ref type="bibr" target="#b17">(Rousell and Zipf, 2017)</ref> or to generate landmarkbased navigation instructions <ref type="bibr" target="#b6">(Dr?ger and Koller, 2012;</ref><ref type="bibr" target="#b2">Cercas Curry et al., 2015)</ref>. Despite having all points of interest on the map available, our approach learns to verbalize only those points of interest that have been deemed salient by inclusion in a human navigation instruction. Previous approaches that learn navigation instructions from data have been confined to simplified grid-based representations of maps for restricted indoor environments <ref type="bibr" target="#b4">(Daniele et al., 2017)</ref>. <ref type="bibr" target="#b24">de Vries et al. (2018)</ref> tackles the problem in a more sophisticated outdoor environment but the model fails to verbalize useful instructions when conditioned on more than one possible landmark. Other work generates navigation instructions from indoor panoramas along a path but provides no explicit evaluation like human navigation success. They rather use the instructions to augment the training routes for a vision and language navigation agent <ref type="bibr" target="#b7">(Fried et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task</head><p>The task addressed in our work is that of automatically generating Natural Language Landmark Navigation Instructions (NLLNI) from realworld open-source geographical data from Open-StreetMap. The instructions are generated a priori <ref type="bibr" target="#b10">(Janarthanam et al., 2012)</ref> for the whole route. Training data for NLLNI was generated by human crowdsourcing workers who were given a route on an OpenStreetMap rendering of lower Manhattan, with the goal of producing a succinct natural language instruction that does not use street names or exact distances, but rather is based on landmarks. Landmarks had to be visible on the map and included, e.g., churches, cinemas, banks, shops, and public amenities such as parks or parking lots. Each generated navigation instruction was validated by another human crowdsourcing worker who had to reach the goal location by following the instruction on Google Street View. NLLNI outputs are distinctively different from navigation instructions produced by OpenRoute-Service, Google Maps, or car navigation systems. While these systems rely on stable GPS signals such that the current location along a grid of streets can be tracked exactly, we aim at use cases where GPS tracking is not available, and knowledge of distances or street names is inexact, for example, pedestrians, cyclists, or users of public transportation. The mode of NLLNI is modeled after human navigation instructions that are naturally based on a small number of distinctive and visible landmarks in order to be memorizable while still being informative enough to reach the goal. A further advantage of NLLNI is that they are based on map inputs which are more widely available and less time dependent than Street View images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Collection</head><p>Because there is no large scale dataset for NLLNI that is generated from map information only, we collect data via crowdsourcing. The annotator is shown a route on the map and writes navigation instructions based on that information <ref type="figure">(Figure 1, top)</ref>. We take the approach of <ref type="bibr" target="#b3">Chen et al. (2019)</ref> and determine correctness of navigation instructions by showing them to other annotators that try to reach the goal location in Street View <ref type="figure">(Figure 1, bottom)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Resources and Preparation</head><p>We use the static Street View dataset provided by <ref type="bibr" target="#b3">Chen et al. (2019)</ref>. This allows us to make the experiments in this work replicable. Because the panorama pictures were taken at the end of 2017, we export an OpenStreetMap extract of Manhattan from that time. OpenStreetMap (OSM) is an open source collection of geodata that can be used to render maps of the world. It features detailed street layouts and annotations for points of interest (POI) like amenities, infrastructure or land use 4 . We discretize the street layout by creating a node every ten meters along the roads. The resulting structure is further referenced to as the OSM graph with nodes consisting of street segments. Based on that graph, we sample routes of length between 35 and 45 nodes. A route is the shortest path between its start and end node. It includes a minimum of three intersections (i.e., a node with more than two edges) and ends in proximity to a POI. We further assure that it is possible to follow the route in Street View by verifying that a corresponding subgraph exists in the Street View graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Crowdsourcing</head><p>We use Amazon Mechanical Turk (AMT) 5 to acquire annotators. Before working on the actual tasks, workers were required to pass a tutorial and qualification test. The tutorial introduces the tasks, teaches basic mechanics of Street View and explains meaning of map icons. A feature of AMT and additional IP address 6 lookup ensures that annotators are located in the United States. This increases the probability of working with native English speakers and people familiar with US street environments. We paid $0.35 per navigation instructions task and $0.20 for the navigation run   <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> taken from the latter. c is the number of instructions out of the 25 which contain the phenomenon at least once. ? is the mean number of times each phenomenon occurs.</p><p>task. Furthermore, we paid a bonus of $0.15 for successfully reaching the goal location and $0.25 for validated navigation instructions. The amounts were chosen on the basis of $10/hour. The annotation procedure involved two phases. First, an annotator wrote navigation instructions for a given route. Afterwards, a different annotator used the instructions to navigate to the goal location. If one of two annotators did so successfully, the navigation instructions were considered valid.</p><p>Navigation Instructions Task As shown in <ref type="figure">Figure 1 (top)</ref>, the annotator sees a route on a map which is rendered without street names. Workers were told to write navigation instructions as if "a tourist is asking for directions in a neighborhood you are familiar with" and to "mention landmarks to support orientation". The navigation instructions were written in a text box below the map which is limited to 330 characters. <ref type="figure">Figure 1</ref> (bottom) shows the Street View interface with navigation instructions faded-in at the bottom. It is possible to look around 360 ? and movement is controlled by the white arrows. In addition there is a button on the bottom left to backtrack which proved to be very helpful. The initial position is the start of the route facing in the correct direction. The annotators finish the navigation run with the bottom right button either when they think the goal location is reached or if they are lost. The task is successful if the annotator stops the run within a 25 meter radius around the goal location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Navigation Run Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dataset</head><p>The data collection resulted in 7,672 navigation instructions that were manually validated in Street View. For additional 1,059 instructions, the validation failed, which amounts to a validation rate of 88%. Of the validated instructions, 1,033 required a second try in the navigation run task. On average, instructions are 257 characters long, with a minimum length of 110, and a maximum of 330 characters. We release the segmented OSM graph, the routes in that graph paired with the collected navigation instructions, and the data split used in  <ref type="figure" target="#fig_1">Figure 3</ref>. The framed middle part is magnified for readability. Some nodes are left out for sake of clear visualization. Also, node colors are for visualization only and not encoded in the graph. Green nodes are part of the route. Blue nodes are neighboring street segments. Orange nodes belong to OSM points of interest. Angles are relative to route direction and start clockwise at 0 ? which is facing forward.  <ref type="table">Table 1</ref> gives a comparison of different datasets with natural language landmark navigation instructions. Our dataset is the only one that uses only map information to generate navigation instructions. The advantage of relying solely on map data is the global availability and longevity of the encoded features. In contrast, navigation instructions written from Street View include temporary features like construction utilities, street advertisements, or passing vehicles. <ref type="table" target="#tab_1">Table 2</ref> shows a qualitative linguistic analysis of the navigation instructions of different datasets. In general, navigation instructions are driven by giving directions in imperative formulation while referencing to entities along the route. In contrast to the Touchdown task where including store names was prohibited, the entities in our instructions are often referenced to by their name. Although the instruction writers in our setting did not see the route in first person perspective, objects are vastly referenced to in egocentric manner (egocentric with respect to the navigating agent). This is because the annotator knows the starting direction and can infer the facing direction for the rest of the route. Because the initial facing direction in Touchdown is random, the first part of their instructions is about rotating the agent. This explains the higher number of occurrences of the state verification phenomenon. In our dataset, state verification is usually used to ensure the correct stopping position. The different setting of data collection is also reflected by the temporal condition phenomenon. Annotators of Touchdown write down instructions while navigating Street View and thus experience the temporal component first hand, while our annotators have a time independent look at the route.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Method</head><p>The underlying OSM geodata of the rendered map is an XML tree of nodes located in the latitudelongitude coordinate system. The nodes are composed into ways and polygons 8 . These elements in connection with their annotations are used to render the visual map. In the next subsection we propose our approach to represent a route and its surrounding map features as a graph that includes all necessary information for generating landmark navigation instructions. The second subsection describes the neural graph-to-text architecture that is trained to learn inductive representations of the individual route graphs and to decode navigation instructions from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Map-to-Graph Representation</head><p>The basis of the graph for a single route is the OSM subgraph (Section 4.1) that includes the ac-tual route nodes. Further, neighboring street segment nodes are added. This is depicted in <ref type="figure" target="#fig_1">Figure 3</ref> as green and blue circles, respectively. In order to decide on the visibility of the POIs, we employ a technique similar to that of <ref type="bibr" target="#b17">Rousell and Zipf (2017)</ref>. For each street segment, the POIs in a radius of 30 meters are identified. If a line drawn between the street segment and the POI is not interrupted by a building polygon, the POI is considered visible from that particular street segment. If the POI itself is (inside) a polygon, then the line is drawn to the closest point on the POI polygon. The orange circles in <ref type="figure" target="#fig_1">Figure 3</ref> show the results of the visibility check and how they naturally fit into the graph structure. Each point of interest in OSM has one or more tags in the form of key and value pairs. They store properties like type or name. Note that we only determine the geometric visibility of the POIs and do not incorporate any hand-crafted salience scores as to what would be a good landmark. Instead, saliency of a landmark is implicitly learned from natural language verbalization of the POI in the human-generated instruction.</p><p>An example graph representation of the route in <ref type="figure" target="#fig_1">Figure 3</ref> is given in <ref type="figure" target="#fig_0">Figure 2</ref>. Formally, a route representation is a directed graph G = (V, E), where V denotes the set of nodes and E the set of edges. A node v consists of a node type v t and a node token v w . There are V t node types and V w node tokens. Street segments are of type &lt;street&gt;. A point of interest has the node type &lt;poi&gt;. An OSM tag key has the node type &lt;tag key&gt; and an OSM tag value has the node type &lt;tag value&gt;. The node token further specifies nodes in the graph. Street segments that belong to the route have a node token &lt;P&gt; according to their sequential position P. The last route segment has the special token &lt;last&gt;.</p><p>Other street segment nodes have the &lt;neighbor&gt; token. The actual key and value literals of an OSM tag are the node tokens of the respective node. The OSM name tag is split into multiple nodes with type &lt;k name N&gt; where N is the word position and the node token is the word at that position.</p><p>All adjacent street segment nodes are connected with an edge in both directions. If a POI is visible from a particular street segment, there is an edge from the corresponding POI node to that street segment node. Each POI node is connected with their tag key nodes. A tag value node is connected to its corresponding tag key node. The name tag nodes of the same POI are connected with each other. Some edges have a geometric interpretation. This is true for edges connecting a street segment with either a POI or with another street segment. These edges (u, v) ? E A , E A ? E have a label attached. The label ang <ref type="bibr">(u, v)</ref> is the binned angle between the nodes relative to route direction. The continuous angle [0 ? , 360 ? ) is assigned to one of 12 bins. Each bin covers 30 ? with the first bin starting at 345 ? . The geometric distance between nodes is not modeled explicitly because street segments are equidistant and POI visibility is determined with a maximum distance. The proposed representation of a route and its surroundings as a directed graph with partially geometric edges is location-and rotationinvariant, which greatly benefits generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph-to-Text Architecture</head><p>By representing a route as a graph, we can frame the generation of NLLNI from maps as a graphto-text problem. The encoder learns a neural representation of the input graph and the sequence decoder generates the corresponding text. The architecture follows the Transformer <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> but uses graph attentional layers <ref type="bibr" target="#b23">(Veli?kovi? et al., 2018)</ref> in the encoder. Graph attention injects the graph structure by masking (multi-head) self-attention to only attend to nodes that are firstorder neighbors in the input graph. The geometric relations between some nodes are treated as edge labels which are modeled by distinct feature transformation matrices during node aggregation <ref type="bibr" target="#b18">(Schlichtkrull et al., 2018)</ref>.</p><p>The input to a layer of the encoder is a set of node representations, x = {x 1 , x 2 , . . . , x N }, x i ? R dm , where N is the number of nodes and d m is the model size. Each layer l : R dm ? R dm takes x and produces new node representations x . The input to the first layer is constructed from the concatenation of type and token embedding:</p><formula xml:id="formula_0">x i = ReLU (W F [E T v t i ||E W v w i ]) where W F ? R 2dm?dm</formula><p>is a weight matrix, E T ? R dm and E W ? R dm are embedding matrices for node types and node tokens, respectively.</p><p>The output of a single graph attention head is the weighted sum of neighboring node representations:</p><formula xml:id="formula_1">x i = j|(v j ,v i )?E ? ij (W U r(i,j) x j )<label>(1)</label></formula><p>The weight coefficient is computed as ? ij = softmax j (e ij ) =  measures the compatibility of two node representations:</p><formula xml:id="formula_2">e ij = LeakyReLU (a T [W V x i ||W U r(i,j) x j ]) (2) where a ? R 2d h , W V ? R dm?d h , d h = d m /h</formula><p>is the attention head dimension and h is the number of heads. In the case of a geometric relation between nodes, the weight matrix W U r(i,j) ? R dm?d h is selected according to the angle label between the nodes: r(i, j) = ang(u i , u j ), otherwise r(i, j) = unlabeled. The output of each head is concatenated and after a skip connection forwarded to the next encoder layer. The encoder layer is applied L times and the final node representations x * are used in the decoder context attention mechanism. Thus, no modification of the Transformer decoder is necessary and L decoder layers are used. Further, the decoder can copy node tokens from the input into the output sequence <ref type="bibr" target="#b19">(See et al., 2017)</ref>.</p><p>The described architecture is able to model all aspects of the input graph. Graph attention models directed edges. Edge labels model the geometric relation between nodes. Heterogeneous nodes are represented by their type embedding and token embedding. The sequentiality of the route is encoded by tokens (&lt;1&gt;, &lt;2&gt;, ...) of the respective nodes. This is analogous to absolute position embeddings which provide word order information for text encoding <ref type="bibr" target="#b22">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines</head><p>We consider two baselines. A rule based system that uses a single heuristic to construct instructions by stringing together all POIs and intersections along the route, and following each intersection by the turning direction. Similar, POIs are followed by 'left' or 'right' depending on which side  of the street they appear. The end of the route is signaled by the 'stop' token. The second baseline is a seq2seq (sequence-to-sequence) model that is trained on pairs of rule based navigation instructions and crowdsourced instructions. The seq2seq model follows the Transformer architecture <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> with copy mechanism and is trained with the same hyperparameters as the graph-to-text model. Examples are given in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setup</head><p>We construct a graph for each route as described above. On average there are 144 nodes in a graph and 3.4 edges per node. There are 8 different node types and a vocabulary of 3,791 node tokens. The hyperparameters for the graph-to-text architecture are set as follows: The embedding and hidden size is set to 256. We use 6 encoder and decoder layers with 8 attention heads. Cross entropy loss is optimized by Adam (Kingma and Ba, 2015) with a learning rate of 0.5 and batch size of 12. The embedding matrix for node tokens and output tokens is shared. Additionally we experiment with pretraining the graph-to-text model with above mentioned rule based instructions as target. This teaches the model sequentiality of route nodes and basic interpretation of the angle labels. We generate 20k instances for pretraining and further fine tune on the human generated instances. Both models and the seq2seq baseline are trained on 5,667 instances of our dataset. The best weights for each model are selected by token accuracy based early stopping on the 605 development instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation Metrics</head><p>BLEU is calculated with SacreBLEU <ref type="bibr" target="#b16">(Post, 2018)</ref> on lower-cased and tokenized text.</p><p>Length is the average length in number of tokens.</p><p>Landmarks is the number of landmark occur-  SDTW is success weighted by normalized Dynamic Time Warping <ref type="bibr" target="#b9">(Ilharco et al., 2019)</ref>. Distance between two nodes is defined as meters along the shortest path between the two nodes and threshold distance is 25 meters. SR is the first try success rate in the navigation run task. Success is achieved if the human navigator stops within a radius of 25 meters around the goal. SNT is success weighted by navigation time:</p><formula xml:id="formula_3">1 N N i=1 S it i t i ,</formula><p>where S i is a binary success indicator that is 1 if the annotator stops within a 25 meter radius around the goal. t i is the time until the navigation run is finished. We empirically estimate the expected navigation timet i as 1.3 seconds 9 per node in the route. This estimation ranges from 45.5 seconds for routes with 35 nodes to 58.5 seconds for routes with 45 nodes. SNT is inspired by SPL <ref type="bibr" target="#b0">(Anderson et al., 2018a</ref>) but considers trajectory time instead of trajectory length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Results and Analysis</head><p>Results of our experimental evaluation are shown in <ref type="table" target="#tab_3">Table 3</ref> and 4. We evaluate on unseen data, i.e., routes without any overlap with routes in the training set, and on partially seen data, i.e., routes 9 Average over all successful navigation runs in the dataset. randomly sampled from the training area with partial overlaps. 10 For the baseline models we perform the human evaluation on a 200 instances subset of the full 700 instances test set.</p><p>On the partially seen test set with 200 instances, our proposed graph-to-text models outperform the baseline models in terms of the success based metrics. In the unseen setup, the rule based baseline achieves a better success rate, but falls short when success is weighted by navigation time. This result shows that the instructions generated by the rule based system are exact by including all possible landmarks, but obviously do not resemble natural language and high evaluation time suggests that they are hard to read. Despite moderate BLEU scores and reasonable amount of produced landmarks, the seq2seq baseline fails to generate useful navigation instructions. The pretrained graph-totext model performs better than its plain counterpart in the unseen setup. It produces more correct landmarks and higher success rates. In the extended evaluation the pretrained graph-to-text model is compared with the reference on 700 instances in each test set. Under the central evaluation metric of success normalized by time (SNT), our model reaches .664 and .629 on partially seen and unseen test data, respectively.</p><p>An example output for each system together with the input map is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. The rule based instruction is complete, but ignores saliency of landmarks and is hard to read. The seq2seq baseline generates a navigation instruction that sounds human-like and also includes salient landmarks found on the map. However, the directions are incorrect in this example. The graph-to-text based models get the directions right and produce fluent natural language sentences. They include landmarks at the correct sequential position. A further qualitative evaluation of instructions generated by the graph-to-text models is given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a dataset and suitable graph-to-text architecture to generate landmark navigation instructions in natural language from OpenStreetMap geographical data. Our neural model includes novel aspects such as a graphical representation of a route using angle labels. Our dataset consists of a few thousand navigation instructions that are verified for successful human navigation. The dataset is large enough to train a neural model to produce navigation instructions that are very similar in several aspects to human-generated instructions on partially seen test data. However, performance naturally drops on unseen data including new types of landmarks in new combinations. All 700 routes that are exclusively in the green rectangle are in the unseen test set. All 605 routes that cross the green border are in the development set. None of those development set routes extend further than the blue rectangle. The training set consists of routes within the red rectangle but outside of the green rectangle. The partially seen test set consists of 700 randomly sampled routes from the training set (and removed from the training set). Partially seen means that subsequences of those routes can be present in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation Navigation Success Rate Analysis</head><p>We analyze the navigation success rate with respect to properties of the corresponding routes. <ref type="figure" target="#fig_4">Figure 6</ref> 35-36 <ref type="formula">(40)</ref>   shows that the length of the route has little influence on the navigation success rate on the partially seen test set. On the unseen data there is tendency in favor of shorter routes for the g2t+pretrain model. The reference instructions do not show such bias. <ref type="figure">Figure 8</ref> shows navigation success with respect to number of turns in a route which is another complexity indicator. The success rate drops with an increasing number of turns for all systems but not for the reference instructions. The analysis reveals that performance of our model drops with increasing route complexity while it is stable for reference instructions. The rule based system appears to be more stable with increasing number of turns in comparison to the learned models. <ref type="table" target="#tab_10">Table 5</ref> and 6 presents a scoring of types of landmarks produced by our pretrained model. A comparison of landmarks produced in human-generated reference instructions to those produced in modelgenerated instructions shows a large overlap on partially seen data, and ranking is similar to handcrafted salient scores used in work in geoinformat-  <ref type="figure">Figure 7</ref>: Navigation success rate in respect of number of intersections in a route. Each node in the route with more than two neighbors is counted as an intersection.   ics <ref type="bibr" target="#b17">(Rousell and Zipf, 2017)</ref>. The distribution of landmarks in the unseen test data is different from the partially seen data. To some extent, the model is able to adapt to the unseen environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Landmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Annotation Instructions</head><p>The AMT workers got the following instructions for the writing task: The goal of this task is to write navigation instructions for a given route. Imagine a tourist is asking for directions in a neighborhood you are familiar with and try to mention useful landmarks to support orientation. Another annotator will later read your instructions in order to find the goal location in StreetView (Navigation Run Task). If the other annotator successfully navigates to the goal location, your instruction is validated.  Go to the light and turn right. Go through the following light, Restaurant on the corner. Pass a bus stop on the right, and go through the light. At the following light, bus stop on the corner, turn left. Go through the following light, and stop after a step or two. Saint Mary's church will be on the right. graph2text: Turn right at the first set of lights. Go through the next set of lights and then turn left at the following lights. Pass Second Time Around half way down the block. Stop at Saint's around Saint's church on the right. g2t+pretrain: Go to the light and turn right. Go straight through 1 light and at the following light, there should be a bus stop on the far left corner. Turn left and go about 1/2 to the next light, stopping in front of Saint Patrick's church on the right and graveyard Memorial's on the right.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Graph representation of the route in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Route rendered on the map with street segments and landmark visibility. our experiments 7 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Route from partially seen test set paired with instructions generated by different systems. rences per instance. Occurrences are identified by token overlap between navigation text and tag values of POIs along the route. E.g., landmarks in the instructions in Figure 1 are: Dunkin Donuts, Bubble Tea &amp; Crepes, Chipotle, Broadway Hotel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Dataset splits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Navigation success rate in respect of route length. Length is measured in number of nodes in a route.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Head straight past the playground and turn left at the light. Turn right at the next light with Fantastic Cafe on the right. Continue down the very long street and stop at the next light with turntable 5060 on the near left corner. rule based: parking left pitch left playground left pitch left bus stop right light left Citibank left light right Fantastic Cafe left Church of the right Dorian Gray Tap left Perbacco left Matcha Cafe Wabi left B4 left stop seq2seq: Go straight to the light and turn left. Go straight to the first light and turn left. Go straight to the first light and stop a couple of steps before the next light. You should see Perbacco on your left. graph2text: Head to the light and turn left. At the next light with Fantastic cafe on your near right corner, turn right. Go to the next light and stop in the middle of the intersection. B4 Gray Tap should be on your left. g2t+pretrain: Head to the light and turn left. At the following light with Fantastic cafe on the far left corner, turn right. Go to the next light with 4th street Deli on the near left corner and stop in the middle of the intersection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Route from partially seen test set with successful navigation for g2t+pretrain. reference: Turn right at the light right ahead of you. Go through the following light. The next light is a double intersection. Turn left at the first of the lights, and go through the second one. Pass a graveyard on the right and stop at the end of it, right as the church starts. It's about 2/3 down the block.rule based: Garden left light right Chef Restaurant Supplies right light straight bus stop left light left light straight grave yard right stop Saint Patrick's right seq2seq:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Route from unseen test set with successful navigation for g2t+pretrain. reference: Go to the light and turn right. Go through the following light with Optical 88. The next light will have Da Gennaro on the far right corner, go through the light. Go through the following light as well, with a school on the corner. Turn right at the following light. Take the first left possible and stop after a few steps.rule based: Hong Kong Supermarket left light right Sweet Moment left light straight 12 Corners right light straight Da Gennaro right Public School 130 right light straight parking left May Wah Vegetarian right Hair Lounge right light right intersection left stop seq2seq: Go straight and take a right at the intersection where Hong Kong supermarket is. Go through the next three intersections and at the fourth one take a right and stop at Hair Lounge. graph2text: Go to the light and turn right. Go through the following light, Optical 88 on the corner. Go through the following light as well, Da Gennaro on the corner. At the following light, Hair Lounge on the corner, turn right. Take a step and stop. g2t+pretrain: Head to the light and turn right. Go past the next 2 lights with Da Gennaro on the right corner. At the 3rd light with May Wah Vegetarian on the far right corner, turn left. Take one step and stop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Route from partially seen test set with unsuccessful navigation for g2t+pretrain. reference: Go straight to the light and turn right. Go through the next 3 lights. At the 4th, there is a Chase and Nike store on the corners. Go through the intersection and stop on the other side at your destination.rule based: Public School 21 left light right Lombardi's Pizza left park left Rice to Riches right drinking water left playground left Ta?m right light straight Tartinery left Duane Reade left light straight restaurant left Premium Laces right Starbucks left light straight MUJI at MoMA right Balthazar left SoHo left light straight Niketown right stop Chase left Chase left shelter left seq2seq: Head to the light and turn right. Go past the next 3 lights with Starbucks on the left corner. At the 4th light with Chase bank on the near left corner, cross the street and stop. graph2text: Head to the light and turn right. Go past the next 2 lights with Starbucks on the far left corner. At the 3rd light with Chase on the far left corner, cross the street and stop. Chase should be on your left. g2t+pretrain: Head to the light and turn right. Go past the next 3 lights with Starbucks on the left corner. At the 4th light with Chase on the near left corner, cross the street. Take one step and stop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Route from unseen test set with unsuccessful navigation for g2t+pretrain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Type of information the annotator uses to write the navigation instructions. #Nodes: Number of nodes in the discretized environment. Avg. Length: Average number of nodes per route. Vocabulary: Number of unique tokens in the instructions. Avg. Tokens: Number of tokens per route instruction. ... at the end of the park on your right... ... head down the block and go through the double lights ...</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Instructions Environment Data Source</cell><cell cols="3">#Nodes Avg. Length Vocabulary Avg. Tokens</cell></row><row><cell>Talk the Walk</cell><cell>786 gridworld</cell><cell cols="3">3D rendering</cell><cell>100</cell><cell>6.8</cell><cell>587</cell><cell>34.5</cell></row><row><cell>Room-to-Room</cell><cell>21,567 indoor</cell><cell cols="3">panoramas</cell><cell>10,800</cell><cell>6.0</cell><cell>3,156</cell><cell>29.0</cell></row><row><cell>Touchdown</cell><cell>9,326 outdoor</cell><cell cols="3">panoramas</cell><cell>29,641</cell><cell>35.2</cell><cell>4,999</cell><cell>89.6</cell></row><row><cell>Talk2Nav</cell><cell>10,714 outdoor</cell><cell cols="4">panoramas and map 21,233</cell><cell>40.0</cell><cell>5,240</cell><cell>68.8</cell></row><row><cell>Room-X-Room</cell><cell>126,069 indoor</cell><cell cols="3">panoramas</cell><cell>10,800</cell><cell>7.0</cell><cell>388K</cell><cell>78.0</cell></row><row><cell>map2seq</cell><cell>7,672 outdoor</cell><cell>map</cell><cell></cell><cell></cell><cell>29,641</cell><cell>40.0</cell><cell>3,826</cell><cell>55.1</cell></row><row><cell cols="8">Table 1: Overview of natural language navigation instructions datasets. The instructions in our dataset rely solely</cell></row><row><cell cols="8">on information present in OpenStreetMap. Dataset: Talk the Walk (MacMahon et al., 2006); Room-to-Room (An-</cell></row><row><cell cols="8">derson et al., 2018b); Touchdown (Chen et al., 2019); Talk2Nav (Vasudevan et al., 2020); Room-X-Room (Ku</cell></row><row><cell cols="8">et al., 2020); map2seq (this work). #Instructions: Number of instructions in the dataset. Environment: Type of</cell></row><row><cell cols="5">navigation environment. Data Source: Phenomenon R-to-R Touchdown map2seq c ? c ? c ?</cell><cell></cell><cell>Example</cell></row><row><cell cols="2">Reference to unique entity 25 3.7 25</cell><cell>9.2</cell><cell cols="5">25 6.3 ... turn right where Dough Boys is on the corner ...</cell></row><row><cell>Coreference</cell><cell>8 0.5 15</cell><cell>1.1</cell><cell>8</cell><cell cols="4">0.5 ... is a bar, Landmark tavern, stop outside of it ...</cell></row><row><cell>Comparison</cell><cell>1 0.0 3</cell><cell>0.1</cell><cell>0</cell><cell cols="4">0.0 ..</cell></row><row><cell>Sequencing</cell><cell>4 0.2 21</cell><cell>1.6</cell><cell cols="5">24 1.8 ... continue straight at the next intersection ...</cell></row><row><cell>Count</cell><cell>4 0.2 9</cell><cell>0.4</cell><cell cols="4">11 0.6 ... go through the next two lights ...</cell></row><row><cell cols="5">Allocentric spatial relation 5 0.2 17 0.5 .Egocentric spatial relation 20 1.2 23 1.2 9 3.6 25 3.2 Imperative 25 4.0 25 5.2 25 5.3 Direction 22 2.8 24 3.7 25 3.Temporal condition 7 0.4 21 1.9 7 0.State verification 2 0.1 18 1.5 12 0.</cell><cell></cell><cell></cell></row></table><note>.. there are two lefts, take the one that is not sharp .... go through the next light with Citibank at the corner. ...5 ... head straight to the light and make a right ...3 ... go straight until you come to the end of a garden area ...6 ... you should see bike rentals on your right ...</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Linguistic analysis of 25 randomly sampled navigation instructions. Numbers for Room-to-Room (An- derson et al., 2018b) and Touchdown</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of navigation instructions produced by models and human reference on partially seen test routes. Evaluation metrics are explained in Section 6.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of navigation instructions produced by models and human reference on unseen test routes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>At the light with Fridays on the corner, turn right. Continue down the long street to the next light with Nine West on the right corner, then turn left. Go to the next light with Brooks Brothers on the right corner, then turn right and stop.rule based: Starbucks Coffee left subway entrance right Best Buy Mobile left Yankees right bus stop left bus stop left light right The Michelangelo left TGI Fridays left Pizza Hut left Bobby Van 's left park right Men 's Wearhouse left fountain left fountain left subway entrance left light left Nine West right Rockefeller Center left subway entrance right Brooks Brothers right light right stop seq2seq: Go straight to the light and make a left. Go straight to the next light and make a left. Go straight to the light and make a right. Stop one step after turning with Brooks Brothers to your right.graph2text: Walk to the light with TGI Fridays on the corner and turn right. Walk down the long block to the next light with Nine West on the left corner, then turn left. Walk to the next light with Brooks Brothers on the far right corner, then turn right. g2t+pretrain: Turn right at the first set of lights with TGI Fridays on the left corner. Pass a park on the right and turn left at the lights. Pass the fountain on the right and turn right at the lights. Take two steps and stop. Brooks Brothers is on the right corner.</figDesc><table /><note>reference:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Frequency of OSM tags of landmark occurrences in the instructions for the partially seen test set, normalized by the number of occurrences in the input graph.</figDesc><table><row><cell></cell><cell></cell><cell>Reference</cell><cell></cell><cell></cell><cell>Model</cell><cell></cell></row><row><cell>Top</cell><cell></cell><cell>OSM tag</cell><cell>Score</cell><cell></cell><cell>OSM tag</cell><cell>Score</cell></row><row><cell>1</cell><cell cols="2">amenity: cinema</cell><cell>0.58</cell><cell cols="2">cuisine: juice</cell><cell>0.64</cell></row><row><cell>2</cell><cell>shop:</cell><cell>wine</cell><cell>0.53</cell><cell cols="2">amenity: pharmacy</cell><cell>0.55</cell></row><row><cell>3</cell><cell>shop:</cell><cell>computer</cell><cell>0.53</cell><cell>shop:</cell><cell cols="2">convenience 0.50</cell></row><row><cell>4</cell><cell cols="2">amenity: pharmacy</cell><cell>0.51</cell><cell cols="2">amenity: cinema</cell><cell>0.46</cell></row><row><cell>5</cell><cell cols="3">cuisine: coffee shop 0.49</cell><cell cols="3">cuisine: coffee shop 0.46</cell></row><row><cell>6</cell><cell cols="2">tourism: hotel</cell><cell>0.44</cell><cell>shop:</cell><cell>computer</cell><cell>0.45</cell></row><row><cell>7</cell><cell>shop:</cell><cell cols="2">convenience 0.42</cell><cell cols="2">tourism: hotel</cell><cell>0.41</cell></row><row><cell>8</cell><cell>shop:</cell><cell>houseware</cell><cell>0.31</cell><cell>shop:</cell><cell>pet</cell><cell>0.39</cell></row><row><cell>9</cell><cell>shop:</cell><cell cols="2">supermarket 0.31</cell><cell>shop:</cell><cell>beauty</cell><cell>0.38</cell></row><row><cell>10</cell><cell cols="2">amenity: bank</cell><cell>0.28</cell><cell>shop:</cell><cell>wine</cell><cell>0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Frequency of OSM tags of landmark occurrences in the instructions for the unseen test set, normalized by the number of occurrences in the input graph.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">openstreetmap.org/wiki/Map_Features 5 www.mturk.com 6 IP addresses were not saved and are not part of the dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">www.cl.uni-heidelberg.de/ statnlpgroup/map2seq/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">www.openstreetmap.org/wiki/Elements</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">The data split is shown in the Appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Christian Buck and Massimiliano Ciaramita for initial fruitful discussions about this work. The research reported in this paper was supported by a Google Focused Research Award on "Learning to Negotiate Answers in Multi-Pass Semantic Parsing".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Roshan Zamir</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<idno>abs/1807.06757</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>On evaluation of embodied navigation agents</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating and evaluating landmarkbased navigation instructions in virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda Cercas</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Gkatzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4715</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Workshop on Natural Language Generation (ENLG)</title>
		<meeting>the 15th European Workshop on Natural Language Generation (ENLG)<address><addrLine>Brighton, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Touchdown: Natural language navigation and spatial reasoning in visual street environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12530" to="12539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Navigational instruction generation as inverse reinforcement learning with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">F</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th ACM/IEEE International Conference on Human-Robot Interaction (HRI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generation of landmark-based navigation instructions from open-source data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="757" to="766" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speaker-follower models for vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3314" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to follow directions in street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6849</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11773" to="11781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective and general evaluation for instruction conditioned navigation using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Visually Grounded Interaction and Language Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A web-based evaluation framework for spatial instruction-giving systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Janarthanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingkun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Walk the talk: Connecting language, knowledge, and action in route instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1475" to="1482" />
		</imprint>
	</monogr>
	<note>AAAI&apos;06</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pedestrian navigation aids: information requirements and design implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">H</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><forename type="middle">J</forename><surname>Tarkiainen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00779-003-0248-5</idno>
	</analytic>
	<monogr>
		<title level="m">Personal and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to navigate in cities without a map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2419" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards a landmark-based pedestrian navigation service using osm data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Rousell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zipf</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijgi6030064</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS International Journal of Geo-Information</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language and spatial cognition: comparing the roles of landmarks and street names in route instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariane</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Denis</surname></persName>
		</author>
		<idno type="DOI">10.1002/acp.1045</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1213" to="1230" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Talk2nav: Long-range vision-andlanguage navigation with dual attention and spatial memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Arun Balajee Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Talk the walk: Navigating new york city through grounded dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiela</surname></persName>
		</author>
		<idno>abs/1807.03367</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
