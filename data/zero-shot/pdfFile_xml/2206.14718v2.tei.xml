<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LViT: Language meets Vision Transformer in Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-16">August 16, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Southwestern Medical Center</orgName>
								<orgName type="institution">University of Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingde</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Hull</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Southwestern Medical Center</orgName>
								<orgName type="institution">University of Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhou</forename><surname>Guo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakai</forename><surname>Jin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqi</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LViT: Language meets Vision Transformer in Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-16">August 16, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has been widely used in medical image segmentation and other aspects. However, the performance of existing medical image segmentation models has been limited by the challenge of obtaining sufficient number of high-quality data with the high cost of data annotation. To overcome the limitation, we propose a new vision-language medical image segmentation model LViT (Language meets Vision Transformer). In our model, medical text annotation is introduced to compensate for the quality deficiency in image data. In addition, the text information can guide the generation of pseudo labels to a certain extent and further guarantee the quality of pseudo labels in semi-supervised learning. We also propose the Exponential Pseudo label Iteration mechanism (EPI) to help extend the semi-supervised version of LViT and the Pixel-Level Attention Module (PLAM) to preserve local features of images. In our model, LV (Language-Vision) loss is designed to supervise the training of unlabeled images using text information directly. To validate the performance of LViT, we construct multimodal medical segmentation datasets (image + text) containing pathological images, X-rays, etc. Experimental results show that our proposed LViT has better segmentation performance in both fully and semi-supervised conditions. Code and datasets are available at https://github.com/HUANGLIZI/LViT. * Part of the work was done when Zihan Li is an intern at DAMO Academy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation is one of most critical tasks in medical image analysis. In clinical practice, accurate segmentation results are often achieved manually or semi-automatically. It remains a challenging task to extract the desired object accurately, especially when the target organ to be extracted is of high complexity in terms of tissue structures. Recent research shows that deep learning can be a promising approach for automatic medical image segmentation, as the knowledge of experts can be learned and extracted by using a certain deep learning method. A summary of existing solutions is shown in <ref type="figure">Figure 1</ref> (1) one shared encoder followed by two separate decoders <ref type="bibr" target="#b0">[1]</ref>; (2) two separate encoders followed by one shared decode <ref type="bibr" target="#b1">[2]</ref>; (3) two separate encoders followed by a modality interaction model <ref type="bibr" target="#b2">[3]</ref>. However, two inherent issues concerning the creation of high quality medical image datasets severely limit the application: one is the difficulty in obtaining high-quality images, and the other one is the high cost of data annotation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. These two issues have dramatically limited the performance improvement of medical image segmentation models. Since it is challenging to improve the quantity and quality of medical images themselves, it may be more feasible to use complementary and easy-to-access information to make up for the quality defects of medical images. Thus, we turn our attention to written medical notes accompanied with medical images. It is well known that text data of medical records are usually generated along with the patients, so no extra cost is needed to access the corresponding text data. The medical text record data and the image data are naturally complementary to each other, so the text information can compensate for the quality deficiency in the medical image data. On the other hand, expert segmentation annotation is often expensive and time-consuming, especially for new diseases like COVID-19, where high-quality annotations are even more difficult to obtain <ref type="bibr" target="#b3">[4]</ref>. In order to address the issue of under-annotated data, some approaches have gone beyond traditional supervised learning by training their models using both labeled and more widely available unlabeled data, such as semi-supervised learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and weakly supervised learning <ref type="bibr" target="#b6">[7]</ref>. However, the performance of these approaches is largely determined by the credibility of pseudo label. This is because the number of pseudo labels is much larger than ground truth labels. Therefore, the critical question to be answered is how to improve the quality of the pseudo label. To effectively address the issue, we develop a model that can be trained using the medical texts written by domain experts. By learning additional expert knowledge from text information, we can improve the quality of pseudo labels.</p><p>In summary, the challenges exist in two aspects: 1) How to improve the segmentation performance by using the existing image-text information; 2) How to make full use of text information to guarantee the quality of pseudo labels. To address the first challenge, We propose the LViT model ( <ref type="figure">Figure  1(b)</ref>), which is innovative in processing images and text. In LViT, the text feature vector is obtained by using the Embedding layer instead of Text Encoder, which can further reduce the number of parameters in the model. In addition, a hybrid CNN-Transformer structure is able to better merge text information and encode global features with Transformer while retaining the CNN's ability to extract local features from images. To address the second challenge, we design an exponential pseudo label iteration mechanism (EPI) for the proposed LViT, aiming to cross-utilize the label information of labeled data and the latent information of unlabeled data. The EPI indirectly introduces text information  <ref type="figure">Figure 1</ref>: Comparison of current medical image segmentation models and our proposed LViT model. to refine the pseudo label progressively using the Exponential Moving Average (EMA) <ref type="bibr" target="#b7">[8]</ref>. In addition, the LV (Language-Vision) loss is designed to directly utilize text to supervise the training of unlabeled medical images. To validate the performance of LViT, we construct multimodal medical image segmentation datasets containing pathological images (MoNuSeg <ref type="bibr" target="#b8">[9]</ref>) and X-rays (QaTa-COV19 <ref type="bibr" target="#b9">[10]</ref>). Results show that LViT has superior segmentation performance, achieving 81.01% Dice score and 68.2% mIoU on the MoNuSeg dataset and 83.66% Dice score and 75.11% mIoU on the QaTa-COV19 dataset. And it is worth noting that LViT using 1/4 of the train set label can still have the same performance as the fully supervised segmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Semantic segmentation can be considered as the work for pixel-level image classification, and thus many image classification networks have been extended <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> to implement semantic segmentation, with FCN <ref type="bibr" target="#b10">[11]</ref> being commonly considered as the first semantic segmentation network for pixel-wise prediction. Among them, U-Net <ref type="bibr" target="#b12">[13]</ref> is considered as a pioneer in medical image segmentation. Based on this, UNet++ <ref type="bibr" target="#b13">[14]</ref> improved the skip connection of U-Net. However, most of the above methods are very sensitive to the quantity and quality of the data, resulting in limited generalization performance of the models. Therefore, some approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> have explored the application of semi-supervised learning to medical image segmentation. The problem of lacking data and its annotation can also be further mitigated by introducing multiple modalities into the learning models. Inspired by this, Radford et al. proposed CLIP <ref type="bibr" target="#b16">[17]</ref>, which uses contrast learning to learn image representations on a dataset of 400 million pairs (image, text) from scratch. And in the natural image semantic segmentation, there are studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> that have begun to use text information to improve the segmentation capabilities of models. In the field of medical image analysis, there are some related works too <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. However, unlike existing multimodal solutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, LViT don't use text encoder and contrastive learning on explicitly aligned images and text. We only use text embedding without additional encoder to reach state-of-the-art (SOTA) performance. And the rationality of our approach will be proved in Section 4.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Embed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottleneck</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed LViT model is a Double-U structure consisting of a U-shaped CNN branch and a U-shaped Transformer branch. The CNN branch acts as the source of information input and the segmentation head of prediction output, and the ViT branch is used to merge image and text information, where we exploit the ability of Transformer to process cross-modality information. After a simple vectorization of the text, the text vector is merged with the image vector and send to the U-shaped ViT branch for processing. Then, we pass the fusion information of the corresponding size back to the U-shape CNN branch at each layer for the final segmentation prediction output. In addition, a pixel-level attention module (PLAM) is set at the skip connection position of the U-shape CNN branch. With PLAM, LViT is able to retain as much local feature information of the image as possible. We also conduct ablation experiments to demonstrate the effectiveness of each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LViT Model</head><p>U-shape CNN Branch. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), the U-shaped CNN branch is used to receive the image information and act as segmentation head to output the prediction mask. </p><formula xml:id="formula_0">DownCNN i = Relu (BN i (Conv i ())) (1) Y DownCNN,i+1 = MaxPool (DownCNN i (Y DownCNN ,i ))<label>(2)</label></formula><p>where Y DownCNN,i represents the input of the i-th DownCNN module, which becomes Y DownCNN,i+1 after the downsampling of the i-th DownCNN module and the MaxPool layer. In addition, we design the CNN-ViT interaction module using methods such as upsampling to align the features from ViT, as the details of CNN-ViT interaction module are shown in the appendix. The reconstructed ViT features are also connected with CNN feature by residuals to form CNN-ViT interaction features. In addition, to further improve the segmentation capability for local features, PLAM is designed at the skip connection in the U-shaped CNN branch. So the CNN-ViT interaction features will be fed into PLAM, then transferred to the UpCNN module to give the upward information layer by layer.</p><p>U-shape ViT Branch. Referring to the U-shaped CNN branch, the U-shaped ViT branch is designed for merging image features and text features. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), the first layer DownViT module receives the text feature input from BERT-Embed <ref type="bibr" target="#b22">[23]</ref> and the image feature input from the first layer DownCNN module. The pretraining model of the BERT-Embed is the BERT_12_768_12 model, which can convert a single word into a 768-dimensional word vector. The specific cross-modal feature merging operation is expressed by the following equations, </p><formula xml:id="formula_1">Y DownViT,1 = ViT (x img, 1 + CTBN (x text )) (3) x img, i = PatchEmbedding (Y DownCNN,i ) (4) x = ViT 1 (x) = MHSA(LN (x)) + x (5) Y = ViT 2 (x ) = MLP (LN (x )) + x<label>(6)</label></formula><formula xml:id="formula_2">Y DownViT ,i+1 = ViT (Y DownViT ,i + x img,i+1 )<label>(7)</label></formula><p>where i=1,2,3. The features of the corresponding size are then transferred back to the CNN-ViT interaction module through the UpViT module. And the feature is merged with the feature from the DownCNN module of the corresponding layer. This will maximize the extraction of image global features and avoid the oscillation of the model performance due to the inaccuracy of text annotation.</p><p>Pixel-Level Attention Module (PLAM). As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), PLAM is designed to preserve the local features of the image and further merge the semantic features in text. Referring to convolutional block attention module (CBAM) <ref type="bibr" target="#b24">[25]</ref>, PLAM uses parallel branches for Global Average Pooling (GAP) and Global Max Pooling (GMP). We also incorporate the concat and add operations.</p><p>The add operation will help merge the corresponding channel features with similar semantics and save computation. In contrast, the concat operation can integrate the feature information more intuitively and help preserve the original features of each part. After concat the feature information, we use the MLP structure and the multiplication operation to help align the feature size, as shown in Eqn. 8,</p><formula xml:id="formula_3">Y = M LP (x cat ) ? x.<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exponential Pseudo-label Iteration mechanism (EPI)</head><p>In this section, we present proposed Exponential Pseudo label Iteration mechanism (EPI), which is designed to help extend the semi-supervised version of LViT. In EPI, the pseudo label is iteratively updated using the idea of EMA <ref type="bibr" target="#b7">[8]</ref>, as shown in <ref type="figure">Figure 3</ref>(a) and Eqn. 9,</p><formula xml:id="formula_4">P t = ? ? P t?1 + (1 ? ?) ? P t<label>(9)</label></formula><p>where P t?1 represents the prediction of model M t?1 , and ? is set as the momentum parameter to 0.99. It is worth noting that here P t?1 is an N -dimensional prediction vector, where N represents the number of category classes, and each dimension represents the prediction probability. Therefore, EPI can gradually optimize the segmentation prediction results of the model for each unlabeled pixel and be robustness to noisy label. This is because we do not simply use the pseudo label predicted by one generation of model as the target for the next-generation model, which can avoid sharp deterioration of the pseudo label quality. The theoretical proof for the effectiveness of EPI algorithm is as follows. Proof: The basic assumption for the EPI algorithm is the model weights will dither around the actual optimum in the last n generations, and therefore the pseudo label predicted by the model will also dither around the actual mask in the last n generations. We expand P t around t in Eqn. 9 to Eqn.10,</p><formula xml:id="formula_5">P t = ? n ? P t?n + (1 ? ?) ? ? n?1 P t?n+1 + ? ? ? + ? 0 P t .<label>(10)</label></formula><p>In particular, we let n = 1/(1 ? ?) and ? n = ? 1 1?? ? 1 e . So for the first 1/(1 ? ?) generations, P t decays to a weighted average of 1/e. Further, we introduce an adjustment gradient g t?1 for the predicted label P t?1 , which leads to Eqn. 11,</p><formula xml:id="formula_6">P t = P t?1 ? g t?1 = P t?2 ? g t?1 ? g t?2 = ? ? ? = P 1 ? n?1 i=1 g i .<label>(11)</label></formula><p>Similarly, we extend Eqn. 10 to Eqn. 12 when t = n and P 0 ? P 1 ,</p><formula xml:id="formula_7">P t = ? n ? P 0 + (1 ? ?) ? ? n?1 P 1 + ? n?2 P 2 + ? ? ? + ? 0 P n = ? n ? P 0 + (1 ? ?) ? ? n?1 P 1 + ? n?2 (P 1 ? g 1 ) + ? ? ? + ? 0 P 1 ? n?1 i=1 g i = ? n ? P 0 + (1 ? ?) ? 1 ? ? n 1 ? ? P 1 ? n?1 i=1 1 ? ? n?i g i 1 ? ? = ? n ? P 0 + (1 ? ? n ) ? P 1 ? n?1 i=1 1 ? ? n?i g i ? P 1 ? n?1 i=1 1 ? ? n?i g i .<label>(12)</label></formula><p>Comparing with Eqn.11, it can be seen from Eqn.12 that the EPI algorithm adds the weight coefficient 1 ? ? n?i for the gradient descent step of the i-th iteration. 1 ? ? n?i will decreases as i increases, so the change of pseudo label is finally stabilized and obtain the pseudo label with high confidence.  <ref type="figure">Figure 3</ref>: Illustration of (a) Exponential Pseudo-label Iteration mechanism (EPI), and (b) LV Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LV loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LV (Language-Vision) Loss</head><p>To further utilize the text information to guide the pseudo-label generation, we design the LV (Language-Vision) loss function, as shown in <ref type="figure">Figure 3</ref>(b). Taking the QaTa-COV19 dataset as an example, we can use different structured text information to form the corresponding mask (the contrastive label in <ref type="figure">Figure  3</ref>(b)). For the MoNuSeg dataset, we directly use the mask in labeled training set as the contrastive label. And the cosine similarity between the texts is shown in Eqn. 13,</p><formula xml:id="formula_8">TextSim = x text ,p ? x text,c |x text ,p | ? |x text ,c |<label>(13)</label></formula><p>where x text,p represents the text feature vector corresponding to the pseudo label, and x text,c represents the text feature vector corresponding to the contrastive label. According to T extSim, we select the contrastive text with the highest similarity and calculate the cosine similarity between the segmentation mask to form L LV corresponding to that text, as shown in Eqn. 14 and 15,</p><formula xml:id="formula_9">ImgSim = ximg,p?ximg,c |ximg,p|?|ximg,c|<label>(14)</label></formula><formula xml:id="formula_10">L LV = 1 ? ImgSim<label>(15)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Two datasets are used in the experiments to evaluate the performance. One is the MoNuSeg dataset <ref type="bibr" target="#b8">[9]</ref> obtained from the MICCAI 2018 MoNuSeg Challenge.</p><p>The training set consists of 30 images with 21,623 single kernel annotations. The other is the QaTa-COV19 dataset <ref type="bibr" target="#b9">[10]</ref>, which is compiled by researchers from Qatar University and Tampere University. The dataset consists of 9258 COVID-19 chest radiographs with manual annotations of COVID-19 lesions for the first time. In addition, text annotations for the datasets are extended by us to be used for training the vision-language model. The loss function we use is shown in Eqn. <ref type="bibr" target="#b15">16</ref>, where L Dice means dice loss and L CE means cross-entropy loss. For the unlabeled data, an additional term on the loss L LV is introduced with ? = 0.1 as shown in Eqn. 17. Dice and mIoU are used to evaluate the segmentation performance. And early stop mechanism is used during training phase. More implementation details can be found in the appendix.</p><formula xml:id="formula_11">L sup = (L Dice + L CE )/2<label>(16)</label></formula><formula xml:id="formula_12">L unsup = (L Dice + L CE )/2 + ? ? L LV<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Art (SOTA) Methods</head><p>We compare the performance of our LViT model with several CNN and Transformer based segmentation models. The number of network parameters and the computational cost of different methods are also reported. Note that the numbers after the methods refer to the ratio of labels used, e.g., LViT-T (1/2) refers to the experimental results of the LViT-T model using 1/2 of the train set labels. And LViT-T means the Tiny version of LViT. The quantitative experimental results are listed in <ref type="table" target="#tab_4">Table 1</ref>.</p><p>Experimental results on the QaTa-COV19 dataset show that LViT-TW/ LViT-T is able to achieve better performance than the previous SOTA method with a smaller number of parameters and a lower computational cost. For example, LViT-T improves the Dice score by 3.24% and the mIoU by 4.3% compared to the suboptimal nnUNet. It is also worth noting that LViT-T still outperforms other SOTA methods even when only 1/4 of the training labels are  The qualitative results of our model and other state-of-the-art methods on MoNuSeg and QaTa-COV19 datasets are shown in <ref type="figure" target="#fig_2">Figure 4</ref>, where four baseline methods are selected for comparison. Qualitative results shows that LViT-T has excellent semantic segmentation capabilities, especially when compared to other SOTA methods where the mis-segmentation phenomenon is greatly reduced.</p><p>As can be seen from the red boxes in <ref type="figure" target="#fig_2">Figure 4</ref>, UNet++, MedT, GTUNet and TransUNet all have more severe mis-segmentation than LViT. It also shows the introduction of text information in our learning mechanism can better guide the training of the model, and consequently lead to more accurate segmentation. The Effectiveness of Proposed Components. Relevant ablation experiments are conducted for our proposed method, and the relevant experimental results are shown in <ref type="table" target="#tab_5">Table 2</ref>. We explore the effectiveness of the improvements from four perspectives, including DownViT, UpViT, PLAM, and Text. The Text refers to the text information. All of them are effective based on experimental results, where the performance improvement brought by Text is the most obvious. Relevant ablation experiments are also performed to address our innovative points in semi-supervision. We explore the effectiveness of the improvement from three perspectives, including EPI, Text, and L LV . From the experimental results, it can be seen that the improvements of EPI, Text, and L LV are effective. Among them, by introducing text annotation information, the Dice score improves by 1.59%, and mIoU improves by 1.66% compared to LViT with only 1/4 of the train set label supervision. And by introducing EPI mechanism, the semi-supervised performance of LViT is guaranteed to be comparable to the fully supervised performance of U-Net. Finally, the continuous improvement of model performance is also ensured by introducing L LV .   <ref type="table" target="#tab_6">Table 3</ref>. According to the experimental results, the increases of model size does not lead to the performance improvement on the QaTa-COV19 dataset. Besides, the model performance is further limited when model size increasing to LViT-LW. This could be due to the fact that the increase of the number of parameters will lead to the increase of the probability of model overfitting, so the performance of LViT-LW/LViT-L may no longer be as good as it should be. In contrast, the positive correlation between model performance and model size is followed on the MoNuSeg dataset. Besides, it is worth noting that LViT has only 1.7M more parameters and 0.1G more computation than LViT-W, which indicates the number of parameters and the amount of computation brought by the text information is far more valuable than the size change of the model itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Interpretability Study</head><p>The interpretability study is performed on the QaTa-COV19 dataset to explore whether the LViT network can notice lesion regions and whether the introduction of text information enhances the attention to lesion regions. We use GradCAM <ref type="bibr" target="#b31">[32]</ref> to compare the activation for regions of attention. <ref type="figure">Figure 5</ref> shows that UNet++, MedT, GTUNet and UCTransNet all have different degrees of misactivation regions. For example, the case of total lung infection only activates half of the lung region in other methods. In contrast, by introducing text information in our model, more regions can be activated, and the edge profile of the activated regions of interest is more consistent with the ground truth. Therefore, the localization of lesion regions can be learned by introducing text embedding. In addition, we conduct more experiments to explore how the introduction of text information specifically helps activate regions, as shown in <ref type="figure">Figure 6</ref>. We perform activation mapping in DownCNN1, DownCNN2, DownCNN3, DownCNN4, and DownViT1, respectively, where DownCNN1 and DownViT1 represent the first layer DownCNN and the first layer DownViT, respectively, where the text information is input to the model through DownViT1, so the difference in activation regions between DownViT1 and DownCNN1 can be approximated as brought by the text information. And the CAM output of LViT-TW and LViT-T shows the final activation difference caused by the text information. It can be seen that the activation effect of the region of interest of DownViT1 is similar to that of DownCNN4. It is also worth noting that image feature of DownViT1 comes from DownCNN1, which failed to activate the lesion region but only activated the lung boundary. However, DownViT1 can directly activate the relevant lesion region by introducing text information. And it indicates that the text information can effectively help locate lesion region in the lung, thus prompting the network to focus more on the region referred to by the text information. By comparing the regions of interest for LViT-TW/LViT-T, we find text information can help reduce the probability of mis-segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth UNet++ MedT GTUNet UCTransNet</head><p>LViT-TW LViT-T <ref type="figure">Figure 5</ref>: Saliency map for interpretability study of different approaches on QaTa-COV19 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth DownCNN1 DownCNN2 DownCNN3 DownCNN4 DownViT1</head><p>LViT-TW LViT-T <ref type="figure">Figure 6</ref>: Saliency map for interpretability study of different layers of LViT on QaTa-COV19 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a new vision-language medical image segmentation model LViT, which leverages medical text annotation to compensate for the quality deficiency in image data and guide the generation of pseudo labels in semisupervised learning. Multimodal medical segmentation datasets (image + text) are constructed to evaluate the performance of LViT, and experimental results show that our model has better segmentation performance in both fully and semi-supervised conditions. Currently, the proposed model is only experimented on 2D medical data. In our future work, we will extend our model to conduct experiments on 3D medical data to verify its generality.</p><p>A The proof of CNN-Transformer structure superiority</p><p>Unlike the previous Vision-and-Language work, our proposed LViT model is innovative in processing images and text. We do not use text encoder and creatively use the interaction between CNN and Vit to extract features.</p><p>Proof: For the sake of description, we assume that the patch size in ViT is equal to the kernel size in CNN, which is S. The input image matrix is M , and the output after convolution is Y cnn .</p><formula xml:id="formula_13">Y cnn,k (i, j) = S ?=0 S ?=0 f k (?, ?) * M (i ? ?, j ? ?) (18) Y cnn (i, j) = [Y cnn,1 (i, j); Y cnn,2 (i, j); . . . ; Y cnn,C (i, j)]<label>(19)</label></formula><p>where f k represents the convolution kernel of the k-th channel, and Y cnn,k (i, j) represents the output of the k-th channel after convolution. The total convolution outputs of C channels form Y cnn (i, j). We can assume that CNN has a certain degree of affine invariance and will learn the shallow features better. Moreover, since the convolutional kernel size is fixed, each can only learn and perceive one aspect of the shallow features from the local information, such as points, lines, and boundary. And since the convolutional kernel f k (?, ?) of each channel shares the weights on the whole image, convolving the whole image with convolutional kernels that focus on boundary features is equivalent to doing whole-image filtering on the image, which will filter out the boundary. From the above, it is clear that CNN has better extraction ability for local shallow features. Similarly, we set the output of matrix M after multi-head self-attention to be Y vit , as shown in the Eqn. 20, 21, 22, 23 and 24,</p><formula xml:id="formula_14">Q h = W Q h ? PatchEmbedding (M )<label>(20)</label></formula><formula xml:id="formula_15">K h = W K h ? PatchEmbedding (M )<label>(21)</label></formula><formula xml:id="formula_16">V h = W V h ? PatchEmbedding (M ) (22) Y vit,h = Softmax Q T h ? K h ? d ? V h<label>(23)</label></formula><formula xml:id="formula_17">Y vit = LN ([Y vit,1 ; Y vit,2 ; . . . ; Y vit,H ])<label>(24)</label></formula><p>where PatchEmbedding represents the embedding layer used to transform M into a sequence vector. Y vit,h denotes the output of the h-th self-attention head, and d prevent the feature gradient from vanishing after Softmax. LN represents the linear layer, which aims to reduce the dimensionality of the output features. And Q h , K h , and V h in the self-attention mechanism are transformations for their own inputs, Softmax</p><formula xml:id="formula_18">Q T h ?K h ? d</formula><p>? V h is computing the similarity between them. So the self-attention is essentially focusing on the invariance of the input features. The input features M are the whole image for ViT, so ViT is easier to learn the global features than CNN and more robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparison between natural image and medical image</head><p>Compared with natural images, medical images typically have the following characteristics:</p><p>1. The positions of human organ in medical images are relatively fixed, whereas the relative positions of different segmented objects in natural images often differ significantly.</p><p>2. Boundaries between different regions in medical images are often blurred. And the small gray-scale value differences in the vicinity of the boundaries makes it difficult to extract highly accurate segmentation boundaries. In contrast, for an image taken with an ordinary digital camera, there are often significant color differences for different objects.</p><p>3. Medical images are usually obtained with specialized equipment. Medical images are taken with different types of equipment and at different environments, which inevitably leading to distribution shifts. In addition, medical images are difficult to share across institutions due to relevant regulations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CNN-ViT Interaction Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of LV (Language-Vision) Loss</head><p>To further utilize the text information to guide the pseudo-label generation, we designed the LV (Language-Vision) loss function. For QaTa-COV19 dataset, we can use different structured text information to form the corresponding mask (the contrastive label). For MoNuSeg dataset, we directly use the mask in labeled training set as the contrastive label. And we calculate the cosine similarity between the texts, as shown in Eqn. 25,</p><formula xml:id="formula_19">TextSim = x text ,p ? x text,c |x text ,p | ? |x text ,c |<label>(25)</label></formula><p>where x text,p represents the text feature vector corresponding to the pseudo label, and x text,c represents the text feature vector corresponding to the contrastive label. After that, according to T extSim, we select the contrastive text with the highest similarity and find the segmentation mask corresponding to that text; we calculate the cosine similarity between the predicted segmentation pseudo-label and the contrastive label using the label similarity, as shown in Eqn. <ref type="bibr" target="#b25">26</ref>  </p><formula xml:id="formula_20">L LV = 1 ? ImgSim<label>(27)</label></formula><p>where x img,p represents the pseudo-label feature vector, and x img,c represents the comparison label feature vector. Compared to Euclidean distance, cosine similarity is not sensitive to absolute values and reflects the degree of similarity more qualitatively, consistent with our task motivation. The contrastive labels mainly provide labeling information of the approximate location instead of refinement for the boundaries. Therefore, the primary purpose of LV loss is to avoid mis-segmentation or mislabelled cases with significant differences. For this reason, we only use LV loss in the unlabeled case because the contrastive labels are of little help for performance improvement when the data is labeled. And in case of no label, LV loss can avoid the sharp deterioration of the pseudo-label quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Implementation Details</head><p>Our proposed approach is implemented using Pytorch. The main parameters of the server are listed below: the operating system is Ubuntu 16.04.12 LTS, the CPU is Intel(R) Xeon(R) Gold 5218, the GPU is a 2-card TESLA V100 32G, and the memory capacity is 128 GB. In addition, for both datasets, we split the train set and the validation set uniformly in the ratio of 80% and 20% from the original train set. Then, the train set is divided into labeled and unlabeled train sets in a specific ratio. The number of samples in the specific dataset is shown in <ref type="table" target="#tab_4">Table 1</ref>. The initial learning rate is set to 3e-4 for the QaTa-COV19 dataset and 1e-3 for the MoNuSeg dataset. We also use an early stop mechanism until the performance of model does not improve for 50 epochs. Different batch sizes are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Loss Function</head><p>We set a weighted loss function using the dice loss and cross-entropy loss to form the final loss function. We also add the loss L LV with ? = 0.1 for the unlabeled data, which are described in the Eqn. 28, 29, 30 and 31,</p><formula xml:id="formula_21">L Dice = 1 ? N i=1 C j=1 1 N C ? 2|pij ?yij | (|pij |+|yij |) (28) L CE = ? N i=1 C j=1 1 N ?y ij log (p ij )<label>(29)</label></formula><formula xml:id="formula_22">L sup = (L Dice + L CE )/2 (30) L unsup = (L Dice + L CE )/2 + ? ? L LV<label>(31)</label></formula><p>where N represents the number of pixels, C represents the number of categories, which is set to 2 in our paper. p ij represents the prediction probability that pixel i belongs to category j, y ij represents whether pixel i belongs to category j. If pixel i belongs to category j, then y ij takes 1, otherwise 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Evaluation Metric</head><p>For the evaluation metrics, the Dice score and the mIoU metric are used to evaluate the performance of our LViT model and other SOTA methods, as shown in the Eqn. 32 and 33,</p><formula xml:id="formula_23">Dice = N i=1 C j=1 1 N C ? 2|pij ?yij | (|pij |+|yij |) = 1 ? L Dice (32) mIoU = N i=1 C j=1 1 N C ? |pij ?yij | |pij ?yij |<label>(33)</label></formula><p>where N, C, p ij and y ij has the same definition as in the section F. As can be seen, the above text is similar to a structured annotation that can be easily extracted from the patient's pathology report, and we believe that structured text annotations can provide approximate location of lesion region, which is beneficial for improving segmentation performance. MoNuSeg dataset. Automatic cell nuclear segmentation techniques can reduce the time required to develop and validate visual biomarkers in pathology analysis. To this end, the MoNuSeg dataset was presented at the MoNuSeg 2018 Challenge, which aims to develop scalable nucleus segmentation techniques in digital pathology and involved 32 teams with more than 80 researchers from different regional institutions. The train set consisted of 30 images with 21,623 single nucleus annotations, and the test set consisted of 14 images. The images in this dataset are from multiple hospitals in The Cancer Genome Atlas (TCGA) and include nuclei from H&amp;E-stained tissue images acquired at the standard 40X magnification. In this work, we introduce text annotations to the MoNuSeg dataset, where the text annotations mainly contain information about the density of the nuclei, e.g., "The nuclei are sparsely distributed." refers to the sparse distribution of nuclei in the image. "The nuclei are evenly distributed." refers to the relatively even distribution of nuclei in the image. All of the above text are manually annotated by viewing the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Details of the datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Ablation study of supervised components on MoNuSeg dateset</head><p>It can be seen from the experimental results that DownViT, UpViT, PLAM, and text have certain effects. By introducing DownViT and UpViT modules, dice scores increased by 3.99% and Miou by 5.34% compared with u-net. By introducing the PLAM module, the dice score of LVIT increased by 0.22%, and Miou increased by 0.3%. By introducing text information, the dice score of LVIT also increased by 0.35%, and Miou increased by 0.49%. To further compare the importance of convolution, we also conducted ablation experiments. It can be seen that without convolution, the performance of LVIT decreases significantly, the dice score decreases by 2.17%, and Miou decreases by 2.83%. We can also find that by introducing text information, Vit structure can have better performance than CNN. We think it is necessary to use CNN-Transformer as the image encoder.  K More saliency maps for interpretability study</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth UNet++ MedT GTUNet UCTransNet</head><p>LViT-TW LViT-T <ref type="figure">Figure 9</ref>: Saliency map for interpretability study of different approaches on QaTa-COV19 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth DownCNN1 DownCNN2 DownCNN3 DownCNN4 DownViT1</head><p>LViT-TW LViT-T <ref type="figure">Figure 10</ref>: Saliency map for interpretability study of different layers on QaTa-COV19 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of (a) the proposed LViT model, and (b) PLAM. The proposed LViT model is a Double-U structure formed by combining a U-shape CNN branch with a U-shaped ViT branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on QaTa-COV19 and MoNuSeg datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A</head><label></label><figDesc>series of ablation experiments are used to verify the performance of the LViT model. The performance of the LViT model is explored in three aspects, including the comparison of proposed components, model size and hyper-parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, with eight different model sizes, namely, LViT-TW/LViT-T, LViT-SW/LViT-S, LViT-MW/LViT-M, LViT-LW/LViT-L, where "W" refers to with-out the text information. And the differences between different versions of LViT are in the number of Transformer layers in the DownViT module and UpViT module, where LViT-TW/LViT-T has only 1 Transformer layer per ViT module, and LViT-SW/LViT-S has 4 Transformer layers per ViT module. LViT-MW/LViT-M has 6 Transformer layers per ViT module, and LViT-LW/LViT-L has 12 Transformer layers per ViT module. The experimental results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>And the convolution operation f (x) is satisfying shift invariance and scale invariance, i.e., if Y (x) = f (x) * M (x), then we have Y (x ? ?) = f (x) * M (x ? ?) and if Y (x) = f (x) * M (x), then we have |?|Y (x/?) = f (x/?) * M (x/?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure. 7</head><label>7</label><figDesc>shows the specific structure of CNN-ViT Interaction Module. Among them, we use the interaction module to help recover the size of ViT feature to be consistent with CNN feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>CNN-ViT Interaction Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>QaTa-COV19 dataset. Researchers at Qatar University et al. presented the QaTa-COV19 dataset, which consists of 9258 COVID-19 breast radiographs and their corresponding true segmentation masks. In this dataset, a train set and a test set are artificially split, with the train set consisting of 7145 lung radiographs and the test set consisting of 2113 lung radiographs. And we extend text annotations on the QaTa-COV19 dataset for the first time, with text annotations focusing on whether both lungs are infected, the number of lesion regions, and the approximate location of the infected areas. For example, "Bilateral pulmonary infection, two infected areas, upper left lung and upper right lung." refers to bilateral lung infection, and there are two infection areas located in the upper left lung and the upper right lung respectively. "Unilateral pulmonary infection, one infected area, middle lower left lung." refers to the presence of unilateral pulmonary infection with only one infected area in the middle upper left lung. "Bilateral pulmonary infection, three infected areas, upper lower left lung and lower right lung." refers to bilateral pulmonary infection with three infected areas in the lung, and the three infected areas are in the upper lower left lungs and in the lower right lung. It is important to note that the terms left and right are used from the perspective of the viewer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>More qualitative results on QaTa-COV19 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison between our method (LViT) and other state-ofthe-art methods on QaTa-COV19 and MoNuSeg datasets. The "W" in LViT-TW refers to without the text information.</figDesc><table><row><cell>QaTa-COV19</cell><cell>MoNuSeg</cell></row></table><note>used. Similarly, it can be seen that LViT-T has a 2.54% higher Dice score and a 4.05% better mIoU score than LViT-TW. This also indicates that introducing text information is able to improve model performance effectively. A similar trend is observed for the MoNuSeg dataset. On the MoNuSeg dataset, compared to UCTransNet, LViT-T improves the Dice value by 0.49% and the mIoU value by 0.63%. Even LViT-TW and LViT-T (1/4) can achieve comparable performance to nnUNet and UCTransNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on effectiveness of supervised components: DownViT, UpViT, PLAM, Text &amp; semi-supervised components: EPI, Text, Loss L LV on QaTa-COV19 dataset. We conduct ablation experiments for model sizes to investigate the specific performance of LViT with different model sizes. The experiments are conducted on two datasets, QaTa-COV19 and MoNuSeg</figDesc><table><row><cell>Method</cell><cell cols="2">CNN DownViT UpViT PLAM Text EPI L LV Dice (%) mIoU (%)</cell></row><row><cell>U-Net</cell><cell>79.02</cell><cell>69.46</cell></row><row><cell></cell><cell>80.73</cell><cell>70.96</cell></row><row><cell></cell><cell>80.85</cell><cell>71.12</cell></row><row><cell>LViT-T</cell><cell>81.12</cell><cell>71.37</cell></row><row><cell></cell><cell>80.52</cell><cell>70.43</cell></row><row><cell></cell><cell>83.66</cell><cell>75.11</cell></row><row><cell></cell><cell>79.08</cell><cell>69.42</cell></row><row><cell>LViT-T (1/4)</cell><cell>80.67</cell><cell>71.08</cell></row><row><cell></cell><cell>80.95</cell><cell>71.31</cell></row><row><cell cols="2">Ablation Study on Model Size.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">Ablation study on different Model Size: LViT-T, LViT-S, LViT-M,</cell></row><row><cell>LViT-L.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Model Size Param(M)</cell><cell>Flops(G)</cell><cell cols="2">QaTa-COV19</cell><cell cols="2">MoNuSeg</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Dice (%) mIoU (%) Dice (%) mIoU (%)</cell></row><row><cell>LViT-TW</cell><cell></cell><cell></cell><cell>81.12</cell><cell>71.37</cell><cell>80.33</cell><cell>67.24</cell></row><row><cell>LViT-T</cell><cell>LViT-TW: 28.0</cell><cell>LViT-TW: 54.0</cell><cell>83.66</cell><cell>75.11</cell><cell>80.36</cell><cell>67.31</cell></row><row><cell>LViT-SW</cell><cell>LViT-SW: 53.1</cell><cell>LViT-SW: 63.8</cell><cell>81.54</cell><cell>71.91</cell><cell>80.27</cell><cell>67.20</cell></row><row><cell>LViT-S</cell><cell>LViT-MW: 69.8</cell><cell>LViT-MW:70.4</cell><cell>83.41</cell><cell>74.84</cell><cell>80.50</cell><cell>67.36</cell></row><row><cell>LViT-MW</cell><cell>LViT-LW: 120.1</cell><cell>LViT-LW: 90.1</cell><cell>81.59</cell><cell>71.82</cell><cell>80.38</cell><cell>67.33</cell></row><row><cell>LViT-M</cell><cell></cell><cell></cell><cell>83.63</cell><cell>75.28</cell><cell>80.39</cell><cell>67.37</cell></row><row><cell>LViT-LW</cell><cell>LViT: LViT-W+1.7</cell><cell>LViT: LViT-W+0.1</cell><cell>81.40</cell><cell>71.43</cell><cell>80.66</cell><cell>67.71</cell></row><row><cell>LViT-L</cell><cell></cell><cell></cell><cell>82.74</cell><cell>73.00</cell><cell>81.01</cell><cell>68.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The specific division of datasets.</figDesc><table><row><cell></cell><cell cols="2">QaTa-COV19 MoNuSeg</cell></row><row><cell>Train set</cell><cell>5716</cell><cell>24</cell></row><row><cell>Val set</cell><cell>1429</cell><cell>6</cell></row><row><cell>Test set</cell><cell>2113</cell><cell>14</cell></row><row><cell cols="3">also set for each dataset since they have different data size. The default batch</cell></row><row><cell cols="3">size is 24 for the QaTa-COV19 dataset, and the default batch size is 4 for the</cell></row><row><cell>MoNuSeg dataset.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on effectiveness of supervised components: DownViT, UpViT, PLAM, Text.</figDesc><table><row><cell cols="7">Method CNN DownViT UpViT PLAM Text Dice (%) mIoU (%)</cell></row><row><cell>U-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.45</cell><cell>62.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.27</cell><cell>67.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.44</cell><cell>67.41</cell></row><row><cell>LViT-L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.66</cell><cell>67.71</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.84</cell><cell>65.37</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.01</cell><cell>68.20</cell></row><row><cell cols="7">J More qualitative results on QaTa-COV19 and</cell></row><row><cell cols="3">MoNuSeg datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Raw image</cell><cell>Ground truth</cell><cell>UNet++</cell><cell>MedT</cell><cell>GTUNet</cell><cell>UCTransNet</cell><cell>LViT-T</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix contains additional details of Section 3, 4 and a discussion about CNN-Transformer hybrid structure superiority.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Med3d: Transfer learning for 3d medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boundary-aware transformers for skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="206" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lymph node gross tumor volume detection and segmentation via distance-based gating using 3d ct/pet imaging in radiotherapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Ying</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhou</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hung</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-supervised tumor segmentation through layer decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual-consistency semi-supervised learning with uncertainty quantification for covid-19 lesion segmentation from ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangjing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3d left atrium segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative localization in cnns for weakly-supervised segmentation of pulmonary nodules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elsa</forename><forename type="middle">D</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruchika</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><forename type="middle">Fahri</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Osegnet: Operational segmentation network for covid-19 detection using chest x-ray images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysen</forename><surname>Degerli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Kiranyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moncef</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabbouj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10185</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mahfuzur Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101766</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semisupervised medical image classification with relation-driven self-ensembling model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quande</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3429" to="3440" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Anton van den Hengel, and Baichuan Sun. The devil is in the labels: Semantic segmentation from sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02002</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Groupvit: Semantic segmentation emerges from text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11094</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gt u-net: A u-net like group transformer network for tooth root segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianni</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="386" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving pneumonia localization via cross-attention on medical images and reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhish</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evrim</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congyu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>R?ckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02889</idno>
		<title level="m">Joint learning of localized representations from medical images and reports</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><forename type="middle">Le</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. Attention u-net: Learning where to look for the pancreas</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">H</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Medical transformer: Gated axial-attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya Maria Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Swin-unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueyue</forename><surname>Hu Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osmar R Zaiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uctransnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04335</idno>
		<title level="m">Rethinking the skip connections in u-net from a channel-wise perspective with transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
