<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10">Oct 2022. AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><forename type="middle">Mengqi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jicong</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Leung</surname></persName>
						</author>
						<title level="a" type="main">Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<title level="j" type="main">JOURNAL OF L A T E X CLASS FILES</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="2022-10">Oct 2022. AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Graph embedding features</term>
					<term>Graph convolutional network</term>
					<term>Centrality</term>
					<term>Human action recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Combining skeleton structure with graph convolutional networks has achieved remarkable performance in human action recognition. Since current research focuses on designing basic graph for representing skeleton data, these embedding features contain basic topological information, which cannot learn more systematic perspectives from skeleton data. In this paper, we overcome this limitation by proposing a novel framework, which unifies 15 graph embedding features into the graph convolutional network for human action recognition, aiming to best take advantage of graph information to distinguish key joints, bones, and body parts in human action, instead of being exclusive to a single feature or domain. Additionally, we fully investigate how to find the best graph features of skeleton structure for improving human action recognition. Besides, the topological information of the skeleton sequence is explored to further enhance the performance in a multi-stream framework. Moreover, the unified graph features are extracted by the adaptive methods on the training process, which further yields improvements. Our model is validated by three large-scale datasets, namely NTU-RGB+D, Kinetics and SYSU-3D, and outperforms the state-of-the-art methods. Overall, our work unified graph embedding features to promotes systematic research on human action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>data, which can dramatically reduce the computational cost of human action recognition. Besides, the skeleton data could show better performance when the background of human action is complicated <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>It is known that recurrent neural networks (RNN) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, convolutional neural networks (CNN) <ref type="bibr" target="#b9">[10]</ref>, temporal convolutional networks (TCN) <ref type="bibr" target="#b10">[11]</ref>, graph convolutional networks (GCN) <ref type="bibr" target="#b11">[12]</ref>, and spatial temporal GCN (ST-GCN) <ref type="bibr" target="#b0">[1]</ref> have been applied to human action recognition. The earliest models for human action recognition often construct all coordinates of human joints from each frame for convolutional learning, like RNN or CNN. These learning models rarely investigate the endogenous relationship between joints and bones, leading to missing abundant unique and significant information in skeleton data. To understand the relationships between joints and bones, recent models build a skeleton graph, where nodes are human joints and edges are natural bones between two connected joints, and implement GCN to extract informative features. Later, the spatial-temporal GCN (ST-GCN) further developed GCN to simultaneously learn topological features of skeleton graphs <ref type="bibr" target="#b0">[1]</ref>. The ST-GCN constructed a skeleton graph to represent the connections of joints, and firstly proposed the temporal edges to link the relationships between consecutive frames. Although the ST-GCN captures the features of joints and bones, structurally key joints and bones are largely ignored, which may contain specific patterns and information of actions. For example, human hand joints and knee joints play key roles in the walking action. While the ST-GCN tries to capture all of the human joints, the correlations between key joints and actions might be weakened during the training process.</p><p>To address such issues, we need a new design that can automatically detect the key human joints and bones, and deeply emphasize their uncovered graph features in terms of their large distance for every human action. Meanwhile, the new model could encode the extracted graph features into training process as well as highlight their dynamic properties, which could fully consider different perspectives from skeleton graphs, and present a general graph embedding application. Additionally, while the skeletons are the structural graphs instead of 2D or 3D grids on different human actions, most of the previous models construct a constant graph as the input signal. Basically, to complete an action, everyone needs the cooperation of various body parts. The phenomenon indicates human actions are based on the relationships between different body parts, but some body parts may play key roles in human actions. For example, walking action strongly depends on legs, knees and feet, while sitting action is closely related to the head and trunk. An exogenous dependency has been proposed for disconnected joints or human parts, but they could not cover wider-range human actions <ref type="bibr" target="#b12">[13]</ref>. These body parts could be considered as subgraphs of skeleton graphs.</p><p>Deep learning on graphs, in particular, have emerged as the dominant models for learning representations on graphs <ref type="bibr" target="#b13">[14]</ref>. These graph learning models condense neighborhood connectivity patterns into low-dimensional embedding features that can be used for a variety of downstream tasks. While graph representation learning has made tremendous progress in recent years <ref type="bibr" target="#b14">[15]</ref>, prevailing models focus on learning useful features for nodes <ref type="bibr" target="#b15">[16]</ref>, edges <ref type="bibr" target="#b16">[17]</ref>, subgraphs <ref type="bibr" target="#b17">[18]</ref> or entire graphs <ref type="bibr" target="#b18">[19]</ref>. Graph-level features provide an overarching view of the graphs at macro details. In contrast, node-level features focus instead on the preservation of the local topological structure in micro details. Besides, subgraph features could effectively capture the unique topology of subgraphs, and incredibly improve the learning performance <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this paper, we systematically study human actions, and propose a new framework to unify 15 graph embedding features with graph convolutional networks and to model characteristic skeletons for human action recognition, called UNFGEF. Compared to existing GCN models, this new model answers what characterizes node features, edge features and subgraph features for skeleton graphs. UNFGEF is expected to provide a ranking that identifies the most important joints, bones and body parts in different human actions. Inspired by graph embedding methods, we categorized the graph features into three domains, such as node centrality, edge centrality, subgraph features. The node centrality contains 10 nodelevel features, including Salton index, Jaccard index, and Katz index, etc <ref type="bibr" target="#b20">[21]</ref>. The edge centrality is calculated by the betweenness centrality, degree centrality, and similarity centrality. Finally, subgraph features are measured by clustering coefficient and motif index. Then, the extracted graph features are encoded into the graph convolution networks for further training. The UNFGEF can identify the relationships between physically connected and disconnected joints simultaneously, which effectively capture micro-level and high-level skeleton structures. Besides, this proposed module shows endogenous dependencies in an attention mechanism to enhance the learning performance of human action recognition. The pipeline of UNFGEF is illustrated in <ref type="figure">Fig. 1</ref>.</p><p>The major contributions of this work are summarized as follows.</p><p>? First, UNFGEF is the first work for embedding various graph features into graph evolutional networks. And it is designed to uncover the overlooked graph information between physically connected and disconnected parts of the human skeleton. ? Second, UNFGEF exploits 15 graph embedding methods in designing the embedding module to meet general demands in human actions. It could offer a new and deep direction of the action recognition task. ? Third, the motion information between consecutive frames is extracted for temporal information modeling.</p><p>Both the spatial and temporal information are fed into a multi-stream framework for the action recognition task. ? Fourth, our model has state-of-the-art performance on three large-scale datasets for skeleton-based action recognition. The relative improvement is about 1%. The finding indicates that these extracted graph features are fundamental factors in the skeleton data, which brings remarkable improvements for human action recognition. Our results have implications for unifying graph embedding features with graph convolutional networks, advancing the science of human action recognition and improving our understanding of general graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skeleton-based action recognition</head><p>Despite the illumination change and scene variation, the reliable skeleton data could be easily and accurately extracted by pose estimation algorithms from depth sensors <ref type="bibr" target="#b21">[22]</ref> or RGB cameras <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and temporal CNN <ref type="bibr" target="#b24">[25]</ref>. The substantial skeleton data spurs the creation of new technological or theoretical action recognition models <ref type="bibr" target="#b25">[26]</ref>. Recently, deep learning approaches have been widely applied to build the spatialtemporal frameworks of skeleton sequences in human action recognition <ref type="bibr" target="#b5">[6]</ref>. For example, RNN models are implemented into human action recognition due to effectively learning long-term sequence data <ref type="bibr" target="#b26">[27]</ref>. Compared to RNN, CNN also showed important implications for human action recognition, owing to its powerful parallelization over every element in the training process. Skeleton data are manually transferred into 2D or 3D images deployed by CNN <ref type="bibr" target="#b11">[12]</ref>, which achieves high performance in action recognition. However, these images used by CNN cannot fully show the topological structures of skeleton data. To address the limitations, GCN successfully applies to human action recognition, and its plausible mechanisms lead to more promising results <ref type="bibr" target="#b0">[1]</ref>. The traditional GCN models only consider physical connections between different joints on human actions. Since the endogenous factors, referring to physically disconnected joints, also play a preeminent role in human action recognition, the skeleton structure conveys much unrevealed information and leaves potential space to improve the performance of GCN <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph convolutional networks</head><p>Mapping the relational data into graphs, the topological structures can be encoded to model the connections among nodes, and provide more promising perspectives underlying the data. Inspiring by this mechanism, GCN is successfully implemented in deep learning research. The various approaches in modeling GCN fall into two categories, including spatial and spectral approaches. Spatial approaches use graph theory to define the nodes and edges for entities on data <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Interestingly, spectral approaches analyze the constructed graph in the frequency domain <ref type="bibr" target="#b13">[14]</ref>. The spectral approach usually leverages the Laplacian eigenvector to transform a graph in the time domain to in frequency domain, potentially resulting in large computation cost. Considering human action recognition, most of the methods choose the spatial approaches ... After the training process, the trained weighted matrices could be fully connected. Finally, the output is calculated by the Softmax classifier in each stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action video Skeleton graph Adjacent matrix</head><p>And then the scores are added by the multi-stream fusion to make the final prediction.</p><p>to construct the GCN due to the large size of skeleton data.</p><p>Since the human body is naturally formed as a graph, not a sequence or an image, the related features are easily extracted from skeleton data. However, their works only focus on the static graph structure and are hard to understand the dynamic information and graph features from human actions. Here, our model could adaptively extract the key information on joints, bones and body parts, which yields a dynamical GCN. Besides, the UNFGEF combines the high-order information to provide a new insight for learning human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph embedding methods</head><p>From the perspective of extracted features, graph embedding models can be classified into two groups: topological embedding methods and content enhanced embedding approaches. First, topological embedding methods encode topological structure features from the graph, and the learning objective is to preserve the topological features. A number of probabilistic models have been designed, such as node2vec <ref type="bibr" target="#b30">[31]</ref> and LINE <ref type="bibr" target="#b31">[32]</ref>. Recently deep learning models also implement the graph embedding features into their training process. These models investigate the first and second order graph features <ref type="bibr" target="#b32">[33]</ref>, or reconstruct the positive node features <ref type="bibr" target="#b33">[34]</ref> via different variants of autoencoders. Meanwhile, community detection, motif extraction, and embedding models for subgraph have made tremendous progress in learning nodes <ref type="bibr" target="#b15">[16]</ref>, edges <ref type="bibr" target="#b16">[17]</ref> or entire graphs <ref type="bibr" target="#b17">[18]</ref>. These methods search for groups of nodes that are well-connected internally while being relatively well-separated from the rest of the graph and typically limited to individual connected components. Second, content enhanced embedding models simultaneously explore topological information and content features, which fully considers all available information during the training process. Henneberg growth model explores massive and complicated topological structures in large-scale graphs <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, these graph embedding methods are hardly combined with graph convolutional networks for human action recognition. Our paper systematically explored available graph features of skeleton graphs, and encode them into graph convolutional architectures. The UNFGEF is different from the current learning representations of human action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPH EMBEDDING FEATURES</head><p>In this paper, three groups of graph features are extracted from the skeleton data, such as node features, edge features and subgraph features. These graph features play a significant role in highlighting the key joints, bones and body parts in human actions. Besides, these graph features elaborately and systematically reflects all information hidden in the skeleton structure. These graph embedding methods have been proposed in many empirical studies on graph learning representatives <ref type="bibr" target="#b20">[21]</ref>.</p><p>A skeleton graph could be modeled by an undirected and weighted network G(V, E), where V is the number of joints, and E is the number of bones, respectively. e ij is an edge between node i and j. In the skeleton graph, each bone is linked by two joints, such as the source node and the target node, respectively. Note that every edge has the length of the corresponding bone information. For example, given two node (joints) coordinates, the source node n i = (x i , y i , z i ) and the target node n j = (x j , y j , z j ), the length of the edge (bone) can be calculated</p><formula xml:id="formula_0">as d(n i , n j ) = (x j ? x i ) 2 + (y j ? y i ) 2 + (z j ? z i ) 2 .</formula><p>In this skeleton graph, there is no cycle, which indicates every edge can only contain a source node and a target node. In this way, the graph adjacent matrix, node features, edge features and subgraph features are calculated, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Node features</head><p>A score v ij is calculated by the node feature algorithm. The score is to measure the common feature between two nodes. Next, the extracted features are listed below:</p><formula xml:id="formula_1">? Common neighbors. It could be written as v CN ij = |?(i) ? ?(j)|,<label>(1)</label></formula><p>where ?(i) is the set of neighbors of node i and | ? | is the cardinality of set. The feature matrix is set toV 1 .</p><formula xml:id="formula_2">? Salton index. It is defined as v SA ij = |?(i) ? ?(j)| k i ? k j ,<label>(2)</label></formula><p>where k i is the degree of i. Its matrix is set toV 2 .</p><formula xml:id="formula_3">? Jaccard index. It is defined as v JI ij = |?(i) ? ?(j)| |?(i) ? ?(j)| .<label>(3)</label></formula><p>Its matrix is set toV 3 .</p><formula xml:id="formula_4">? Hub promoted index. It is defined as v HP I ij = |?(i) ? ?(j)| min(k i , k j ) .<label>(4)</label></formula><p>The calculated matrix is set toV 4 .</p><formula xml:id="formula_5">? Srensen Index. It is defined as v SRI ij = 2|?(i) ? ?(j)| k i + k j .<label>(5)</label></formula><p>Its matrix is set toV 5 .</p><formula xml:id="formula_6">? Adamic-Adar Index. It is defined as v AA ij = z??(i)??(j) 1 log kz .<label>(6)</label></formula><p>Its matrix is set toV 6 .</p><formula xml:id="formula_7">? Local path index. It is defined as v LP I ij = (A 2 ) ij + ?(A 3 ) ij ,<label>(7)</label></formula><p>where ? is a hyper parameter and set to 0.1. A is the adjacent matrix, and (A k ) ij is the number of path length k linking with the nodes i and j. Its matrix is set toV 7 .</p><formula xml:id="formula_8">? Katz index. It is defined as v KI ij = ?(A) ij + ? 2 (A 2 ) ij + ? 3 (A 3 ) ij + ? ? ? ,<label>(8)</label></formula><p>where ? is a parameter. If ? &lt; 1 ?max , it can be rewritten as</p><formula xml:id="formula_9">v KI ij = (1 ? ?A) ?1 ? I N ,<label>(9)</label></formula><p>where I N is the identity matrix, and ? max is the largest eigenvalue of A. Its matrix is set toV 8 .</p><formula xml:id="formula_10">? Preferential attachment index. It is defined as v P AI ij = |?(i)| ? |?(j)|.<label>(10)</label></formula><p>Its matrix is set toV 9 . ? Closeness centrality. It is defined as</p><formula xml:id="formula_11">v cc ij = N N j=1 d ij ,<label>(11)</label></formula><p>where d ij is the shortest path length between nodes i and j. Its matrix is set toV <ref type="bibr" target="#b9">10</ref> . Based on 10 mentioned methods, we identify the node features from skeleton graphs. Next, we use attention mechanism to encode 10 node features (feature matrices) into GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Edge features</head><p>In the skeleton graph, the bone can be served as an edge. Due to the different coordinates of each node, the distances of bones have a diverse length. Considering graph theory, the related algorithm is to identify the importance of edges in graph. Here, the edge features are calculated below:</p><p>? Betweenness centrality. It is defined as</p><formula xml:id="formula_12">e BC ij = (l,q) =(i,j) S lq (e ij )d lq (e ij ) S lq d lq ,<label>(12)</label></formula><p>where e ij is the edge weight between node i and node j, S lq is the number of all existing shortest paths from node l to node q, S lq (e ij ) is the number of all shortest paths from node l to node q that pass through edge e ij , d lq is the total distance length from node l to node q, and d lq (e ij ) is the total distance length from node l to node q that pass through edge e ij . Its matrix is set to B 1 . ? Degree centrality. It is defined as</p><formula xml:id="formula_13">e DC ij = e ij k e ik .<label>(13)</label></formula><p>Its matrix is set to B 2 . ? Similarity centrality. It is defined as</p><formula xml:id="formula_14">e SC ij = f (i, j)e ij k f (i, k)e ik ,<label>(14)</label></formula><p>where f (i, j) is the similarity score function for node i and j. Its matrix is set to B 3 . Three edge features could yield related weighted matrices. With frame changing, the weighted matrices automatically update. Additionally, these edge features play key roles in reflecting bone characteristics in human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Subgraph features</head><p>Different body parts are associated with completing human actions. However, there exists a problem of how to evaluate the influence of every body part during human actions and measure the correlations between two body parts. In graph theory, the body part could be considered as a subgraph. A subgraph S is a graph whose node set N (S) is a subset of the node set N (G), i.e. N (S) ? N (G). Similarly, the subgraph edge set E(S) is a subset of the edge set E(G), that is E(S) ? E(G). The subgraph features are extracted by the listing algorithms:</p><p>? Clustering coefficient. It is defined as</p><formula xml:id="formula_15">s CC i = 2L i k i (k i ? 1) ,<label>(15)</label></formula><p>where L i is the number of edges between neighbors of node i. Its matrix is set to S 1 . ? Motif index. It is defined as</p><formula xml:id="formula_16">s MI i = M 1 i + M 2 i + M 3 i + ? ? ? ,<label>(16)</label></formula><p>where M k i denotes the frequency of node i in subgraphs M k , and k is the number of nodes in subgraph. Its matrix is set to S 2 . The associations with subgraphs are measured by every node, that is, a weighted matrix with N ? N dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architecture</head><p>Shown in <ref type="figure">Fig. 1</ref>, the pipeline of this network architecture contains four components, namely graph generator, feature encoder, GCN-TCN blocks, and multi-stream fusion. In the first component, the original action video would be transferred into the skeleton data. Then the adjacent matrix is generated from the skeleton data and is encoded into three streams for training and prediction. Second, the feature encoder calculated 15 different features mentioned in the above sections. These features are categorized into three domains, namely node features, edge features, and subgraph features. These extracted features would be trained by the GCN-TCN blocks, which could take the best advantages of the skeleton data to learn more information. The GCN-TCN blocks play key roles in training and updating parameters. Finally, the framework would fuse the output scores of the softmax layers from the three streams. In the following sections, the last two components of the proposed network architecture are elaborated in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GCN-TCN blocks</head><p>One basic GCN-TCN block is composed of two layers, namely the GCN layer and TCN layer, based on ST-GCN model <ref type="bibr" target="#b0">[1]</ref>. Its pipeline is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Compared to the original GCN layer, the GCN layer has 5 layers, and appends two more layers, including the dropout layer and aggregation layer. The next TCN layer has 3 layers, namely convolution, batch normalization, and RELU layers. Each GCN-TCN block adds a residual connection to avoid gradient vanishing. Let x ? R N be a feature vector for every node in a graph. The spectral convolution on a graph can be formulated as</p><formula xml:id="formula_17">Z =D ? 1 2?D ? 1 2 X? = AX?,<label>(17)</label></formula><p>where A =D ? 1</p><formula xml:id="formula_18">2?D ? 1 2 = I N ?N + D ? 1 2 AD ? 1 2</formula><p>(A is considered as its adjacency matrix with N ? N dimensions), D i = j? ij , X ? R N ?C and ? ? R C?F (C is the dimensions of feature vector per node and F is the filter channels). Note that Z is the convolved or extracted feature matrix, and ? is the filtering-parameters matrix. Thus, the complexity of this normalized formulation is O(EF C).</p><p>To implement the UNFGEF in human action recognition, let X = X in be the input feature matrix on total frames per one video sample. For encoding graph embedding features, A of the Eq. 17 would be modified. For example, the node features could be encoded by this method</p><formula xml:id="formula_19">Z 1 = ( A + J)X in ? 1 ,<label>(18)</label></formula><p>where J = kV k is 10 node features, andV k means the extracted node feature matrix, like Salton index and Jaccard index, etc. Z 1 and ? 1 are the output and parameters of the node stream, respectively.</p><p>Similar to node features, let B be 3 edge features and S be 2 subgraph features. The Eq. 18 could be rewritten as</p><formula xml:id="formula_20">Z 2 = ( A + B)X in ? 2 ,<label>(19)</label></formula><formula xml:id="formula_21">Z 3 = ( A + S)X in ? 3 ,<label>(20)</label></formula><p>where B = k B k , S = k S k , and B k and S k represent the extracted edge and subgraph feature matrices, respectively. Z 2 and ? 2 are the output and parameters of the edge stream, whereas Z 3 and ? 3 are the output and parameters of the subgraph stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-stream Fusion</head><p>Compared to previous works, the three streams, such as node, edge, and subgraph streams, consist of 15 graph features, which is the first to systematically investigate the skeleton structures and uncover neglected information. The three streams are calculated from mentioned graph embedding features. Then, 15 graph embedding features pass about 10 GCN-TCN blocks. Each softmax score is calculated by</p><formula xml:id="formula_22">P 1 = Sof tmax (ReLU (Z 1 )) ,<label>(21)</label></formula><formula xml:id="formula_23">P 2 = Sof tmax (ReLU (Z 2 )) ,<label>(22)</label></formula><formula xml:id="formula_24">P 3 = Sof tmax (ReLU (Z 3 )) ,<label>(23)</label></formula><p>where Z 1 , Z 2 , Z 3 are outputs of node, edge, and subgraph streams, respectively. The multi-class cross-entropy loss is written as</p><formula xml:id="formula_25">L = ? i?yL Y i log(p i ),<label>(24)</label></formula><p>where p i is the predicted probability of class i and y L is the number of action classes. if Y = i , Y i is set as 1, otherwise it is 0. Fused by three softmax scores, the final score can be obtained</p><formula xml:id="formula_26">R = P 1 + P 2 + P 3 ,<label>(25)</label></formula><p>where P 1 , P 2 , P 3 are softmax scores of node, edge, and subgraph streams, respectively. The final score could yield the score ranking and predict the action classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNFGEF design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-process layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message passing layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-process layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-layer design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-layer design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT</head><p>To evaluate the proposed framework, i.e. UNFGEF, we conduct the experiments on three large-scale action recognition datasets: NTU-RGB+D <ref type="bibr" target="#b22">[23]</ref>, Kinetics <ref type="bibr" target="#b21">[22]</ref> and SYSU-3D <ref type="bibr" target="#b36">[37]</ref>. First, since the size of the NTU-RGB+D dataset is smaller than that of the Kinetics dataset, we perform the ablation studies of our model on the NTU-RGB+D dataset, which examines every component of the proposed model on the action recognition performance. Then, the performance of UNFGEF is verified and compared with that of state-of-the-art approaches.</p><p>A. Datasets 1) NTU-RGB+D: NTU-RGB+D is currently one of the largest dataset widely used in the skeleton-based action recognition, which consists of 56,880 video samples categorized into 60 action classes. All of video samples are acted by 40 volunteers, who come from different age groups from 10 to 35. Each action is recorded by three Kinect V2 cameras concurrently from different horizontal angles, like ?45 ? , 0 ? , 45 ? . This dataset contains 3D skeletal data for each video sample. Here, it records 25 joints for each frame on each subject sample, while each video sample has one or two subjects. The dataset recommends two benchmarks. The first one is the cross-subject (X-sub), which is composed of a training set (40,320 video samples) and a testing set <ref type="bibr">(16,560 video samples)</ref>. The second one is the cross-view (X-view), where the training set contains 37,920 video samples captured by cameras No. 2 and No. 3, and the testing set contains 18,960 video samples captured by camera No. 1. In the comparison, we report the top-1 accuracy on two benchmarks.</p><p>2) Kinetics: Kinetics is another large-scale dataset for human action recognition, containing 300,000 video samples. All of video samples cover 400 human action classes. The video samples are recorded by YouTube and have various subjects. It only contains RGB video samples without raw skeleton data. We obtain 18 joints' positions of every frame using the publicly available OpenPose toolbox. The captured skeleton data contains two dimensions of coordinates, i.e., (x, y), and the confidence score (c) for each joint. The dataset consists of a training set (240,000 video samples) and a testing set (20,000 video samples). Following the evaluation method, we report the top-1 and top-5 accuracy on the benchmark.</p><p>3) SYSU-3D: SYSU-3D is a dataset recorded by Kinetic camera. It consists of 480 skeleton video samples of 12 action categories performed by 40 subjects. Each video sample has 20 joints. There are two standard evaluation cases for this dataset, i.e., cross-subject (CS) setting and same-subject (SS) setting. For the cross-subject setting, half of the subjects are used for training and the rest are for testing. For the same subject setting, half of the samples from each activity are used for training and the rest are for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training details</head><p>UNFGEF is implemented on the PyTorch deep learning framework. The kernel size of this model is set to 4. And it contains three streams to train the skeleton data. The optimization method uses the stochastic gradient descent with Nesterov momentum (0.9). And the loss function chooses the cross-entropy function for back propagating gradients. The batch size is 32 or 64. The learning rate is also set to 0.1 and the decay rate is 0.0001. The UNFGEF model is trained on 2 GTX-1080Ti GPUs. For different datasets, we set the specific configurations of UNFGEF.</p><p>The number of GCN-TCN block are 10. The numbers of output channels are 64, 64, 64, 64, 128, 128, 128, 256, 256 and 256 in each block. Note that the drop rate of GCN layers is set to 0.4. At the beginning of 10 basic blocks, there exists a batch normalization layer, which normalizes the input skeleton data. Through 10 basic blocks, it is followed by a global average pooling layer, adjusting different feature dimensions of video samples into the same dimensions. Finally, the final result is calculated by the Softmax layer to yield the classification results.</p><p>For the NTU-RGB+D dataset, each video sample contains no more than two persons. The feature dimensions are 64, 128 and 256, respectively. The number of frames in each video sample is set to 300. If the frame number is less than 300 frames, we repeat the video sample until it reaches 300 frames. In Eq. 18, X in should be the total features from 300 frames. The training process ends at the 50th epoch.</p><p>For the Kinetics dataset, the input configuration of Kinetics is set to 150 frames with 2 persons in each video sample. Here, we randomly select 150 frames from the input skeleton data, and disorder the joint coordinates with randomly chosen translations and rotations. Here, X in should be the total features from 150 frames in Eq. 18. The training process converges to the 60th epoch.</p><p>For the SYSU-3D dataset, it contains one person per each frame, and has two standard evaluation protocols, i.e., Setting-1 (cross-subject) and Setting-2 (same-subject). We randomly select 200 frames from the input skeleton data, and disorder the joint coordinates with randomly chosen translations and rotations. The training process stops at the 60th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study</head><p>We examine the influence of each graph embedding feature in UNFGEF with the X-view benchmark on the NTU-RGB+D dataset. The performance of ST-GCN on the NTU-RGB+D dataset is 88.3%. By highlighting 15 graph embedding features with multi-stream framework, the result is improved to 96.4%, which brings a definite improvement. The detail is introduced in the below parts. 1) Graph embedding features: As mentioned in Section 3, there are three types of graph embedding features in the UNFGEF framework, i.e., node features J, edge features B, and subgraph features S. Here, we only investigate effects of these graph features. The different combinations of the graph features are tested on the UNFGEF framework, including UNFGEF only with J, UNFGEF only with B, UNFGEF only with S, UNFGEF with J and B, UNFGEF with J and S, UNFGEF with B and S, and UNFGEF with J, B and S. And the results are shown in <ref type="table" target="#tab_2">Table I</ref>. UNFGEF (J+B+S) outperforms the baseline model ST-GCN by 8.2% on the Xview NTU-RGB+D dataset, which strongly demonstrate the importance of graph embedding features. The results show that each graph feature extracted from the graph is significant for human action recognition. Besides, deleting any one of streams will dramatically reduce the performance. Shown in <ref type="figure" target="#fig_1">fig. 3</ref>, the accuracy of our new framework achieves more than 93.0% on the majority of action classes, except A.10 to A.12 and A.16. Most importantly, the node features have a greater impact than edge features and subgraph features in the UNFGEF framework. If the UNFGEF framework only has subgraph features, the performance is not good. After combining all graph features together, we can utilize all in information and achieve better outcomes. The result indicates that the learned graph features are important, which also demonstrates the significance of the node features. The confusion matrices of UNFGEF on 60 action classes of the NTU-RGB+D dataset are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The performance on the cross-view benchmark is better than that on the crosssubject benchmark. 2) Visualization of the centrality graphs.: <ref type="figure" target="#fig_3">Fig. 5</ref> is a visualization of three types of graph embedding features for one layer on the same action, i.e. node features, edge features, and subgraph features, respectively. The skeleton graphs are plotted from the physical connections of the human body. The red joints, bones, and body parts represent the highlighted parts by these graph features. In the drinking-water action class, the highlights of body parts move from the left arm to the upper body. For the standing-up action, the low trunk and upper legs play the key roles during the action. It indicates that the node and edge of the human trunk play significant roles in this action. Besides, it also suggests that a traditional skeleton graph is not the best choice for the action recognition task, and different actions need graphs with different skeleton structures. The node features pay more attention to the adjacent joints in the physical skeleton graph. For the subgraph features, the neck part and the hipbone part are detected to have a stronger connection, although they are far away from each other in the physical skeleton. Compared to previous works, these extracted graph features clearly capture the low-level features, like key joints and bones, and high-level structures, namely body parts. This information could uncover endogenous factors in human actions and give new perspectives on human action recognition and graph convolutional networks. Thus, graph embedding features are more relevant to the action classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the state-of-the-art</head><p>We compare the performance of our model with state-of-theart action recognition methods on the NTU-RGB+D dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b47">[48]</ref> , Kinetics dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b49">[50]</ref> , and SYSU dataset <ref type="bibr" target="#b50">[51]</ref>- <ref type="bibr" target="#b53">[54]</ref>. The results of three comparisons are shown in <ref type="table" target="#tab_2">Table II, Table III and Table IV,</ref> respectively. The methods used for comparison are categorized as RNN-based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, CNN-based methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b46">[47]</ref>, and GCN-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Our model outperforms state-of-the-art methods with a large margin on these datasets, which validates the superiority of our model. For the NTU-RGB+D dataset, the proposed UNFGEF framework achieves 8.9% and 8.2% improvements over the most influence method, i.e. ST-GCN, on the cross-subject and cross-view benchmarks, respectively. In <ref type="bibr" target="#b47">[48]</ref>, the authors encode joint and bone features into GCN and exploit two-stream framework for skeletonbased human action recognition. Compared to this model, our model outperforms it by 1.9% and 1.4% on both benchmarks. In <ref type="table" target="#tab_2">Table III,</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Setting-1 (%) Setting-2 (%) VA-RNN(aug.) <ref type="bibr" target="#b50">[51]</ref> 80.5 79.7 VA-CNN(aug.) <ref type="bibr" target="#b50">[51]</ref> 85.1 84.0 EleAttG <ref type="bibr" target="#b51">[52]</ref> 85.7 85.7 SGN <ref type="bibr" target="#b52">[53]</ref> 86.9 86.5 SLnL-rFA+ML <ref type="bibr" target="#b53">[54]</ref> 88. ods, such as ST-GCN <ref type="bibr" target="#b0">[1]</ref>, 2s-AGCN <ref type="bibr" target="#b47">[48]</ref>, and AS-GCN <ref type="bibr" target="#b49">[50]</ref>. Compared to 2s-AGCN <ref type="bibr" target="#b47">[48]</ref>, the accuracy of the proposed model relatively increases by 1.5% and 1.8% on Top-1 and Top-5 benchmarks. Our model also achieves state-of-the-art performance on the Kinetics-Skeleton dataset. On the SYSU dataset, the recognition results are listed in <ref type="table" target="#tab_2">Table IV</ref>. Even though SLnL-rFA+ML <ref type="bibr" target="#b53">[54]</ref> achieves to 88.3% and 88.1% on setting-1 and setting-2 benchmarks, the UNFGEF model raises by 3.8% and 3.4%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this work, we proposed a novel framework of unifying graph embedding features (UNFGEF) with graph convolutional networks for human action recognition. UNFGEF captures the key joints, bones, and body parts from human actions and embeds them into graph convolution networks to adaptively learn fusion features. On the contrary, the existing methods always ignore the importance of graph information on skeleton data, i.e., node features, bone features, and subgraph features. However, these embedding features could show the low-level features and high-level information of the skeleton data. Our UNFGEF uses a multi-stream framework to explicitly exploit 15 graph embedding structures, which improves the performance significantly. UNFGEF is validated on three large-scale datasets, namely NTU-RGB+D, Kinetics, and SYSU-3D, and outperforms state-of-the-art methods on all the three datasets. The finding indicates that these extracted graph features are fundamental mechanisms and factors hidden in the skeleton topology. This novel framework provides some new insights on future studies of skeleton-based action recognition. For example, how to incorporate the correlations between adjacent frames, such as similarities, distances, and structures into the UNFGEF framework becomes a natural question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The layer design of UNFGEF. It contains two types of layers, such as inter-layer design and intra-layer design. In inter-layer design, there are three steps, including pre-processing layers, message passing layers and post-processing layers. The core layer is the message passing layer, consisting of multiple GCN-TCN blocks. For intra-layer design, it shows the details of GCN and TCN layers. Compared to TCN layer, GCN layer contains dropout and aggregation layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The ablation study of four methods on 20 action classes. The four methods are UNFGEF only with J, UNFGEF only with B, UNFGEF only with S, and UNFGEF with J, B and S. The x axis is the top-1 accuracy on the X-view NTU-RGB+D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Cross-subject NTU-RGB+D (b) Cross-view NTU-RGB+D Confusion matrices of the UNFGEF framework on the NTU RGB+D dataset. We can see that the most significant classification errors occur among classes that are physically similar. (a) Cross-subject evaluation. (b) Cross-view evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>UNFGEF surpasses recent GCN-based meth-Visualization of weighted skeleton graphs during training process on the NTU RGB+D dataset. The size of nodes and edges indicates its parameter weight. From left to right, skeleton graphs reflect the node features, edge features, and subgraph features, respectively. Each sequence visualizes an example of an action in NTU RGB+D. (a) Drink water. (b) Standing up (from sitting position). Three frames are respectively selected at t = 0, t = 1, t = T . Each skeleton graph contains 18 key joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Node features Edge features Subgraph features 11 features 3Z2 Z1 Softmax Predicted Scores FC layer UNFGEF Data generator Feature encoder GCN-TCN blocks Multi-stream fusion FC layer FC layer</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Z1-11</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Z1-1</cell><cell>Z1-2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Z2-3</cell></row><row><cell>features</cell><cell></cell><cell></cell><cell cols="2">Z2-1</cell><cell>Z2-2</cell></row><row><cell>2 features</cell><cell></cell><cell></cell><cell cols="2">Z3-1</cell><cell>Z3-2</cell><cell>Z3</cell></row><row><cell>GCN</cell><cell>TCN</cell><cell>GCN</cell><cell>TCN</cell></row></table><note>Fig. 1. The network architecture of UNFGEF model. The action video could be divided into each frame. Then, the related skeleton graph is built by each frame. We use adjacent matrix to represent the skeleton graph. Based on graph embedding algorithms, various graph features are extracted and categorized into three domains, such as node features, edge features and subgraph features. Next, these features are separately embedded into GCN and TCN networks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISONS</head><label>I</label><figDesc>OF THE TOP-1 ACCURACY ON THE X-VIEW NTU-RGB+D DATASET WHEN THE DIFFERENT COMBINATIONS OF GRAPH EMBEDDING FEATURES ARE TESTED ON UNFGEF.</figDesc><table><row><cell>Methods</cell><cell>Accuracy(%)</cell></row><row><cell>ST-GCN</cell><cell>88.3</cell></row><row><cell>UNFGEF(J)</cell><cell>95.7</cell></row><row><cell>UNFGEF(B)</cell><cell>94.9</cell></row><row><cell>UNFGEF(S)</cell><cell>94.1</cell></row><row><cell>UNFGEF(J+B)</cell><cell>95.9</cell></row><row><cell>UNFGEF(J+S)</cell><cell>95.7</cell></row><row><cell>UNFGEF(B+S)</cell><cell>94.9</cell></row><row><cell cols="2">UNFGEF(J+B+S) UNFGEF(J+B+S) UNFGEF(J+B+S) 96.5 96.5 96.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISONS</head><label>II</label><figDesc>OF THE TOP-1 ACCURACY WITH STATE-OF-THE-ART METHODS ON THE NTU-RGB+D DATASET. THE-ART METHODS ON THE KINETICS-SKELETON DATASET.</figDesc><table><row><cell>Methods</cell><cell>X-sub (%)</cell><cell>X-view (%)</cell></row><row><cell>Deep LSTM [23]</cell><cell>60.7</cell><cell>67.3</cell></row><row><cell>2s-3DCNN [38]</cell><cell>66.8</cell><cell>72.6</cell></row><row><cell>ST-LSTM [39]</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell>STA-LSTM [3]</cell><cell>73.4</cell><cell>81.2</cell></row><row><cell>VA-LSTM [40]</cell><cell>79.2</cell><cell>87.7</cell></row><row><cell>ARRN-LSTM [41]</cell><cell>80.7</cell><cell>88.8</cell></row><row><cell>Ind-RNN [42]</cell><cell>81.8</cell><cell>88.0</cell></row><row><cell>TCN [11]</cell><cell>74.3</cell><cell>83.1</cell></row><row><cell>C-CNN+MTLN [43]</cell><cell>79.6</cell><cell>84.8</cell></row><row><cell>Synthesized CNN [44]</cell><cell>80.0</cell><cell>87.2</cell></row><row><cell>ST-GCN [1]</cell><cell>81.5</cell><cell>88.3</cell></row><row><cell cols="2">CNN-Motion+Trans [45] 83.2</cell><cell>89.3</cell></row><row><cell>ResNet152 [46]</cell><cell>85.0</cell><cell>92.3</cell></row><row><cell>DPRL+GCNN [47]</cell><cell>83.5</cell><cell>89.8</cell></row><row><cell>2s-AGCN [48]</cell><cell>88.5</cell><cell>95.1</cell></row><row><cell>UNFGEF (ours) UNFGEF (ours) UNFGEF (ours)</cell><cell>90.4 90.4 90.4</cell><cell>96.5 96.5 96.5</cell></row><row><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="3">COMPARISONS OF THE TOP-1 AND TOP-5 ACCURACY WITH</cell></row><row><cell>STATE-OF-Methods</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell>Feature Encoding [49]</cell><cell>14.9</cell><cell>25.8</cell></row><row><cell>Deep LSTM [23]</cell><cell>16.4</cell><cell>35.3</cell></row><row><cell>TCN [11]</cell><cell>20.3</cell><cell>40.0</cell></row><row><cell>ST-GCN [1]</cell><cell>30.7</cell><cell>52.8</cell></row><row><cell>AS-GCN [50]</cell><cell>34.8</cell><cell>56.3</cell></row><row><cell>2s-AGCN [48]</cell><cell>36.1</cell><cell>58.7</cell></row><row><cell>UNFGEF (ours) UNFGEF (ours) UNFGEF (ours)</cell><cell>37.6 37.6 37.6</cell><cell>60.5 60.5 60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISONS</head><label>IV</label><figDesc>OF STATE-OF-THE-ART METHODS ON THE SYSU-3D DATASET.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tangent fisher vector on matrix manifolds for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3052" to="3064" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action-attending graphic neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3657" to="3670" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action-stage emphasized spatiotemporal vlad for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2799" to="2812" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">View-invariant deep architecture for human action recognition using two-stream motion and shape temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dhiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Vishwakarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3835" to="3844" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Memory attention networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08254</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multidomain and multi-task learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="853" to="867" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward efficient action recognition: Principal backpropagation for training two-stream networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1773" to="1782" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">View-invariant human action recognition based on a 3d bio-constrained skeleton model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3959" to="3972" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sta-cnn: Convolutional spatial-temporal attention learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5783" to="5793" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01189</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sub2vec: Feature learning for subgraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Compositionbased multi-relational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03082</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motifnet: a motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Data Science Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evolvegcn: Evolving graph convolutional networks for dynamic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schardl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Subgraph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8017" to="8029" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Link weight prediction using supervised learning methods and its application to yelp layered network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1507" to="1518" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pose flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00977</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04724</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning class regularized features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02651</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive rnn tree for large-scale human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1444" to="1452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatio-temporal naive-bayes nearestneighbor (st-nbnn) for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4171" to="4180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with multi-stream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9532" to="9545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international on conference on information and knowledge management</title>
		<meeting>the 24th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Henneberg growth of social networks: modeling the facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="701" to="712" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Controllability robustness of henneberg-growth complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Two-stream 3d convolutional neural network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Skeletonbased relational modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02556</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actionalstructural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1963" />
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semanticsguided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Joint learning in the spatio-temporal and frequency domains for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2207" to="2220" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>PLACE PHOTO HERE Dong Yang Biography text here</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mengqi Li Biography text here</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Jicong Fan Biography text here</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Zhao Zhang Biography text here</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
