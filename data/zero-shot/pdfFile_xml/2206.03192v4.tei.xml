<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Data Distribution Iteration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changnan</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Data Distribution Iteration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To obtain higher sample efficiency and superior final performance simultaneously has been one of the major challenges for deep reinforcement learning (DRL). Previous work could handle one of these challenges but typically failed to address them concurrently. In this paper, we try to tackle these two challenges simultaneously. To achieve this, we firstly decouple these challenges into two classic RL problems: data richness and exploration-exploitation trade-off. Then, we cast these two problems into the training data distribution optimization problem, namely to obtain desired training data within limited interactions, and address them concurrently via i) explicit modeling and control of the capacity and diversity of behavior policy and ii) more fine-grained and adaptive control of selective/sampling distribution of the behavior policy using a monotonic data distribution optimization. Finally, we integrate this process into Generalized Policy Iteration (GPI) and obtain a more general framework called Generalized Data Distribution Iteration (GDI). We use the GDI framework to introduce operator-based versions of well-known RL methods from DQN to Agent57. Theoretical guarantee of the superiority of GDI compared with GPI is concluded. We also demonstrate our state-ofthe-art (SOTA) performance on Arcade Learning Environment (ALE), wherein our algorithm has achieved 9620.33% mean human normalized score (HNS), 1146.39% median HNS and surpassed 22 human world records using only 200M training frames. Our performance is comparable to Agent57's while we consume 500 times less data. We argue that there is still a long way to go before obtaining real superhuman agents in ALE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>To obtain higher sample efficiency and superior final performance simultaneously has been one of the major challenges for deep reinforcement learning (DRL). Previous work could handle one of these challenges but typically failed to address them concurrently. In this paper, we try to tackle these two challenges simultaneously. To achieve this, we firstly decouple these challenges into two classic RL problems: data richness and exploration-exploitation trade-off. Then, we cast these two problems into the training data distribution optimization problem, namely to obtain desired training data within limited interactions, and address them concurrently via i) explicit modeling and control of the capacity and diversity of behavior policy and ii) more fine-grained and adaptive control of selective/sampling distribution of the behavior policy using a monotonic data distribution optimization. Finally, we integrate this process into Generalized Policy Iteration (GPI) and obtain a more general framework called Generalized Data Distribution Iteration (GDI). We use the GDI framework to introduce operator-based versions of well-known RL methods from DQN to Agent57. Theoretical guarantee of the superiority of GDI compared with GPI is concluded. We also demonstrate our state-ofthe-art (SOTA) performance on Arcade Learning Environment (ALE), wherein our algorithm has achieved 9620.33% mean human normalized score (HNS), 1146.39% median HNS and surpassed 22 human world records using only 200M training frames. Our performance is comparable to Agent57's while we consume 500 times less data. We argue that there is still a long way to go before obtaining real superhuman agents in ALE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GDI-I3 (Ours)</head><p>GDI-H3 <ref type="formula">(</ref>  <ref type="figure">Figure 1</ref>. Performance of algorithms of Atari 57 games on mean HNS(%) and corresponding learning/sample efficiency calculated by <ref type="bibr">Mean HNS Training Scale (frames)</ref> . For more benchmark results, can see App. J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning (RL) algorithms, when combined with high-capacity deep neural networks, have shown promise in domains ranging from video games <ref type="bibr" target="#b12">(Mnih et al., 2015)</ref> to robotic manipulation <ref type="bibr" target="#b21">(Schulman et al., 2015;</ref><ref type="bibr" target="#b23">2017b)</ref>. However, it still suffers from high sample complexity and unsatisfactory final performance, especially compared to human learning <ref type="bibr" target="#b27">(Tsividis et al., 2017)</ref>. Prior work could handle one of these problems but commonly failed to tackle both of them simultaneously.</p><p>Model-free RL methods typically obtain remarkable final performance via finding a way to encourage exploration and improve the data richness (e.g., Seen Conditions All Conditions ) that guarantees traversal of all possible conditions. These methods <ref type="bibr">Badia et al., 2020a)</ref> could perform remarkably well when interactions are (nearly) limitless but normally fail when interactions are limited. We argue that when interactions are limited, finding a way to guarantee traversal of all unseen conditions is unreasonable, and perhaps we should find a way to traverse the nontrivial conditions (e.g., unseen  and high-value <ref type="bibr" target="#b10">(Kumar et al., 2020)</ref>) first and avoid traversing the trivial/low-value conditions repeatedly. In other words, we should explicitly control the training data distribution in RL and maximize the probabil-arXiv:2206.03192v4 <ref type="bibr">[cs.</ref>LG] 20 Jun 2022 The Isomorphism architecture of GDI, wherein the behavior policy space (e.g., the soft entropy policy space, ? ? ? = ? Softmax</p><formula xml:id="formula_0">A ? 1 ? 1 + (1 ? ) ? Softmax A ? 2 ? 2</formula><p>) is constructed by the base policy with shared parameters (i.e., ?1 = ?2 = ?) and indexed by ? = (?1, ?2, ). (b) The Heterogeneous architecture of GDI, wherein the behavior policy space is constructed by the base policy with different parameters (i.e., ?1 = ?2) and indexed by ?. For more details, can see <ref type="bibr">Sec. 4 and 5.</ref> ity of nontrivial conditions being traversed, namely the data distribution optimization (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>In RL, training data distribution is normally controlled by the behavior policy <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018;</ref><ref type="bibr" target="#b12">Mnih et al., 2015)</ref>, so that the data richness can be controlled by the capacity and diversity of the behavior policy. Wherein the capacity describes how many different behavior policies there are in the policy space, and the diversity describes how many different behavior policies are selected/sampled from the policy space to generate training data (discussed in <ref type="bibr">Sec. 4.2)</ref>. When interactions are limitless, increasing the capacity and maximizing the diversity via randomly sampling behavior policies (most prior works have achieved SOTA in this way) can significantly improve the data richness and guarantee traversal of almost all unseen conditions, which induces better final performance <ref type="bibr">(Badia et al., 2020a)</ref> and generalization <ref type="bibr">(Ghosh et al., 2021)</ref>. However, perhaps surprisingly, this is not the case when interactions are limited, where each interaction is rare and the selection of the behavior policy becomes important. In conclusion, we should increase the probability of the traversal of unseen conditions (i.e., exploration) via increasing the capacity and diversity of the behavior policy and maximize the probability of high-value conditions (i.e., exploitation) being traversed via optimizing the selective distribution of the behavior policy. It's also known as the exploration-exploitation trade-off problem.</p><p>From this perspective, we can understand why the prior SOTA algorithms, such as Agent57 and Go-Explore, failed to obtain high sample efficiency. They have collected massive data to guarantee the traversal of unseen conditions but ignore the different values of data. Therefore, they wasted many trials to collect useless/low-value data, which accounts for their low sample efficiency. In other words, they failed to tackle the data distribution optimization problem.</p><p>In this paper, we argue that the sample efficiency of modelfree methods can be significantly improved (even outperform the SOTA model-based schemes <ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>) without degrading the final performance via data distribution optimization. To achieve this, we propose a data distribution optimization operator E to iteratively optimize the selective distribution of the behavior policy and thereby optimize the training data distribution. Specifically, we construct a parameterized policy space indexed by ? called the soft entropy space, which enjoys a larger capacity than Agent57. The behavior policies are sampled from this policy space via a sampling distribution. Then, we adopt a meta-learning method to optimize the sampling distribution of behavior policies iteratively and thereby achieve a more fine-grained exploration and exploitation trade-off. Moreover, training data collected by the optimized behavior policies will be used for RL optimization via the operator T . This process will be illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, generalized in Sec. 4, proved superior in Sec. 4.6 and implemented in Sec. 5.1.</p><p>The main contributions of our work are:</p><p>1. A General RL Framework. Efficient learning within limited interactions induces the data distribution optimization problem. To tackle this problem, we firstly explicitly control the diversity and capacity of the behavior policy (see <ref type="bibr">Sec. 4</ref>.2) and then optimize the sampling distribution of behavior policies iteratively via a data distribution optimization operator (see <ref type="bibr">Sec. 4.4)</ref>. After integrating them into GPI, we obtain a general RL framework, GDI (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Data richness. As claimed by <ref type="bibr">(Ghosh et al., 2021)</ref>, generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully observed MDPs into POMDPs, which makes generalization in RL much more difficult. Therefore, data richness (e.g., Seen Conditions All Conditions ) is vital for the generalization and performance of RL agents. When interactions are limited, more diverse behavior policies increase the data richness and thereby reduce the proportion of unseen conditions and improve generalization and performance. Therefore, we can recast this problem into the problem to control the capacity and diversity of the behavior policy. There are two promising ways to handle this issue. Firstly, some RL methods adopt intrinsic reward to encourage exploration, where unsupervised objectives, auxiliary tasks and other techniques induce the intrinsic reward <ref type="bibr" target="#b15">(Pathak et al., 2017)</ref>. Other methods <ref type="bibr">(Badia et al., 2020a)</ref> introduced a diversity-based regularizer into the RL objective and trained a family of policies with different degrees of exploratory behaviors. Despite both obtaining SOTA performance, adopting intrinsic rewards and entropy regularization has increased the uncertainty of environmental transition. We argue that the inability to effectively tackle the data distribution optimization accounts for their low learning efficiency.</p><p>Exploration and exploitation trade-off. Exploration and exploitation trade-off remains one of the significant challenges in DRL <ref type="bibr">(Badia et al., 2020b;</ref><ref type="bibr" target="#b25">Sutton &amp; Barto, 2018)</ref>. In general, methods that guarantee to find an optimal policy require the number of visits to each state-action pair to approach infinity. The entropy of policy would collapse to zero swiftly after a finite number of steps may never learn to act optimally; they may instead converge prematurely to suboptimal policies and never gather the data they need to learn to act optimally. Therefore, to ensure that all state-action pairs are encountered infinitely, off-policy learning methods are widely used <ref type="bibr" target="#b13">(Mnih et al., 2016;</ref><ref type="bibr">Espeholt et al., 2018)</ref>, and agents must learn to adjust the entropy (exploitation degree) of the behavior policy. Adopting stochastic policies into the behavior policy has been widely used in RL algorithms <ref type="bibr" target="#b12">(Mnih et al., 2015;</ref><ref type="bibr" target="#b3">Hessel et al., 2017)</ref>, such as the -greedy <ref type="bibr" target="#b31">(Watkins, 1989)</ref>. These methods can perform remarkably well in dense reward scenarios <ref type="bibr" target="#b12">(Mnih et al., 2015)</ref>, but fail to learn in sparse reward environments. Recent approaches <ref type="bibr">(Badia et al., 2020a)</ref> have proposed to train a family of policies and provide intrinsic rewards and entropy regularization to agents to drive exploration. Among these methods, the intrinsic rewards are proportional to some notion of saliency, quantifying how different the current state is from those already visited. They have achieved SOTA performance at the cost of a relatively lower sample efficiency. We argue that these algorithms overemphasize the role of exploration to traverse unseen conditions but ignore the value of data and thereby waste many trails to collect low-value data, accounting for their low sample efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>The RL problem can be formulated as a Markov Decision Process <ref type="bibr">(Howard, 1960, MDP)</ref> defined by (S, A, p, r, ?, ? 0 ). Considering a discounted episodic MDP, the initial state s 0 is sampled from the initial distribution ? 0 (s) : S ? ?(S), where we use ? to represent the probability simplex. At each time t, the agent chooses an action a t ? A according to the policy ?(a t |s t ) : S ? ?(A) at state s t ? S. The environment receives a t , produces the reward r t ? r(s, a) : S ? A ? R and transfers to the next state s t+1 according to the transition distribution p (s | s, a) : S ? A ? ?(S). The process continues until the agent reaches a terminal state or a maximum time step. Define the discounted state visitation distribution as d ? ?0 (s) = (1 ? ?)E s0??0 [ ? t=0 ? t P(s t = s|s 0 )]. The goal of reinforcement learning is to find the optimal policy ? * that maximizes the expected sum of discounted rewards, denoted by J <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018)</ref>:</p><formula xml:id="formula_1">? * = argmax ? E st?d ? ? 0 E ? ? k=0 ? k r t+k |s t<label>(1)</label></formula><p>where ? ? (0, 1) is the discount factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Notation Definition</head><p>Let's introduce our notations first, which are also summarized in App. A.</p><p>Define ? to be an index set, ? ? R k . ? ? ? is an index in ?. (?, B| ? , P ? ) is a probability space, where B| ? is a Borel ?-algebra restricted to ?. Under the setting of meta-RL, ? can be regarded as the set of all possible meta information. Under the setting of population-based training (PBT) <ref type="bibr" target="#b6">(Jaderberg et al., 2017)</ref>, ? can be regarded as the set of the whole population.</p><p>Define ? to be a set of all possible values of parameters (e.g., parameters of value function network and policy network). ? ? ? is some specific value of parameters. For each index ?, there exists a specific mapping between each parameter of ? and ?, denoted as ? ? , to indicate the parameters in ? corresponding to ? (e.g., in -greedy behavior policies). Under the setting of linear regression y = w ? x, ? = {w ? R n } and ? = w. If ? represents using only the first half features to perform regression, assume w = (w 1 , w 2 ), then ? ? = w 1 . Under the setting of RL, ? ? defines a parameterized policy indexed by ?, denoted as ? ? ? .</p><p>Define D def = {d ? ?0 | ? ? ?(A) S , ? 0 ? ?(S)} to be the set of all states visitation distributions. For the parameterized policies, denote D ?,?,?0 def = {d ? ? ? ?0 | ? ? ?, ? ? ?}. Note that (?, B| ? , P ? ) is a probability space on ?, which induces a probability space on D ?,?,?0 , with the probability measure given by P D (D ?0,?,?0 ) = P ? (? 0 ), ?? 0 ? B| ? .</p><p>We use x to represent one sample, which contains all necessary information for learning. As for DQN, x = (s t , a t , r t , s t+1 ). As for R2D2, x = (s t , a t , r t , . . . , s t+N , a t+N , r t+N , s t+N +1 ). As for IM-PALA, x also contains the distribution of the behavior policy. The content of x depends on the algorithm, but it's assumed to be sufficient for learning. We use X to represent the set of samples. At training stage t, given the parameter ? = ? (t) , the distribution of the index set P ? = P (t) ? (e.g., sampling distribution of behavior policy) and the distribution of the initial state ? 0 , we denote the set of samples as</p><formula xml:id="formula_2">X (t) ?0 def = d ? ? 0 ?P (t) D {x|x ? d ? ?0 } = ??P (t) ? {x|x ? d ? ? ?0 , ? = ? (t) ? } ??P (t) ? X (t)</formula><p>?0,? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Capacity and Diversity Control of Behavior Policy</head><p>We consider the problem that behavior policies ? are sampled from a policy space {? ? ? |? ? ?} which is parameterized by the policy network and indexed by the index set ?. The capacity of ? describes how many different behavior policies are there in the policy space, controlled by the base policy's capacity (e.g., shared parameters or not) and the size of the index set |?|. Noting that there are two sets of parameters, namely ? and ?. The diversity describes how many different behavior policies are actually selected from the policy space to generate training data, controlled by the sampling/selective distribution P ? (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>After the capacity of the base policy is determined, we can explicitly control the data richness via the size of the index set and the sampling distribution P ? . On the condition that interactions are limitless, increasing the size of the index set can significantly improve the data richness and thus is more important for a superior final performance since the diversity can be maximized via adopting a uniform distribution (most prior works have achieved SOTA in this way). However, it's data inefficient and the condition may never hold. Considering interactions are limited, the optimization of the sampling distribution, namely to select suitable behavior policies to generate training data, is crucial for sample efficiency because each interaction is rare. It's also known as the exploration-exploitation trade-off problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Distribution Optimization Problem</head><p>In conclusion, the final performance can be significantly improved via increasing the data richness controlled by the capacity and diversity of behavior policy. The sample efficiency is significantly influenced by the explorationexploitation trade-off, namely the sampling/selective distribution of the behavior policy. In general, on the condition that the capacity of behavior policy is determined and training data is totally generated by behavior policies, these problems can be cast into the data distribution optimization problem: Definition 4.1 (Data Distribution Optimization Problem). Finding a selective distribution P ? that samples behavior policies ? ? ? from a parameterized policy space that indexed by ? and maximizing some target function L E , where the L E can be any target function (e.g., RL target) that describes what kind of data do agents desire (i.e., a measure of the importance/value of the sample trajectory).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalized Data Distribution Iteration</head><p>Now we introduce our main algorithm to handle the data distribution optimization problem in RL.</p><formula xml:id="formula_3">T defined as ? (t+1) = T (? (t) , {X (t) ?0,? } ??P (t) ?</formula><p>) is a typical optimization operator of RL algorithms, which utilizes the</p><formula xml:id="formula_4">Algorithm 1 Generalized Data Distribution Iteration Initialize ?, ?, P (0) ? , ? (0) . for t = 0, 1, 2, . . . do Sample {X (t) ?0,? } ??P (t) ? . {Data Sampling} ? (t+1) = T (? (t) , {X (t) ?0,? } ??P (t) ? ). {Generalized Pol- icy Iteration} P (t+1) ? = E(P (t) ? , {X (t) ?0,? } ??P (t) ?</formula><p>). {Data Distribution Iteration} end for collected samples to update the parameters for maximizing some function L T . For instance, L T may contain the policy gradient and the state value evaluation for the policy-based methods, may contain generalized policy iteration for the value-based methods, and may also contain some auxiliary tasks or intrinsic rewards for specially designed methods.</p><formula xml:id="formula_5">E defined as P (t+1) ? = E(P (t) ? , {X (t) ?0,? } ??P (t) ? ) is a data distribution optimization operator. It uses the samples {X (t) ?0,? } ??P (t) ?</formula><p>to update P ? and maximize some function L E , namely,</p><formula xml:id="formula_6">P (t+1) ? = arg max P? L E ({X (t) ?0,? } ??P? ).</formula><p>Since P ? is parameterized, we abuse the notation and use P ? to represent the parameter of P ? . If E is a first-order optimization operator, then we can write E explicitly as</p><formula xml:id="formula_7">P (t+1) ? = P (t) ? + ?? P (t) ? L E ({X (t) ?0,? } ??P (t) ? ).</formula><p>If E is a second-order optimization operator, like natural gradient, we can write E formally as</p><formula xml:id="formula_8">P (t+1) ? = P (t) ? + ?F(P (t) ? ) ? ? P (t) ? L E ({X (t) ?0,? } ??P (t) ? ), F(P (t) ? ) = ? P (t) ? log P (t) ? ? ? P (t) ? log P (t) ? ,</formula><p>where ? denotes the Moore-Penrose pseudoinverse of the matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">An Operator View of RL Methods</head><p>We can further divide all algorithms into two categories, GDI-I n and GDI-H n . n represents the degree of freedom of ?, which is the dimension of selective distribution. I represents Isomorphism. We say one algorithm belongs to GDI-I n , if ? = ? ? , ?? ? ?. H represents Heterogeneous. We say one algorithm belongs to GDI-H n , if ? ?1 = ? ?2 , ?? 1 , ? 2 ? ?. By definition, GDI-H n is a much larger set than GDI-I n , but many algorithms belong to GDI-I n rather than GDI-H n . We say one algorithm is "w/o E" if it doesn't contain the operator E, which means its E is an identical mapping and the data distribution is not additionally optimized. Now, we could understand some well-known RL methods from the view of GDI.</p><p>For DQN, RAINBOW, PPO and IMPALA, they are in GDI-</p><formula xml:id="formula_9">I 0 w/o E. Let |?| = 1, WLOG, assume ? = {? 0 }. Then, the probability measure P ? collapses to P ? (? 0 ) = 1. ? = {? ?0 }. E is an identical mapping of P (t) ? .</formula><p>T is the first-order operator that optimizes the loss functions.</p><p>For Ape-X and R2D2, they are in GDI-</p><formula xml:id="formula_10">I 1 w/o E. Let ? = { l | l = 1, . . . , 256}. P ? is uniform, P ? ( l ) = |?| ?1 .</formula><p>Since all actors and the learner share parameters, we have</p><formula xml:id="formula_11">? 1 = ? 2 for ? 1 , 2 ? ?, hence ? = ?? {? } = {? l }, ? l = 1, . . . , 256. E is an identical mapping, because P (t)</formula><p>? is always a uniform distribution. T is the first-order operator that optimizes the loss functions.</p><formula xml:id="formula_12">For LASER, it's in GDI-H 1 w/o E. Let ? = {i| i = 1, . . . , K} to be the number of learners. P ? is uniform, P ? (i) = |?| ?1 . Since different learners don't share param- eters, ? i1 ? ? i2 = ? for ?i 1 , i 2 ? ?, hence ? = i?? {? i }. E is an identical mapping. T can be formulated as a union of ? (t+1) i = T i (? (t) i , {X (t) ?0,? } ??P (t) ? )</formula><p>, which represents optimizing ? i of the ith learner with shared samples from other learners.</p><p>For PBT, it's in GDI-H n+1 , where n is the number of searched hyperparameters. Let ? = {h} ? {i|i = 1, . . . , K}, where h represents the hyperparameters being searched and K is the population size</p><formula xml:id="formula_13">. ? = i=1,...,K {? i,h }, where ? i,h1 = ? i,h2 for ?(h 1 , i), (h 2 , i) ? ?.</formula><p>E is the meta-controller that adjusts h for each i, which can be formally written as P</p><formula xml:id="formula_14">(t+1) ? (?, i) = E i (P (t) ? (?, i), {X (t) ?0,(h,i) } h?P (t)</formula><p>? (?,i) ), which optimizes P ? according to the performance of all agents in the population. T can also be formulated as a union of T i , but is ?</p><formula xml:id="formula_15">(t+1) i = T i (? (t) i , {X (t) ?0,(h,i) } h?P (t)</formula><p>? (?,i) ), which represents optimizing the ith agent with samples from the ith agent.</p><p>For NGU and Agent57, it's in GDI-I 2 . Let ? = {? i |i = 1, . . . , m} ? {? j |j = 1, . . . , n}, where ? is the weight of the intrinsic value function and ? is the discount factor. Since all actors and the learner share variables, ? = (?,?)?? {? (?,?) } = {? (?,?) } for ?(?, ?) ? ?. E is an optimization operator of a multi-arm bandit controller with UCB, which aims to maximize the expected cumulative rewards by adjusting P ? . Different from above, T is identical to our general definition</p><formula xml:id="formula_16">? (t+1) = T (? (t) , {X (t) ?0,? } ??P (t) ? )</formula><p>, which utilizes samples from all ?s to update the shared ?.</p><p>For Go-Explore, it's in GDI-H 1 . Let ? = {? }, where ? represents the stopping time of switching between robustification and exploration. ? = {? r } ? {? e }, where ? r is the robustification model and ? e is the exploration model. E is a search-based controller, which defines the next P ? for better exploration. T can be decomposed into (T r , T e ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Monotonic Data Distribution Optimization</head><p>We see that many algorithms can be formulated as a special case of GDI. For algorithms without a meta-controller, whose data distribution optimization operator E is an identical mapping, the guarantee that the learned policy could converge to the optimal policy has been widely studied, for instance, GPI in <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018)</ref> and policy gradient in <ref type="bibr">(Agarwal et al., 2019)</ref>. However, for algorithms with a meta-controller, whose data distribution optimization operator E is non-identical, though most algorithms in this class show superior performance, it still lacks a general study on why the data distribution optimization operator E helps. In this section, with a few assumptions, we show that given the same optimization operator T , a GDI with a non-identical data distribution optimization operator E is always superior to that without E.</p><p>For brevity, we denote the expectation of L E , L T for each ? ? ? as L E (?, ? ? ) and L T (?, ? ? ), calculated as</p><formula xml:id="formula_17">L E (?, ? ? ) = E x?? ? ? [L E ({X ?0,? })] L T (?, ? ? ) = E x?? ? ? [L T ({X ?0,? })]</formula><p>and denote the expectation of L E (?, ? ? ), L T (?, ? ? ) for any <ref type="bibr">(Agarwal et al., 2019)</ref> shows V ? is ?-smooth, namely bounded second-order derivative, for direct parameterization. If ?(A) S is compact, continuity implies uniform continuity. Assumption 2 (Formulation of E Assumption). Assume P</p><formula xml:id="formula_18">P ? as L E (P ? , ?) = E ??P? [L E (?, ? ? )], L T (P ? , ?) = E ??P? [L T (?, ? ? )]. Assumption 1 (Uniform Continuous Assumption). For ? &gt; 0, ?s ? S, ? ? &gt; 0, s.t.|V ?1 (s) ? V ?2 (s)| &lt; , ? d ? (? 1 , ? 2 ) &lt; ?, where d ? is a metric on ?(A) S . If ? is parameterized by ?, then for ? &gt; 0, ?s ? S, ? ? &gt; 0, s.t.|V ? ? 1 (s) ? V ? ? 2 (s)| &lt; , ? ||? 1 ? ? 2 || &lt; ?. Remark. (Dadashi et al., 2019) shows V ? is infinitely dif- ferentiable everywhere on ?(A) S if |S| &lt; ?, |A| &lt; ?.</formula><formula xml:id="formula_19">(t+1) ? = E(P (t) ? , {X (t) ?0,? } ??P (t) ? ) can be writ- ten as P (t+1) ? (?) = P (t) ? (?) exp(?L E (?,? (t) ? )) Z (t+1) , Z (t+1) = E ??P (t) ? [exp(?L E (?, ? (t) ? ))].</formula><p>Remark. The assumption is actually general. Regarding ? as an action space and r ? = L E (?, ?</p><formula xml:id="formula_20">(t) ? ), when solving arg max P? E ??P? [L E (?, ? (t) ? )] = arg max P? E ??P? [r ? ],</formula><p>the data distribution optimization operator E is equivalent to solving a multi-arm bandit (MAB) problem. For the first-order optimization, <ref type="bibr" target="#b22">(Schulman et al., 2017a)</ref> shows that the solution of a KL-regularized version,</p><formula xml:id="formula_21">arg max P? E ??P? [r ? ] ? ?KL(P ? ||P (t) ? )</formula><p>, is exactly the assumption. For the second-order optimization, let P ? = sof tmax({r ? }), <ref type="bibr">(Agarwal et al., 2019)</ref> shows that the natural policy gradient of a softmax parameterization also induces exactly the assumption.</p><formula xml:id="formula_22">Assumption 3 (First-Order Optimization Co-Monotonic Assumption). For ? ? 1 , ? 2 ? ?, we have [L E (? 1 , ? ?1 ) ? L E (? 2 , ? ?2 )] ? [L T (? 1 , ? ?1 ) ? L T (? 2 , ? ?2 )] ? 0. Assumption 4 (Second-Order Optimization Co-Monotonic Assumption). For ? ? 1 , ? 2 ? ?, ? ? 0 &gt; 0, s.t. ? 0 &lt; ? &lt; ? 0 , we have [L E (? 1 , ? ?1 )?L E (? 2 , ? ?2 )]?[G ? L T (? 1 , ? ?1 )? G ? L T (? 2 , ? ?2 )] ? 0, where ? ? ? = ? ? + ?? ? ? L T (?, ? ? ) and G ? L T (?, ? ? ) = 1 ? [L T (?, ? ? ? ) ? L T (?, ? ? )].</formula><p>Under assumptions <ref type="formula" target="#formula_1">(1) (2) (3)</ref>, if T is a first-order operator, namely a gradient accent operator, to maximize L T , GDI can be guaranteed to be superior to that w/o E. Under assumptions <ref type="formula" target="#formula_1">(1)</ref> (2) (4), if T is a second-order operator, namely a natural gradient operator, to maximize L T , GDI can also be guaranteed to be superior to that w/o E. Theorem 1 (First-Order Optimization with Superior Target). Under assumptions <ref type="formula" target="#formula_1">(1)</ref> </p><formula xml:id="formula_23">(2) (3), we have L T (P (t+1) ? , ? (t+1) ) = E ??P (t+1) ? [L T (?, ? (t+1) ? )] ? E ??P (t) ? [L T (?, ? (t+1) ? )] = L T (P (t)</formula><p>? , ? (t+1) ). Proof. By Theorem 4 (see App. C), the upper triangular transport inequality, let f (?) = L T (?, ? ? ) and g(?) = L E (?, ? ? ), the proof is done.</p><p>Remark (Superiority of Target). In Algorithm 1, if E updates P (t) ? at time t, then the operator T at time t + 1 can be written as</p><formula xml:id="formula_24">? (t+2) = ? (t+1) + ?? ? (t+1) L T (P (t+1) ? , ? (t+1) ). If P (t)</formula><p>? hasn't been updated at time t, then the operator T at time t + 1 can be written as ? (t+2) = ? (t+1) + ?? ? (t+1) L T (P (t) ? , ? (t+1) ). Theorem 1 shows that the target of T at time t + 1 becomes higher if P (t) ? is updated by E at time t.</p><p>Example 1 (Practical Implementation). Let L E (?, ? ? ) = J ? ? ? and L T (?, ? ? ) = J ? ? ? . E can update P ? by the Monte-Carlo estimation of J ? ? ? . T is to maximize J ? ? ? , which can be any RL algorithms.</p><p>Theorem 2 (Second-Order Optimization with Superior Improvement). Under assumptions</p><formula xml:id="formula_25">(1) (2) (4), we have E ??P (t+1) ? [G ? L T (?, ? (t+1) ? )] ? E ??P (t) ? [G ? L T (?, ? (t+1) ? )]</formula><p>, more specifically, Proof. By Theorem 4 (see App. C), the upper triangular transport inequality, let f (?) = G ? L T (?, ? ? ) and g(?) = L E (?, ? ? ), the proof is done.</p><formula xml:id="formula_26">E ??P (t+1) ? [L T (?, ? (t+1),? ? ) ? L T (?, ? (t+1) ? )] ? E ??P (t) ? [L T (?, ? (t+1),? ? ) ? L T (?, ? (t+1) ? )]</formula><p>Remark (Superiority of Improvement). Theorem 2 shows that, if P ? is updated by E, the expected improvement of T is higher. <ref type="bibr">(Agarwal et al., 2019)</ref> shows that, for direct parameterization, the natural policy gradient gives</p><formula xml:id="formula_27">Example 2 (Practical Implementation). Let L E (?, ? ? ) = E s?d ? ? 0 E a??(?|s) exp( A ? (s,?))/Z [A ? (s, a)], where ? = ? ? ? . Let L T (?, ? ? ) = J ? ? ? . If we optimize L T (?, ? ? ) by nat- ural gradient,</formula><formula xml:id="formula_28">? (t+1) ? ? (t) exp( A ? (t) ), by Lemma 4 (see App. C), the performance difference lemma, V ? (s 0 ) ? V ? (s 0 ) = 1 1?? E s?d ? s 0 E a??(?|s) [A ? (s, a)]</formula><p>, hence if we ignore the gap between the states visitation distributions of ? (t) and ? (t+1) ,</p><formula xml:id="formula_29">L E (?, ? (t) ? ) ? 1 1?? E s?d ? ? 0 [V ? (t+1) (s) ? V ? (t) (s)], where ? (t) = ? ? (t) ? .</formula><p>Hence, E is actually putting more measure on ? that can achieve more improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>In this section, we designed our experiment to answer the following questions:</p><p>? How to implement RL algorithms based on GDI step by step (see Sec. 5.1)? Whether the proposed methods can outperform all prior SOTA RL algorithms in both sample efficiency and final performance (see Tab. 1)?</p><p>? How to construct a behavior policy space (see Sec. 5.1)? What's the impact of the size of the index set ?, namely, whether the data richness can be improved via increasing the capacity and diversity (see <ref type="figure" target="#fig_1">Fig. 3</ref>)?</p><p>? How to design a data distribution optimization operator (e.g., a meta-controller) to tackle the exploration and exploitation trade-off (see Sec. 5.1)? How much performance would be degraded without data distribution optimization, namely no meta-controller (see <ref type="figure" target="#fig_1">Fig. 3</ref>)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Practical Implementation Based on GDI</head><p>Policy Space Construction To illustrate the effectiveness of GDI, we give two representative practical implementations of GDI, namely GDI-I 3 and GDI-H 3 , the capacity of whose behavior policy space is larger than Agent57. Let ? = {?|? = (? 1 , ? 2 , )}. The behavior policy belongs to a soft entropy policy space including policies ranging from very exploratory to purely exploitative and thereby the optimization of the sampling distribution of behavior policy P ? can be reframed into the trade-off between exploration and exploitation. We define the behavior policy ? ? ? as</p><formula xml:id="formula_30">? ? ? = ?Softmax A ?1 ? 1 +(1? )?Softmax A ?2 ? 2<label>(2)</label></formula><p>wherein ? ? ? constructs a parameterized policy space, and the index set ? is constructed by ? = (? 1 , ? 2 , ). For GDI-I 3 , A ?1 and A ?2 are identical advantage functions <ref type="bibr" target="#b29">(Wang et al., 2016)</ref>. Namely, they are estimated by an isomorphic family of trainable variables ?. The learning policy is also ? ? ? . For GDI-H 3 , A ?1 and A ?2 are different, and they are estimated by two different families of trainable variables (i.e., ? 1 = ? 2 ). Since GDI needn't assume A ?1 and A ?2 are learned from the same MDP, we adopt two kinds of reward shaping to learn A ?1 and A ?2 respectively, which can see App. G. More implementation details see App. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Distribution Optimization Operator</head><p>The operator E, which optimizes P ? , is achieved by Multi-Arm Bandits <ref type="bibr">(Sutton &amp; Barto, 2018, MAB)</ref>, where assumption <ref type="formula" target="#formula_30">(2)</ref>   system to highlight the superiority of GDI from multiple levels (see App. H). Furthermore, to avoid any issues that aggregated metrics may have, App. K provides full learning curves for all games and detailed comparison tables of raw and normalized scores. More details see App. F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of GDI</head><p>The aggregated results across games are reported in Tab. 1. Our agents obtain the highest mean HNS with the minimal training frames, leading to the best learning efficiency. Furthermore, our agents have surpassed 22 human world records within 38 playtime days, which is 500 times more efficient than Agent57. Extensive experiments have demonstrated the fact that either GDI-I 3 or GDI-H 3 could obtain superhuman performance with remarkable learning efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of the Results</head><p>Agent57 could obtain the highest median HNS but relatively lower learning efficiency via i) a relatively larger behavior policy space and a metacontroller ii) intrinsic rewards and nearly unlimited data. However, Agent57 fails to distinguish the value of data and thereby collects many useless/low-value samples. Other algorithms are struggling to match our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>Ablation Study Design In the ablation study, we further investigate the effects of several properties of GDI. In the first experiment, we demonstrate the effectiveness of the capacity and diversity control via exploring how different sizes of the index set of the policy space influence the performance and data richness. In the second experiment, we highlight the effectiveness of data distribution optimization operator E via ablating E. More details can see App. L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Capacity and Diversity Control</head><p>In this experiment, we firstly implement a GDI-I 1 algorithm with Boltzmann policy space (i.e., ? ? ? = Softmax( A ? )) to explore the impact of the capacity and diversity control. Then, we explore whether the data richness is indeed improved via a case study of t-SNE of GDI-I 3 and GDI-I 1 . Results are illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, from which we could find the visited states of GDI-I 3 are indeed richer than GDI-I 1 , which concludes its better performance. In the same way, the behavior policy space of GDI-I 3 is a sub-space (i.e., ? 1 = ? 2 ) of that of GDI-H 3 , leading to further performance improvement. <ref type="figure" target="#fig_1">Fig. 3</ref>, we could also find that not using a meta-controller (e.g., the index ? of behavior policy takes a fixed value) will dramatically degrade performance, which confirms the effectiveness of the data distribution optimization and echoes the previous theoretical proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Data Distribution Optimization From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Simultaneously obtaining superior sample efficiency and better final performance is an important and challenging problem in RL. In this paper, we present the first attempt to address this problem from training data distribution control, namely to obtain any desired (e.g., nontrivial) data within limited interactions. To tackle this problem, we firstly cast it into a data distribution optimization problem. Then, we handle this problem via i) explicitly modeling and controlling the diversity of the behavior policies and ii) adaptively tackling the exploration-exploitation trade-off using metalearning. After integrating this process into GPI, we surprisingly find a more general framework GDI and then we give an operation-version of recent SOTA algorithms. Under the guidance of GDI, we propose feasible implementations and achieve the superhuman final performance with remarkable learning efficiency within only 38 playtime days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Summary of Notation and Abbreviation</head><p>In this section, we briefly summarize some common notations and abbreviations in this paper for the convenience of readers, which are illustrated in Tab. 2 and Tab. 3. the states visitation distribution of ? with the initial state distribution ? 0 J ? the expectation of the returns with the states visitation distribution of ? V ? the state value function of ? Q ? the state-action value function of ? ? discount-rate parameter  </p><formula xml:id="formula_31">? t temporal-difference error at t ? set of indexes ? one index in ? P ? one</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Background on RL</head><p>The RL problem can be formulated by a Markov decision process <ref type="bibr">(Howard, 1960, MDP)</ref> defined by the tuple (S, A, p, r, ?, ? 0 ). Considering a discounted episodic MDP, the initial state s 0 will be sampled from the distribution denoted by ? 0 (s) : S ? ?(S). At each time t, the agent choose an action a t ? A according to the policy ?(a t |s t ) : S ? ?(A) at state s t ? S. The environment receives the action, produces a reward r t ? r(s, a) : S ? A ? R and transfers to the next state s t+1 submitted to the transition distribution p (s | s, a) : S ? A ? ?(S). The process continues until the agent reaches a terminal state or a maximum time step. Define return G t = ? k=0 ? k r t+k , state value function V ? (s t ) = E ? k=0 ? k r t+k |s t , state-action value function Q ? (s t , a t ) = E ? k=0 ? k r t+k |s t , a t , and advantage function A ? (s t , a t ) = Q ? (s t , a t ) ? V ? (s t ), wherein ? ? (0, 1) is the discount factor. The connections between V ? and Q ? is given by the Bellman equation,</p><formula xml:id="formula_32">T Q ? (s t , a t ) = E ? [r t + ?V ? (s t+1 )], where V ? (s t ) = E ? [Q ? (s t , a t )].</formula><p>The goal of reinforcement learning is to find the optimal policy ? * that maximizes the expected sum of discounted rewards, denoted by J <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018)</ref>:</p><formula xml:id="formula_33">? * = argmax ? J ? (? ) = argmax ? E ? [G t ] = argmax ? E ? [ ? k=0 ? k r t+k ]</formula><p>Model-free reinforcement learning (MFRL) has made many impressive breakthroughs in a wide range of Markov decision processes <ref type="bibr" target="#b28">(Vinyals et al., 2019;</ref><ref type="bibr" target="#b16">Pedersen, 2019;</ref><ref type="bibr">Badia et al., 2020a, MDP)</ref>. MFRL mainly consists of two categories, valued-based methods <ref type="bibr" target="#b12">(Mnih et al., 2015;</ref><ref type="bibr" target="#b3">Hessel et al., 2017)</ref> and policy-based methods <ref type="bibr" target="#b21">(Schulman et al., 2015;</ref><ref type="bibr" target="#b23">2017b;</ref><ref type="bibr">Espeholt et al., 2018)</ref>.</p><p>Value-based methods learn state-action values and select actions according to these values. One merit of value-based methods is to accurately control the exploration rate of the behavior policies by some trivial mechanism, such like -greedy. The drawback is also apparent. The policy improvement of valued-based methods totally depends on the policy evaluation. Unless the selected action is changed by a more accurate policy evaluation, the policy won't be improved. So the policy improvement of each policy iteration is limited, which leads to a low learning efficiency. Previous works equip valued-based methods with many appropriated designed structures, achieving a more promising learning efficiency <ref type="bibr" target="#b29">(Wang et al., 2016;</ref><ref type="bibr" target="#b17">Schaul et al., 2015;</ref><ref type="bibr" target="#b9">Kapturowski et al., 2018)</ref>.</p><p>In practice, value-based methods maximize J by policy iteration <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018)</ref>. The policy evaluation is fulfilled by minimizing E ? [(G ? Q ? ) 2 ], which gives the gradient ascent direction E ? [(G ? Q ? )?Q ? ]. The policy improvement is usually achieved by -greedy.</p><p>Q-learning is a typical value-based methods, which updates the state-action value function Q(s, a) with Bellman Optimality Equation <ref type="bibr" target="#b30">(Watkins &amp; Dayan, 1992)</ref>:</p><formula xml:id="formula_34">? t = r t+1 + ? arg max a Q (s t+1 , a) ? Q (s t , a t ) Q (s t , a t ) ? Q (s t , a t ) + ?? t</formula><p>wherein ? t is the temporal difference error <ref type="bibr" target="#b24">(Sutton, 1988)</ref>, and ? is the learning rate.</p><p>A refined structure design of Q ? is achieved by <ref type="bibr" target="#b29">(Wang et al., 2016)</ref>. It estimates Q ? by a summation of two separated networks, Q ? = A ? + V ? , which has been widely studied in <ref type="bibr" target="#b29">(Wang et al., 2016;</ref><ref type="bibr">Xiao et al., 2021a)</ref>.</p><p>Policy gradient <ref type="bibr">(Williams, 1992, PG)</ref> methods is an outstanding representative of policy-based RL algorithms, which directly parameterizes the policy and updates through optimizing the following objective:</p><formula xml:id="formula_35">J (?) = E ? ? t=0 log ? ? (a t | s t ) R(? )</formula><p>wherein R(? ) is the cumulative return on trajectory ? . In PG method, policy improves via ascending along the gradient of the above equation, denoted as policy gradient:</p><formula xml:id="formula_36">? ? J (? ? ) = E ? ?? ? ? t=0 ? ? log ? ? (a t | s t ) R(? )</formula><p>One merit of policy-based methods is that they incorporate a policy improvement phase every training step, suggesting a higher learning efficiency than value-based methods. Nevertheless, policy-based methods easily fall into a suboptimal solution, where the entropy drops to 0 <ref type="bibr" target="#b1">(Haarnoja et al., 2018)</ref>. The actor-critic methods introduce a value function as the baseline to reduce the variance of the policy gradient <ref type="bibr" target="#b13">(Mnih et al., 2016)</ref>, but maintain the other characteristics unchanged.</p><p>Actor-Critic <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018</ref>, AC) reinforcement learning updates the policy gradient with an value-based critic, which can reduce variance of estimates and thereby ensure more stable and rapid optimization.</p><formula xml:id="formula_37">? ? J (?) = E ? ? t=0 ? t ? ? log ? ? (a t | s t )</formula><p>wherein ? t is the critic to guide the improvement directions of policy improvement, which can be the state-action value function Q ? (s t , a t ), the advantage function A ? (s t , a t ) = Q ? (s t , a t ) ? V ? (s t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Retrace</head><p>When large scale training is involved, the off-policy problem is inevitable. Denote ? to be the behavior policy, ? to be the target policy, and c t = min{ ?t ?t ,c} to be the clipped importance sampling. For brevity, denote c [t:t+k] = k i=0 c t+i . ReTrace <ref type="bibr" target="#b14">(Munos et al., 2016)</ref> estimates Q(s t , a t ) by clipped per-step importance sampling</p><formula xml:id="formula_38">Q?(s t , a t ) = E ? [Q(s t , a t ) + k?0 ? k c [t+1:t+k] ? Q t+k Q], where ? Q t Q def = r t + ?Q(s t+1 , a t+1 ) ? Q(s t , a t ).</formula><p>The above operator is a contraction mapping, and Q converges to Q? ReT race that corresponds to some? ReT race .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Vtrace</head><p>Policy-based methods maximize J by policy gradient. It's shown <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018)</ref> that ?J = E ? [G? log ?]. When involved with a baseline, it becomes an actor-critic algorithm such as</p><formula xml:id="formula_39">?J = E ? [(G ? V ? )? log ?], where V ? is optimized by minimizing E ? [(G ? V ? ) 2 ], i.e. gradient ascent direction E ? [(G ? V ? )?V ? ].</formula><p>IMPALA <ref type="bibr">(Espeholt et al., 2018)</ref> introduces V-Trace off-policy actor-critic algorithm to correct for the discrepancy between target policy and behavior policy. Denote ? t = min{ ?t ?t ,?}. V-Trace estimates V (s t ) by</p><formula xml:id="formula_40">V?(s t ) = E ? [V (s t ) + k?0 ? k c [t:t+k?1] ? t+k ? V t+k V ], where ? V t V def = r t + ?V (s t+1 ) ? V (s t ).</formula><p>Ifc ??, the above operator is a contraction mapping, and V converges to V? that corresponds to? The policy gradient is given by</p><formula xml:id="formula_41">E ? ? t (r t + ?V?(s t+1 ) ? V (s t ))? log ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Theoretical Proof</head><p>For a monotonic sequence of numbers which satisfies a = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; b, we call it a split of interval <ref type="bibr">[a, b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 (Discretized Upper Triangular Transport Inequality for Increasing Functions in</head><formula xml:id="formula_42">R 1 ). Assume ? is a continuous probability measure supported on [0, 1]. Let 0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 to be any split of [0, 1]. Define?(x i ) = ?([x i , x i+1 )). Define? (x i ) =?(x i ) exp(x i )/Z, Z = i? (x i ) exp(x i ).</formula><p>Then there exists a probability measure ? :</p><formula xml:id="formula_43">{x i } i=0,...,n ? {x i } i=0,...,n ? [0, 1], s.t. ? ? ? ? ? ? ? ? ? ? ? ? ? j ?(x i , y j ) =?(x i ), i = 0, . . . , n; i ?(x i , y j ) =?(y j ), j = 0, . . . , n; ?(x i , y j ) = 0, i &gt; j.<label>(3)</label></formula><p>Then for any monotonic increasing function f :</p><formula xml:id="formula_44">{x i } i=0,...,n ? R, we have E?[f ] ? E?[f ].</formula><p>Proof of Lemma 1. For any couple of measures (?, ?), we say the couple satisfies Upper Triangular Transport Condition (UTTC), if there exists ? s.t. <ref type="formula" target="#formula_43">(3)</ref> holds.</p><formula xml:id="formula_45">Given 0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1, we prove the existence of ? by induction. Define? m (x i ) = ? ? ? ? ? ?([x i , x i+1 )), i &lt; m, ?([x i , 1)), i = m, 0, i &gt; m. Define? m (x i ) =? m (x i ) exp(x i )/Z m , Z m = i? m (x i ) exp(x i ).</formula><p>Noting if we prove that (? m ,? m ) satisfies UTTC for m = n, it's equivalent to prove the existence of ? in <ref type="formula" target="#formula_43">(3)</ref>.</p><p>To clarify the proof, we use x i to represent the point for?-axis in coupling and y j to represent the point for?-axis, but they are actually identical, i.e. x i = y j when i = j.</p><p>When m = 0, it's obvious that (? 0 ,? 0 ) satisfies UTTC, as</p><formula xml:id="formula_46">? 0 (x i , y j ) = 1, i = 0, j = 0, 0, else.</formula><p>Assume UTTC holds for m, i.e. there exists ? m s.t. (? m ,? m ) satisfies UTTC, we want to prove it also holds for m + 1.</p><p>By definition of? m , we have</p><formula xml:id="formula_47">? ? ? ? ?? m (x i ) =? m+1 (x i ), i &lt; m, ? m (x i ) =? m+1 (x i ) +? m+1 (x i+1 ), i = m, ? m (x m+1 ) =? m (x i ) =? m+1 (x i ) = 0, i &gt; m + 1.</formula><p>By definition of? m , we have</p><formula xml:id="formula_48">? ? ? ? ? ? ? ? ? ? ?? m (x i ) =? m+1 (x i ) ? Z m+1 Z m , i &lt; m, ? m (x i ) = ? m+1 (x i ) +? m+1 (x i+1 ) exp(x i ? x i+1 ) ? Z m+1 Z m , i = m, ? m (x m+1 ) =? m (x i ) =? m+1 (x i ) = 0, i &gt; m + 1.</formula><p>Multiplying ? m by Zm Zm+1 , we get the following UTTC</p><formula xml:id="formula_49">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? j Z m Z m+1 ? m (x i , y j ) = Z m Z m+1? m+1 (x i ), i &lt; m; j Z m Z m+1 ? m (x i , y j ) = Z m Z m+1 (? m+1 (x i ) +? m+1 (x i+1 )), i = m; j Z m Z m+1 ? m (x i , y j ) = 0, i = m + 1; j Z m Z m+1 ? m (x i , y j ) =? m+1 (x i ) = 0, i &gt; m + 1; i Z m Z m+1 ? m (x i , y j ) =? m+1 (y j ), j &lt; m; i Z m Z m+1 ? m (x i , y j ) =? m+1 (y i ) +? m+1 (y j+1 ) exp(y j ? y j+1 ), j = m; i Z m Z m+1 ? m (x i , y j ) = 0, j = m + 1; i Z m Z m+1 ? m (x i , y j ) =? m+1 (y j ) = 0, j &gt; m + 1; Z m Z m+1 ? m (x i , y j ) = 0, i &gt; j. By definition of Z m , Z m+1 ? Z m =? m+1 (x m+1 )(exp(x m+1 ) ? exp(x m )) &gt; 0,<label>(4)</label></formula><formula xml:id="formula_50">so we have Zm Zm+1? m+1 (x i ) &lt;? m+1 (x i ). Noticing that? m+1 (y i+1 ) exp(y i ? y i+1 ) &lt;? m+1 (y i+1 ) and Zm Zm+1? m+1 (x i ) &lt;? m+1 (x i ), we decompose the measure of Zm Zm+1 ? m at (x i , y m ) to (x i , y m )</formula><p>, (x i , y m+1 ) for i = 0, . . . , m ? 1, and complement a positive measure at (x i , y m+1 ) to make up the difference between Zm Zm+1? m+1 (x i ) and? m+1 (x i ). For i = m, we decompose the measure at (x m , y m ) to (x m , y m ), (x m , y m+1 ), (x m+1 , y m+1 ) and also complement a proper positive measure.</p><formula xml:id="formula_51">Now we define ? m+1 by ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? m+1 (x i , y j ) = Z m Z m+1 ? m (x i , y j ), i &lt; m and j &lt; m, ? m+1 (x i , y j ) = Z m Z m+1 ? m (x i , y j ) + Z m+1 ? Z m Z m+1? m+1 (x i ) ?? m+1 (y j ) ? m+1 (y j ) +? m+1 (y j+1 ) , i &lt; m and j = m, ? m+1 (x i , y j ) = Z m Z m+1 ? m (x i , y j ) + Z m+1 ? Z m Z m+1? m+1 (x i ) ?? m+1 (y j+1 ) ? m+1 (y j ) +? m+1 (y j+1 )</formula><p>, i &lt; m and j = m + 1,</p><formula xml:id="formula_52">? m+1 (x i , y j ) = 0, i &gt; j or i &gt; m + 1 or j &gt; m + 1, ? m+1 (x m , y m ) = u, ? m+1 (x m , y m+1 ) = v, ? m+1 (x m+1 , y m+1 ) = w,</formula><p>where we assume u, v, w to be the solution of the following equations</p><formula xml:id="formula_53">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? u + v + w =? m+1 (x m ) +? m+1 (x m+1 ), w u + v =? m+1 (x m+1 ) ? m+1 (x m ) , v + w u =? m+1 (x m+1 ) ? m+1 (x m ) , u, v, w ? 0.<label>(5)</label></formula><p>It's obvious that</p><formula xml:id="formula_54">? ? ? ? ? ? ? ? ? ? ? ? ? j ? m+1 (x i , y j ) =? m+1 (x i ) = 0, i &gt; m + 1, i ? m+1 (x i , y j ) =? m+1 (y j ) = 0, j &gt; m + 1, ?(x i , y j ) = 0, i &gt; j. For j &lt; m, since i Zm Zm+1 ? m (x i , y j ) =? m+1 (y j ), we have i ? m+1 (x i , y j ) =? m+1 (y j ), j &lt; m. For i &lt; m, since j Zm Zm+1 ? m (x i , y j ) = Zm Zm+1? m+1 (x i ) &lt;? m+1 (x i ), we add Zm+1?Zm Zm+1? m+1 (x i )? m+1(ym) ?m+1(ym)+?m+1(ym+1) , Zm+1?Zm Zm+1? m+1 (x i )? m+1(ym+1) ?m+1(ym)+?m+1(ym+1) to ? m+1 (x i , y m ), ? m+1 (x i , y m+1 ), respectively. So we have j ? m+1 (x i , y j ) =? m+1 (x i ), i &lt; m.</formula><p>For i = m, m + 1, since assumption <ref type="formula" target="#formula_53">(5)</ref> </p><formula xml:id="formula_55">holds, we have u + v + w =? m+1 (x m ) +? m+1 (x m+1 ), w u+v =? m+1(xm+1) ?m+1(xm) , it's obvious that u + v =? m+1 (x m ), w =? m+1 (x m+1 ), which is j ? m+1 (x i , y j ) =? m+1 (x i ), i = m, m + 1.</formula><p>For j = m, m + 1, we firstly have</p><formula xml:id="formula_56">j=m,m+1 i ? m+1 (x i , y j ) = j i ? m+1 (x i , y j ) ? j =m,m+1 i ? m+1 (x i , y j ) = i j ? m+1 (x i , y j ) ? j =m,m+1? m+1 (y j ) = i? m+1 (x i ) ? j =m,m+1? m+1 (y j ) = 1 ? (1 ?? m+1 (y m ) ?? m+1 (y m+1 )) =? m+1 (y m ) +? m+1 (y m+1 ).</formula><p>By definition of ? m+1 , we know ?m+1(xi,ym) ?m(xi,ym) =? m+1(xm+1) ?m+1(xm) for i &lt; m. By assumption <ref type="formula" target="#formula_53">(5)</ref>, we know v+w u =? m+1(xm+1) ?m+1(xm) . Combining three equations above together, we have</p><formula xml:id="formula_57">i ? m+1 (x i , y j ) =? m+1 (y j ), j = m, m + 1.</formula><p>Now we only need to prove assumption <ref type="formula" target="#formula_53">(5)</ref> holds. With linear algebra, we solve <ref type="formula" target="#formula_53">(5)</ref> and have</p><formula xml:id="formula_58">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? u = w 1 +? m+1(xm+1) ?m+1(xm) ?m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) , v = w? m+1(xm+1) ?m+1(xm) ?? m+1(xm+1) ?m+1(xm) ?m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) , w = (? m+1 (x m ) +? m+1 (x m+1 ))? m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) . It's obvious that u, w ? 0. v ? 0 also holds, becaus? ? m+1 (x m+1 ) ? m+1 (x m ) ?? m+1 (x m+1 ) ? m+1 (x m ) =? m+1 (x m+1 ) exp(x m+1 ) ? m+1 (x m ) exp(x m ) ?? m+1 (x m+1 ) ? m+1 (x m ) =? m+1 (x m+1 ) ? m+1 (x m ) (exp(x m+1 ? x m ) ? 1) ? 0.<label>(6)</label></formula><p>So we can find a proper solution of assumption <ref type="bibr" target="#b51">(5)</ref>.</p><p>So ? m+1 defined above satisfies UTTC for (? m+1 ,? m+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By induction, for any</head><formula xml:id="formula_59">0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1, there exists ? s.t. UTTC (3) holds for (?,?).</formula><p>Then for any monotonic increasing function, since ?(</p><formula xml:id="formula_60">x i , y j ) = 0 when i &gt; j, we know ?(x i , y j )f (x i ) ? ?(x i , y j )f (y j ). Hence we have E?[f ] = i? (x i )f (x i ) = i j ?(x i , y j )f (x i ) ? i j ?(x i , y j )f (y j ) = j i ?(x i , y j )f (y j ) = j? (y j )f (y j ) = E?[f ].</formula><p>Lemma 2 (Discretized Upper Triangular Transport Inequality for Co-Monotonic Functions in R 1 ). Assume ? is a continuous probability measure supported on [0, 1]. Let 0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 to be any split of [0, 1]. Let f, g : {x i } i=0,...,n ? R to be two co-monotonic functions that satisfy</p><formula xml:id="formula_61">(f (x i ) ? f (x j )) ? (g(x i ) ? g(x j )) ? 0, ? i, j. Define?(x i ) = ?([x i , x i+1 )). Defin? ?(x i ) =?(x i ) exp(g(x i ))/Z, Z = i? (x i ) exp(g(x i )).</formula><p>Then we have</p><formula xml:id="formula_62">E?[f ] ? E?[f ].</formula><p>Proof of Lemma 2. If the Upper Triangular Transport Condition (UTTC) holds for (?,?), i.e. there exists a probability measure ? :</p><formula xml:id="formula_63">{x i } i=0,...,n ? {x i } i=0,...,n ? [0, 1], s.t. ? ? ? ? ? ? ? ? ? ? ? ? ? j ?(x i , y j ) =?(x i ), i = 0, . . . , n; i ?(x i , y j ) =?(y j ), j = 0, . . . , n; ?(x i , y j ) = 0, g(x i ) &gt; g(y j ),</formula><p>then we finish the proof by</p><formula xml:id="formula_64">E?[f ] = i? (x i )f (x i ) = i j ?(x i , y j )f (x i ) ? i j ?(x i , y j )f (y j ) = j i ?(x i , y j )f (y j ) = j? (y j )f (y j ) = E?[f ], where ?(x i , y j )f (x i ) ? ?(x i , y j )f (y j ) is because of ?(x i , y j ) = 0, g(x i ) &gt; g(y j ) and (f (x i )?f (x j ))?(g(x i )?g(x j )) ? 0.</formula><p>Now we only need to prove UTTC holds for (?,?).</p><p>Given 0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1, we prove the existence of ? by induction. With g to be the transition function in the definition of?, we mimic the proof of Lemma 1 and sort (x 0 , . . . , x n ) in the increasing order of g, which is</p><formula xml:id="formula_65">g(x k0 ) ? g(x k1 ) ? ? ? ? ? g(x kn ). Define? m (x ki ) = ? ? ? ? ? ?([x ki , min{1, x k l | x k l &gt; x ki , l ? m})), i ? m, x ki = min{x k l | l ? m}, ?([0, min{1, x k l | x k l &gt; x ki , l ? m})), i ? m, x ki = min{x k l | l ? m}, 0, i &gt; m. Define? m (x ki ) =? m (x ki ) exp(g(x ki ))/Z m , Z m = i? m (x ki ) exp(g(x ki )).</formula><p>To clarify the proof, we use x ki to represent the point for?-axis in coupling and y kj to represent the point for?-axis, but they are actually identical, i.e. x ki = y kj when i = j.</p><p>When m = 0, it's obvious that (? 0 ,? 0 ) satisfies UTTC, as ? 0 (x ki , y kj ) = 1, i = 0, j = 0, 0, else.</p><p>Assume UTTC holds for m, i.e. there exists ? m s.t. (? m ,? m ) satisfies UTTC, we want to prove it also holds for m + 1.</p><formula xml:id="formula_66">When x km+1 &gt; min{x k l | l ? m}, let x k * = max{x k l | x k l &lt; x km+1 , l ? m} to be the closest left neighbor of x km+1 in {x k l | l ? m}. Then we have? m (x k * ) =? m+1 (x k * ) +? m+1 (x k m+1 )</formula><p>.</p><formula xml:id="formula_67">When x km+1 &lt; min{x k l | l ? m}, let x k * = min{x k l | l ? m} to be the leftmost point in {x k l | l ? m}. Then we hav? ? m (x k * ) =? m+1 (x k * ) +? m+1 (x k m+1 ).</formula><p>In either case, we always have?</p><formula xml:id="formula_68">m (x k * ) =? m+1 (x k * ) +? m+1 (x km+1 ). By definition of? m and? m , we have ? ? ? ? ?? m (x ki ) =? m+1 (x ki ), i ? m, k i = k * , ? m (x ki ) =? m+1 (x ki ) +? m+1 (x km+1 ), i ? m, k i = k * , ? m (x km+1 ) =? m (x ki ) =? m+1 (x ki ) = 0, i &gt; m + 1, ? ? ? ? ? ? ? ? ? ? ?? m (x ki ) =? m+1 (x ki ) ? Z m+1 Z m , i ? m, k i = k * , ? m (x ki ) = ? m+1 (x ki ) +? m+1 (x km+1 ) exp g(x ki ) ? g(x km+1 ) ? Z m+1 Z m , i ? m, k i = k * , ? m (x m+1 ) =? m (x i ) =? m+1 (x i ) = 0, i &gt; m + 1. If g(x k * ) = g(x km+1 ), it's easy to check that? m+1(xk m+1 ) ?m+1(x k * ) =? m+1(xk m+1 )</formula><p>?m+1(x k * ) , we can simply define the following ? m+1 which achieves UTTC for (? m+1 ,? m+1 ):</p><formula xml:id="formula_69">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? m+1 (x k * , y kj ) = ? m (x k * , y kj )? m+1 (x k * ) ? m+1 (x k * ) +? m+1 (x km+1 ) , j ? m, k j = k * , ? m+1 (x km+1 , y kj ) = ? m (x km+1 , y kj )? m+1 (x km+1 ) ? m+1 (x k * ) +? m+1 (x km+1 ) , j ? m, k j = k * , ? m+1 (x ki , y k * ) = ? m (x ki , y k * )? m+1 (y k * ) ? m+1 (y k * ) +? m+1 (y km+1 ) , i ? m, k i = k * , ? m+1 (x ki , y km+1 ) = ? m (x ki , y km+1 )? m+1 (y km+1 ) ? m+1 (y k * ) +? m+1 (x km+1 ) , i ? m, k i = k * , ? m+1 (x k * , y k * ) = ? m (x k * , y k * )? m+1 (x k * ) ? m+1 (x k * ) +? m+1 (x km+1 ) , ? m+1 (x km+1 , y km+1 ) = ? m (x km+1 , y km+1 )? m+1 (x km+1 ) ? m+1 (x k * ) +? m+1 (x km+1 ) , ? m+1 (x ki , y kj ) = 0, others. If g(x k * ) &lt; g(x km+1 )</formula><p>, recalling the proof of Lemma 1, it's crucial to prove inequalities (4) and (6). Inequality (4) guarantees that Zm Zm+1 &lt; 1, so we can shrinkage ? m entrywise by Zm Zm+1 and add some proper measure at proper points. Inequality (6) guarantees that (x m , y m ) can be decomposed to (x m , y m ), (x m , y m+1 ), (x m+1 , y m+1 ). Following the idea, we check that</p><formula xml:id="formula_70">Z m+1 ? Z m =? m+1 (x km+1 ) exp(g(x km+1 ) ? g(x k * )) &gt; 0, ? m+1 (x km+1 ) ? m+1 (x k * ) ?? m+1 (x km+1 ) ? m+1 (x k * ) =? m+1 (x km+1 ) exp(g(x km+1 )) ? m+1 (x k * ) exp(g(x k * )) ?? m+1 (x km+1 ) ? m+1 (x k * ) =? m+1 (x km+1 ) ? m+1 (x k * ) exp(g(x km+1 ) ? g(x k * )) ? 1 &gt; 0.</formula><p>Replacing x m , x m+1 in the proof of Lemma 1 by x k * , x km+1 , we can construct ? m+1 all the same way as in the proof of Lemma 1.</p><p>By induction, we prove UTTC for (?,?). The proof is done.</p><p>Theorem 3 (Upper Triangular Transport Inequality for Co-Monotonic Functions in R 1 ). Assume ? is a continuous probability measure supported on [0, 1]. Let f, g : [0, 1] ? R to be two co-monotonic functions that satisfy</p><formula xml:id="formula_71">(f (x) ? f (y)) ? (g(x) ? g(y)) ? 0, ? x, y ? [0, 1]. f is continuous. Define ?(x) = ?(x) exp(g(x))/Z, Z = [0,1] ?(x) exp(g(x)</formula><p>).</p><p>Then we have</p><formula xml:id="formula_72">E ? [f ] ? E ? [f ]. Proof of Theorem 3. For ? &gt; 0, since f is continuous, f is uniformly continuous, so there exists ? &gt; 0 s.t. |f (x) ? f (y)| &lt; , ?x, y ? [0, 1]. We can split [0, 1] by 0 &lt; x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 s.t. x i+1 ? x i &lt; ?. Define? and? as in Lemma 2. Since x i+1 ? x i &lt; ?,</formula><p>by uniform continuity and the definition of the expectation, we have</p><formula xml:id="formula_73">|E ? [f ] ? E?[f ]| &lt; , |E ? [f ] ? E?[f ]| &lt; , By Lemma 2, we have E?[f ] ? E?[f ]. So we have E ? [f ] &lt; E?[f ] + ? E?[f ] + &lt; E ? [f ] + 2 .</formula><p>Since is arbitrary, we prove</p><formula xml:id="formula_74">E ? [f ] ? E ? [f ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 (Discretized Upper Triangular Transport Inequality for Co-Monotonic Functions in</head><formula xml:id="formula_75">R p ). Assume ? is a continuous probability measure supported on [0, 1] p . Let 0 = x d 0 &lt; x d 1 &lt; ? ? ? &lt; x d n &lt; 1 to be any split of [0, 1], d = 1, . . . , p. Denote x i def = (x 1 i1 , . . . , x p ip ). Define?(x i ) = ?( d=1,...,p [x d i d , x d i d +1 )). Let f, g : {x i } i?{0,...,n} p ? R to be two co-monotonic functions that satisfy (f (x i ) ? f (x j )) ? (g(x i ) ? g(x j )) ? 0, ? i, j.</formula><p>Define?</p><formula xml:id="formula_76">(x i ) =?(x i ) exp(g(x i ))/Z, Z = i? (x i ) exp(g(x i )).</formula><p>Then there exists a probability measure ? :</p><formula xml:id="formula_77">{x i } i?{0,...,n} p ? {x j } j?{0,...,n} p ? [0, 1], s.t. j ?(x i , y j ) =?(x i ), ? i; i ?(x i , y j ) =?(y j ), ? j; ?(x i , y j ) = 0, g(x i ) &gt; g(y j ).</formula><p>Then we have</p><formula xml:id="formula_78">E?[f ] ? E?[f ].</formula><p>Proof of Lemma 3. The proof is almost identical to the proof of Lemma 2, except for the definition of (? m ,? m ) in R p .</p><p>Given {x i } i?{0,...,n} p , we sort x i in the increasing order of g, which is</p><formula xml:id="formula_79">g(x k0 ) ? g(x k1 ) ? ? ? ? ? g(x k (n+1) p ?1 ), where {k i } i?{0,...,(n+1) p ?1} is a permutation of {i} i?{0,...,n} p . For i, j ? {0, . . . , n} p , we define the partial order i &lt; j on {0, . . . , n} p , if ?0 ? d 0 ? n, s.t. i d ? j d , ?d &lt; d 0 and i d0 &lt; j d0 . It's obvious that ? ? ? ? ? ?i ? {0, . . . , n} p , i ? i, ?i, j ? {0, . . . , n} p , i &lt; j ? j ? i, ?i, j, k ? {0, . . . , n} p , i &lt; j, j &lt; k ? i &lt; k. We define i = j if i d = j d , ? 0 ? d ? n.</formula><p>So we define the partial order relation, and we can further define the min function and the max function on {0, . . . , n} p . Now using this partial order relation, we defin?</p><formula xml:id="formula_80">? m (x ki ) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? k?ki,k&lt;min{k l |k l &gt;ki,l?m}? (x k ), i ? m, k i = min{k l | l ? m}, k&lt;min{k l |k l &gt;ki,l?m}? (x k ), i ? m, k i = min{k l | l ? m}, 0, i &gt; m.</formula><p>With this definition of? m , other parts are identical to the proof of Lemma 2. The proof is done.</p><p>Theorem 4 (Upper Triangular Transport Inequality for Co-Monotonic Functions in R p ). Assume ? is a continuous</p><formula xml:id="formula_81">probability measure supported on [0, 1] p . Denote x def = (x 1 , . . . , x p ). Let f, g : [0, 1] p ? R to be two co-monotonic functions that satisfy (f (x) ? f (y)) ? (g(x) ? g(y)) ? 0, ? x, y ? [0, 1] p . f is continuous. Define ?(x) = ?(x) exp(g(x))/Z, Z = [0,1] p ?(x) exp(g(x)</formula><p>).</p><p>Let f, g : [0, 1] p ? R to be two co-monotonic functions that satisfy</p><formula xml:id="formula_82">(f (x) ? f (y)) ? (g(x) ? g(y)) ? 0, ? x, y ? [0, 1] p .</formula><p>Then we have</p><formula xml:id="formula_83">E ? [f ] ? E ? [f ]. Proof of Theorem 4. For ? &gt; 0, since f is continuous, f is uniformly continuous, so there exists ? &gt; 0 s.t. |f (x) ? f (y)| &lt; , ?x, y ? [0, 1] p . We can split [0, 1] by 0 &lt; x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 s.t. x i+1 ? x i &lt; ?/ ? p. Define x d i = x i , ?0 ? d ? p. Define? and? as in Lemma 3. Since x i+1 ? x i &lt; ?/ ? p, |(x 0 i+1 , . . . , x p i+1 ) ? (x 0 i , . . . , x p i )| &lt; ?,</formula><p>by uniform continuity and the definition of the expectation, we have</p><formula xml:id="formula_84">|E ? [f ] ? E?[f ]| &lt; , |E ? [f ] ? E?[f ]| &lt; , By Lemma 3, we have E?[f ] ? E?[f ]. So we have E ? [f ] &lt; E?[f ] + ? E?[f ] + &lt; E ? [f ] + 2 .</formula><p>Since is arbitrary, we prove</p><formula xml:id="formula_85">E ? [f ] ? E ? [f ].</formula><p>Lemma 4 (Performance Difference Lemma). For any policies ?, ? and any state s 0 , we have</p><formula xml:id="formula_86">V ? (s 0 ) ? V ? (s 0 ) = 1 1 ? ? E s?d ? s 0 E a??(?|s) A ? (s, a) .</formula><p>Proof. See <ref type="bibr" target="#b8">(Kakade &amp; Langford, 2002)</ref>.</p><formula xml:id="formula_87">Generalized Data Distribution Iteration D. Algorithm Pseudocode D.1. GDI-I 3</formula><p>In this section, we provide the implementation pseudocode of GDI-I 3 , which is shown in Algorithm 2.</p><formula xml:id="formula_88">A = A ? (s t ) , V = V ? (s t ) , A = A ? E ? [A], Q =? + V. (7) ? = (? 1 , ? 2 , ), ? ? ? = ? Softmax A ? 1 Exploration +(1 ? ) ? Softmax A ? 2 Exploitation (8)</formula><p>Generalized Data Distribution Iteration Algorithm 2 GDI-I 3 Algorithm. Initialize Parameter Server (PS) and Data Collector (DC).</p><formula xml:id="formula_89">// LEARNER Initialize d push .</formula><p>Initialize ? as Eq. <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref> In this section, we provide the implementation pseudocode of GDI-H 3 , which is shown in Algorithm 3. </p><formula xml:id="formula_90">A ?1 = A ?1 (s t ) , V ?1 = V ?1 (s t ) , A ?1 = A ?1 ? E ? [A ?1 ], Q ?1 =? ?1 + V ?1 . A ?2 = A ?2 (s t ) , V ?2 = V ?2 (s t ) , A ?2 = A ?2 ? E ? [A ?2 ], Q ?2 =? ?2 + V ?2 . (9) ? = (? 1 , ? 2 , ), ? ? ? = ? Softmax A ?1 ? 1 + (1 ? ) ? Softmax A ?2 ? 2<label>(10</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Adaptive Controller Formalism</head><p>In practice, we use a Bandits Controller (BC) to control the behavior sampling distribution adaptively, which has been widely used in prior works <ref type="bibr">(Badia et al., 2020a;</ref><ref type="bibr">Xiao et al., 2021b)</ref>. More details on Bandits can see <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018)</ref>. The whole algorithm is shown in Algorithm 4. As the behavior policy can be parameterized and thereby sampling behaviors from the policy space is equivalent to sampling indexes x from the index set.</p><p>Let's firstly define a bandit as B = Bandit <ref type="bibr">(mode, l, r, lr, d, acc, ta, to, w, N)</ref>.</p><p>? mode is the mode of sampling, with two choices, argmax and random, wherein argmax greedily chooses the behaviors with top estimated value from the policy space, and random samples behaviors according to a distribution calculated by Sof tmax(V ).</p><p>? l is the left boundary of the index set, and each x is clipped to x = max{x, l}.</p><p>? r is the right boundary of the index set, and each x is clipped to x = min{x, r}.</p><p>? acc is the accuracy of space to be optimized, where each x is located in the (min{max{x, l}, r} ? l)/acc th block.</p><p>? tile coding is a representation method of continuous space <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018)</ref>, and each kind of tile coding can be uniquely determined by l, r, to and ta, wherein to represents the tile offset and ta represents the accuracy of the tile coding.</p><p>? to is the offset of each tile coding, which represents the relative offset of the basic coordinate system (normally we select the space to be optimized as basic coordinate system).</p><p>? ta is the accuracy of each tile coding, where each x is located in the (min{max{x ? to, l}, r} ? l)/ta th tile.</p><p>? M btt represents block-to-tile, which is a mapping from the block of the original space to the tile coding space.</p><p>? M ttb represents tile-to-block, which is a mapping from the tile coding space to the block of the original space.</p><p>? w is a vector in R (r?l)/ta , which represents the weight of each tile.</p><p>? N is a vector in R (r?l)/ta , which counts the number of sampling of each tile.</p><p>? lr is the learning rate.</p><p>? d is an integer, which represents how many candidates is provided by each bandit when sampling.</p><p>During the evaluation process, we evaluate the value of the ith tile by</p><formula xml:id="formula_91">V i = M btt (blocki) k w k len(M btt (block i ))<label>(11)</label></formula><p>During the training process, for each sample (x, g), where g is the target value. Since x locates in the jth tile of kth tile_coding, we update B by</p><formula xml:id="formula_92">? ? ? ? ? j = (min{max{x ? to k , l}, r} ? l)/ta k , w j ? w j + lr * (g ? V i ) N j ? N j + 1<label>(12)</label></formula><p>During the sampling process, we firstly evaluate B by (11) and get (V 1 , ..., V (r?l)/acc ). We calculate the score of ith tile by</p><formula xml:id="formula_93">score i = V i ? ?({V j } j=1,..., (r?l)/acc ) ?({V j } j=1,..., (r?l)/acc ) + c ? log(1 + j N j ) 1 + N i .<label>(13)</label></formula><p>For different modes, we sample the candidates by the following mechanism,</p><p>? if mode = argmax, find blocks with top-d scores, then sample d candidates from these blocks, one uniformly from a block;</p><p>? if mode = random, sample d blocks with scores as the logits without replacement, then sample d candidates from these blocks, one uniformly from a block;</p><p>In practice, we define a set of bandits</p><formula xml:id="formula_94">B m = {B m } m=1,...,M . At each step, we sample d candidates {c m,i } i=1,...,d from each B m , so we have a set of m ? d candidates {c m,i } m=1,...,M ;i=1,...,d .</formula><p>Then we sample uniformly from these m ? d candidates to get x. At last, we transform the selected x to ? = {? 1 , ? 2 , } by ? 1,2 = 1 exp(x1,2)?1 and = x 3 When we receive (?, g), we transform ? to x by x 1,2 = log(1 + 1/? 1,2 ), and x 3 = . Then we update each B m by <ref type="formula" target="#formula_1">(12)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiment Details</head><p>The overall training architecture is on the top of the Learner-Actor framework <ref type="bibr">(Espeholt et al., 2018)</ref>, which supports large-scale training. Additionally, the recurrent encoder with LSTM <ref type="bibr" target="#b18">(Schmidhuber, 1997)</ref> is used to handle the partially observable MDP problem <ref type="bibr">(Bellemare et al., 2013)</ref>. The burn-in technique is adopted to deal with the representational drift <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>, and we train each sample twice. A complete description of the hyperparameters can see App. G. We employ additional environments to evaluate the scores during training, and the undiscounted episode returns averaged over 32 environments with different seeds have been recorded. Details on relevant evaluation criteria can see App. H.</p><p>We evaluated all agents on 57 Atari 2600 games from the arcade learning environment (Bellemare et al., 2013, ALE) by recording the average score of the population of agents during training. We have demonstrated our evaluation metrics for ALE in App. H, and we will describe more details in the following. Besides, all the experiment is accomplished using a single CPU with 92 cores and a single Tesla-V100-SXM2-32GB GPU.</p><p>Noting that episodes will be truncated at 100K frames (or 30 minutes of simulated play) as other baseline algorithms <ref type="bibr" target="#b3">(Hessel et al., 2017;</ref><ref type="bibr">Badia et al., 2020a;</ref><ref type="bibr" target="#b19">Schmitt et al., 2020;</ref><ref type="bibr">Badia et al., 2020b;</ref><ref type="bibr" target="#b9">Kapturowski et al., 2018)</ref> and thereby we calculate the mean playtime over 57 games which is called Playtime. In addition to comparing the mean and median human normalized scores (HNS), we also report the performance based on human world records among these algorithms and the related learning efficiency to further highlight the significance of our algorithm. Inspired by <ref type="bibr" target="#b26">(Toromanoff et al., 2019)</ref>, human world records normalized score (HWRNS) and SABER are better descriptors for evaluating algorithms on human top level on Atari games, which simultaneously give rise to more challenges and lead the related research into a new journey to train the superhuman agent instead of just paying attention to the human average level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Hyperparameters</head><p>In this section, we firstly detail the hyperparameters we use to pre-process the environment frames received from the Arcade Learning Environment. The hyperparameters that we used in all experiments are almost the same as Agent57 (Badia et al., 2020a), NGU (Badia et al., 2020b), MuZero <ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref> and R2D2 <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>. In Tab. 4, we detail these pre-processing hyperparameters. Then we will detail the hyperparameters we used for Atari experiments, which is demonstrated in Tab. 5.   </p><formula xml:id="formula_95">(r) + 1.0) ? (2 ? 1 {r?0} ? 1 {r&lt;0} ) GDI-H 3 Reward Shape 1 log(abs(r) + 1.0) ? (2 ? 1 {r?0} ? 1 {r&lt;0} ) GDI-H 3 Reward Shape 2 sign(r) ? ((abs(r) + 1.0) 0.25 ? 1.0) + 0.001 ? r</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Evaluation Metrics for ALE</head><p>In this section, we will mainly introduce the evaluation metrics in ALE, including those that have been commonly used by previous works like the raw score and the normalized score over all the Atari games based on human average score baseline, and some novel evaluation criteria for the superhuman Atari benchmark such as the normalized score based on human world records, learning efficiency, and human world record breakthrough. For the summary of benchmark results on these evaluation metrics can see App. J. For more details on these evaluation metrics, we refer to see <ref type="bibr">(Fan, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1. Raw Score</head><p>Raw score refers to using tables (e.g., <ref type="table">Table of</ref> Scores) or figures (e.g., Training Curve) to show the total scores of RL algorithms on all Atari games, which can be calculated by the sum of the undiscounted reward of the gth game of Atari using algorithm i as follows:</p><formula xml:id="formula_96">G g,i = E st?d ? ? 0 E ? ? k=0 r t+k |s t , g ? [1, 57]<label>(14)</label></formula><p>As Bellemare et al. <ref type="formula" target="#formula_1">(2013)</ref> firstly put it, raw score over the whole 57 Atari games can reflect the performance and generality of RL agents to a certain extent. However, this evaluation metric has many limitations:</p><p>1. It is difficult to compare the performance of the two algorithms directly.</p><p>2. Its value is easily affected by the score scale. For example, the score scale of Pong is <ref type="bibr">[-21,21]</ref>, but that of Chopper Command is [0,999900], so the Chopper Command will dominate the mean score of those games.</p><p>In recent RL advances, this metric is used to avoid any issues that aggregated metrics may have (Badia et al., 2020a). Furthermore, this paper used these metrics to prove whether the RL agents have surpassed the human world records, which will be introduced in detail later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Normalized Scores</head><p>To handle the drawbacks of the raw score, some methods <ref type="bibr">(Bellemare et al., 2013;</ref><ref type="bibr" target="#b12">Mnih et al., 2015)</ref> proposed the normalized score. The normalized score of the gth game of Atari using algorithm i can be calculated as follows:</p><formula xml:id="formula_97">Z g,i = G g,i ? G g,base G g,ref erence ? G g,base<label>(15)</label></formula><p>As <ref type="bibr">Bellemare et al. (2013)</ref> put it, we can compare games with different scoring scales by normalizing scores, which makes the numerical values become comparable. In practice, we can make G g,base = r g,min and G g,ref erence = r g,max , where [r g,min , r g,max ] is the score scale of the gth game. Then Equ. (15) becomes Z g,i = Gg,i?rg,min ri,max?rg,min , which is a Min-Max Scaling and thereby Z g,i ? [0, 1] become comparable across the 57 games. It seems this metric can be served to compare the performance between two different algorithms. However, the Min-Max normalized score fail to intuitively reflect the gap between the algorithm and the average level of humans. Thus, we need a human baseline normalized score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2.1. HUMAN AVERAGE SCORE BASELINE</head><p>As we mentioned above, recent reinforcement learning advances <ref type="bibr">(Badia et al., 2020a;</ref><ref type="bibr" target="#b9">Kapturowski et al., 2018;</ref><ref type="bibr" target="#b20">Schrittwieser et al., 2020;</ref><ref type="bibr" target="#b4">Hessel et al., 2021;</ref> are seeking agents that can achieve superhuman performance. Thus, we need a metric to intuitively reflect the level of the algorithms compared to human performance. Since being proposed by <ref type="bibr">(Bellemare et al., 2013)</ref>, the Human Normalized Score (HNS) is widely used in the RL research <ref type="bibr" target="#b11">(Machado et al., 2018)</ref>. HNS can be calculated as follows:</p><formula xml:id="formula_98">HNS g,i = G g,i ? G g,random G g,human average ? G g,random<label>(16)</label></formula><p>wherein g denotes the gth game of Atari, i represents the algorithm i, G g,human average represents the human average score baseline <ref type="bibr" target="#b26">(Toromanoff et al., 2019)</ref>, and G g,random represents the performance of a random policy. Adopting HNS as an evaluation metric has the following advantages:</p><p>1. Intuitive comparison with human performance. HNS g,i ? 100% means algorithm i have surpassed the human average performance in game g. Therefore, we can directly use HNS to reflect which games the RL agents have surpassed the average human performance.</p><p>2. Performance across algorithms become comparable. Like Max-Min Scaling, the human normalized score can also make two different algorithms comparable. The value of HNS g,i represents the degree to which algorithm i surpasses the average level of humans in game g.</p><p>Mean HNS represents the mean performance of the algorithms across the 57 Atari games based on the human average score. However, it is susceptible to interference from individual high-scoring games like the hard-exploration problems in Atari . While taking it as the only evaluation metric, Go-Explore  has achieved SOTA compared to Agent57(Badia et al., 2020a), NGU(Badia et al., 2020b), R2D2 <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>. However, Go-Explore fails to handle many other games in Atari like Demon Attack, Breakout, Boxing, Phoenix. Additionally, Go-Explore fails to balance the trade-off between exploration and exploitation, which makes it suffer from the low sample efficiency problem, which will be discussed later.</p><p>Median HNS represents the median performance of the algorithms across the 57 Atari games based on the human average score. Some methods <ref type="bibr" target="#b20">(Schrittwieser et al., 2020;</ref><ref type="bibr" target="#b4">Hessel et al., 2021</ref>) have adopted it as a more reasonable metric for comparing performance between different algorithms. The median HNS has overcome the interference from individual high-scoring games. However, As far as we can see, there are at least two problems while only referring to it as the evaluation metrics. First of all, the median HNS only represents the mediocre performance of an algorithm. How about the top performance? One algorithm  can easily achieve high median HNS, but at the same time obtain a poor mean HNS by adjusting the hyperparameters of algorithms for games near the median score. It shows that these metrics can show the generality of the algorithms but fail to reflect the algorithm's potential. Moreover, adopting these metrics will urge us to pursue rather mediocre methods.</p><p>In practice, we often use mean HNS or median HNS to show the final performance or generality of an algorithm. Dispute upon whether the mean value or the median value is more representative to show the generality and performance of the algorithms lasts for several years <ref type="bibr" target="#b12">(Mnih et al., 2015;</ref><ref type="bibr" target="#b3">Hessel et al., 2017;</ref><ref type="bibr" target="#b2">Hafner et al., 2020;</ref><ref type="bibr" target="#b4">Hessel et al., 2021;</ref><ref type="bibr">Bellemare et al., 2013;</ref><ref type="bibr" target="#b11">Machado et al., 2018)</ref>. To avoid any issues that aggregated metrics may have, we advocate calculating both of them in the final results because they serve different purposes, and we could not evaluate any algorithm via a single one of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2.2. CAPPED NORMALIZED SCORE</head><p>Capped Normalized Score is also widely used in many reinforcement learning advances <ref type="bibr" target="#b26">(Toromanoff et al., 2019;</ref><ref type="bibr">Badia et al., 2020a)</ref>. Among them, Agent57 (Badia et al., 2020a) adopts the capped human normalized score (CHNS) as a better descriptor for evaluating general performance, which can be calculated as CHNS = max{min{HNS, 1}, 0}. Agent57 claimed CHNS emphasizes the games that are below the average human performance benchmark and used CHNS to judge whether an algorithm has surpassed the human performance via CHNS ? 100%. The mean/median CHNS represents the mean/median completeness of surpassing human performance. However, there are several problems while adopting these metrics:</p><p>1. CHNS fails to reflect the real performance in specific games. For example, CHNS ? 100% represents the algorithms surpassed the human performance but failed to reveal how good the algorithm is in this game. From the view of CHNS, Agent57 (Badia et al., 2020a) has achieved SOTA performance across 57 Atari games, but while referring to the mean HNS or median HNS, Agent57 lost to MuZero.</p><p>2. It is still controversial that using CHNS ? 100% to represent the superhuman performance because it underestimates the human performance <ref type="bibr" target="#b26">(Toromanoff et al., 2019)</ref>.</p><p>3. CHNS ignores the low sample efficiency problem as other metrics using normalized scores.</p><p>In practice, CHNS can serve as an indicator to reflect whether RL agents can surpass the average human performance. The mean/median CHNS can be used to reflect the generality of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2.3. HUMAN WORLD RECORDS BASELINE</head><p>As <ref type="bibr" target="#b26">(Toromanoff et al., 2019)</ref> put it, the Human Average Score Baseline potentially underestimates human performance relative to what is possible. To better reflect the performance of the algorithm compared to the human world record, we introduced a complete human world record baseline extended from <ref type="bibr" target="#b2">(Hafner et al., 2020;</ref><ref type="bibr" target="#b26">Toromanoff et al., 2019)</ref> to normalize the raw score, which is called the Human World Records Normalized Score (HWRNS), which can be calculated as follows:</p><formula xml:id="formula_99">HWRNS g,i = G g,i ? G g,random G g,human world records ? G g,random<label>(17)</label></formula><p>wherein g denotes the gth game of Atari, i represents the RL algorithm, G i,human represents the human world records, and G g,random represents means the performance of a random policy. Adopting HWRNS as an evaluation metric of algorithm performance has the following advantages:</p><p>1. Intuitive comparison with human world records. As HNS g,i ? 100% means algorithm i have surpassed the human world records performance in game g. We can directly use HWRNS to reflect which games the RL agents have surpassed the human world records, which can be used to calculate the human world records breakthrough in Atari benchmarks.</p><p>2. Performance across algorithms become comparable. Like the Max-Min Scaling, the HWRNS can also make two different algorithms comparable. The value of HWRNS g,i represents the degree to which algorithm i has surpassed the human world records in game g.</p><p>Mean HWRNS represents the mean performance of the algorithms across the 57 Atari games. Compared to mean HNS, mean HWRNS put forward higher requirements on the algorithm. Poor performance algorithms like SimPLe <ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref> will can be directly distinguished from other algorithms. It requires the algorithms to pursue a better performance across all the games rather than concentrate on one or two of them because breaking through any human world record is a huge milestone, which puts forward significant challenges to the performance and generality of the algorithm. For example, current model-free SOTA algorithms on HNS is Agent57 (Badia et al., 2020a), which only acquires 125.92% mean HWRNS, while GDI-H 3 obtained 154.27% mean HWRNS and thereby became the new state-of-the-art.</p><p>Median HWRNS represents the median performance of the algorithms across the 57 Atari games. Compared to Median HNS, median HWRNS also puts forward higher requirements for the algorithm. For example, current SOTA RL algorithms like Muzero <ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref> obtain much higher median HNS over GDI-H 3 but relatively lower median HWRNS.</p><p>Capped HWRNS Capped HWRNS (also called SABER) is firstly proposed and used by <ref type="bibr" target="#b26">(Toromanoff et al., 2019)</ref>, which is calculated by SABER = max{min{HWRNS, 2}, 0}. SABER also has the same problems as CHNS, and we will not repeat them here. For more details on SABER, can see <ref type="bibr" target="#b26">(Toromanoff et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3. Learning Efficiency</head><p>As we mentioned above, traditional SOTA algorithms typically ignore the low learning efficiency problem, which makes the data used for training continuously increasing (e.g., from 10B <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref> to 100B <ref type="figure" target="#fig_0">(Badia et al., 2020a)</ref>).</p><p>Increasing the training volume hinders the application of reinforcement learning algorithms into the real world. In this paper, we advocate not to improve the final performance via improving the learning efficiency instead of increasing the training volume. We advocate achieving SOTA within 200M training frames for Atari. To evaluate the learning efficiency of an algorithm, we introduce three promising metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3.1. TRAINING SCALE</head><p>As one of the commonly used metrics to reveal the learning efficiency for machine learning algorithms, training scale can also serve the purpose in RL problems. In ALE, the training scale means the scale of video frames used for training. Training frames for world modeling or planning via real-world models also need to be counted in model-based settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3.2. PLAYTIME</head><p>Playtime is a unique metric of Atari, which means the equivalent real-time gameplay <ref type="bibr" target="#b11">(Machado et al., 2018)</ref>. We can use the following formula to calculate this metric:</p><p>Playtime (day) = Num.Frames 108000*2*24 <ref type="formula" target="#formula_1">(18)</ref> For example, 200M training frames equal to 38.5 days real-time gameplay, and 100B training frames equal to 19250 days (52.7 years) real-time gameplay <ref type="bibr">(Badia et al., 2020a)</ref>. As far as we know, no Atari human world record was achieved by playing a game continuously for more than 52.7 years because it is less than 52.7 years since the birth of the Atari games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3.3. LEARNING EFFICIENCY</head><p>As we mentioned several times while discussing the drawbacks of the normalized score, learning efficiency has been ignored in massive SOTA algorithms. Many SOTA algorithms achieved SOTA through training with vast amounts of data, which may equal 52.7 years continuously playing for a human. In this paper, we argue it is unreasonable to rely on the increase of data to improve the algorithm's performance. Thus, we proposed the following metric to evaluate the learning efficiency of an algorithm:</p><p>Learning Efficiency = Related Evaluation Metric Num.Frames</p><p>For example, the learning efficiency of an algorithm over means HNS is mean HNS Num.Frames , which means the algorithms obtaining higher mean HNS via lower training frames are better than those acquiring more training data methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4. Human World Record Breakthrough</head><p>As we mentioned above, we need higher requirements to prove RL agents achieve real superhuman performance. Therefore, like the CHNS <ref type="figure" target="#fig_0">(Badia et al., 2020a)</ref>, the Human World Record Breakthrough (HWRB) can serve as the metric to reveal whether the algorithm has achieved the real superhuman performance, which can be calculated by HWRB =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Atari Benchmark</head><p>In this section, we introduce some SOTA algorithms in the Atari Benchmarks. For more details on evaluation metrics for ALE, can see App. H. For summary of the benchmark results on those evaluation metrics can see App. J. <ref type="bibr" target="#b3">(Hessel et al., 2017</ref>) is a classic value-based RL algorithm among the DQN algorithm family, which has fruitfully combined six extensions of the DQN algorithm family. It is recognized to achieve state-of-the-art performance on the ALE benchmark. Thus, we select it as one of the representative algorithms of the SOTA DQN algorithms. I.1.2. IMPALA IMPALA, namely the Importance Weighted Actor Learner Architecture <ref type="bibr">(Espeholt et al., 2018)</ref>, is a classic distributed off-policy actor-critic framework, which decouples acting from learning and learning from experience trajectories using V-trace. IMPALA actors communicate trajectories of experience (sequences of states, actions, and rewards) to a centralized learner, which boosts distributed large-scale training. Thus, we select it as one of the representative algorithms of the traditional distributed RL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1. Model-Free Reinforcement Learning</head><formula xml:id="formula_101">I.1.1. RAINBOW Rainbow</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1.3. LASER</head><p>LASER ) is a classic Actor-Critic algorithm, which investigated the combination of Actor-Critic algorithms with a uniform large-scale experience replay. It trained populations of actors with shared experiences and claimed to achieve SOTA in Atari. Thus, we select it as one of the SOTA RL algorithms within 200M training frames. I.1.4. R2D2 <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref> Like IMPALA, R2D2 <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref> is also a classic distributed RL algorithms. It trained RNN-based RL agents from distributed prioritized experience replay, which achieved SOTA in Atari. Thus, we select it as one of the representative value-based distributed RL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1.5. NGU</head><p>One of the classical problems in ALE for RL agents is the hard exploration problems <ref type="bibr">Bellemare et al., 2013;</ref><ref type="bibr">Badia et al., 2020a)</ref> like Private Eye, Montezuma's Revenge, Pitfall!. NGU (Badia et al., 2020b), or Never Give Up, try to ease this problem by augmenting the reward signal with an internally generated intrinsic reward that is sensitive to novelty at two levels: short-term novelty within an episode and long-term novelty across episodes. It then learns a family of policies for exploring and exploiting (sharing the same parameters) to obtain the highest score under the exploitative policy. NGU has achieved SOTA in Atari and thus we selected it as one of the representative population-based model-free RL algorithms. I.1.6. AGENT57</p><p>Agent57 <ref type="figure" target="#fig_0">(Badia et al., 2020a</ref>) is the SOTA model-free RL algorithms on CHNS or Median HNS of Atari Benchmark. Built on the NGU agents, Agent57 proposed a novel state-action value function parameterization method and adopted an adaptive exploration over a family of policies, which overcome the drawback of NGU (Badia et al., 2020a). We select it as one of the SOTA model-free RL algorithms. I.1.7. GDI GDI, or Generalized Data Distribution Iteration, claimed to have achieved SOTA on mean/median HWRNS, mean HNS, HWRB, median SABER of Atari Benchmark. GDI is one of the novel Reinforcement Learning paradigms, which combined a data distribution optimization operator into the traditional generalized policy iteration (GPI) <ref type="bibr" target="#b25">(Sutton &amp; Barto, 2018</ref>) and thus achieved human-level learning efficiency. Thus, we select them as one of the SOTA model-free RL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2. Model-Based Reinforcement Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2.1. SIMPLE</head><p>As one of the classic model-based RL algorithms on Atari, SimPLe, or Simulated Policy Learning <ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref>, adopted a video prediction model to enable RL agents to solve Atari problems with higher sample efficiency. It claimed to outperform the SOTA model-free algorithms in most games, so we selected it as representative model-based RL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2.2. DREAMER-V2</head><p>Dreamer-V2 <ref type="bibr" target="#b2">(Hafner et al., 2020)</ref> built world models to facilitate generalization across the experience and allow learning behaviors from imagined outcomes in the compact latent space of the world model to increase sample efficiency. Dreamer-V2 is claimed to achieve SOTA in Atari and thus we select it as one of the SOTA model-based RL algorithms within the 200M training scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2.3. MUZERO</head><p>Muzero <ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref> combined a tree-based search with a learned model and has achieved superhuman performance on Atari. We thus selected it as one of the SOTA model-based RL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3. Other SOTA algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3.1. GO-EXPLORE</head><p>As mentioned in NGU, a grand challenge in reinforcement learning is intelligent exploration, which is called the hardexploration problem <ref type="bibr" target="#b11">(Machado et al., 2018)</ref>. Go-Explore  adopted three principles to solve this problem. Firstly, agents remember previously visited states. Secondly, agents first return to a promising state and then explore it. Finally, solve simulated environment through any available means, and then robustify via imitation learning. Go-Explore has achieved SOTA in Atari, so we select it as one of the SOTA algorithms of the hard exploration problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3.2. MUESLI</head><p>Muesli  proposed a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. It acts directly with a policy network and has a computation speed comparable to model-free baselines. As it claimed to achieve SOTA in Atari within 200M training frames, we select it as one of the SOTA RL algorithms within 200M training frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.4. Summary of Benchmark Results</head><p>This part summarizes the results among all the algorithms we mentioned above on the human world record benchmark for Atari. In Figs, we illustrated the benchmark results on HNS, HWRNS, SABER, and the corresponding training scale. 6, 9 and 12, HWRB and corresponding game time and learning efficiency in <ref type="figure" target="#fig_1">Fig. 13</ref>. From those results, we see GDI has achieved SOTA in learning efficiency, HWRB, HWRNS, mean HNS, and median SABER within 200M training frames. Agent57 has achieved SOTA in mean SABER, and Muzero <ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref> has achieved SOTA in median HNS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GDI-I3 (Ours)</head><p>GDI-H3 <ref type="formula">(</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Summary of Benchmark Results</head><p>In this section, we illustrate the benchmark results of all the SOTA algorithms mentioned in this paper. For more details on these algorithms, can see App. I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1. RL Benchmarks on HNS</head><p>We report several milestones of Atari benchmarks on HNS, including DQN <ref type="bibr" target="#b12">(Mnih et al., 2015)</ref>,       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Experimental Results</head><p>In this section, we report the performance of GDI-H 3 , GDI-I 3 , and many well-known SOTA algorithms, including both the model-based and model-free methods (see App. I). First of all, we summarize the performance of all the algorithms over all the evaluation criteria of our evaluation system in App. K.1 which is mentioned in App. F. In the following three parts, we visualize the performance of GDI-H 3 , GDI-I 3 over HNS in App. K.2, HWRNS in App. K.3, SABER in App. K.4 via histogram. Furthermore, we detail all the original scores of all the algorithms and provide raw data that calculates those evaluation criteria, wherein we first provide all the human world records in 57 Atari games and calculate the HNS in App. K.5, HWRNS in App. K.6 and SABER in App. K.7 of all 57 Atari games. We further provide all the evaluation curves of GDI-H 3 and GDI-I 3 over 57 Atari games in the App. K.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1. Full Performance Comparison</head><p>In this part, we summarize the performance of all mentioned algorithms over all the evaluation criteria in Tab. 6. In the following sections, we will detail the performance of each algorithm on all Atari games one by one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2. Figure of HNS</head><p>In this part, we visualize the HNS using GDI-H 3 and GDI-I 3 in all 57 games. The HNS histogram of GDI-I 3 is illustrated in <ref type="figure" target="#fig_3">Fig. 14.</ref> The HNS histogram of GDI-H 3 is illustrated in <ref type="figure" target="#fig_4">Fig. 15</ref>.    <ref type="bibr" target="#b3">(Hessel et al., 2017)</ref>. IMPALA's scores are from <ref type="bibr">(Espeholt et al., 2018)</ref>. LASER's scores are from , with no sweep at 200M. As there are many versions of R2D2 and NGU, we use original papers'. R2D2's scores are from <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>. NGU's scores are from <ref type="figure" target="#fig_0">(Badia et al., 2020b)</ref>. Agent57's scores are from <ref type="bibr">(Badia et al., 2020a)</ref>. MuZero's scores are from <ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref>. DreamerV2's scores are from <ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>. SimPLe's scores are from <ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref>. Go-Explore's scores are from . Muesli's scores are from . In the following, we detail the raw scores and HNS of each algorithm on 57 Atari games.     <ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref> and DreamerV2 <ref type="bibr" target="#b2">(Hafner et al., 2020)</ref> haven't evaluated all 57 Atari Games in their paper. For fairness, we set the score on those games as N/A, which will not be considered when calculating the median and mean HNS.   <ref type="bibr" target="#b2">(Hafner et al., 2020;</ref><ref type="bibr" target="#b26">Toromanoff et al., 2019)</ref>. Rainbow's scores are from <ref type="bibr" target="#b3">(Hessel et al., 2017)</ref>. IMPALA's scores are from <ref type="bibr">(Espeholt et al., 2018)</ref>. LASER's scores are from , with no sweep at 200M. As there are many versions of R2D2 and NGU, we use original papers'. R2D2's scores are from <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>. NGU's scores are from <ref type="figure" target="#fig_0">(Badia et al., 2020b)</ref>. Agent57's scores are from <ref type="figure" target="#fig_0">(Badia et al., 2020a)</ref>. MuZero's scores are from <ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref>. DreamerV2's scores are from <ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>. SimPLe's scores are from <ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref>. Go-Explore's scores are from . Muesli's scores are from . In the following, we detail the raw scores and HWRNS of each algorithm on 57 Atari games.     <ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref> and DreamerV2 <ref type="bibr" target="#b2">(Hafner et al., 2020)</ref> haven't evaluated all 57 Atari Games in their paper. For fairness, we set the score on those games as N/A, which will not be considered when calculating the median and mean HWRNS and human world record breakthrough (HWRB  <ref type="bibr" target="#b2">(Hafner et al., 2020;</ref><ref type="bibr" target="#b26">Toromanoff et al., 2019)</ref>. Rainbow's scores are from <ref type="bibr" target="#b3">(Hessel et al., 2017)</ref>. IMPALA's scores are from <ref type="bibr">(Espeholt et al., 2018)</ref>. LASER's scores are from , with no sweep at 200M. As there are many versions of R2D2 and NGU, we use original papers'. R2D2's scores are from <ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>. NGU's scores are from <ref type="figure" target="#fig_0">(Badia et al., 2020b</ref>). Agent57's scores are from <ref type="figure" target="#fig_0">(Badia et al., 2020a)</ref>. MuZero's scores are from <ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref>. DreamerV2's scores are from <ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>. SimPLe's scores are from <ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref>. Go-Explore's scores are from . Muesli's scores are from . In the following, we detail the raw scores and SABER of each algorithm on 57 Atari games.           </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alien</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.5. Atari Games</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="57.">zaxxon</head><p>Generalized Data Distribution Iteration</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. Ablation Study</head><p>In this section, we firstly demonstrate the settings of our ablation studies. Then, we offer the t-SNE of three Atari games as a study case to further show the data richness among different capacities of the policy space via t-SNE. <ref type="table" target="#tab_3">Table 22</ref>. Summary of Algorithms of Ablation Study. The behavior policies are sampled from the policy space {? ? ? |? ? ?} which is parameterized by the policy network and indexed by the index set ? via a sampling distribution P?. Wherein the sampling distribution is iteratively optimized via a data distribution optimization E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Category <ref type="table" target="#tab_3">Table 23</ref>. Summary of the ablation groups in the Ablation Study. The corresponding algorithms can see Tab. L. We investigate the effects of several properties of GDI via ablating the Ablation Variables (e.g., removing the meta-controller from GDI-I 3 , and explore the impact of the meta-controller) and keeping the Control Variables (e.g., Hyperparameters) remains the same. </p><formula xml:id="formula_102">? ? ? ? P (0) ? E GDI-I 3 GDI-I 3 ? Softmax A ? ?1 + (1 ? ) ? Softmax A ? ?2 {?|? = ( , ? 1 , ? 2 )} Uniform MAB GDI-H 3 GDI-H 3 ? Softmax A ? 1 ?1 + (1 ? ) ? Softmax A ? 2 ?2 {?|? = ( , ? 1 , ? 2 )} Uniform MAB GDI-I 0 w/o E GDI-I 0 ? Softmax A ? ?1 + (1 ? ) ? Softmax A ? ?2 {?|? = ( , ? 1 , ? 2 )} One Point Identical Mapping GDI-I 1 GDI-I 1 Softmax A ? ? {?|? = (? )} Uniform MAB</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.1. Ablation Study Design</head><p>To prove the effectiveness of capacity and diversity control and the data distribution optimization operator E. All the implemented algorithms in the ablation study have been summarized in Tab. L.</p><p>We have summarized all the ablation experimental groups of the ablation study in Tab. L. The operator E is achieved with MAB (see App. E).The operator T is achieved with Vtrace, Retrace and policy gradient. Except for the Control Variable listed in the Tab. L, other settings and the shared hyperparameters remain the same in all ablation groups. The hyperparameters can see App. G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.2. t-SNE</head><p>In all the t-SNE, we mark the state generated by GDI-I 3 as A i and mark the state generated by GDI-I 1 as B i , where i = 1, 2, 3 represents three stages of the training process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Algorithm Architecture Diagram. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figures of ablation study. (a) shows how the ablation groups (see App. L) perform compared with the baseline (i.e., GDI-I 3 ). Noting that the performance has been normalized by GDI-I 3 (e.g., Mean HNS of GDI-I 1 Mean HNS of GDI-I 3 ), and w/o E means without the meta-controller. (b) and (c) illustrate the data richness (e.g., Seen Conditions All Conditions ) of GDI-I 1 and GDI-I 3 via t-SNE of visited states (see App. L.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>a|s) = min {??(a|s), ?(a|s)} b?A min {??(b|s), ?(b|s)} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>SOTA algorithms of Atari 57 games on mean and median HNS (%) and playtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>SOTA algorithms of Atari 57 games on mean and median HNS (%) and corresponding learning efficiency calculated by MEAN HNS/MEDIAN HNS TRAINING FRAMES .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>RAINBOW<ref type="bibr" target="#b3">(Hessel et al., 2017)</ref>,IMPALA (Espeholt et al., 2018), LASER, R2D2<ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>,NGU (Badia et al.,  2020b), Agent57 (Badia et al., 2020a), Go-Explore, MuZero<ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref>, DreamerV2<ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>, SimPLe<ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref> and Muesli. We summarize mean HNS and median HNS of these algorithms with their playtime (human playtime), learning efficiency , and training scale inFig 4, 5and 6.J.2. RL Benchmarks on HWRNSWe report several milestones of Atari benchmarks on Human World Records Normalized Score (HWRNS), including DQN<ref type="bibr" target="#b12">(Mnih et al., 2015)</ref>, RAINBOW<ref type="bibr" target="#b3">(Hessel et al., 2017)</ref>,IMPALA (Espeholt et al., 2018), LASER, R2D2<ref type="bibr" target="#b9">(Kapturowski et al., 2018)</ref>,NGU (Badia et al., 2020b), Agent57 (Badia et al., 2020a), Go-Explore, MuZero<ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref>, DreamerV2<ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>, SimPLe<ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref> and Muesli(Hessel   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>SOTA algorithms of Atari 57 games on mean and median HWRNS (%) and corresponding playtime.et al., 2021). We summarize mean HWRNS and median HWRNS of these algorithms with their playtime (day), learning efficiency , and training scale inFig 7, 8 and 9. J.3. RL Benchmarks on SABER We report several milestones of Atari benchmarks on Standardized Atari BEnchmark for RL (SABER), including DQN (Mnih et al., 2015), RAINBOW (Hessel et al., 2017), IMPALA (Espeholt et al., 2018), LASER (Schmitt et al., 2020), R2D2 (Kapturowski et al., 2018), NGU (Badia et al., 2020b), Agent57 (Badia et al., 2020a), Go-Explore (Ecoffet et al., 2019), MuZero<ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref>, DreamerV2<ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>, SimPLe<ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref> and Muesli. We summarize mean SABER and median SABER of these algorithms with their playtime, learning efficiency, and training scale in Figs 10, 11 and 12.J.4. RL Benchmarks on HWRBWe report several milestones of Atari benchmarks on HWRB, including DQN<ref type="bibr" target="#b12">(Mnih et al., 2015)</ref>, RAINBOW<ref type="bibr" target="#b3">(Hessel et al., 2017)</ref>,IMPALA (Espeholt et al., 2018), LASER, R2D2<ref type="bibr" target="#b9">(Kapturowski et al., 2018</ref>), NGU  (Badia et al., 2020b), Agent57 (Badia et al., 2020a, Go-Explore, MuZero<ref type="bibr" target="#b20">(Schrittwieser et al., 2020)</ref>, DreamerV2<ref type="bibr" target="#b2">(Hafner et al., 2020)</ref>, SimPLe<ref type="bibr" target="#b7">(Kaiser et al., 2019)</ref> and Muesli. We summarize HWRB of these algorithms with their playtime, learning efficiency , and training scale in Figs 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>SOTA algorithms of Atari 57 games on mean and median SABER (%) and corresponding playtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 .</head><label>13</label><figDesc>SOTA algorithms of Atari 57 games on HWRB. HWRB of SimPLe is 0, so it's not shown in the up-rightfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>HNS (%) of Atari 57 games using GDI-I 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig. 3 Fig. 20 Fig. 21 Fig. 22 Fig. 23</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experiment results of Atari. Playtime is the equivalent human playtime, HWRB is the human world record breakthrough, HNS is the human normalized score, HWRNS is the human world records normalized score, SABER = max{min{HWRNS, 2}, 0}.GDI-H 3 GDI-I 3 Muesli RAINBOW LASER R2D2 NGU Agent57</figDesc><table><row><cell cols="4">Training Scale (Num. Frames) 2E+8 2E+8 2E+8</cell><cell>2E+8</cell><cell cols="4">2E+8 1E+10 3.5E+10 1E+11</cell></row><row><cell>Playtime (Day)</cell><cell>38.5</cell><cell>38.5</cell><cell>38.5</cell><cell>38.5</cell><cell>38.5</cell><cell cols="3">1929 6751.5 19290</cell></row><row><cell>HWRB</cell><cell>22</cell><cell>17</cell><cell>5</cell><cell>4</cell><cell>7</cell><cell>15</cell><cell>8</cell><cell>18</cell></row><row><cell>Mean HNS(%)</cell><cell cols="8">9620.33 7810.1 2538.12 873.54 1740.94 3373.48 3169.07 4762.17</cell></row><row><cell>Median HNS(%)</cell><cell cols="8">1146.39 832.5 1077.47 230.99 454.91 1342.27 1174.92 1933.49</cell></row><row><cell>Mean HWRNS(%)</cell><cell cols="3">154.27 117.98 75.52</cell><cell>28.39</cell><cell cols="4">45.39 98.78 76.00 125.92</cell></row><row><cell>Median HWRNS(%)</cell><cell cols="3">50.63 35.78 24.86</cell><cell>4.92</cell><cell cols="4">8.08 33.62 21.19 43.62</cell></row><row><cell>Mean SABER(%)</cell><cell cols="3">71.26 61.66 48.74</cell><cell>28.39</cell><cell cols="4">36.78 60.43 50.47 76.26</cell></row><row><cell>Median SABER(%)</cell><cell cols="3">50.63 35.78 24.86</cell><cell>4.92</cell><cell cols="4">8.08 33.62 21.19 43.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Summary of Notation</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>s</cell><cell>state</cell></row><row><cell>a</cell><cell>action</cell></row><row><cell>S</cell><cell>set of all states</cell></row><row><cell>A</cell><cell>set of all actions</cell></row><row><cell>?</cell><cell>probability simplex</cell></row><row><cell>?</cell><cell>behavior policy</cell></row><row><cell>?</cell><cell>target policy</cell></row><row><cell>G t</cell><cell>cumulative discounted reward or return at t</cell></row><row><cell>d ? ?0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Summary ofAbbreviation</figDesc><table><row><cell>Abbreviation</cell><cell>Description</cell></row><row><cell>Sec.</cell><cell>Section (Badia et al., 2020a)</cell></row><row><cell>Figs.</cell><cell>Figures (Hafner et al., 2020)</cell></row><row><cell>Fig.</cell><cell>Figure (Badia et al., 2020a)</cell></row><row><cell>Eq.</cell><cell>Equation (Badia et al., 2020a)</cell></row><row><cell>Tab.</cell><cell>Table (Badia et al., 2020a)</cell></row><row><cell>App.</cell><cell>Appendix (Badia et al., 2020a)</cell></row><row><cell>SOTA</cell><cell>State-of-The-Art (Badia et al., 2020a)</cell></row><row><cell>RL</cell><cell>Reinforcement Learning (Sutton &amp; Barto, 2018)</cell></row><row><cell>DRL</cell><cell>Deep Reinforcement Learning (Sutton &amp; Barto, 2018)</cell></row><row><cell>GPI</cell><cell>Generalized Policy Iteration (Sutton &amp; Barto, 2018)</cell></row><row><cell>PG</cell><cell>Policy Gradient (Sutton &amp; Barto, 2018)</cell></row><row><cell>AC</cell><cell>Actor Critic (Sutton &amp; Barto, 2018)</cell></row><row><cell>ALE</cell><cell>Atari Learning Environment (Bellemare et al., 2013)</cell></row><row><cell>HNS</cell><cell>Human Normalized Score (Bellemare et al., 2013)</cell></row><row><cell>HWRB</cell><cell>Human World Records Breakthrough</cell></row><row><cell>HWRNS</cell><cell>Human World Records Normalized Score</cell></row><row><cell>SABER</cell><cell>Standardized Atari BEnchmark for RL (Toromanoff et al., 2019)</cell></row><row><cell>CHWRNS</cell><cell>Capped Human World Records Normalized Score</cell></row><row><cell>WLOG</cell><cell>Without Loss of Generality</cell></row><row><cell>w/o</cell><cell>Without</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. Initialize count = 0. while T rue do Load data from DC. Estimate qs and vs by proper off-policy algorithms. (For instance, ReTrace (B.1) for qs and V-Trace (B.2) for vs.) Update ? via policy gradient and policy evaluation. if count mod d push = 0 then Push ? to PS. end if count ? count + 1. end while // ACTOR Initialize d pull , M . Initialize ? as Eq. (7) and (8). Initialize {B m } m=1,...,M and sample ? as in Algorithm 4. Initialize count = 0, G = 0. while T rue do Calculate ? ? ? (?|s). Sample a ? ? ? ? (?|s). s, r, done ? p(?|s, a). G ? G + r. if done then Update {B m } m=1,...,M with (?, G) as in Algorithm 4. Send data to DC and reset the environment. G ? 0. Sample ? as in Algorithm 4 end if if count mod d pull = 0 then Pull ? from PS and update ?.</figDesc><table><row><cell>D.2. GDI-H 3</cell></row><row><cell>end if</cell></row><row><cell>count ? count + 1.</cell></row><row><cell>end while</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, qs 2 and vs 1 , vs 2 by proper off-policy algorithms. (For instance, ReTrace (B.1) for qs1, qs 2 and V-Trace (B.2) for vs 1 , vs 2 .) Update ? 1 , ? 2 via policy gradient and policy evaluation, respectively. if count mod d push = 0 then Push ? 1 , ? 2 to PS. end if count ? count + 1. end while // ACTOR Initialize d pull , M . Initialize ? 1 , ? 2 as Eq. (9) and (10). Initialize {B m } m=1,...,M and sample ? as in Algorithm 4. Initialize count = 0, G = 0. while T rue do Calculate ? ? ? (?|s). Sample a ? ? ? ? (?|s). s, r, done ? p(?|s, a). G ? G + r. if done then Update {B m } m=1,...,M with (?, G) as in Algorithm 4. Send data to DC and reset the environment. G ? 0. Sample ? as in Algorithm 4 end if if count mod d pull = 0 then Pull ? from PS and update ?.</figDesc><table><row><cell>Algorithm 3 GDI-H 3 Algorithm.</cell><cell></cell></row><row><cell>Initialize Parameter Server (PS) and Data Collector (DC).</cell><cell></cell></row><row><cell>// LEARNER</cell><cell>)</cell></row><row><cell>Initialize d push .</cell><cell></cell></row><row><cell>Initialize ? as Eq. (9) and (10).</cell><cell></cell></row><row><cell>Initialize count = 0.</cell><cell></cell></row><row><cell>while T rue do</cell><cell></cell></row><row><cell>Load data from DC.</cell><cell></cell></row><row><cell>Estimate qs 1 end if</cell><cell></cell></row><row><cell>count ? count + 1.</cell><cell></cell></row><row><cell>end while</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>.</head><label></label><figDesc>Algorithm 4 Bandits Controller for m = 1, ..., M do Sample mode ? {argmax, random} and other initialization parameters Initialize B m = Bandit(mode, l, r, lr, d, acc, to, ta, w, N) Ensemble B m to constitute B m end for while T rue do for m = 1, ..., M do Evaluate B m by (11). Sample candidates c m,1 , ..., c m,d from B m via (13) following its mode. end for Sample x from {c m,i } m=1,...,M ;i=1,...,d . Execute x and receive the return G. for m = 1, ..., M do Update B m with (x, G) by (12).</figDesc><table><row><cell>end for</cell></row><row><cell>end while</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Atari pre-processing hyperparameters.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Random modes and difficulties</cell><cell>No</cell></row><row><cell>Sticky action probability</cell><cell>0.0</cell></row><row><cell>Life information</cell><cell>Not allowed</cell></row><row><cell>Image Size</cell><cell>(84, 84)</cell></row><row><cell>Num. Action Repeats</cell><cell>4</cell></row><row><cell>Num. Frame Stacks</cell><cell>4</cell></row><row><cell>Action Space</cell><cell>Full</cell></row><row><cell>Max episode length</cell><cell>100000</cell></row><row><cell>Random noops range</cell><cell>30</cell></row><row><cell>Grayscaled/RGB</cell><cell>Grayscaled</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Hyperparameters for Atari experiments.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Num. Frames</cell><cell>200M (2E+8)</cell></row><row><cell>Replay</cell><cell>2</cell></row><row><cell>Num. Environments</cell><cell>160</cell></row><row><cell>GDI-I 3 Reward Shape</cell><cell>log(abs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 6 .</head><label>6</label><figDesc>Full performance comparison on Atari. The units of training scale is sampled frames. The units of playtime is huamn playtime</figDesc><table><row><cell>Median SABER</cell></row></table><note>(day). HNS(%), HWRNS(%), and SABER(%) adopts the percentage format (i.e., %). Bold scores indicate the SOTA performance Algorithms Training Scale Playtime HWRB Mean HNS Median HNS Mean HWRNS Median HWRNS Mean SABER</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Tennis Time Pilot Tutankham Up n Down Venture Video Pinball Wizard of Wor Yars Revenge</head><label></label><figDesc></figDesc><table><row><cell>Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Berzerk Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Defender Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar Hero Ice Hockey Jamesbond Kangaroo Krull Kung Fu Master Montezuma Revenge Ms Pacman Name This Game Phoenix Pitfall Pong Private Eye Qbert Riverraid Road Runner Robotank Seaquest Skiing Solaris Space Invaders Star Gunner Surround</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zaxxon</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1%</cell><cell>100%</cell><cell>10,000%</cell><cell>1,000,000%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Table of Scores Based on Human Average Records In this part, we detail the raw score of several representative SOTA algorithms , including the SOTA 200M model-free algorithms, SOTA 10B+ model-free algorithms, SOTA model-based algorithms and other SOTA algorithms. 1 Additionally, we calculate the Human Normalized Score (HNS) of each game with each algorithm. First of all, we demonstrate the sources of the scores that we used. Random scores and average human's scores are from (Badia et al., 2020a). Rainbow's scores are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 7 .</head><label>7</label><figDesc>Score table of SOTA 200M model-free algorithms on HNS(%) (GDI-I 3 ). Games RND HUMAN RAINBOW HNS IMPALA HNS LASER HNS GDI-I 3</figDesc><table><row><cell>HNS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 8 .</head><label>8</label><figDesc>Score table of SOTA 200M model-free algorithms on HNS(%) (GDI-H 3 ). Games RND HUMAN RAINBOW HNS IMPALA HNS LASER HNS GDI-H 3</figDesc><table><row><cell>HNS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 9 .</head><label>9</label><figDesc>Score table of 10B+ SOTA model-free algorithms on HNS(%).</figDesc><table><row><cell>Games</cell><cell>R2D2</cell><cell>HNS</cell><cell>NGU</cell><cell cols="4">HNS AGENT57 HNS GDI-I 3</cell><cell cols="2">HNS GDI-H 3</cell><cell>HNS</cell></row><row><cell>Scale</cell><cell>10B</cell><cell></cell><cell>35B</cell><cell></cell><cell>100B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="8">109038.4 1576.97 248100 3592.35 297638.17 4310.30 43384 625.45</cell><cell cols="2">48735 703.00</cell></row><row><cell>Amidar</cell><cell cols="7">27751.24 1619.04 17800 1038.35 29660.08 1730.42 1442</cell><cell>83.81</cell><cell>1065</cell><cell>61.81</cell></row><row><cell>Assault</cell><cell cols="10">90526.44 17379.53 34800 6654.66 67212.67 12892.66 63876 12250.50 97155 18655.23</cell></row><row><cell>Asterix</cell><cell cols="10">999080 12044.30 950700 11460.94 991384.42 11951.51 759910 9160.41 999999 12055.38</cell></row><row><cell>Asteroids</cell><cell cols="10">265861.2 568.12 230500 492.36 150854.61 321.70 751970 1609.72 760005 1626.94</cell></row><row><cell>Atlantis</cell><cell cols="10">1576068 9662.56 1653600 10141.80 1528841.76 9370.64 3803000 23427.66 3837300 23639.67</cell></row><row><cell>Bank Heist</cell><cell cols="7">46285.6 6262.20 17400 2352.93 23071.5 3120.49 1401</cell><cell>187.68</cell><cell>1380</cell><cell>184.84</cell></row><row><cell>Battle Zone</cell><cell cols="10">513360 1388.64 691700 1871.27 934134.88 2527.36 478830 1295.20 824360 2230.29</cell></row><row><cell>Beam Rider</cell><cell cols="10">128236.08 772.05 63600 381.80 300509.8 1812.19 162100 976.51 422390 2548.07</cell></row><row><cell>Berzerk</cell><cell cols="7">34134.8 1356.81 36200 1439.19 61507.83 2448.80 7607</cell><cell>298.53</cell><cell cols="2">14649 579.46</cell></row><row><cell>Bowling</cell><cell cols="4">196.36 125.92 211.9 137.21</cell><cell>251.18</cell><cell cols="2">165.76 201.9</cell><cell>129.94</cell><cell>205.2</cell><cell>132.34</cell></row><row><cell>Boxing</cell><cell>99.16</cell><cell>825.50</cell><cell>99.7</cell><cell>830.00</cell><cell>100</cell><cell>832.50</cell><cell>100</cell><cell>832.50</cell><cell>100</cell><cell>832.50</cell></row><row><cell>Breakout</cell><cell cols="4">795.36 2755.76 559.2 1935.76</cell><cell>790.4</cell><cell>2738.54</cell><cell>864</cell><cell>2994.10</cell><cell>864</cell><cell>2994.10</cell></row><row><cell>Centipede</cell><cell cols="10">532921.84 5347.83 577800 5799.95 412847.86 4138.15 155830 1548.84 195630 1949.80</cell></row><row><cell cols="11">Chopper Command 960648 14594.29 999900 15191.11 999900 15191.11 999999 15192.62 999999 15192.62</cell></row><row><cell>Crazy Climber</cell><cell cols="10">312768 1205.59 313400 1208.11 565909.85 2216.18 201000 759.39 241170 919.76</cell></row><row><cell>Defender</cell><cell cols="10">562106 3536.22 664100 4181.16 677642.78 4266.80 893110 5629.27 970540 6118.89</cell></row><row><cell>Demon Attack</cell><cell cols="10">143664.6 7890.07 143500 7881.02 143161.44 7862.41 675530 37131.12 787985 43313.70</cell></row><row><cell>Double Dunk</cell><cell cols="4">23.12 1896.36 -14.1 204.55</cell><cell>23.93</cell><cell>1933.18</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell></row><row><cell>Enduro</cell><cell cols="2">2376.68 276.20</cell><cell>2000</cell><cell>232.42</cell><cell cols="6">2367.71 275.16 14330 1665.31 14300 1661.82</cell></row><row><cell>Fishing Derby</cell><cell>81.96</cell><cell>328.28</cell><cell>32</cell><cell>233.84</cell><cell>86.97</cell><cell>337.75</cell><cell>59</cell><cell>285.71</cell><cell>65</cell><cell>296.22</cell></row><row><cell>Freeway</cell><cell>34</cell><cell>114.86</cell><cell>28.5</cell><cell>96.28</cell><cell>32.59</cell><cell>110.10</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell></row><row><cell>Frostbite</cell><cell cols="8">11238.4 261.70 206400 4832.76 541280.88 12676.32 10485 244.05</cell><cell cols="2">11330 263.84</cell></row><row><cell>Gopher</cell><cell cols="10">122196 5658.66 113400 5250.47 117777.08 5453.59 488830 22672.63 473560 21964.01</cell></row><row><cell>Gravitar</cell><cell>6750</cell><cell cols="5">206.93 14200 441/32 19213.96 599.07</cell><cell>5905</cell><cell>180.34</cell><cell>5915</cell><cell>180.66</cell></row><row><cell>Hero</cell><cell cols="8">37030.4 120.82 69400 229.44 114736.26 381.58 38330 125.18</cell><cell cols="2">38225 124.83</cell></row><row><cell>Ice Hockey</cell><cell>71.56</cell><cell>683.97</cell><cell>-4.1</cell><cell>58.68</cell><cell>63.64</cell><cell cols="2">618.51 44.94</cell><cell>463.97</cell><cell>47.11</cell><cell>481.90</cell></row><row><cell>Jamesbond</cell><cell cols="10">23266 8486.85 26600 9704.53 135784.96 49582.16 594500 217118.70 620780 226716.95</cell></row><row><cell>Kangaroo</cell><cell>14112</cell><cell cols="7">471.34 35100 1174.92 24034.16 803.96 14500 484.34</cell><cell cols="2">14636 488.90</cell></row><row><cell>Krull</cell><cell cols="10">145284.8 13460.12 127400 11784.73 251997.31 23456.61 97575 8990.82 594540 55544.92</cell></row><row><cell>Kung Fu Master</cell><cell cols="10">200176 889.40 212100 942.45 206845.82 919.07 140440 623.64 1666665 7413.57</cell></row><row><cell cols="2">Montezuma Revenge 2504</cell><cell>52.68</cell><cell cols="2">10400 218.80</cell><cell cols="2">9352.01 196.75</cell><cell>3000</cell><cell>63.11</cell><cell>2500</cell><cell>52.60</cell></row><row><cell>Ms Pacman</cell><cell cols="8">29928.2 445.81 40800 609.44 63994.44 958.52 11536 169.00</cell><cell cols="2">11573 169.55</cell></row><row><cell cols="9">Name This Game 45214.8 745.61 23900 375.35 54386.77 904.94 34434 558.34</cell><cell cols="2">36296 590.68</cell></row><row><cell>Phoenix</cell><cell cols="10">811621.6 125.11 959100 14786.66 908264.15 14002.29 894460 13789.30 959580 14794.07</cell></row><row><cell>Pitfall</cell><cell>0</cell><cell>3.43</cell><cell>7800</cell><cell cols="3">119.97 18756.01 283.66</cell><cell>0</cell><cell>3.43</cell><cell>-4.3</cell><cell>3.36</cell></row><row><cell>Pong</cell><cell>21</cell><cell>118.13</cell><cell>19.6</cell><cell>114.16</cell><cell>20.67</cell><cell>117.20</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell></row><row><cell>Private Eye</cell><cell>300</cell><cell>0.40</cell><cell cols="5">100000 143.75 79716.46 114.59 15100</cell><cell>21.68</cell><cell>15100</cell><cell>21.68</cell></row><row><cell>Qbert</cell><cell cols="8">161000 1210.10 451900 3398.79 580328.14 4365.06 27800 207.93</cell><cell cols="2">28657 214.38</cell></row><row><cell>Riverraid</cell><cell cols="8">34076.4 207.47 36700 224.10 63318.67 392.79 28075 169.44</cell><cell cols="2">28349 171.17</cell></row><row><cell>Road Runner</cell><cell cols="10">498660 6365.59 128600 1641.52 243025.8 3102.24 878600 11215.78 999999 12765.53</cell></row><row><cell>Robotank</cell><cell cols="2">132.4 1342.27</cell><cell>9.1</cell><cell>71.13</cell><cell cols="6">127.32 1289.90 108.2 1092.78 113.4 1146.39</cell></row><row><cell>Seaquest</cell><cell cols="10">999991.84 2381.55 1000000 2381.57 999997.63 2381.56 943910 2247.98 1000000 2381.57</cell></row><row><cell>Skiing</cell><cell cols="4">-29970.32 -100.87 -22977.9 -46.08</cell><cell>-4202.6</cell><cell cols="2">101.05 -6774</cell><cell>80.90</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>Solaris</cell><cell>4198.4</cell><cell>26.71</cell><cell>4700</cell><cell>31.23</cell><cell cols="3">44199.93 387.39 11074</cell><cell>88.70</cell><cell>9105</cell><cell>70.95</cell></row><row><cell>Space Invaders</cell><cell cols="10">55889 3665.48 43400 2844.22 48680.86 3191.48 140460 9226.80 154380 10142.17</cell></row><row><cell>Star Gunner</cell><cell cols="10">521728 5435.68 414600 4318.13 839573.53 8751.40 465750 4851.72 677590 7061.61</cell></row><row><cell>Surround</cell><cell>9.96</cell><cell>120.97</cell><cell>-9.6</cell><cell>2.42</cell><cell>9.5</cell><cell>118.18</cell><cell>-7.8</cell><cell>13.33</cell><cell>2.606</cell><cell>76.40</cell></row><row><cell>Tennis</cell><cell>24</cell><cell>308.39</cell><cell>10.2</cell><cell>219.35</cell><cell>23.84</cell><cell>307.35</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell></row><row><cell>Time Pilot</cell><cell cols="10">348932 20791.28 344700 20536.51 405425.31 24192.24 216770 12834.99 450810 26924.45</cell></row><row><cell>Tutankham</cell><cell cols="4">393.64 244.71 191.1 115.04</cell><cell cols="3">2354.91 1500.33 423.9</cell><cell>264.08</cell><cell>418.2</cell><cell>260.44</cell></row><row><cell>Up N Down</cell><cell cols="10">542918.8 4860.17 620100 5551.77 623805.73 5584.98 986440 8834.45 966590 8656.58</cell></row><row><cell>Venture</cell><cell>1992</cell><cell>167.75</cell><cell>1700</cell><cell>143.16</cell><cell cols="2">2623.71 220.94</cell><cell>2035</cell><cell>171.37</cell><cell>2000</cell><cell>168.42</cell></row><row><cell>Video Pinball</cell><cell cols="10">483569.72 2737.00 965300 5463.58 992340.74 5616.63 925830 5240.18 978190 5536.54</cell></row><row><cell>Wizard of Wor</cell><cell cols="10">133264 3164.81 106200 2519.35 157306.41 3738.20 64293 1519.90 63735 1506.59</cell></row><row><cell>Yars Revenge</cell><cell cols="10">918854.32 1778.73 986000 1909.15 998532.37 1933.49 972000 1881.96 968090 1874.36</cell></row><row><cell>Zaxxon</cell><cell cols="10">181372 1983.85 111100 1215.07 249808.9 2732.54 109140 1193.63 216020 2362.89</cell></row><row><cell>MEAN HNS(%)</cell><cell></cell><cell>3373.48</cell><cell></cell><cell>3169.07</cell><cell></cell><cell>4762.17</cell><cell></cell><cell>7810.1</cell><cell></cell><cell>9620.33</cell></row><row><cell>MEDIAN HNS(%)</cell><cell></cell><cell>1342.27</cell><cell></cell><cell>1174.92</cell><cell></cell><cell>1933.49</cell><cell></cell><cell>832.5</cell><cell></cell><cell>1146.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 10 .</head><label>10</label><figDesc>Score table of SOTA model-based algorithms on HNS(%). SimPLe</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 11 .</head><label>11</label><figDesc>Score table of other SOTA algorithms on HNS(%). Go-Explore and Muesli. Atari Games Table of Scores Based on Human World Records In this part, we detail the raw score of several representative SOTA algorithms , including the SOTA 200M model-free algorithms, SOTA 10B+ model-free algorithms, SOTA model-based algorithms and other SOTA algorithms. 2 Additionally, we calculate the human world records normalized world score (HWRNS) of each game with each algorithm. First of all, we demonstrate the sources of the scores that we used. Random scores are from(Badia et al., 2020a). Human world records (HWR) are from</figDesc><table><row><cell>Games</cell><cell cols="5">Muesli HNS Go-Explore HNS GDI-I 3</cell><cell>HNS</cell><cell>GDI-H 3</cell><cell>HNS</cell></row><row><cell>Scale</cell><cell>200M</cell><cell></cell><cell>10B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="6">139409 2017.12 959312 13899.77 43384 625.45</cell><cell>48735</cell><cell>703.00</cell></row><row><cell>Amidar</cell><cell cols="2">21653 1263.18</cell><cell>19083</cell><cell cols="2">1113.22 1442</cell><cell>83.81</cell><cell>1065</cell><cell>61.81</cell></row><row><cell>Assault</cell><cell cols="2">36963 7070.94</cell><cell>30773</cell><cell cols="5">5879.64 63876 12250.50 97155 18655.23</cell></row><row><cell>Asterix</cell><cell cols="8">316210 3810.30 999500 12049.37 759910 9160.41 999999 12055.38</cell></row><row><cell>Asteroids</cell><cell cols="3">484609 1036.84 112952</cell><cell cols="5">240.48 751970 1609.72 760005 1626.94</cell></row><row><cell>Atlantis</cell><cell cols="8">1363427 8348.18 286460 1691.24 3803000 23427.66 3837300 23639.67</cell></row><row><cell>Bank Heist</cell><cell cols="2">1213 162.24</cell><cell>3668</cell><cell>494.49</cell><cell>1401</cell><cell>187.68</cell><cell>1380</cell><cell>184.84</cell></row><row><cell>Battle Zone</cell><cell cols="8">414107 1120.04 998800 2702.36 478830 1295.20 824360 2230.29</cell></row><row><cell>Beam Rider</cell><cell cols="8">288870 1741.91 371723 2242.15 162100 976.51 422390 2548.07</cell></row><row><cell>Berzerk</cell><cell cols="5">44478 1769.43 131417 5237.69 7607</cell><cell>298.53</cell><cell>14649</cell><cell>579.46</cell></row><row><cell>Bowling</cell><cell>191</cell><cell>122.02</cell><cell>247</cell><cell>162.72</cell><cell>202</cell><cell>129.94</cell><cell>205.2</cell><cell>132.34</cell></row><row><cell>Boxing</cell><cell>99</cell><cell>824.17</cell><cell>91</cell><cell>757.50</cell><cell>100</cell><cell>832.50</cell><cell>100</cell><cell>832.50</cell></row><row><cell>Breakout</cell><cell cols="2">791 2740.63</cell><cell>774</cell><cell>2681.60</cell><cell>864</cell><cell>2994.10</cell><cell>864</cell><cell>2994.10</cell></row><row><cell>Centipede</cell><cell cols="8">869751 8741.20 613815 6162.78 155830 1548.84 195630 1949.80</cell></row><row><cell cols="9">Chopper Command 101289 1527.76 996220 15135.16 999999 15192.62 999999 15192.62</cell></row><row><cell>Crazy Climber</cell><cell cols="2">175322 656.88</cell><cell>235600</cell><cell cols="5">897.52 201000 759.39 241170 919.76</cell></row><row><cell>Defender</cell><cell cols="2">629482 3962.26</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">893110 5629.27 970540 6118.89</cell></row><row><cell>Demon Attack</cell><cell cols="8">129544 7113.74 239895 13180.65 675530 37131.12 787985 43313.70</cell></row><row><cell>Double Dunk</cell><cell>-3</cell><cell>709.09</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell></row><row><cell>Enduro</cell><cell cols="2">2362 274.49</cell><cell>1031</cell><cell cols="5">119.81 14330 1665.31 14300 1661.82</cell></row><row><cell>Fishing Derby</cell><cell>51</cell><cell>269.75</cell><cell>67</cell><cell>300.00</cell><cell>59</cell><cell>285.71</cell><cell>65</cell><cell>296.22</cell></row><row><cell>Freeway</cell><cell>33</cell><cell>111.49</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell></row><row><cell>Frostbite</cell><cell cols="6">301694 7064.73 999990 23420.19 10485 244.05</cell><cell>11330</cell><cell>263.84</cell></row><row><cell>Gopher</cell><cell cols="8">104441 4834.72 134244 6217.75 488830 22672.63 473560 21964.01</cell></row><row><cell>Gravitar</cell><cell cols="2">11660 361.41</cell><cell>13385</cell><cell>415.68</cell><cell>5905</cell><cell>180.34</cell><cell>5915</cell><cell>180.66</cell></row><row><cell>Hero</cell><cell cols="2">37161 121.26</cell><cell>37783</cell><cell cols="3">123.34 38330 125.18</cell><cell>38225</cell><cell>124.83</cell></row><row><cell>Ice Hockey</cell><cell>25</cell><cell>299.17</cell><cell>33</cell><cell cols="2">365.29 44.94</cell><cell>463.97</cell><cell>47.11</cell><cell>481.90</cell></row><row><cell>Jamesbond</cell><cell cols="8">19319 7045.29 200810 73331.26 594500 217118.70 620780 226716.95</cell></row><row><cell>Kangaroo</cell><cell cols="2">14096 470.80</cell><cell>24300</cell><cell cols="3">812.87 14500 484.34</cell><cell>14636</cell><cell>488.90</cell></row><row><cell>Krull</cell><cell cols="2">34221 3056.02</cell><cell>63149</cell><cell cols="5">5765.90 97575 8990.82 594540 55544.92</cell></row><row><cell cols="3">Kung Fu Master 134689 598.06</cell><cell>24320</cell><cell cols="5">107.05 140440 623.64 1666665 7413.57</cell></row><row><cell cols="2">Montezuma Revenge 2359</cell><cell>49.63</cell><cell>24758</cell><cell>520.86</cell><cell>3000</cell><cell>63.11</cell><cell>2500</cell><cell>52.60</cell></row><row><cell>Ms Pacman</cell><cell cols="2">65278 977.84</cell><cell cols="4">456123 6860.25 11536 169.00</cell><cell>11573</cell><cell>169.55</cell></row><row><cell cols="7">Name This Game 105043 1784.89 212824 3657.16 34434 558.34</cell><cell>36296</cell><cell>590.68</cell></row><row><cell>Phoenix</cell><cell cols="3">805305 12413.69 19200</cell><cell cols="5">284.50 894460 13789.30 959580 14794.07</cell></row><row><cell>Pitfall</cell><cell>0</cell><cell>3.43</cell><cell>7875</cell><cell>121.09</cell><cell>0</cell><cell>3.43</cell><cell>-4.3</cell><cell>3.36</cell></row><row><cell>Pong</cell><cell>20</cell><cell>115.30</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell></row><row><cell>Private Eye</cell><cell cols="2">10323 14.81</cell><cell>69976</cell><cell cols="2">100.58 15100</cell><cell>21.68</cell><cell>15100</cell><cell>21.68</cell></row><row><cell>Qbert</cell><cell cols="6">157353 1182.66 999975 7522.41 27800 207.93</cell><cell>28657</cell><cell>214.38</cell></row><row><cell>Riverraid</cell><cell cols="2">47323 291.42</cell><cell>35588</cell><cell cols="3">217.05 28075 169.44</cell><cell>28349</cell><cell>171.17</cell></row><row><cell>Road Runner</cell><cell cols="8">327025 4174.55 999900 12764.26 878600 11215.78 999999 12765.53</cell></row><row><cell>Robotank</cell><cell>59</cell><cell>585.57</cell><cell>143</cell><cell>1451.55</cell><cell>108</cell><cell>1092.78</cell><cell cols="2">113.4 1146.39</cell></row><row><cell>Seaquest</cell><cell cols="8">815970 1943.26 539456 1284.68 943910 2247.98 1000000 2381.57</cell></row><row><cell>Skiing</cell><cell cols="2">-18407 -10.26</cell><cell>-4185</cell><cell cols="2">101.19 -6774</cell><cell>80.90</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>Solaris</cell><cell>3031</cell><cell>16.18</cell><cell>20306</cell><cell cols="2">171.95 11074</cell><cell>88.70</cell><cell>9105</cell><cell>70.95</cell></row><row><cell>Space Invaders</cell><cell cols="2">59602 3909.65</cell><cell>93147</cell><cell cols="5">6115.54 140460 9226.80 154380 10142.17</cell></row><row><cell>Star Gunner</cell><cell cols="8">214383 2229.49 609580 6352.14 465750 4851.72 677590 7061.61</cell></row><row><cell>Surround</cell><cell>9</cell><cell>115.15</cell><cell>N/A</cell><cell>N/A</cell><cell>-8</cell><cell>13.33</cell><cell>2.606</cell><cell>76.40</cell></row><row><cell>Tennis</cell><cell>12</cell><cell>230.97</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell></row><row><cell>Time Pilot</cell><cell cols="8">359105 21403.71 183620 10839.32 216770 12834.99 450810 26924.45</cell></row><row><cell>Tutankham</cell><cell>252</cell><cell>154.03</cell><cell>528</cell><cell>330.73</cell><cell>424</cell><cell>264.08</cell><cell>418.2</cell><cell>260.44</cell></row><row><cell>Up N Down</cell><cell cols="8">649190 5812.44 553718 4956.94 986440 8834.45 966590 8656.58</cell></row><row><cell>Venture</cell><cell cols="2">2104 177.18</cell><cell>3074</cell><cell>258.86</cell><cell>2035</cell><cell>171.37</cell><cell>2000</cell><cell>168.42</cell></row><row><cell>Video Pinball</cell><cell cols="8">685436 3879.56 999999 5659.98 925830 5240.18 978190 5536.54</cell></row><row><cell>Wizard of Wor</cell><cell cols="8">93291 2211.48 199900 4754.03 64293 1519.90 63735 1506.59</cell></row><row><cell>Yars Revenge</cell><cell cols="8">557818 1077.47 999998 1936.34 972000 1881.96 968090 1874.36</cell></row><row><cell>Zaxxon</cell><cell cols="2">65325 714.30</cell><cell>18340</cell><cell cols="5">200.28 109140 1193.63 216020 2362.89</cell></row><row><cell>MEAN HNS(%)</cell><cell></cell><cell>2538.12</cell><cell></cell><cell>4989.31</cell><cell></cell><cell>7810.1</cell><cell></cell><cell>9620.33</cell></row><row><cell>MEDIAN HNS(%)</cell><cell></cell><cell>1077.47</cell><cell></cell><cell>1451.55</cell><cell></cell><cell>832.5</cell><cell></cell><cell>1146.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 12 .</head><label>12</label><figDesc>Score table of SOTA 200M model-free algorithms on HWRNS(%) (GDI-I 3 ).</figDesc><table><row><cell>Games</cell><cell>RND</cell><cell cols="9">HWR RAINBOW HWRNS IMPALA HWRNS LASER HWRNS GDI-I 3 HWRNS</cell></row><row><cell>Scale</cell><cell></cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="2">227.8 251916</cell><cell>9491.7</cell><cell>3.68</cell><cell>15962.1</cell><cell>6.25</cell><cell cols="4">976.51 14.04 43384 17.15</cell></row><row><cell>Amidar</cell><cell>5.8</cell><cell>104159</cell><cell>5131.2</cell><cell>4.92</cell><cell>1554.79</cell><cell>1.49</cell><cell>1829.2</cell><cell>1.75</cell><cell>1442</cell><cell>1.38</cell></row><row><cell>Assault</cell><cell>222.4</cell><cell>8647</cell><cell cols="8">14198.5 165.90 19148.47 224.65 21560.4 253.28 63876 755.57</cell></row><row><cell>Asterix</cell><cell cols="3">210 1000000 428200</cell><cell>42.81</cell><cell>300732</cell><cell cols="5">30.06 240090 23.99 759910 75.99</cell></row><row><cell>Asteroids</cell><cell cols="3">719 10506650 2712.8</cell><cell cols="3">0.02 108590.05 1.03</cell><cell>213025</cell><cell cols="3">2.02 751970 7.15</cell></row><row><cell>Atlantis</cell><cell cols="3">12850 10604840 826660</cell><cell cols="3">7.68 849967.5 7.90</cell><cell>841200</cell><cell cols="3">7.82 3803000 35.78</cell></row><row><cell>Bank Heist</cell><cell>14.2</cell><cell>82058</cell><cell>1358</cell><cell>1.64</cell><cell>1223.15</cell><cell>1.47</cell><cell>569.4</cell><cell>0.68</cell><cell>1401</cell><cell>1.69</cell></row><row><cell>Battle Zone</cell><cell>236</cell><cell>801000</cell><cell>62010</cell><cell>7.71</cell><cell>20885</cell><cell cols="5">2.58 64953.3 8.08 478830 59.77</cell></row><row><cell>Beam Rider</cell><cell cols="2">363.9 999999</cell><cell>16850.2</cell><cell cols="7">1.65 32463.47 3.21 90881.6 9.06 162100 16.18</cell></row><row><cell>Berzerk</cell><cell cols="2">123.7 1057940</cell><cell>2545.6</cell><cell>0.23</cell><cell>1852.7</cell><cell cols="3">0.16 25579.5 2.41</cell><cell>7607</cell><cell>0.71</cell></row><row><cell>Bowling</cell><cell>23.1</cell><cell>300</cell><cell>30</cell><cell>2.49</cell><cell>59.92</cell><cell>13.30</cell><cell>48.3</cell><cell>9.10</cell><cell>201.9</cell><cell>64.57</cell></row><row><cell>Boxing</cell><cell>0.1</cell><cell>100</cell><cell>99.6</cell><cell>99.60</cell><cell>99.96</cell><cell>99.96</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>Breakout</cell><cell>1.7</cell><cell>864</cell><cell>417.5</cell><cell>48.22</cell><cell>787.34</cell><cell>91.11</cell><cell>747.9</cell><cell>86.54</cell><cell>864</cell><cell>100.00</cell></row><row><cell>Centipede</cell><cell cols="2">2090.9 1301709</cell><cell>8167.3</cell><cell cols="3">0.47 11049.75 0.69</cell><cell cols="4">292792 22.37 155830 11.83</cell></row><row><cell>Chopper Command</cell><cell>811</cell><cell>999999</cell><cell>16654</cell><cell>1.59</cell><cell>28255</cell><cell>2.75</cell><cell cols="4">761699 76.15 999999 100.00</cell></row><row><cell>Crazy Climber</cell><cell cols="4">10780.5 219900 168788.5 75.56</cell><cell>136950</cell><cell cols="5">60.33 167820 75.10 201000 90.96</cell></row><row><cell>Defender</cell><cell cols="2">2874.5 6010500</cell><cell>55105</cell><cell>0.87</cell><cell>185203</cell><cell>3.03</cell><cell>336953</cell><cell cols="3">5.56 893110 14.82</cell></row><row><cell>Demon Attack</cell><cell cols="3">152.1 1556345 111185</cell><cell cols="3">7.13 132826.98 8.53</cell><cell>133530</cell><cell cols="3">8.57 675530 43.40</cell></row><row><cell>Double Dunk</cell><cell>-18.6</cell><cell>21</cell><cell>-0.3</cell><cell>46.21</cell><cell>-0.33</cell><cell>46.14</cell><cell>14</cell><cell>82.32</cell><cell>24</cell><cell>107.58</cell></row><row><cell>Enduro</cell><cell>0</cell><cell>9500</cell><cell>2125.9</cell><cell>22.38</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell cols="2">14330 150.84</cell></row><row><cell>Fishing Derby</cell><cell>-91.7</cell><cell>71</cell><cell>31.3</cell><cell>75.60</cell><cell>44.85</cell><cell>83.93</cell><cell>45.2</cell><cell>84.14</cell><cell>59</cell><cell>92.89</cell></row><row><cell>Freeway</cell><cell>0</cell><cell>38</cell><cell>34</cell><cell>89.47</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>34</cell><cell>89.47</cell></row><row><cell>Frostbite</cell><cell>65.2</cell><cell>454830</cell><cell>9590.5</cell><cell>2.09</cell><cell>317.75</cell><cell>0.06</cell><cell>5083.5</cell><cell>1.10</cell><cell>10485</cell><cell>2.29</cell></row><row><cell>Gopher</cell><cell cols="2">257.6 355040</cell><cell>70354.6</cell><cell cols="7">19.76 66782.3 18.75 114820.7 32.29 488830 137.71</cell></row><row><cell>Gravitar</cell><cell>173</cell><cell>162850</cell><cell>1419.3</cell><cell>0.77</cell><cell>359.5</cell><cell>0.11</cell><cell>1106.2</cell><cell>0.57</cell><cell>5905</cell><cell>3.52</cell></row><row><cell>Hero</cell><cell cols="3">1027 1000000 55887.4</cell><cell cols="5">5.49 33730.55 3.27 31628.7 3.06</cell><cell>38330</cell><cell>3.73</cell></row><row><cell>Ice Hockey</cell><cell>-11.2</cell><cell>36</cell><cell>1.1</cell><cell>26.06</cell><cell>3.48</cell><cell>31.10</cell><cell>17.4</cell><cell>60.59</cell><cell cols="2">44.92 118.94</cell></row><row><cell>Jamesbond</cell><cell>29</cell><cell>45550</cell><cell>19809</cell><cell>43.45</cell><cell>601.5</cell><cell cols="5">1.26 37999.8 83.41 594500 1305.93</cell></row><row><cell>Kangaroo</cell><cell>52</cell><cell cols="2">1424600 14637.5</cell><cell>1.02</cell><cell>1632</cell><cell>0.11</cell><cell>14308</cell><cell>1.00</cell><cell>14500</cell><cell>1.01</cell></row><row><cell>Krull</cell><cell cols="2">1598 104100</cell><cell>8741.5</cell><cell>6.97</cell><cell>8147.4</cell><cell>6.39</cell><cell>9387.5</cell><cell>7.60</cell><cell cols="2">97575 93.63</cell></row><row><cell>Kung Fu Master</cell><cell cols="2">258.5 1000000</cell><cell>52181</cell><cell>5.19</cell><cell>43375.5</cell><cell>4.31</cell><cell cols="4">607443 60.73 140440 14.02</cell></row><row><cell>Montezuma Revenge</cell><cell>0</cell><cell>1219200</cell><cell>384</cell><cell>0.03</cell><cell>0</cell><cell>0.00</cell><cell>0.3</cell><cell>0.00</cell><cell>3000</cell><cell>0.25</cell></row><row><cell>Ms Pacman</cell><cell cols="2">307.3 290090</cell><cell>5380.4</cell><cell>1.75</cell><cell>7342.32</cell><cell>2.43</cell><cell>6565.5</cell><cell>2.16</cell><cell>11536</cell><cell>3.87</cell></row><row><cell>Name This Game</cell><cell cols="2">2292.3 25220</cell><cell>13136</cell><cell cols="7">47.30 21537.2 83.94 26219.5 104.36 34434 140.19</cell></row><row><cell>Phoenix</cell><cell cols="3">761.5 4014440 108529</cell><cell cols="3">2.69 210996.45 5.24</cell><cell cols="4">519304 12.92 894460 22.27</cell></row><row><cell>Pitfall</cell><cell cols="2">-229.4 114000</cell><cell>0</cell><cell>0.20</cell><cell>-1.66</cell><cell>0.20</cell><cell>-0.6</cell><cell>0.20</cell><cell>0</cell><cell>0.20</cell></row><row><cell>Pong</cell><cell>-20.7</cell><cell>21</cell><cell>20.9</cell><cell>99.76</cell><cell>20.98</cell><cell>99.95</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>Private Eye</cell><cell>24.9</cell><cell>101800</cell><cell>4234</cell><cell>4.14</cell><cell>98.5</cell><cell>0.07</cell><cell>96.3</cell><cell>0.07</cell><cell cols="2">15100 14.81</cell></row><row><cell>Qbert</cell><cell cols="3">163.9 2400000 33817.5</cell><cell cols="5">1.40 351200.12 14.63 21449.6 0.89</cell><cell>27800</cell><cell>1.15</cell></row><row><cell>Riverraid</cell><cell cols="3">1338.5 1000000 22920.8</cell><cell cols="5">2.16 29608.05 2.83 40362.7 3.91</cell><cell>28075</cell><cell>2.68</cell></row><row><cell>Road Runner</cell><cell cols="2">11.5 2038100</cell><cell>62041</cell><cell>3.04</cell><cell>57121</cell><cell>2.80</cell><cell>45289</cell><cell cols="3">2.22 878600 43.11</cell></row><row><cell>Robotank</cell><cell>2.2</cell><cell>76</cell><cell>61.4</cell><cell>80.22</cell><cell>12.96</cell><cell>14.58</cell><cell>62.1</cell><cell>81.17</cell><cell cols="2">108.2 143.63</cell></row><row><cell>Seaquest</cell><cell>68.4</cell><cell>999999</cell><cell>15898.9</cell><cell>1.58</cell><cell>1753.2</cell><cell>0.17</cell><cell>2890.3</cell><cell cols="3">0.28 943910 94.39</cell></row><row><cell>Skiing</cell><cell cols="2">-17098 -3272</cell><cell cols="8">-12957.8 29.95 -10180.38 50.03 -29968.4 -93.09 -6774 74.67</cell></row><row><cell>Solaris</cell><cell cols="2">1236.3 111420</cell><cell>3560.3</cell><cell>2.11</cell><cell>2365</cell><cell>1.02</cell><cell>2273.5</cell><cell>0.94</cell><cell>11074</cell><cell>8.93</cell></row><row><cell>Space Invaders</cell><cell>148</cell><cell>621535</cell><cell>18789</cell><cell cols="7">3.00 43595.78 6.99 51037.4 8.19 140460 22.58</cell></row><row><cell>Star Gunner</cell><cell>664</cell><cell>77400</cell><cell>127029</cell><cell cols="7">164.67 200625 260.58 321528 418.14 465750 606.09</cell></row><row><cell>Surround</cell><cell>-10</cell><cell>9.6</cell><cell>9.7</cell><cell>100.51</cell><cell>7.56</cell><cell>89.59</cell><cell>8.4</cell><cell>93.88</cell><cell>-7.8</cell><cell>11.22</cell></row><row><cell>Tennis</cell><cell>-23.8</cell><cell>21</cell><cell>0</cell><cell>53.13</cell><cell>0.55</cell><cell>54.35</cell><cell>12.2</cell><cell>80.36</cell><cell>24</cell><cell>106.70</cell></row><row><cell>Time Pilot</cell><cell>3568</cell><cell>65300</cell><cell>12926</cell><cell cols="7">15.16 48481.5 72.76 105316 164.82 216770 345.37</cell></row><row><cell>Tutankham</cell><cell>11.4</cell><cell>5384</cell><cell>241</cell><cell>4.27</cell><cell>292.11</cell><cell>5.22</cell><cell>278.9</cell><cell>4.98</cell><cell>423.9</cell><cell>7.68</cell></row><row><cell>Up N Down</cell><cell>533.4</cell><cell>82840</cell><cell>125755</cell><cell cols="7">152.14 332546.75 403.39 345727 419.40 986440 1197.85</cell></row><row><cell>Venture</cell><cell>0</cell><cell>38900</cell><cell>5.5</cell><cell>0.01</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>2000</cell><cell>5.23</cell></row><row><cell>Video Pinball</cell><cell>0</cell><cell cols="2">89218328 533936.5</cell><cell cols="3">0.60 572898.27 0.64</cell><cell>511835</cell><cell cols="3">0.57 925830 1.04</cell></row><row><cell>Wizard of Wor</cell><cell cols="2">563.5 395300</cell><cell>17862.5</cell><cell>4.38</cell><cell>9157.5</cell><cell cols="3">2.18 29059.3 7.22</cell><cell cols="2">64439 16.14</cell></row><row><cell>Yars Revenge</cell><cell cols="3">3092.9 15000105 102557</cell><cell cols="7">0.66 84231.14 0.54 166292.3 1.09 972000 6.46</cell></row><row><cell>Zaxxon</cell><cell>32.5</cell><cell>83700</cell><cell>22209.5</cell><cell cols="3">26.51 32935.5 39.33</cell><cell>41118</cell><cell cols="3">49.11 109140 130.41</cell></row><row><cell>MEAN HWRNS(%)</cell><cell>0.00</cell><cell>100.00</cell><cell></cell><cell>28.39</cell><cell></cell><cell>34.52</cell><cell></cell><cell>45.39</cell><cell></cell><cell>117.98</cell></row><row><cell cols="2">MEDIAN HWRNS(%) 0.00</cell><cell>100.00</cell><cell></cell><cell>4.92</cell><cell></cell><cell>4.31</cell><cell></cell><cell>8.08</cell><cell></cell><cell>35.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 13 .</head><label>13</label><figDesc>Score table of SOTA 200M model-free algorithms on HWRNS(%) (GDI-H 3 ).</figDesc><table><row><cell>Games</cell><cell>RND</cell><cell cols="9">HWR RAINBOW HWRNS IMPALA HWRNS LASER HWRNS GDI-H 3 HWRNS</cell></row><row><cell>Scale</cell><cell></cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="2">227.8 251916</cell><cell>9491.7</cell><cell>3.68</cell><cell>15962.1</cell><cell>6.25</cell><cell cols="4">976.51 14.04 48735 19.27</cell></row><row><cell>Amidar</cell><cell>5.8</cell><cell>104159</cell><cell>5131.2</cell><cell>4.92</cell><cell>1554.79</cell><cell>1.49</cell><cell>1829.2</cell><cell>1.75</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>Assault</cell><cell>222.4</cell><cell>8647</cell><cell cols="8">14198.5 165.90 19148.47 224.65 21560.4 253.28 97155 1150.59</cell></row><row><cell>Asterix</cell><cell cols="3">210 1000000 428200</cell><cell>42.81</cell><cell>300732</cell><cell cols="5">30.06 240090 23.99 999999 100.00</cell></row><row><cell>Asteroids</cell><cell cols="3">719 10506650 2712.8</cell><cell cols="3">0.02 108590.05 1.03</cell><cell>213025</cell><cell cols="3">2.02 760005 7.23</cell></row><row><cell>Atlantis</cell><cell cols="3">12850 10604840 826660</cell><cell cols="3">7.68 849967.5 7.90</cell><cell>841200</cell><cell cols="3">7.82 3837300 36.11</cell></row><row><cell>Bank Heist</cell><cell>14.2</cell><cell>82058</cell><cell>1358</cell><cell>1.64</cell><cell>1223.15</cell><cell>1.47</cell><cell>569.4</cell><cell>0.68</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>Battle Zone</cell><cell>236</cell><cell>801000</cell><cell>62010</cell><cell>7.71</cell><cell>20885</cell><cell cols="5">2.58 64953.3 8.08 824360 102.92</cell></row><row><cell>Beam Rider</cell><cell cols="2">363.9 999999</cell><cell>16850.2</cell><cell cols="7">1.65 32463.47 3.21 90881.6 9.06 422390 42.22</cell></row><row><cell>Berzerk</cell><cell cols="2">123.7 1057940</cell><cell>2545.6</cell><cell>0.23</cell><cell>1852.7</cell><cell cols="3">0.16 25579.5 2.41</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>Bowling</cell><cell>23.1</cell><cell>300</cell><cell>30</cell><cell>2.49</cell><cell>59.92</cell><cell>13.30</cell><cell>48.3</cell><cell>9.10</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>Boxing</cell><cell>0.1</cell><cell>100</cell><cell>99.6</cell><cell>99.60</cell><cell>99.96</cell><cell>99.96</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>Breakout</cell><cell>1.7</cell><cell>864</cell><cell>417.5</cell><cell>48.22</cell><cell>787.34</cell><cell>91.11</cell><cell>747.9</cell><cell>86.54</cell><cell>864</cell><cell>100.00</cell></row><row><cell>Centipede</cell><cell cols="2">2090.9 1301709</cell><cell>8167.3</cell><cell cols="3">0.47 11049.75 0.69</cell><cell cols="4">292792 22.37 195630 14.89</cell></row><row><cell>Chopper Command</cell><cell>811</cell><cell>999999</cell><cell>16654</cell><cell>1.59</cell><cell>28255</cell><cell>2.75</cell><cell cols="4">761699 76.15 999999 100.00</cell></row><row><cell>Crazy Climber</cell><cell cols="4">10780.5 219900 168788.5 75.56</cell><cell>136950</cell><cell cols="5">60.33 167820 75.10 241170 110.17</cell></row><row><cell>Defender</cell><cell cols="2">2874.5 6010500</cell><cell>55105</cell><cell>0.87</cell><cell>185203</cell><cell>3.03</cell><cell>336953</cell><cell cols="3">5.56 970540 16.11</cell></row><row><cell>Demon Attack</cell><cell cols="3">152.1 1556345 111185</cell><cell cols="3">7.13 132826.98 8.53</cell><cell>133530</cell><cell cols="3">8.57 787985 50.63</cell></row><row><cell>Double Dunk</cell><cell>-18.6</cell><cell>21</cell><cell>-0.3</cell><cell>46.21</cell><cell>-0.33</cell><cell>46.14</cell><cell>14</cell><cell>82.32</cell><cell>24</cell><cell>107.58</cell></row><row><cell>Enduro</cell><cell>0</cell><cell>9500</cell><cell>2125.9</cell><cell>22.38</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell cols="2">14300 150.53</cell></row><row><cell>Fishing Derby</cell><cell>-91.7</cell><cell>71</cell><cell>31.3</cell><cell>75.60</cell><cell>44.85</cell><cell>83.93</cell><cell>45.2</cell><cell>84.14</cell><cell>65</cell><cell>96.31</cell></row><row><cell>Freeway</cell><cell>0</cell><cell>38</cell><cell>34</cell><cell>89.47</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>34</cell><cell>89.47</cell></row><row><cell>Frostbite</cell><cell>65.2</cell><cell>454830</cell><cell>9590.5</cell><cell>2.09</cell><cell>317.75</cell><cell>0.06</cell><cell>5083.5</cell><cell>1.10</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>Gopher</cell><cell cols="2">257.6 355040</cell><cell>70354.6</cell><cell cols="7">19.76 66782.3 18.75 114820.7 32.29 473560 133.41</cell></row><row><cell>Gravitar</cell><cell>173</cell><cell>162850</cell><cell>1419.3</cell><cell>0.77</cell><cell>359.5</cell><cell>0.11</cell><cell>1106.2</cell><cell>0.57</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>Hero</cell><cell cols="3">1027 1000000 55887.4</cell><cell cols="5">5.49 33730.55 3.27 31628.7 3.06</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>Ice Hockey</cell><cell>-11.2</cell><cell>36</cell><cell>1.1</cell><cell>26.06</cell><cell>3.48</cell><cell>31.10</cell><cell>17.4</cell><cell>60.59</cell><cell cols="2">47.11 123.54</cell></row><row><cell>Jamesbond</cell><cell>29</cell><cell>45550</cell><cell>19809</cell><cell>43.45</cell><cell>601.5</cell><cell cols="5">1.26 37999.8 83.41 620780 1363.66</cell></row><row><cell>Kangaroo</cell><cell>52</cell><cell cols="2">1424600 14637.5</cell><cell>1.02</cell><cell>1632</cell><cell>0.11</cell><cell>14308</cell><cell>1.00</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>Krull</cell><cell cols="2">1598 104100</cell><cell>8741.5</cell><cell>6.97</cell><cell>8147.4</cell><cell>6.39</cell><cell>9387.5</cell><cell cols="3">7.60 594540 578.47</cell></row><row><cell>Kung Fu Master</cell><cell cols="2">258.5 1000000</cell><cell>52181</cell><cell>5.19</cell><cell>43375.5</cell><cell>4.31</cell><cell cols="4">607443 60.73 1666665 166.68</cell></row><row><cell>Montezuma Revenge</cell><cell>0</cell><cell>1219200</cell><cell>384</cell><cell>0.03</cell><cell>0</cell><cell>0.00</cell><cell>0.3</cell><cell>0.00</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>Ms Pacman</cell><cell cols="2">307.3 290090</cell><cell>5380.4</cell><cell>1.75</cell><cell>7342.32</cell><cell>2.43</cell><cell>6565.5</cell><cell>2.16</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>Name This Game</cell><cell cols="2">2292.3 25220</cell><cell>13136</cell><cell cols="7">47.30 21537.2 83.94 26219.5 104.36 36296 148.31</cell></row><row><cell>Phoenix</cell><cell cols="3">761.5 4014440 108529</cell><cell cols="3">2.69 210996.45 5.24</cell><cell cols="4">519304 12.92 959580 23.89</cell></row><row><cell>Pitfall</cell><cell cols="2">-229.4 114000</cell><cell>0</cell><cell>0.20</cell><cell>-1.66</cell><cell>0.20</cell><cell>-0.6</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>Pong</cell><cell>-20.7</cell><cell>21</cell><cell>20.9</cell><cell>99.76</cell><cell>20.98</cell><cell>99.95</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>Private Eye</cell><cell>24.9</cell><cell>101800</cell><cell>4234</cell><cell>4.14</cell><cell>98.5</cell><cell>0.07</cell><cell>96.3</cell><cell>0.07</cell><cell cols="2">15100 14.81</cell></row><row><cell>Qbert</cell><cell cols="3">163.9 2400000 33817.5</cell><cell cols="5">1.40 351200.12 14.63 21449.6 0.89</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>Riverraid</cell><cell cols="3">1338.5 1000000 22920.8</cell><cell cols="5">2.16 29608.05 2.83 40362.7 3.91</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>Road Runner</cell><cell cols="2">11.5 2038100</cell><cell>62041</cell><cell>3.04</cell><cell>57121</cell><cell>2.80</cell><cell>45289</cell><cell cols="3">2.22 999999 49.06</cell></row><row><cell>Robotank</cell><cell>2.2</cell><cell>76</cell><cell>61.4</cell><cell>80.22</cell><cell>12.96</cell><cell>14.58</cell><cell>62.1</cell><cell>81.17</cell><cell cols="2">113.4 150.68</cell></row><row><cell>Seaquest</cell><cell>68.4</cell><cell>999999</cell><cell>15898.9</cell><cell>1.58</cell><cell>1753.2</cell><cell>0.17</cell><cell>2890.3</cell><cell cols="3">0.28 1000000 100.00</cell></row><row><cell>Skiing</cell><cell cols="2">-17098 -3272</cell><cell cols="8">-12957.8 29.95 -10180.38 50.03 -29968.4 -93.09 -6025 86.77</cell></row><row><cell>Solaris</cell><cell cols="2">1236.3 111420</cell><cell>3560.3</cell><cell>2.11</cell><cell>2365</cell><cell>1.02</cell><cell>2273.5</cell><cell>0.94</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>Space Invaders</cell><cell>148</cell><cell>621535</cell><cell>18789</cell><cell cols="7">3.00 43595.78 6.99 51037.4 8.19 154380 24.82</cell></row><row><cell>Star Gunner</cell><cell>664</cell><cell>77400</cell><cell>127029</cell><cell cols="7">164.67 200625 260.58 321528 418.14 677590 882.15</cell></row><row><cell>Surround</cell><cell>-10</cell><cell>9.6</cell><cell>9.7</cell><cell>100.51</cell><cell>7.56</cell><cell>89.59</cell><cell>8.4</cell><cell>93.88</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>Tennis</cell><cell>-23.8</cell><cell>21</cell><cell>0</cell><cell>53.13</cell><cell>0.55</cell><cell>54.35</cell><cell>12.2</cell><cell>80.36</cell><cell>24</cell><cell>106.70</cell></row><row><cell>Time Pilot</cell><cell>3568</cell><cell>65300</cell><cell>12926</cell><cell cols="7">15.16 48481.5 72.76 105316 164.82 450810 724.49</cell></row><row><cell>Tutankham</cell><cell>11.4</cell><cell>5384</cell><cell>241</cell><cell>4.27</cell><cell>292.11</cell><cell>5.22</cell><cell>278.9</cell><cell>4.98</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>Up N Down</cell><cell>533.4</cell><cell>82840</cell><cell>125755</cell><cell cols="7">152.14 332546.75 403.39 345727 419.40 966590 1173.73</cell></row><row><cell>Venture</cell><cell>0</cell><cell>38900</cell><cell>5.5</cell><cell>0.01</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>Video Pinball</cell><cell>0</cell><cell cols="2">89218328 533936.5</cell><cell cols="3">0.60 572898.27 0.64</cell><cell>511835</cell><cell cols="3">0.57 978190 1.10</cell></row><row><cell>Wizard of Wor</cell><cell cols="2">563.5 395300</cell><cell>17862.5</cell><cell>4.38</cell><cell>9157.5</cell><cell cols="3">2.18 29059.3 7.22</cell><cell cols="2">63735 16.00</cell></row><row><cell>Yars Revenge</cell><cell cols="3">3092.9 15000105 102557</cell><cell cols="7">0.66 84231.14 0.54 166292.3 1.09 968090 6.43</cell></row><row><cell>Zaxxon</cell><cell>32.5</cell><cell>83700</cell><cell>22209.5</cell><cell cols="3">26.51 32935.5 39.33</cell><cell>41118</cell><cell cols="3">49.11 216020 258.15</cell></row><row><cell>MEAN HWRNS(%)</cell><cell>0.00</cell><cell>100.00</cell><cell></cell><cell>28.39</cell><cell></cell><cell>34.52</cell><cell></cell><cell>45.39</cell><cell></cell><cell>154.27</cell></row><row><cell cols="2">MEDIAN HWRNS(%) 0.00</cell><cell>100.00</cell><cell></cell><cell>4.92</cell><cell></cell><cell>4.31</cell><cell></cell><cell>8.08</cell><cell></cell><cell>50.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 14 .</head><label>14</label><figDesc>Score table of SOTA 10B+ model-free algorithms on HWRNS(%). Games R2D2 HWRNS NGU HWRNS AGENT57 HWRNS GDI-I 3 HWRNS GDI-H 3 HWRNS</figDesc><table><row><cell>Scale</cell><cell>10B</cell><cell></cell><cell>35B</cell><cell></cell><cell>100B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="10">109038.4 43.23 248100 98.48 297638.17 118.17 43384 17.15 48735 19.27</cell></row><row><cell>Amidar</cell><cell cols="6">27751.24 26.64 17800 17.08 29660.08 28.47</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>Assault</cell><cell cols="10">90526.44 1071.91 34800 410.44 67212.67 795.17 63876 755.57 97155 1150.59</cell></row><row><cell>Asterix</cell><cell>999080</cell><cell cols="9">99.91 950700 95.07 991384.42 99.14 759910 75.99 999999 100.00</cell></row><row><cell>Asteroids</cell><cell cols="10">265861.2 2.52 230500 2.19 150854.61 1.43 751970 7.15 760005 7.23</cell></row><row><cell>Atlantis</cell><cell cols="10">1576068 14.76 1653600 15.49 1528841.76 14.31 3803000 35.78 3837300 36.11</cell></row><row><cell>Bank Heist</cell><cell cols="4">46285.6 56.40 17400 21.19</cell><cell>23071.5</cell><cell>28.10</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>Battle Zone</cell><cell>513360</cell><cell cols="9">64.08 691700 86.35 934134.88 116.63 478830 59.77 824360 102.92</cell></row><row><cell>Beam Rider</cell><cell cols="3">128236.08 12.79 63600</cell><cell>6.33</cell><cell cols="6">300509.8 30.03 162100 16.18 422390 42.22</cell></row><row><cell>Berzerk</cell><cell>34134.8</cell><cell>3.22</cell><cell>36200</cell><cell>3.41</cell><cell>61507.83</cell><cell>5.80</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>Bowling</cell><cell>196.36</cell><cell>62.57</cell><cell>211.9</cell><cell>68.18</cell><cell>251.18</cell><cell>82.37</cell><cell>201.9</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>Boxing</cell><cell>99.16</cell><cell>99.16</cell><cell>99.7</cell><cell>99.70</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>Breakout</cell><cell>795.36</cell><cell>92.04</cell><cell>559.2</cell><cell>64.65</cell><cell>790.4</cell><cell>91.46</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>Centipede</cell><cell cols="10">532921.84 40.85 577800 44.30 412847.86 31.61 155830 11.83 195630 14.89</cell></row><row><cell>Chopper Command</cell><cell>960648</cell><cell cols="3">96.06 999900 99.99</cell><cell>999900</cell><cell cols="5">99.99 999999 100.00 999999 100.00</cell></row><row><cell>Crazy Climber</cell><cell cols="10">312768 144.41 313400 144.71 565909.85 265.46 201000 90.96 241170 110.17</cell></row><row><cell>Defender</cell><cell>562106</cell><cell cols="9">9.31 664100 11.01 677642.78 11.23 893110 14.82 970540 16.11</cell></row><row><cell>Demon Attack</cell><cell cols="10">143664.6 9.22 143500 9.21 143161.44 9.19 675530 43.40 787985 50.63</cell></row><row><cell>Double Dunk</cell><cell>23.12</cell><cell cols="2">105.35 -14.1</cell><cell>11.36</cell><cell>23.93</cell><cell>107.40</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>Enduro</cell><cell cols="2">2376.68 25.02</cell><cell>2000</cell><cell>21.05</cell><cell>2367.71</cell><cell cols="5">24.92 14330 150.84 14300 150.53</cell></row><row><cell>Fishing Derby</cell><cell>81.96</cell><cell>106.74</cell><cell>32</cell><cell>76.03</cell><cell>86.97</cell><cell>109.82</cell><cell>59</cell><cell>92.89</cell><cell>65</cell><cell>96.31</cell></row><row><cell>Freeway</cell><cell>34</cell><cell>89.47</cell><cell>28.5</cell><cell>75.00</cell><cell>32.59</cell><cell>85.76</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>Frostbite</cell><cell>11238.4</cell><cell cols="6">2.46 206400 45.37 541280.88 119.01 10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>Gopher</cell><cell>122196</cell><cell cols="9">34.37 113400 31.89 117777.08 33.12 488830 137.71 473560 133.41</cell></row><row><cell>Gravitar</cell><cell>6750</cell><cell>4.04</cell><cell>14200</cell><cell>8.62</cell><cell cols="2">19213.96 11.70</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>Hero</cell><cell>37030.4</cell><cell>3.60</cell><cell>69400</cell><cell cols="4">6.84 114736.26 11.38 38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>Ice Hockey</cell><cell>71.56</cell><cell>175.34</cell><cell>-4.1</cell><cell>15.04</cell><cell>63.64</cell><cell cols="5">158.56 37.89 118.94 47.11 123.54</cell></row><row><cell>Jamesbond</cell><cell>23266</cell><cell cols="9">51.05 26600 58.37 135784.96 298.23 594500 1305.93 620780 1363.66</cell></row><row><cell>Kangaroo</cell><cell>14112</cell><cell>0.99</cell><cell>35100</cell><cell>2.46</cell><cell>24034.16</cell><cell>1.68</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>Krull</cell><cell cols="10">145284.8 140.18 127400 122.73 251997.31 244.29 97575 93.63 594540 578.47</cell></row><row><cell>Kung Fu Master</cell><cell>200176</cell><cell cols="9">20.00 212100 21.19 206845.82 20.66 140440 14.02 1666665 166.68</cell></row><row><cell>Montezuma Revenge</cell><cell>2504</cell><cell>0.21</cell><cell>10400</cell><cell>0.85</cell><cell>9352.01</cell><cell>0.77</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>Ms Pacman</cell><cell cols="7">29928.2 10.22 40800 13.97 63994.44 21.98 11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>Name This Game</cell><cell cols="10">45214.8 187.21 23900 94.24 54386.77 227.21 34434 140.19 36296 148.31</cell></row><row><cell>Phoenix</cell><cell cols="10">811621.6 20.20 959100 23.88 908264.15 22.61 894460 22.27 959580 23.89</cell></row><row><cell>Pitfall</cell><cell>0</cell><cell>0.20</cell><cell>7800</cell><cell>7.03</cell><cell cols="2">18756.01 16.62</cell><cell>0</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>Pong</cell><cell>21</cell><cell>100.00</cell><cell>19.6</cell><cell>96.64</cell><cell>20.67</cell><cell>99.21</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>Private Eye</cell><cell>300</cell><cell cols="9">0.27 100000 98.23 79716.46 78.30 15100 14.81 15100 14.81</cell></row><row><cell>Qbert</cell><cell>161000</cell><cell cols="6">6.70 451900 18.82 580328.14 24.18 27800</cell><cell>1.15</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>Riverraid</cell><cell>34076.4</cell><cell>3.28</cell><cell>36700</cell><cell>3.54</cell><cell>63318.67</cell><cell>6.21</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>Road Runner</cell><cell>498660</cell><cell cols="3">24.47 128600 6.31</cell><cell cols="6">243025.8 11.92 878600 43.11 999999 49.06</cell></row><row><cell>Robotank</cell><cell>132.4</cell><cell>176.42</cell><cell>9.1</cell><cell>9.35</cell><cell>127.32</cell><cell>169.54</cell><cell>108</cell><cell cols="3">143.63 113.4 150.68</cell></row><row><cell>Seaquest</cell><cell cols="10">999991.84 100.00 1000000 100.00 999997.63 100.00 943910 94.39 1000000 100.00</cell></row><row><cell>Skiing</cell><cell cols="4">-29970.32 -93.10 -22977.9 -42.53</cell><cell>-4202.6</cell><cell>93.27</cell><cell cols="2">-6774 74.67</cell><cell cols="2">-6025 86.77</cell></row><row><cell>Solaris</cell><cell>4198.4</cell><cell>2.69</cell><cell>4700</cell><cell>3.14</cell><cell cols="3">44199.93 38.99 11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>Space Invaders</cell><cell>55889</cell><cell>8.97</cell><cell>43400</cell><cell>6.96</cell><cell>48680.86</cell><cell cols="5">7.81 140460 22.58 154380 24.82</cell></row><row><cell>Star Gunner</cell><cell cols="10">521728 679.03 414600 539.43 839573.53 1093.24 465750 606.09 677590 882.15</cell></row><row><cell>Surround</cell><cell>9.96</cell><cell>101.84</cell><cell>-9.6</cell><cell>2.04</cell><cell>9.5</cell><cell>99.49</cell><cell>-7.8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>Tennis</cell><cell>24</cell><cell>106.70</cell><cell>10.2</cell><cell>75.89</cell><cell>23.84</cell><cell>106.34</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>Time Pilot</cell><cell cols="10">348932 559.46 344700 552.60 405425.31 650.97 216770 345.37 450810 724.49</cell></row><row><cell>Tutankham</cell><cell>393.64</cell><cell>7.11</cell><cell>191.1</cell><cell>3.34</cell><cell>2354.91</cell><cell>43.62</cell><cell>423.9</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>Up N Down</cell><cell cols="10">542918.8 658.98 620100 752.75 623805.73 757.26 986440 1197.85 966590 1173.73</cell></row><row><cell>Venture</cell><cell>1992</cell><cell>5.12</cell><cell>1700</cell><cell>4.37</cell><cell>2623.71</cell><cell>6.74</cell><cell>2000</cell><cell>5.23</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>Video Pinball</cell><cell cols="10">483569.72 0.54 965300 1.08 992340.74 1.11 925830 1.04 978190 1.10</cell></row><row><cell>Wizard of Wor</cell><cell>133264</cell><cell cols="9">33.62 106200 26.76 157306.41 39.71 64439 16.14 63735 16.00</cell></row><row><cell>Yars Revenge</cell><cell cols="10">918854.32 6.11 986000 6.55 998532.37 6.64 972000 6.46 968090 6.43</cell></row><row><cell>Zaxxon</cell><cell cols="10">181372 216.74 111100 132.75 249808.9 298.53 109140 130.41 216020 258.15</cell></row><row><cell>MEAN HWRNS(%)</cell><cell></cell><cell>98.78</cell><cell></cell><cell>76.00</cell><cell></cell><cell>125.92</cell><cell></cell><cell>117.98</cell><cell></cell><cell>154.27</cell></row><row><cell>MEDIAN HWRNS(%)</cell><cell></cell><cell>33.62</cell><cell></cell><cell>21.19</cell><cell></cell><cell>43.62</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 15 .</head><label>15</label><figDesc>Score table of SOTA model-based algorithms on HWRNS(%). SimPLe</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>Atari Games Table of Scores Based on SABERIn this part, we detail the raw score of several representative SOTA algorithms , including the SOTA 200M model-free algorithms, SOTA 10B+ model-free algorithms, SOTA model-based algorithms and other SOTA algorithms.<ref type="bibr" target="#b50">3</ref> Additionally, we calculate the capped human world records normalized world score (CHWRNS) or called SABER<ref type="bibr" target="#b26">(Toromanoff et al., 2019)</ref> of each game with each algorithm. First of all, we demonstrate the sources of the scores that we used. Random scores are from(Badia et al., 2020a). Human world records (HWR) are from</figDesc><table><row><cell>K.7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Games</cell><cell cols="10">MuZero HWRNS DreamerV2 HWRNS SimPLe HWRNS GDI-I 3 HWRNS GDI-H 3 HWRNS</cell></row><row><cell>Scale</cell><cell>20B</cell><cell></cell><cell>200M</cell><cell></cell><cell>1M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="2">741812.63 294.64</cell><cell>3483</cell><cell>1.29</cell><cell>616.9</cell><cell>0.15</cell><cell cols="4">43384 17.15 48735 19.27</cell></row><row><cell>Amidar</cell><cell cols="2">28634.39 27.49</cell><cell>2028</cell><cell>1.94</cell><cell>74.3</cell><cell>0.07</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>Assault</cell><cell cols="2">143972.03 1706.31</cell><cell>7679</cell><cell cols="2">88.51 527.2</cell><cell>3.62</cell><cell cols="4">63876 755.57 97155 1150.59</cell></row><row><cell>Asterix</cell><cell>998425</cell><cell>99.84</cell><cell>25669</cell><cell cols="7">2.55 1128.3 0.09 759910 75.99 999999 100.00</cell></row><row><cell>Asteroids</cell><cell cols="2">678558.64 6.45</cell><cell>3064</cell><cell>0.02</cell><cell>793.6</cell><cell cols="5">0.00 751970 7.15 760005 7.23</cell></row><row><cell>Atlantis</cell><cell cols="2">1674767.2 15.69</cell><cell>989207</cell><cell cols="7">9.22 20992.5 0.08 3803000 35.78 3837300 36.11</cell></row><row><cell>Bank Heist</cell><cell>1278.98</cell><cell>1.54</cell><cell>1043</cell><cell>1.25</cell><cell>34.2</cell><cell>0.02</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>Battle Zone</cell><cell>848623</cell><cell>105.95</cell><cell>31225</cell><cell cols="7">3.87 4031.2 0.47 478830 59.77 824360 102.92</cell></row><row><cell>Beam Rider</cell><cell cols="2">454993.53 45.48</cell><cell>12413</cell><cell>1.21</cell><cell>621.6</cell><cell cols="5">0.03 162100 16.18 422390 42.22</cell></row><row><cell>Berzerk</cell><cell>85932.6</cell><cell>8.11</cell><cell>751</cell><cell>0.06</cell><cell>N/A</cell><cell>N/A</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>Bowling</cell><cell>260.13</cell><cell>85.60</cell><cell>48</cell><cell>8.99</cell><cell>30</cell><cell>2.49</cell><cell>202</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>Boxing</cell><cell>100</cell><cell>100.00</cell><cell>87</cell><cell>86.99</cell><cell>7.8</cell><cell>7.71</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>Breakout</cell><cell>864</cell><cell>100.00</cell><cell>350</cell><cell>40.39</cell><cell>16.4</cell><cell>1.70</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>Centipede</cell><cell cols="2">1159049.27 89.02</cell><cell>6601</cell><cell>0.35</cell><cell>N/A</cell><cell cols="5">N/A 155830 11.83 195630 14.89</cell></row><row><cell>Chopper Command</cell><cell cols="2">991039.7 99.10</cell><cell>2833</cell><cell>0.20</cell><cell>979.4</cell><cell cols="5">0.02 999999 100.00 999999 100.00</cell></row><row><cell>Crazy Climber</cell><cell cols="2">458315.4 214.01</cell><cell>141424</cell><cell cols="7">62.47 62583.6 24.77 201000 90.96 241170 110.17</cell></row><row><cell>Defender</cell><cell cols="2">839642.95 13.93</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="5">N/A 893110 14.82 970540 16.11</cell></row><row><cell>Demon Attack</cell><cell cols="2">143964.26 9.24</cell><cell>2775</cell><cell>0.17</cell><cell>208.1</cell><cell cols="5">0.00 675530 43.40 787985 50.63</cell></row><row><cell>Double Dunk</cell><cell>23.94</cell><cell>107.42</cell><cell>22</cell><cell cols="2">102.53 N/A</cell><cell>N/A</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>Enduro</cell><cell>2382.44</cell><cell>25.08</cell><cell>2112</cell><cell>22.23</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">14330 150.84 14300 150.53</cell></row><row><cell>Fishing Derby</cell><cell>91.16</cell><cell>112.39</cell><cell>93.24</cell><cell cols="2">286.77 -90.7</cell><cell>0.61</cell><cell>59</cell><cell>92.89</cell><cell>65</cell><cell>96.31</cell></row><row><cell>Freeway</cell><cell>33.03</cell><cell>86.92</cell><cell>34</cell><cell>89.47</cell><cell>16.7</cell><cell>43.95</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>Frostbite</cell><cell cols="2">631378.53 138.82</cell><cell>15622</cell><cell>3.42</cell><cell>236.9</cell><cell>0.04</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>Gopher</cell><cell cols="2">130345.58 36.67</cell><cell>53853</cell><cell cols="2">15.11 596.8</cell><cell cols="5">0.10 488830 137.71 473560 133.41</cell></row><row><cell>Gravitar</cell><cell>6682.7</cell><cell>4.00</cell><cell>3554</cell><cell>2.08</cell><cell>173.4</cell><cell>0.00</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>Hero</cell><cell>49244.11</cell><cell>4.83</cell><cell>30287</cell><cell cols="3">2.93 2656.6 0.16</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>Ice Hockey</cell><cell>67.04</cell><cell>165.76</cell><cell>29</cell><cell>85.17</cell><cell>-11.6</cell><cell>-0.85</cell><cell>38</cell><cell cols="3">118.94 47.11 123.54</cell></row><row><cell>Jamesbond</cell><cell cols="2">41063.25 90.14</cell><cell>9269</cell><cell cols="2">20.30 100.5</cell><cell cols="5">0.16 594500 1305.93 620780 1363.66</cell></row><row><cell>Kangaroo</cell><cell>16763.6</cell><cell>1.17</cell><cell>11819</cell><cell>0.83</cell><cell>51.2</cell><cell>0.00</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>Krull</cell><cell cols="2">269358.27 261.22</cell><cell>9687</cell><cell cols="3">7.89 2204.8 0.59</cell><cell cols="4">97575 93.63 594540 578.47</cell></row><row><cell>Kung Fu Master</cell><cell>204824</cell><cell>20.46</cell><cell>66410</cell><cell cols="7">6.62 14862.5 1.46 140440 14.02 1666665 166.68</cell></row><row><cell>Montezuma Revenge</cell><cell>0</cell><cell>0.00</cell><cell>1932</cell><cell>0.16</cell><cell>N/A</cell><cell>N/A</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>Ms Pacman</cell><cell cols="2">243401.1 83.89</cell><cell>5651</cell><cell>1.84</cell><cell>1480</cell><cell>0.40</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>Name This Game</cell><cell cols="2">157177.85 675.54</cell><cell>14472</cell><cell cols="3">53.12 2420.7 0.56</cell><cell cols="4">34434 140.19 36296 148.31</cell></row><row><cell>Phoenix</cell><cell cols="2">955137.84 23.78</cell><cell>13342</cell><cell>0.31</cell><cell>N/A</cell><cell cols="5">N/A 894460 22.27 959580 23.89</cell></row><row><cell>Pitfall</cell><cell>0</cell><cell>0.20</cell><cell>-1</cell><cell>0.20</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>Pong</cell><cell>21</cell><cell>100.00</cell><cell>19</cell><cell>95.20</cell><cell>12.8</cell><cell>80.34</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>Private Eye</cell><cell>15299.98</cell><cell>15.01</cell><cell>158</cell><cell>0.13</cell><cell>35</cell><cell>0.01</cell><cell cols="4">15100 14.81 15100 14.81</cell></row><row><cell>Qbert</cell><cell>72276</cell><cell>3.00</cell><cell>162023</cell><cell cols="3">6.74 1288.8 0.05</cell><cell>27800</cell><cell>1.15</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>Riverraid</cell><cell cols="2">323417.18 32.25</cell><cell>16249</cell><cell cols="3">1.49 1957.8 0.06</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>Road Runner</cell><cell cols="2">613411.8 30.10</cell><cell>88772</cell><cell cols="7">4.36 5640.6 0.28 878600 43.11 999999 49.06</cell></row><row><cell>Robotank</cell><cell>131.13</cell><cell>174.70</cell><cell>65</cell><cell>85.09</cell><cell>N/A</cell><cell>N/A</cell><cell>108</cell><cell cols="3">143.63 113.4 150.68</cell></row><row><cell>Seaquest</cell><cell cols="2">999976.52 100.00</cell><cell>45898</cell><cell>4.58</cell><cell>683.3</cell><cell cols="5">0.06 943910 94.39 1000000 100.00</cell></row><row><cell>Skiing</cell><cell cols="2">-29968.36 -93.09</cell><cell>-8187</cell><cell>64.45</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">-6774 74.67</cell><cell cols="2">-6025 86.77</cell></row><row><cell>Solaris</cell><cell>56.62</cell><cell>-1.07</cell><cell>883</cell><cell>-0.32</cell><cell>N/A</cell><cell>N/A</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>Space Invaders</cell><cell>74335.3</cell><cell>11.94</cell><cell>2611</cell><cell>0.40</cell><cell>N/A</cell><cell cols="5">N/A 140460 22.58 154380 24.82</cell></row><row><cell>Star Gunner</cell><cell cols="2">549271.7 714.93</cell><cell>29219</cell><cell>37.21</cell><cell>N/A</cell><cell cols="5">N/A 465750 606.09 677590 882.15</cell></row><row><cell>Surround</cell><cell>9.99</cell><cell>101.99</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>-8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>Tennis</cell><cell>0</cell><cell>53.13</cell><cell>23</cell><cell cols="2">104.46 N/A</cell><cell>N/A</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>Time Pilot</cell><cell cols="2">476763.9 766.53</cell><cell>32404</cell><cell>46.71</cell><cell>N/A</cell><cell cols="5">N/A 216770 345.37 450810 724.49</cell></row><row><cell>Tutankham</cell><cell>491.48</cell><cell>8.94</cell><cell>238</cell><cell>4.22</cell><cell>N/A</cell><cell>N/A</cell><cell>424</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>Up N Down</cell><cell cols="2">715545.61 868.72</cell><cell>648363</cell><cell cols="7">787.09 3350.3 3.42 986440 1197.85 966590 1173.73</cell></row><row><cell>Venture</cell><cell>0.4</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>N/A</cell><cell>N/A</cell><cell>2030</cell><cell>5.23</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>Video Pinball</cell><cell cols="2">981791.88 1.10</cell><cell>22218</cell><cell>0.02</cell><cell>N/A</cell><cell cols="5">N/A 925830 1.04 978190 1.10</cell></row><row><cell>Wizard of Wor</cell><cell>197126</cell><cell>49.80</cell><cell>14531</cell><cell>3.54</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">64439 16.14 63735 16.00</cell></row><row><cell>Yars Revenge</cell><cell cols="2">553311.46 3.67</cell><cell>20089</cell><cell cols="7">0.11 5664.3 0.02 972000 6.46 968090 6.43</cell></row><row><cell>Zaxxon</cell><cell cols="2">725853.9 867.51</cell><cell>18295</cell><cell>21.83</cell><cell>N/A</cell><cell cols="5">N/A 109140 130.41 216020 258.15</cell></row><row><cell>MEAN HWRNS(%)</cell><cell></cell><cell>152.10</cell><cell></cell><cell>4.29</cell><cell></cell><cell>4.80</cell><cell></cell><cell>117.98</cell><cell></cell><cell>154.27</cell></row><row><cell>MEDIAN HWRNS(%)</cell><cell></cell><cell>49.80</cell><cell></cell><cell>4.29</cell><cell></cell><cell>0.13</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 17 .</head><label>17</label><figDesc>Score table of SOTA 200M model-free algorithms on SABER(%) (GDI-I 3 ).</figDesc><table><row><cell>Games</cell><cell>RND</cell><cell cols="9">HWR RAINBOW SABER IMPALA SABER LASER SABER GDI-I 3 SABER</cell></row><row><cell>Scale</cell><cell></cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="2">227.8 251916</cell><cell>9491.7</cell><cell>3.68</cell><cell cols="2">15962.1 6.25</cell><cell cols="4">976.51 14.04 43384 17.15</cell></row><row><cell>Amidar</cell><cell>5.8</cell><cell>104159</cell><cell>5131.2</cell><cell>4.92</cell><cell cols="2">1554.79 1.49</cell><cell cols="2">1829.2 1.75</cell><cell>1442</cell><cell>1.38</cell></row><row><cell>Assault</cell><cell>222.4</cell><cell>8647</cell><cell cols="8">14198.5 165.90 19148.47 200.00 21560.4 200.00 63876 200.00</cell></row><row><cell>Asterix</cell><cell cols="3">210 1000000 428200</cell><cell cols="7">42.81 300732 30.06 240090 23.99 759910 75.99</cell></row><row><cell>Asteroids</cell><cell cols="3">719 10506650 2712.8</cell><cell cols="7">0.02 108590.05 1.03 213025 2.02 751970 7.15</cell></row><row><cell>Atlantis</cell><cell cols="3">12850 10604840 826660</cell><cell cols="7">7.68 849967.5 7.90 841200 7.82 3803000 35.78</cell></row><row><cell>Bank Heist</cell><cell>14.2</cell><cell>82058</cell><cell>1358</cell><cell>1.64</cell><cell cols="2">1223.15 1.47</cell><cell>569.4</cell><cell>0.68</cell><cell>1401</cell><cell>1.69</cell></row><row><cell>Battle Zone</cell><cell>236</cell><cell>801000</cell><cell>62010</cell><cell>7.71</cell><cell>20885</cell><cell cols="5">2.58 64953.3 8.08 478830 59.77</cell></row><row><cell>Beam Rider</cell><cell cols="2">363.9 999999</cell><cell>16850.2</cell><cell cols="7">1.65 32463.47 3.21 90881.6 9.06 162100 16.18</cell></row><row><cell>Berzerk</cell><cell cols="2">123.7 1057940</cell><cell>2545.6</cell><cell>0.23</cell><cell>1852.7</cell><cell cols="3">0.16 25579.5 2.41</cell><cell>7607</cell><cell>0.71</cell></row><row><cell>Bowling</cell><cell>23.1</cell><cell>300</cell><cell>30</cell><cell>2.49</cell><cell>59.92</cell><cell>13.30</cell><cell>48.3</cell><cell>9.10</cell><cell cols="2">201.9 64.57</cell></row><row><cell>Boxing</cell><cell>0.1</cell><cell>100</cell><cell>99.6</cell><cell>99.60</cell><cell>99.96</cell><cell>99.96</cell><cell>100</cell><cell>100.00</cell><cell cols="2">100 100.00</cell></row><row><cell>Breakout</cell><cell>1.7</cell><cell>864</cell><cell>417.5</cell><cell>48.22</cell><cell cols="2">787.34 91.11</cell><cell cols="2">747.9 86.54</cell><cell cols="2">864 100.00</cell></row><row><cell>Centipede</cell><cell cols="2">2090.9 1301709</cell><cell>8167.3</cell><cell cols="7">0.47 11049.75 0.69 292792 22.37 155830 11.83</cell></row><row><cell>Chopper Command</cell><cell>811</cell><cell>999999</cell><cell>16654</cell><cell>1.59</cell><cell>28255</cell><cell cols="5">2.75 761699 76.15 999999 100.00</cell></row><row><cell>Crazy Climber</cell><cell cols="10">10780.5 219900 168788.5 75.56 136950 60.33 167820 75.10 201000 90.96</cell></row><row><cell>Defender</cell><cell cols="2">2874.5 6010500</cell><cell>55105</cell><cell>0.87</cell><cell>185203</cell><cell cols="5">3.03 336953 5.56 893110 14.82</cell></row><row><cell>Demon Attack</cell><cell cols="3">152.1 1556345 111185</cell><cell cols="7">7.13 132826.98 8.53 133530 8.57 675530 43.10</cell></row><row><cell>Double Dunk</cell><cell>-18.6</cell><cell>21</cell><cell>-0.3</cell><cell>46.21</cell><cell>-0.33</cell><cell>46.14</cell><cell>14</cell><cell>82.32</cell><cell>24</cell><cell>107.58</cell></row><row><cell>Enduro</cell><cell>0</cell><cell>9500</cell><cell>2125.9</cell><cell>22.38</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell cols="2">14330 150.84</cell></row><row><cell>Fishing Derby</cell><cell>-91.7</cell><cell>71</cell><cell>31.3</cell><cell>75.60</cell><cell>44.85</cell><cell>83.93</cell><cell>45.2</cell><cell>84.14</cell><cell>59</cell><cell>95.08</cell></row><row><cell>Freeway</cell><cell>0</cell><cell>38</cell><cell>34</cell><cell>89.47</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>34</cell><cell>89.47</cell></row><row><cell>Frostbite</cell><cell>65.2</cell><cell>454830</cell><cell>9590.5</cell><cell>2.09</cell><cell>317.75</cell><cell>0.06</cell><cell cols="2">5083.5 1.10</cell><cell cols="2">10485 2.29</cell></row><row><cell>Gopher</cell><cell cols="2">257.6 355040</cell><cell cols="8">70354.6 19.76 66782.3 18.75 114820.7 32.29 488830 137.71</cell></row><row><cell>Gravitar</cell><cell>173</cell><cell>162850</cell><cell>1419.3</cell><cell>0.77</cell><cell>359.5</cell><cell>0.11</cell><cell cols="2">1106.2 0.57</cell><cell>5905</cell><cell>3.52</cell></row><row><cell>Hero</cell><cell cols="3">1027 1000000 55887.4</cell><cell cols="5">5.49 33730.55 3.27 31628.7 3.06</cell><cell cols="2">38330 3.73</cell></row><row><cell>Ice Hockey</cell><cell>-11.2</cell><cell>36</cell><cell>1.1</cell><cell>26.06</cell><cell>3.48</cell><cell>31.10</cell><cell>17.4</cell><cell cols="3">60.59 44.92 118.94</cell></row><row><cell>Jamesbond</cell><cell>29</cell><cell>45550</cell><cell>19809</cell><cell>43.45</cell><cell>601.5</cell><cell cols="5">1.26 37999.8 83.41 594500 200.00</cell></row><row><cell>Kangaroo</cell><cell>52</cell><cell cols="2">1424600 14637.5</cell><cell>1.02</cell><cell>1632</cell><cell>0.11</cell><cell>14308</cell><cell>1.00</cell><cell cols="2">14500 1.01</cell></row><row><cell>Krull</cell><cell cols="2">1598 104100</cell><cell>8741.5</cell><cell>6.97</cell><cell>8147.4</cell><cell>6.39</cell><cell cols="2">9387.5 7.60</cell><cell cols="2">97575 93.63</cell></row><row><cell>Kung Fu Master</cell><cell cols="2">258.5 1000000</cell><cell>52181</cell><cell>5.19</cell><cell cols="6">43375.5 4.31 607443 60.73 140440 14.02</cell></row><row><cell>Montezuma Revenge</cell><cell>0</cell><cell>1219200</cell><cell>384</cell><cell>0.03</cell><cell>0</cell><cell>0.00</cell><cell>0.3</cell><cell>0.00</cell><cell>3000</cell><cell>0.25</cell></row><row><cell>Ms Pacman</cell><cell cols="2">307.3 290090</cell><cell>5380.4</cell><cell>1.75</cell><cell cols="2">7342.32 2.43</cell><cell cols="2">6565.5 2.16</cell><cell cols="2">11536 3.87</cell></row><row><cell cols="3">Name This Game 2292.3 25220</cell><cell>13136</cell><cell cols="7">47.30 21537.2 83.94 26219.5 104.36 34434 140.19</cell></row><row><cell>Phoenix</cell><cell cols="3">761.5 4014440 108529</cell><cell cols="7">2.69 210996.45 5.24 519304 12.92 894460 22.27</cell></row><row><cell>Pitfall</cell><cell cols="2">-229.4 114000</cell><cell>0</cell><cell>0.20</cell><cell>-1.66</cell><cell>0.20</cell><cell>-0.6</cell><cell>0.20</cell><cell>0</cell><cell>0.20</cell></row><row><cell>Pong</cell><cell>-20.7</cell><cell>21</cell><cell>20.9</cell><cell>99.76</cell><cell>20.98</cell><cell>99.95</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>Private Eye</cell><cell>24.9</cell><cell>101800</cell><cell>4234</cell><cell>4.14</cell><cell>98.5</cell><cell>0.07</cell><cell>96.3</cell><cell>0.07</cell><cell cols="2">15100 14.81</cell></row><row><cell>Qbert</cell><cell cols="3">163.9 2400000 33817.5</cell><cell cols="5">1.40 351200.12 14.63 21449.6 0.89</cell><cell cols="2">27800 1.03</cell></row><row><cell>Riverraid</cell><cell cols="3">1338.5 1000000 22920.8</cell><cell cols="5">2.16 29608.05 2.83 40362.7 3.91</cell><cell cols="2">28075 2.68</cell></row><row><cell>Road Runner</cell><cell cols="2">11.5 2038100</cell><cell>62041</cell><cell>3.04</cell><cell>57121</cell><cell>2.80</cell><cell>45289</cell><cell cols="3">2.22 878600 43.11</cell></row><row><cell>Robotank</cell><cell>2.2</cell><cell>76</cell><cell>61.4</cell><cell>80.22</cell><cell>12.96</cell><cell>14.58</cell><cell>62.1</cell><cell cols="3">81.17 108.2 143.63</cell></row><row><cell>Seaquest</cell><cell>68.4</cell><cell>999999</cell><cell>15898.9</cell><cell>1.58</cell><cell>1753.2</cell><cell>0.17</cell><cell cols="4">2890.3 0.28 943910 94.39</cell></row><row><cell>Skiing</cell><cell cols="2">-17098 -3272</cell><cell cols="8">-12957.8 29.95 -10180.38 50.03 -29968.4 -93.09 -6774 74.67</cell></row><row><cell>Solaris</cell><cell cols="2">1236.3 111420</cell><cell>3560.3</cell><cell>2.11</cell><cell>2365</cell><cell>1.02</cell><cell cols="2">2273.5 0.94</cell><cell cols="2">11074 8.93</cell></row><row><cell>Space Invaders</cell><cell>148</cell><cell>621535</cell><cell>18789</cell><cell cols="7">3.00 43595.78 6.99 51037.4 8.19 140460 22.58</cell></row><row><cell>Star Gunner</cell><cell>664</cell><cell>77400</cell><cell cols="8">127029 164.67 200625 200.00 321528 418.14 465750 200.00</cell></row><row><cell>Surround</cell><cell>-10</cell><cell>9.6</cell><cell>9.7</cell><cell>100.51</cell><cell>7.56</cell><cell>89.59</cell><cell>8.4</cell><cell>93.88</cell><cell>-7.8</cell><cell>11.22</cell></row><row><cell>Tennis</cell><cell>-23.8</cell><cell>21</cell><cell>0</cell><cell>53.13</cell><cell>0.55</cell><cell>54.35</cell><cell>12.2</cell><cell>80.36</cell><cell>24</cell><cell>106.70</cell></row><row><cell>Time Pilot</cell><cell>3568</cell><cell>65300</cell><cell>12926</cell><cell cols="7">15.16 48481.5 72.76 105316 164.82 216770 200.00</cell></row><row><cell>Tutankham</cell><cell>11.4</cell><cell>5384</cell><cell>241</cell><cell>4.27</cell><cell>292.11</cell><cell>5.22</cell><cell>278.9</cell><cell>4.98</cell><cell>423.9</cell><cell>7.68</cell></row><row><cell>Up N Down</cell><cell>533.4</cell><cell>82840</cell><cell cols="8">125755 152.14 332546.75 200.00 345727 200.00 986440 200.00</cell></row><row><cell>Venture</cell><cell>0</cell><cell>38900</cell><cell>5.5</cell><cell>0.01</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>Video Pinball</cell><cell>0</cell><cell cols="9">89218328 533936.5 0.60 572898.27 0.64 511835 0.57 925830 1.04</cell></row><row><cell>Wizard of Wor</cell><cell cols="2">563.5 395300</cell><cell>17862.5</cell><cell>4.38</cell><cell>9157.5</cell><cell cols="3">2.18 29059.3 7.22</cell><cell cols="2">64439 16.18</cell></row><row><cell>Yars Revenge</cell><cell cols="3">3092.9 15000105 102557</cell><cell cols="7">0.66 84231.14 0.54 166292.3 1.09 972000 6.46</cell></row><row><cell>Zaxxon</cell><cell>32.5</cell><cell>83700</cell><cell cols="8">22209.5 26.51 32935.5 39.33 41118 49.11 109140 130.41</cell></row><row><cell>MEAN SABER(%)</cell><cell>0.00</cell><cell>100.00</cell><cell></cell><cell>28.39</cell><cell></cell><cell>29.45</cell><cell></cell><cell>36.78</cell><cell></cell><cell>61.66</cell></row><row><cell cols="2">MEDIAN SABER(%) 0.00</cell><cell>100.00</cell><cell></cell><cell>4.92</cell><cell></cell><cell>4.31</cell><cell></cell><cell>8.08</cell><cell></cell><cell>35.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 18 .</head><label>18</label><figDesc>Score table of SOTA 200M model-free algorithms on SABER(%) (GDI-H 3 ). Atari Games Learning Curves K.8.1. ATARI GAMES LEARNING CURVES OF GDI-I 3</figDesc><table><row><cell>Games</cell><cell>RND</cell><cell cols="9">HWR RAINBOW SABER IMPALA SABER LASER SABER GDI-H 3 SABER</cell></row><row><cell>Scale</cell><cell></cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>Alien</cell><cell cols="2">227.8 251916</cell><cell>9491.7</cell><cell>3.68</cell><cell cols="2">15962.1 6.25</cell><cell cols="4">976.51 14.04 48735 19.27</cell></row><row><cell>Amidar</cell><cell>5.8</cell><cell>104159</cell><cell>5131.2</cell><cell>4.92</cell><cell cols="2">1554.79 1.49</cell><cell cols="2">1829.2 1.75</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>Assault</cell><cell>222.4</cell><cell>8647</cell><cell cols="8">14198.5 165.90 19148.47 200.00 21560.4 200.00 97155 200.00</cell></row><row><cell>Asterix</cell><cell cols="3">210 1000000 428200</cell><cell cols="7">42.81 300732 30.06 240090 23.99 999999 100.00</cell></row><row><cell>Asteroids</cell><cell cols="3">719 10506650 2712.8</cell><cell cols="7">0.02 108590.05 1.03 213025 2.02 760005 7.23</cell></row><row><cell>Atlantis</cell><cell cols="3">12850 10604840 826660</cell><cell cols="7">7.68 849967.5 7.90 841200 7.82 3837300 36.11</cell></row><row><cell>Bank Heist</cell><cell>14.2</cell><cell>82058</cell><cell>1358</cell><cell>1.64</cell><cell cols="2">1223.15 1.47</cell><cell>569.4</cell><cell>0.68</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>Battle Zone</cell><cell>236</cell><cell>801000</cell><cell>62010</cell><cell>7.71</cell><cell>20885</cell><cell cols="5">2.58 64953.3 8.08 824360 102.92</cell></row><row><cell>Beam Rider</cell><cell cols="2">363.9 999999</cell><cell>16850.2</cell><cell cols="7">1.65 32463.47 3.21 90881.6 9.06 422390 42.22</cell></row><row><cell>Berzerk</cell><cell cols="2">123.7 1057940</cell><cell>2545.6</cell><cell>0.23</cell><cell>1852.7</cell><cell cols="3">0.16 25579.5 2.41</cell><cell cols="2">14649 1.37</cell></row><row><cell>Bowling</cell><cell>23.1</cell><cell>300</cell><cell>30</cell><cell>2.49</cell><cell>59.92</cell><cell>13.30</cell><cell>48.3</cell><cell>9.10</cell><cell cols="2">205.2 65.76</cell></row><row><cell>Boxing</cell><cell>0.1</cell><cell>100</cell><cell>99.6</cell><cell>99.60</cell><cell>99.96</cell><cell>99.96</cell><cell>100</cell><cell>100.00</cell><cell cols="2">100 100.00</cell></row><row><cell>Breakout</cell><cell>1.7</cell><cell>864</cell><cell>417.5</cell><cell>48.22</cell><cell cols="2">787.34 91.11</cell><cell cols="2">747.9 86.54</cell><cell cols="2">864 100.00</cell></row><row><cell>Centipede</cell><cell cols="2">2090.9 1301709</cell><cell>8167.3</cell><cell cols="7">0.47 11049.75 0.69 292792 22.37 195630 14.89</cell></row><row><cell>Chopper Command</cell><cell>811</cell><cell>999999</cell><cell>16654</cell><cell>1.59</cell><cell>28255</cell><cell cols="5">2.75 761699 76.15 999999 100.00</cell></row><row><cell>Crazy Climber</cell><cell cols="10">10780.5 219900 168788.5 75.56 136950 60.33 167820 75.10 241170 110.17</cell></row><row><cell>Defender</cell><cell cols="2">2874.5 6010500</cell><cell>55105</cell><cell>0.87</cell><cell>185203</cell><cell cols="5">3.03 336953 5.56 970540 16.11</cell></row><row><cell>Demon Attack</cell><cell cols="3">152.1 1556345 111185</cell><cell cols="7">7.13 132826.98 8.53 133530 8.57 787985 50.63</cell></row><row><cell>Double Dunk</cell><cell>-18.6</cell><cell>21</cell><cell>-0.3</cell><cell>46.21</cell><cell>-0.33</cell><cell>46.14</cell><cell>14</cell><cell>82.32</cell><cell>24</cell><cell>107.58</cell></row><row><cell>Enduro</cell><cell>0</cell><cell>9500</cell><cell>2125.9</cell><cell>22.38</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell cols="2">14300 150.53</cell></row><row><cell>Fishing Derby</cell><cell>-91.7</cell><cell>71</cell><cell>31.3</cell><cell>75.60</cell><cell>44.85</cell><cell>83.93</cell><cell>45.2</cell><cell>84.14</cell><cell>65</cell><cell>96.31</cell></row><row><cell>Freeway</cell><cell>0</cell><cell>38</cell><cell>34</cell><cell>89.47</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>34</cell><cell>89.47</cell></row><row><cell>Frostbite</cell><cell>65.2</cell><cell>454830</cell><cell>9590.5</cell><cell>2.09</cell><cell>317.75</cell><cell>0.06</cell><cell cols="2">5083.5 1.10</cell><cell cols="2">11330 2.48</cell></row><row><cell>Gopher</cell><cell cols="2">257.6 355040</cell><cell cols="8">70354.6 19.76 66782.3 18.75 114820.7 32.29 473560 133.41</cell></row><row><cell>Gravitar</cell><cell>173</cell><cell>162850</cell><cell>1419.3</cell><cell>0.77</cell><cell>359.5</cell><cell>0.11</cell><cell cols="2">1106.2 0.57</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>Hero</cell><cell cols="3">1027 1000000 55887.4</cell><cell cols="5">5.49 33730.55 3.27 31628.7 3.06</cell><cell cols="2">38225 3.72</cell></row><row><cell>Ice Hockey</cell><cell>-11.2</cell><cell>36</cell><cell>1.1</cell><cell>26.06</cell><cell>3.48</cell><cell>31.10</cell><cell>17.4</cell><cell cols="3">60.59 47.11 123.54</cell></row><row><cell>Jamesbond</cell><cell>29</cell><cell>45550</cell><cell>19809</cell><cell>43.45</cell><cell>601.5</cell><cell cols="5">1.26 37999.8 83.41 620780 200.00</cell></row><row><cell>Kangaroo</cell><cell>52</cell><cell cols="2">1424600 14637.5</cell><cell>1.02</cell><cell>1632</cell><cell>0.11</cell><cell>14308</cell><cell>1.00</cell><cell cols="2">14636 1.02</cell></row><row><cell>Krull</cell><cell cols="2">1598 104100</cell><cell>8741.5</cell><cell>6.97</cell><cell>8147.4</cell><cell>6.39</cell><cell cols="4">9387.5 7.60 594540 200.00</cell></row><row><cell>Kung Fu Master</cell><cell cols="2">258.5 1000000</cell><cell>52181</cell><cell>5.19</cell><cell cols="6">43375.5 4.31 607443 60.73 1666665 166.68</cell></row><row><cell>Montezuma Revenge</cell><cell>0</cell><cell>1219200</cell><cell>384</cell><cell>0.03</cell><cell>0</cell><cell>0.00</cell><cell>0.3</cell><cell>0.00</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>Ms Pacman</cell><cell cols="2">307.3 290090</cell><cell>5380.4</cell><cell>1.75</cell><cell cols="2">7342.32 2.43</cell><cell cols="2">6565.5 2.16</cell><cell cols="2">11573 3.89</cell></row><row><cell cols="3">Name This Game 2292.3 25220</cell><cell>13136</cell><cell cols="7">47.30 21537.2 83.94 26219.5 104.36 36296 148.31</cell></row><row><cell>Phoenix</cell><cell cols="3">761.5 4014440 108529</cell><cell cols="7">2.69 210996.45 5.24 519304 12.92 959580 23.89</cell></row><row><cell>Pitfall</cell><cell cols="2">-229.4 114000</cell><cell>0</cell><cell>0.20</cell><cell>-1.66</cell><cell>0.20</cell><cell>-0.6</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>Pong</cell><cell>-20.7</cell><cell>21</cell><cell>20.9</cell><cell>99.76</cell><cell>20.98</cell><cell>99.95</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>Private Eye</cell><cell>24.9</cell><cell>101800</cell><cell>4234</cell><cell>4.14</cell><cell>98.5</cell><cell>0.07</cell><cell>96.3</cell><cell>0.07</cell><cell cols="2">15100 14.81</cell></row><row><cell>Qbert</cell><cell cols="3">163.9 2400000 33817.5</cell><cell cols="5">1.40 351200.12 14.63 21449.6 0.89</cell><cell cols="2">28657 1.19</cell></row><row><cell>Riverraid</cell><cell cols="3">1338.5 1000000 22920.8</cell><cell cols="5">2.16 29608.05 2.83 40362.7 3.91</cell><cell cols="2">28349 2.70</cell></row><row><cell>Road Runner</cell><cell cols="2">11.5 2038100</cell><cell>62041</cell><cell>3.04</cell><cell>57121</cell><cell>2.80</cell><cell>45289</cell><cell cols="3">2.22 999999 49.06</cell></row><row><cell>Robotank</cell><cell>2.2</cell><cell>76</cell><cell>61.4</cell><cell>80.22</cell><cell>12.96</cell><cell>14.58</cell><cell>62.1</cell><cell cols="3">81.17 113.4 150.68</cell></row><row><cell>Seaquest</cell><cell>68.4</cell><cell>999999</cell><cell>15898.9</cell><cell>1.58</cell><cell>1753.2</cell><cell>0.17</cell><cell cols="4">2890.3 0.28 1000000 100.00</cell></row><row><cell>Skiing</cell><cell cols="2">-17098 -3272</cell><cell cols="8">-12957.8 29.95 -10180.38 50.03 -29968.4 -93.09 -6025 86.77</cell></row><row><cell>Solaris</cell><cell cols="2">1236.3 111420</cell><cell>3560.3</cell><cell>2.11</cell><cell>2365</cell><cell>1.02</cell><cell cols="2">2273.5 0.94</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>Space Invaders</cell><cell>148</cell><cell>621535</cell><cell>18789</cell><cell cols="7">3.00 43595.78 6.99 51037.4 8.19 154380 24.82</cell></row><row><cell>Star Gunner</cell><cell>664</cell><cell>77400</cell><cell cols="8">127029 164.67 200625 200.00 321528 418.14 677590 200.00</cell></row><row><cell>Surround</cell><cell>-10</cell><cell>9.6</cell><cell>9.7</cell><cell>100.51</cell><cell>7.56</cell><cell>89.59</cell><cell>8.4</cell><cell cols="3">93.88 2.606 64.32</cell></row><row><cell>Tennis</cell><cell>-23.8</cell><cell>21</cell><cell>0</cell><cell>53.13</cell><cell>0.55</cell><cell>54.35</cell><cell>12.2</cell><cell>80.36</cell><cell>24</cell><cell>106.70</cell></row><row><cell>Time Pilot</cell><cell>3568</cell><cell>65300</cell><cell>12926</cell><cell cols="7">15.16 48481.5 72.76 105316 164.82 450810 200.00</cell></row><row><cell>Tutankham</cell><cell>11.4</cell><cell>5384</cell><cell>241</cell><cell>4.27</cell><cell>292.11</cell><cell>5.22</cell><cell>278.9</cell><cell>4.98</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>Up N Down</cell><cell>533.4</cell><cell>82840</cell><cell cols="8">125755 152.14 332546.75 200.00 345727 200.00 966590 200.00</cell></row><row><cell>Venture</cell><cell>0</cell><cell>38900</cell><cell>5.5</cell><cell>0.01</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>Video Pinball</cell><cell>0</cell><cell cols="9">89218328 533936.5 0.60 572898.27 0.64 511835 0.57 978190 1.10</cell></row><row><cell>Wizard of Wor</cell><cell cols="2">563.5 395300</cell><cell>17862.5</cell><cell>4.38</cell><cell>9157.5</cell><cell cols="3">2.18 29059.3 7.22</cell><cell cols="2">63735 16.00</cell></row><row><cell>Yars Revenge</cell><cell cols="3">3092.9 15000105 102557</cell><cell cols="7">0.66 84231.14 0.54 166292.3 1.09 968090 6.43</cell></row><row><cell>Zaxxon</cell><cell>32.5</cell><cell>83700</cell><cell cols="8">22209.5 26.51 32935.5 39.33 41118 49.11 216020 200.00</cell></row><row><cell>MEAN SABER(%)</cell><cell>0.00</cell><cell>100.00</cell><cell></cell><cell>28.39</cell><cell></cell><cell>29.45</cell><cell></cell><cell>36.78</cell><cell></cell><cell>71.26</cell></row><row><cell cols="2">MEDIAN SABER(%) 0.00</cell><cell>100.00</cell><cell></cell><cell>4.92</cell><cell></cell><cell>4.31</cell><cell></cell><cell>8.08</cell><cell></cell><cell>50.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head></head><label></label><figDesc>Generalized Data Distribution Iteration K.8.2. ATARI GAMES LEARNING CURVES OF GDI-H 3</figDesc><table><row><cell>-1e+4 -5e+3 0 5e+3 1e+4 1.5e+4 2e+4 2.5e+4 3e+4 3.5e+4 4e+4 4.5e+4 5e+4 5.5e+4 6e+4</cell><cell>-200 0 200 400 600 800 1e+3 1.2e+3 1.4e+3</cell><cell></cell><cell>-1e+4 0 1e+4 2e+4 3e+4 4e+4 5e+4 6e+4 7e+4 8e+4 9e+4 1e+5 1.1e+5 1.2e+5</cell></row><row><cell>-20k 0 20k 40k 60k 80k 100k120k140k160k18</cell><cell cols="2">-20k 0 20k 40k 60k 80k 100k120k140k160k18</cell><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell></row><row><cell>1. alien</cell><cell></cell><cell>2. amidar</cell><cell>3. assault</cell></row><row><cell>-1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 8e+5 9e+5 1e+6 1.1e+6 1.2e+6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4. asterix</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>51. Tutankham</cell></row><row><cell>-1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 8e+5 9e+5 1e+6 1.1e+6 1.2e+6</cell><cell>-400 -200 0 200 400 600 800 1e+3 1.2e+3 1.4e+3 1.6e+3 1.8e+3 2e+3 2.2e+3 2.4e+3</cell><cell></cell><cell>-1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 8e+5 9e+5 1e+6 1.1e+6 1.2e+6</cell></row><row><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell><cell>0</cell><cell>20k 40k 60k 80k 100k 120k 140k 16</cell><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell></row><row><cell>52. Up_N_Down</cell><cell></cell><cell>53. Venture</cell><cell>54. Video_Pinball</cell></row><row><cell>-1e+4 0 1e+4 2e+4 3e+4 4e+4 5e+4 6e+4 7e+4 8e+4</cell><cell>-1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 8e+5 9e+5 1e+6 1.1e+6 1.2e+6</cell><cell></cell><cell>-2e+4 0 2e+4 4e+4 6e+4 8e+4 1e+5 1.2e+5 1.4e+5</cell></row><row><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell><cell cols="2">-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell></row><row><cell>55. Wizard_of_Wor</cell><cell></cell><cell>56. Yars_Revenge</cell><cell>57. Zaxxon</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">i=1 (HWRNS ? 1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">200M and 10B+ represent the training scale.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">200M and 10B+ represent the training scale.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">200M and 10B+ represent the training scale.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful for the careful reading and insightful reviews of meta-reviewers and reviewers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G.</p><p>On the theory of policy gradient methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261, 2019.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rahme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mastering atari with discrete world models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02193</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rainbow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02298</idno>
		<title level="m">Combining improvements in deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muesli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06159</idno>
		<title level="m">Combining improvements in policy optimization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dynamic programming and markov processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>John Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09846</idno>
		<title level="m">Population based training of neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Model-based reinforcement learning for atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Osinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00374</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximately optimal approximate reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th International Conference on Machine Learning. Citeseer</title>
		<meeting>19th International Conference on Machine Learning. Citeseer</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent experience replay in distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Discor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07305</idno>
		<title level="m">Corrective feedback in reinforcement learning via distribution correction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno>abs/1705.05363</idno>
		<ptr target="http://arxiv.org/abs/1705.05363" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Re: Human-level performance in 3d multiplayer games with population-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<title level="m">Prioritized experience replay</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Off-policy actorcritic with shared experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8545" to="8554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mastering atari, go, chess and shogi by planning with a learned model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="issue">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Equivalence between policy gradients and soft q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06440</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04683</idno>
		<title level="m">Is deep reinforcement learning really superhuman on atari? leveling the playing field</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human learning in atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tsividis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pouncy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grandmaster level in starcraft ii using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7782</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards. King&apos;s College</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Cambridge United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A bridge between gradient of policy improvement and policy evaluation. CoRR, abs/2105.03923, 2021a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casa</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2105.03923" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An entropy regularization free mechanism for policy-based reinforcement learning. CoRR, abs/2106.00707, 2021b</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.00707" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Go-Explore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ecoffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Games Muesli HWRNS Go-Explore HWRNS GDI-I 3 HWRNS GDI-H 3 HWRNS Scale 200M</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Table 16. Score table of other SOTA algorithms on HWRNS(%)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Montezuma Revenge 2359</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">Table 19. Score table of SOTA 10B+ model-free algorithms on SABER(%)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<idno>109038.4 43.23 248100 98.48 297638.17 118.17 43384 17.15 48735 19.27 Amidar 27751.24 26.64 17800 17.08 29660.08 28.47 1442 1.38 1065 1.02 Assault 90526.44 200.00 34800 200.00 67212.67 200.00 63876 200.00 97155 200.00</idno>
		<title level="m">Games R2D2 SABER NGU SABER AGENT57 SABER GDI-I 3 SABER GDI-H 3 SABER Scale 10B 35B 100B 200M 200M Alien</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kung Fu Master</surname></persName>
		</author>
		<idno>20.00 212100 21.19 206845.82 20.66 140440 14.02 1666665 166.68</idno>
		<imprint>
			<date type="published" when="200176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Montezuma Revenge 2504</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Up N Down</surname></persName>
		</author>
		<idno>542918.8 200.00 620100 200.00 623805.73 200.00 986440 200.00 966590 200.00 Venture 1992</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SimPLe (Kaiser et al., 2019) and DreamerV2 (Hafner et al., 2020) haven&apos;t evaluated all 57 Atari Games in their paper. For fairness, we set the score on those games as N/A, which will not be considered when calculating the median and mean SABER</title>
	</analytic>
	<monogr>
		<title level="m">Games MuZero SABER DreamerV2 SABER SimPLe SABER GDI-I 3 SABER GDI-H 3 SABER(%)</title>
		<imprint/>
	</monogr>
	<note>Table 20. Score table of SOTA model-based algorithms on SABER(%)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>N/A N/A</surname></persName>
		</author>
		<idno>130.41 216020 200.00</idno>
		<imprint>
			<date type="published" when="109140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Go-Explore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ecoffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generalized Data Distribution Iteration Table 21. Score table of other SOTA algorithms on SABER(%)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Games Muesli SABER Go-Explore SABER GDI-I 3 SABER GDI-H 3 SABER Scale 200M</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Montezuma Revenge 2359</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">We sample 1k states from each stage of GDI-I 3 and GDI-I 1 . We highlight 1k</title>
	</analytic>
	<monogr>
		<title level="m">Figure 20. t-SNE of Seaquest. t-SNE is drawn from 6k states</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Figure 21. t-SNE of ChopperCommand. t-SNE is drawn from 6k states. We sample 1k states from each stage of GDI-I 3 and GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">We sample 1k states from each stage of GDI-H 3 and GDI-I 1 . We highlight 1k states of each stage of GDI-H 3 and GDI-I 1</title>
	</analytic>
	<monogr>
		<title level="m">Figure 22. t-SNE of Krull. t-SNE is drawn from 6k states</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">GDI-I 3 on Seaquest 2. GDI-I 1 on Seaquest</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">GDI-I 3 on ChopperCommand 4. GDI-I 1 on ChopperCommand</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Overview of t-SNE in Atari games. Each t-SNE figure is drawn from 6k states. We highlight 3k states of GDI-I 3 , GDI-H 3 and GDI-I 1</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>GDI-I 1 on Krull Figure 23. respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
