<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Julian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avnish</forename><surname>Narayan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Shively</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Bellathur</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>1 , UC Berkeley 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>4 , Robotics at Google 5</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>meta-learning</term>
					<term>multi-task reinforcement learning</term>
					<term>benchmarks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multitask learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While reinforcement learning (RL) has achieved some success in domains such as assembly <ref type="bibr" target="#b0">[1]</ref>, ping pong <ref type="bibr" target="#b1">[2]</ref>, in-hand manipulation <ref type="bibr" target="#b2">[3]</ref>, and hockey <ref type="bibr" target="#b3">[4]</ref>, state-of-the-art methods require substantially more experience than humans to acquire only one narrowly-defined skill. If we want robots to be broadly useful in realistic environments, we instead need algorithms that can learn a wide variety of skills reliably and efficiently. Fortunately, in most specific domains, such as robotic manipulation or locomotion, many individual tasks share common structure that can be reused to acquire related tasks more efficiently. For example, most robotic manipulation tasks involve grasping or moving objects in the workspace. However, while current methods can learn to individual skills like screwing on a bottle cap <ref type="bibr" target="#b0">[1]</ref> and hanging a mug <ref type="bibr" target="#b4">[5]</ref>, we need algorithms that can efficiently learn shared structure across many related tasks, and use that structure to learn new skills quickly, such as screwing a jar lid or hanging a bag. Recent advances in machine learning have provided unparalleled generalization capabilities in domains such as images <ref type="bibr" target="#b5">[6]</ref> and speech <ref type="bibr" target="#b6">[7]</ref>, suggesting that this should be possible; however, we have yet to see such generalization to diverse tasks in reinforcement learning settings.</p><p>Recent works in meta-learning and multi-task reinforcement learning have shown promise for addressing this gap. Multi-task RL methods aim to learn a single policy that can solve multiple tasks more efficiently than learning the tasks individually, while meta-learning methods train on many tasks, and optimize for fast adaptation to a new task. While these methods have made progress, the development of both classes of approaches has been limited by the lack of established benchmarks and evaluation protocols that reflect realistic use cases. On one hand, multi-task RL methods have largely been evaluated on disjoint and overly diverse tasks such as the Atari suite <ref type="bibr" target="#b7">[8]</ref>, where there is little efficiency to be gained by learning across games <ref type="bibr" target="#b8">[9]</ref>. On the other hand, meta-RL methods have been evaluated on very narrow task distributions. For example, one popular evaluation of metalearning involves choosing different running directions for simulated legged robots <ref type="bibr" target="#b9">[10]</ref>, which then enables fast adaptation to new directions. While these are technically distinct tasks, they are a far cry from the promise of a meta-learned model that can adapt to any new task within some domain. In order to study the capabilities of current multi-task and meta-reinforcement learning methods and make it feasible to design new algorithms that actually generalize and adapt quickly on meaningfully distinct tasks, we need evaluation protocols and task suites that are broad enough to enable this sort of generalization, while containing sufficient shared structure for generalization to be possible.</p><p>The key contributions of this work are a suite of 50 diverse simulated manipulation tasks and an extensive empirical evaluation of how previous methods perform on sets of such distinct tasks. We contend that multi-task and meta reinforcement learning methods that aim to efficiently learn many tasks and quickly generalize to new tasks should be evaluated on distributions of tasks that are diverse and exhibit shared structure. To this end, we present a benchmark of simulated manipulation tasks with everyday objects, all of which are contained in a shared, table-top environment with a simulated Sawyer arm. By providing a large set of distinct tasks that share common environment and control structure, we believe that this benchmark will allow researchers to test the generalization capabilities of the current multi-task and meta RL methods, and help to identify new research avenues to improve the current approaches. Our empirical evaluation of existing methods on this benchmark reveals that, despite some impressive progress in multi-task and meta-reinforcement learning over the past few years, current methods are generally not able to learn diverse task sets, much less generalize successfully to entirely new tasks. We provide an evaluation protocol with evaluation modes of varying difficulty, and observe that current methods show varying amounts of success on these modes This opens the door for future developments in multi-task and meta reinforcement learning: instead of focusing on further increasing performance on current narrow task suites, we believe that it is essential for future work in these areas to focus on increasing the capabilities of algorithms to handle highly diverse task sets.</p><p>By doing so, we can enable meaningful generalization across many tasks and achieve the full potential of meta-learning as a means of incorporating past experience to make it possible for robots to acquire new skills as quickly as people can.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous works that have proposed benchmarks for reinforcement learning have largely focused on single task learning settings <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. One popular benchmark used to study multi-task learning is the Arcade Learning Environment, a suite of dozens of Atari 2600 games <ref type="bibr" target="#b13">[14]</ref>. While having a tremendous impact on the multi-task reinforcement learning research community <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, the Atari games included in the benchmark have significant differences in visual appearance, controls, and objectives, making it challenging to acquire any efficiency gains through shared learning. In fact, many prior multi-task learning methods have observed substantial negative transfer between the Atari games <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. In contrast, we would like to study a case where positive transfer between the different tasks should be possible. We therefore propose a set of related yet diverse tasks that share the same robot, action space, and workspace.</p><p>Meta-reinforcement learning methods have been evaluated on a number of different problems, including maze navigation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, continuous control domains with parametric variation across tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, bandit problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>, levels of an arcade game <ref type="bibr" target="#b24">[25]</ref>, and locomotion tasks with varying dynamics <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Complementary to these evaluations, we aim to develop a testbed of tasks and an evaluation protocol that are reflective of the challenges in applying meta- <ref type="figure" target="#fig_9">Figure 1</ref>: Meta-World contains 50 manipulation tasks, designed to be diverse yet carry shared structure that can be leveraged for efficient multi-task RL and transfer to new tasks via meta-RL. In the most difficult evaluation, the method must use experience from 45 training tasks (left) to quickly learn distinctly new test tasks (right). A larger view of the environments can be found on the next page.</p><p>learning to robotic manipulation problems, including both parameteric and non-parametric variation in tasks.</p><p>There is a long history of robotics benchmarks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, datasets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, competitions <ref type="bibr" target="#b37">[38]</ref> and standardized object sets <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> that have played an important role in robotics research. Similarly, there exists a number of robotics simulation benchmarks including visual navigation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>, autonomous driving <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>, grasping <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>, single-task manipulation <ref type="bibr" target="#b51">[52]</ref>, among others. In this work, our aim is to continue this trend and provide a large suite of tasks that will allow researchers to study multi-task learning, meta-learning, and transfer in general. Further, unlike these prior simulation benchmarks, we particularly focus on providing a suite of many diverse manipulation tasks and a protocol for multi-task and meta RL evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Multi-Task and Meta-RL Problem Statements</head><p>Our proposed benchmark is aimed at making it possible to study generalization in meta-RL and multi-task RL. In this section, we define the meta-RL and multi-task RL problem statements, and describe some of the challenges associated with task distributions in these settings.</p><p>We use the formalism of Markov decision processes (MDPs), where each task T corresponds to a different finite horizon MDP, represented by a tuple (S, A, P, R, H, ?), where s ? S correspond to states, a ? A correspond to the available actions, P (s t+1 |s t , a t ) represents the stochastic transition dynamics, R(s, a) is a reward function, H is the horizon and ? is the discount factor. In standard reinforcement learning, the goal is to learn a policy ?(a|s) that maximizes the expected return, which is the sum of (discounted) rewards over all time. In multi-task and meta-RL settings, we assume a distribution of tasks p(T ). Different tasks may vary in any aspect of the Markov decision process, though efficiency gains in adaptation to new tasks are only possible if the tasks share some common structure. For example, as we describe in the next section, the tasks in our proposed benchmark have the same action space and horizon, and structurally similar rewards and state spaces. <ref type="bibr" target="#b1">2</ref> Multi-task RL problem statement. The goal of multi-task RL is to learn a single, taskconditioned policy ?(a|s, z), where z indicates an encoding of the task ID. This policy should maximize the average expected return across all tasks from the task distribution p(T ), given by</p><formula xml:id="formula_0">E T ?p(T ) [E ? [ T t=0 ? t R t (s t , a t )]</formula><p>]. The information about the task can be provided to the policy in various ways, e.g. using a one-hot task identification encoding z that is passed in addition to the cur-rent state. There is no separate test set of tasks, and multi-task RL algorithms are typically evaluated on their average performance over the training tasks.</p><p>Meta-RL problem statement. Meta-reinforcement learning aims to leverage the set of training task to learn a policy ?(a|s) that can quickly adapt to new test tasks that were not seen during training, where both training and test tasks are assumed to be drawn from the same task distribution p(T ). Typically, the training tasks are referred to as the meta-training set, to distinguish from the adaptation (training) phase performed on the (meta-) test tasks. During meta-training, the learning algorithm has access to M tasks {T i } M i=1 that are drawn from the task distribution p(T ). At meta-test time, a new task T j ? p(T ) is sampled that was not seen during meta-training, and the meta-trained policy must quickly adapt to this task to achieve the highest return with a small number of samples. A key premise in meta-RL is that a sufficiently powerful meta-RL method can meta-learn a model that effectively implements a highly efficient reinforcement learning procedure, which can then solve entirely new tasks very quickly -much more quickly than a conventional reinforcement learning algorithm learning from scratch. However, in order for this to happen, the meta-training distribution p(T ) must be sufficiently broad to encompass these new tasks. Unfortunately, most prior work in meta-RL evaluates on very narrow task distributions, with only one or two dimensions of parametric variation, such as the running direction for a simulated robot <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Meta-World</head><p>If we want meta-RL methods to generalize effectively to entirely new tasks, we must meta-train on broad task distributions that are representative of the range of tasks that a particular agent might need to solve in the future. To this end, we propose a new multi-task and meta-RL benchmark, which we call Meta-World. In this section, we motivate the design decisions behind the Meta-World tasks, discuss the range of tasks, describe the representation of the actions, observations, and rewards, and present a set of evaluation protocols of varying difficulty for both meta-RL and multi-task RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Space of Manipulation Tasks: Parametric and Non-Parametric Variability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parametric Task Variation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Parametric Task Variation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reach Puck</head><p>Open Window A task, T , in Meta-World is defined as the tuple (reward function, initial object position, target position) Meta-learning makes two critical assumptions: first, that the meta-training and meta-test tasks are drawn from the same distribution, p(T ), and second, that the task distribution p(T ) exhibits shared structure that can be utilized for efficient adaptation to new tasks. If p(T ) is defined as a family of variations within a particular control task, as in prior work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>, then it is unreasonable to hope for generalization to entirely new control tasks. For example, an agent has little hope of being able to quickly learn to open a door, without having ever experienced doors before, if it has only been trained on a set of meta-training tasks that are homogeneous and narrow. Thus, to enable meta-RL methods to adapt to entirely new tasks, we propose a much larger suite of tasks consisting of 50 qualitatively-distinct manipulation tasks, where continuous parameter variation cannot be used to describe the differences between tasks.</p><p>With such non-parametric variation, however, there is the danger that tasks will not exhibit enough shared structure, or will lack the task overlap needed for the method to avoid memorizing each of the tasks. Motivated by this challenge, we design each task to include parametric variation in object and goal positions, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Introducing this parametric variability not only creates a substantially larger (infinite) variety of tasks, but also makes it substantially more practical to expect that a meta-trained model will generalize to acquire entirely new tasks more quickly, since varying the positions provides for wider coverage of the space of possible manipulation tasks. Without parametric variation, the model could for example memorize that any object at a particular location is a door, while any object at another location is a drawer. If the locations are not fixed, this kind of memorization is much less likely, and the model is forced to generalize more broadly. With enough tasks and variation within tasks, pairs of qualitatively-distinct tasks are more likely to overlap, serving as a catalyst for generalization. For example, closing a drawer and pushing a block can appear as nearly the same task for some initial and goal positions of each object.</p><p>Note that this kind of parametric variation, which we introduce for each task, essentially represents the entirety of the task distribution for previous meta-RL evaluations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>, which test on single tasks (e.g., running towards a goal) with parametric variability (e.g., variation in the goal position). Our full task distribution is therefore substantially broader, since it includes this parametric variability for each of the 50 tasks.</p><p>To provide shared structure, the 50 environments require the same robotic arm to interact with different objects, with different shapes, joints, and connectivity. The tasks themselves require the robot to execute a combination of reaching, pushing, and grasping, depending on the task. By recombining these basic behavioral building blocks with a variety of objects with different shapes and articulation properties, we can create a wide range of manipulation tasks. For example, the open door task involves pushing or grasping an object with a revolute joint, while the open drawer task requires pushing or grasping an object with a sliding joint. More complex tasks require a combination of these building blocks, which must be executed in the right order. We visualize all of the tasks in Meta-World in <ref type="figure" target="#fig_9">Figure 1</ref>, and include a description of all tasks in Appendix A.</p><p>All of the tasks are implemented in the MuJoCo physics engine <ref type="bibr" target="#b52">[53]</ref>, which enables fast simulation of physical contact. To make the interface simple and accessible, we base our suite on the Multiworld interface <ref type="bibr" target="#b53">[54]</ref> and the OpenAI Gym environment interfaces <ref type="bibr" target="#b10">[11]</ref>, making additions and adaptations of the suite relatively easy for researchers already familiar with Gym.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Actions, Observations, and Rewards</head><p>In order to represent policies for multiple tasks with one model, the observation and action spaces must contain significant shared structure across tasks. All of our tasks are performed by a simulated Sawyer robot. The action space is a 2-tuple consisting of the change in 3D space of the end-effector followed by a normalized torque that the gripper fingers should apply. The actions in this space range between ?1 and 1. For all tasks, the robot must either manipulate one object with a variable goal position, or manipulate two objects with a fixed goal position. The observation space is represented as a 6-tuple of the 3D Cartesian positions of the end-effector, a normalized measurement of how open the gripper is, the 3D position of the first object, the quaternion of the first object, the 3D position of the second object, the quaternion of the second object, all of the previous measurements in the environment, and finally the 3D position of the goal. If there is no second object or the goal is not meant to be included in the observation, then the quantities corresponding to them are zeroed out. The observation space is always 39 dimensional.</p><p>Designing reward functions for Meta-World requires two major considerations. First, to guarantee that our tasks are within the reach of current single-task reinforcement learning algorithms, which is a prerequisite for evaluating multi-task and meta-RL algorithms, we design well-shaped reward functions for each task that make each of the tasks at least individually solvable. More importantly, the reward functions must exhibit shared structure across tasks. Critically, even if the reward function admits the same optimal policy for multiple tasks, varying reward scales or structures can make the tasks appear completely distinct for the learning algorithm, masking their shared structure and leading to preferences for tasks with high-magnitude rewards <ref type="bibr" target="#b7">[8]</ref>. Accordingly, we adopt a structured, multi-component reward function for all tasks, which leads to effective policy learning for each of the task components. For instance, in a task that involves a combination of reaching, grasping, and placing an object, let o ? R 3 be the object position, where o = (o x , o y , o z ), h ? R 3 be the position of the robot's gripper, z target ? R be the target height of lifting the object, and g ? R 3 be goal position. With the above definition, the multi-component reward function R is the combination of a reaching reward, a grasping reward, and a placing reward or subsets thereof for simpler tasks that only involve reaching and/or pushing. With this design, the reward functions across all tasks have a similar magnitude that ranges between 0 and 10, where 10 always corresponds to the rewardfunction being solved, and conform to similar structure, as desired. The full form of the reward function and a list of all task rewards is provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3:</head><p>Visualization of three of our multi-task and meta-learning evaluation protocols, ranging from within task adaptation in ML1, to multi-task training across 10 distinct task families in MT10, to adapting to new tasks in ML10. Our most challenging evaluation mode ML45 is shown in <ref type="figure" target="#fig_9">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Protocol</head><p>With the goal of providing a challenging benchmark to facilitate progress in multi-task RL and meta-RL, we design an evaluation protocol with varying levels of difficulty, ranging from the level of current goal-centric meta-RL benchmarks to a setting where methods must learn distinctly new, challenging manipulation tasks based on diverse experience across 45 tasks. We hence divide our evaluation into five categories, which we describe next. We then detail our evaluation criteria.</p><p>Meta-Learning 1 (ML1): Few-shot adaptation to goal variation within one task. The simplest evaluation aims to verify that previous meta-RL algorithms can adapt to new object or goal configurations on only one type of task. ML1 uses single Meta-World Tasks, with the meta-training "tasks" corresponding to 50 random initial object and goal positions, and meta-testing on 50 heldout positions. This resembles the evaluations in prior works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. We evaluate algorithms on three individual tasks from Meta-World: reaching, pushing, and pick and place, where the variation is over reaching position or goal object position. The goal positions are not provided in the observation, forcing meta-RL algorithms to adapt to the goal through trial-and-error.</p><p>Multi-Task 1 (MT1): Learning one multi-task policy that generalizes to 50 tasks belonging to the same environment. This evaluation aims to verify how well multi-task algorithms can learn across a large related task distribution. MT1 uses single Meta-World environments, with the training "tasks" corresponding to 50 random initial object and goal positions. The goal positions are provided in the observation and are a fixed set, as to focus on the ability of algorithms in acquiring a distinct skill across multiple goals, rather than generalization and robustness.</p><p>Multi-Task 10, Multi-Task 50 (MT10, MT50): Learning one multi-task policy that generalizes to 50 tasks belonging to 10 and 50 training environments, for a total of 500, and 2,500 training tasks. A first step towards adapting quickly to distinctly new tasks is the ability to train a single policy that can solve multiple distinct training tasks. The multi-task evaluation in Meta-World tests the ability to learn multiple tasks at once, without accounting for generalization to new tasks. In our experiments, the algorithm is typically provided with a one-hot vector indicating the current task. The positions of objects and goal positions are fixed in all tasks in this evaluation, so as to focus on acquiring the distinct skills, rather than generalization and robustness.</p><p>Meta-Learning 10, Meta-Learning 45 (ML10, ML45): Few-shot adaptation to new test tasks with 10 and 50 meta-training tasks. With the objective to test generalization to new tasks, we hold 6 out 5 tasks and meta-train policies on 10 and 45 tasks. We randomize object and goals positions and intentionally select training tasks with structural similarity to the test tasks. Task IDs are not provided as input, requiring a meta-RL algorithm to identify the tasks from experience.</p><p>Success metrics. Since values of reward are not directly indicative how successful a policy is, we define an interpretable success metric for each task, which will be used as the evaluation criterion for all of the above evaluation settings. Since all of our tasks involve manipulating one or more objects into a goal configuration, this success metric is typically based on the distance between the task-relevant object and its final goal pose, i.e. o ? g 2 &lt; , where is a small distance threshold such as 5 cm. For the complete list of success metrics and thresholds for each task, see Appendix 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results and Analysis</head><p>The first, most basic goal of our experiments is to verify that each of the 50 presented tasks are indeed solveable by existing single-task reinforcement learning algorithms. We provide this verification in Appendix B. Beyond verifying the individual tasks, the goals of our experiments are to study the following questions: (1) can existing state-of-the-art meta-learning algorithms quickly learn qualitatively new tasks when meta-trained on a sufficiently broad, yet structured task distribution, and (2) how do different multi-task and meta-learning algorithms compare in this setting? To answer these questions, we evaluate various multi-task and meta-learning algorithms on the Meta-World benchmark. We include the training curves of all evaluations in <ref type="figure" target="#fig_9">Figure 15</ref> in the Appendix C. Videos of the tasks and evaluations, along with all source code, are on the project webpage 3 .</p><p>In the multi-task evaluation, we evaluate the following RL algorithms: multi-task proximal policy optimization (PPO) <ref type="bibr" target="#b54">[55]</ref>: a policy gradient algorithm adapted to the multi-task setting by providing the one-hot task ID as input, multi-task trust region policy optimization (TRPO) <ref type="bibr" target="#b55">[56]</ref>: an on-policy policy gradient algorithm adapted to the multi-task setting using the one-hot task ID as input, multi-task soft actor-critic (SAC) <ref type="bibr" target="#b56">[57]</ref>: an off-policy actor-critic algorithm adapted to the multi-task setting using the one-hot task ID as input, and an on-policy version of task embeddings (TE) [58]: a multi-task reinforcement learning algorithm that parameterizes the learned policies via shared skill embedding space. For the meta-RL evaluation, we study three algorithms: RL 2 <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>: an on-policy meta-RL algorithm that corresponds to training a GRU network with hidden states maintained across episodes within a task and trained with PPO, model-agnostic metalearning (MAML) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>: an on-policy gradient-based meta-RL algorithm that embeds policy gradient steps into the meta-optimization, and is trained with PPO, and probabilistic embeddings for actor-critic RL (PEARL) <ref type="bibr" target="#b21">[22]</ref>: an off-policy actor-critic meta-RL algorithm, which learns to encode experience into a probabilistic embedding of the task that is fed to the actor and the critic. We use the baselines in the Garage <ref type="bibr" target="#b58">[59]</ref> reinforcement learning library, which we developed for benchmarking Meta-World.</p><p>We show results of the simplest meta-learning evaluation mode, ML1, in <ref type="figure">Figure 4</ref>. We find that there is room for improvement even in this very simple setting. Next, we look at results of multitask learning across distinct tasks, starting with MT10 in <ref type="figure">Figure 5</ref> and in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We find that multi-task SAC is able to the learn the MT10 task suite well, achieving around 68% success rate averaged across tasks, while multi-task PPO and TRPO are only able to achieve around a 30% success rate. However, as we scale to 50 distict tasks with MT50, we find that MT-SAC and MT-PPO only achieve around a 35-38% success rate, indicating that there is significant room for improvement in these methods</p><p>Finally, we study the ML10 and ML45 meta-learning benchmarks, which require learning the metatraining tasks and generalizing to new meta-test tasks with small amounts of experience. From <ref type="figure">Figure 8</ref> and <ref type="table" target="#tab_2">Table 1</ref>  Success Rate</p><formula xml:id="formula_1">Average door-close-v2 shelf-place-v2 drawer-open-v2 sweep-into-v2 lever-pull-v2</formula><p>Test Environment ML-10 Maximum Per-Task Success Rates (N=10) <ref type="figure">Figure 6</ref>: Performance of the tested meta-RL algorithms on 10 seeds. RL 2 shows the highest performance on the training tasks (86.9%), however its ability to generalize is not that much greater than MAML (35.8% for RL 2 and 31.6% for MAML). Success Rate</p><formula xml:id="formula_2">Average pick-place-wall-v2 dial-turn-v2 door-close-v2 shelf-place-v2 reach-wall-v2 push-back-v2 pick-out-of-hole-v2 push-wall-v2 push-v2 handle-pull-v2 soccer-v2 sweep-into-v2 pick-place-v2 button-press-v2 disassemble-v2 plate-slide-v2 box-close-v2 peg-unplug-side-v2 handle-press-side-v2 window-open-v2 coffee-button-v2 handle-press-v2 drawer-open-v2 peg-insert-side-v2 reach-v2 window-close-v2 handle-pull-side-v2</formula><p>door-open-v2 lever-pull-v2 faucet-open-v2 button-press-topdown-v2 bin-picking-v2 door-lock-v2 button-press-wall-v2 plate-slide-side-v2 plate-slide-back-v2 assembly-v2 button-press-topdown-wall-v2</p><formula xml:id="formula_3">drawer-close-v2 faucet-close-v2 hammer-v2 coffee-push-v2 stick-push-v2 coffee-pull-v2 basketball-v2 plate-slide-back-side-v2 hand-insert-v2 door-unlock-v2 sweep-v2 stick-pull-v2</formula><p>Environment MT-50 Maximum Per-Task Success Rates (N=10) MT-SAC MT-PPO TE-PPO MT-TRPO <ref type="figure">Figure 7</ref>: Performance of the tested MTRL algorithms on 10 seeds. In MT-10, MT-SAC showed the highest performance, however its performance does not scale to MT-50, the more difficult benchmark. MT-PPO exhibits the better performance in this benchmark.</p><formula xml:id="formula_4">SuccessRate Average basketball-v2 sweep-v2 shelf-place-v2 assembly-v2 faucet-close-v2 door-open-v2 pick-place-v2 pick-place-wall-v2 door-close-v2 faucet-open-v2 coffee-push-v2 plate-slide-v2 stick-push-v2 sweep-into-v2 hammer-v2 lever-pull-v2 peg-unplug-side-v2 handle-pull-v2 peg-insert-side-v2 reach-wall-v2 handle-press-side-v2 plate-slide-side-v2 coffee-button-v2 push-v2 handle-pull-side-v2 button-press-topdown-v2</formula><p>pick-out-of-hole-v2 button-press-wall-v2 stick-pull-v2 handle-press-v2 reach-v2 dial-turn-v2 button-press-v2 drawer-close-v2 window-open-v2 plate-slide-back-v2 soccer-v2 coffee-pull-v2 push-wall-v2 plate-slide-back-side-v2 disassemble-v2 drawer-open-v2 push-back-v2 window-close-v2 button-press-topdown-wall-v2 The best performance in each benchmark is bolden. For MT10 and MT50, we show the average training success rate of multi-task SAC and multi-task PPO respectively outperform other methods. For ML10 and ML45, we show the meta-train and meta-test success rates. RL 2 achieves best meta-train performance in ML10 and ML45, while MAML and RL2 get the best generalization performance in ML10 and ML45 meta-test tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Directions for Future Work</head><p>We proposed an open-source benchmark for meta-reinforcement learning and multi-task learning, which consists of a large number of simulated robotic manipulation tasks.</p><p>Unlike previous evaluation benchmarks in meta-RL, our benchmark specifically emphasizes generalization to distinctly new tasks, not just in terms of parametric variation in goals, but completely new objects and interaction scenarios.</p><p>While meta-RL can in principle make it feasible for agents to acquire new skills more quickly by leveraging past experience, previous evaluation benchmarks utilize very narrow task distributions, making it difficult to understand the degree to which meta-RL actually enables this kind of generalization. The aim of our benchmark is to make it possible to develop new meta-RL algorithms that actually exhibit this sort of generalization. Our experiments show that current meta-RL methods in fact cannot yet generalize effectively to entirely new tasks and do not even learn the meta-training tasks effectively when meta-trained across multiple distinct tasks. This suggests a number of directions for future work, which we describe below.</p><p>Future directions for algorithm design. The main conclusion from our experimental evaluation with our proposed benchmark is that current meta-RL algorithms generally struggle in settings where the meta-training tasks are highly diverse. This issue mirrors the challenges observed in multi-task RL, which is also challenging with our task suite, and has been observed to require considerable additional algorithmic development to attain good results in prior work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. A number of recent works have studied algorithmic improvements in the area of multi-task reinforcement learning, as well as potential explanations for the difficulty of RL in the multi-task setting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b59">60]</ref>. Incorporating some of these methods into meta-RL, as well as developing new techniques to enable meta-RL algorithms to train on broader task distributions, would be a promising direction for future work to enable meta-RL methods to generalize effectively across diverse tasks, and our proposed benchmark suite can provide future algorithms development with a useful gauge of progress towards the eventual goal of broad task generalization.</p><p>Future extensions of the benchmark. While the presented benchmark is significantly broader and more challenging than existing evaluations of meta-reinforcement learning algorithms, there are a number of extensions to the benchmark that would continue to improve and expand upon its applicability to realistic robotics tasks. First, in many situations, the poses of objects are not directly accessible to a robot in the real world. Hence, one interesting and important direction for future work is to consider image observations and sparse rewards. Sparse rewards can be derived already using the success metrics, while support for image rendering is already supported by the code. However, for meta-learning algorithms, special care needs to be taken to ensure that the task cannot be inferred directly from the image, else meta-learning algorithms will memorize the training tasks rather than learning to adapt. Another natural extension would be to consider including a breadth of compositional long-horizon tasks, where there exist combinatorial numbers of tasks. Such tasks would be a straightforward extension, and provide the possibility to include many more tasks with shared structure. Another challenge when deploying robot learning and meta-learning algorithms is the manual effort of resetting the environment. To simulate this case, one simple extension of the benchmark is to significantly reduce the frequency of resets available to the robot while learning. Lastly, in many real-world situations, the tasks are not available all at once. To reflect this challenge in the benchmark, we can add an evaluation protocol that matches that of online meta-learning problem statements <ref type="bibr" target="#b60">[61]</ref>. We leave these directions for future work, either to be done by ourselves or in the form of open-source contributions. To summarize, we believe that the proposed form of the task suite represents a significant step towards evaluating multi-task and meta-learning algorithms on diverse robotic manipulation problems that will pave the way for future research in these areas. <ref type="figure" target="#fig_9">Figure 10</ref>: Enlarged image of <ref type="figure" target="#fig_6">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Task Descriptions</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we include a description of each of the 50 Meta-World tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Benchmark Verification with Single-Task Learning</head><p>In this section, we aim to verify that each of the benchmark tasks are individually solvable provided enough data. To do so, we consider two state-of-the-art single task reinforcement learning methods, Success Rate door-close-v2 sweep-into-v2 button-press-wall-v2 button-press-topdown-v2 push-back-v2 plate-slide-v2 coffee-button-v2 handle-press-v2 window-open-v2 reach-wall-v2 drawer-close-v2 peg-insert-side-v2 dial-turn-v2 handle-press-side-v2 button-press-v2 plate-slide-back-side-v2 reach-v2 plate-slide-side-v2 coffee-push-v2 door-unlock-v2 plate-slide-back-v2</p><formula xml:id="formula_5">soccer-v2 basketball-v2 door-open-v2 drawer-open-v2 button-press-topdown-wall-v2 sweep-v2 push-wall-v2 window-close-v2 handle-pull-side-v2 hand-insert-v2 pick-place-v2 door-lock-v2 stick-push-v2 handle-pull-v2</formula><p>push-v2 peg-unplug-side-v2 coffee-pull-v2 lever-pull-v2 faucet-close-v2 stick-pull-v2 pick-out-of-hole-v2</p><p>shelf-place-v2 faucet-open-v2 hammer-v2 box-close-v2 bin-picking-v2 assembly-v2 disassemble-v2 pick-place-wall-v2 Environment Single Task Success Rates PPO SAC <ref type="figure" target="#fig_9">Figure 11</ref>: Performance of independent policies trained on individual tasks using soft actor-critic (SAC) and proximal policy optimization (PPO) on 3 seeds. We verify that SAC can solve all of the tasks and PPO can also solve most of the tasks. proximal policy optimization (PPO) <ref type="bibr" target="#b54">[55]</ref> and soft actor-critic (SAC) <ref type="bibr" target="#b56">[57]</ref>. This evaluation is purely for validation of the tasks, and not an official evaluation protocol of the benchmark. Details of the hyperparameters are provided in Appendix D. The results of this experiment are illustrated in <ref type="figure" target="#fig_9">Figure 11</ref>. We indeed find that SAC can learn to perform all of the 50 tasks to some degree, while PPO can solve a large majority of the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Learning curves</head><p>In evaluating meta-learning algorithms, we care not just about performance but also about efficiency, i.e. the amount of data required by the meta-training process. While the adaptation process for all algorithms is extremely efficient, requiring only a few trajectories, the meta-learning process can be very inefficient. In <ref type="figure" target="#fig_0">Figure 12</ref>, we show full learning curves of the three meta-learning methods on ML1. In <ref type="figure" target="#fig_9">Figure 15</ref>, we show full learning curves of MT10, ML10, MT50 and ML45. The MT10 and MT50 learning curves show the efficiency of multi-task learning, a critical evaluation metric, since sample efficiency gains are a primary motivation for using multi-task learning. Unsurprisingly, we find that off-policy algorithms such as soft actor-critic are able to learn with substantially less data than on-policy algorithms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameter Details</head><p>In this section, we provide hyperparameter values for each of the methods in our experimental evaluation.          </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Reward Functions</head><p>The variables that will be discussed are the following: The following tolerance function is used frequently:</p><formula xml:id="formula_6">O ? R 3 : object</formula><formula xml:id="formula_7">L(x, b min , b max , m) = ? ? ? 1 b min ? x ? b max S bmin?x m , 0.1 x &lt; b min S x?bmax m , 0.1 x ? b max</formula><p>Where S is defined to be a long-tail sigmoid:</p><formula xml:id="formula_8">S(a 1 , a 2 ) = 1 a 2 ? 1 ? 1 a 2 1 + 1 ?1</formula><p>With these basics in place, we define a caging tensor that describes behaviour in an axis which intersects the gripper's actuated fingers (in code, the Y axis):</p><formula xml:id="formula_9">C LR (c 1 , c 2 ) = L h L,(y) h R,(y) ? o (y) , c 1 , c 2 , h L,(y) h R,(y) ? o i,(y) ? c 2</formula><p>A similar caging value describes behaviour in the other two axes (in code, X and Z axes):</p><formula xml:id="formula_10">C P (c 3 ) = L o (xz) ? h (xz) , 0, c 3 , o i,(xz) ? h i,(xz) ? c 3</formula><p>These get lumped together as follows (T H0 is the Hamacher product):</p><formula xml:id="formula_11">C(c 1 , c 2 , c 3 ) = T H0 (T H0 (C LR,(0) , C LR,(1) ), C P (c 3 ))</formula><p>The caging reward has two modes: medium density and high density. The arguments c 1 , c 2 , c 3 are passed to C R cage,dense (c 1 , c 2 , c 3 ) = 0.5(C + T H0 (C, g)) C &gt; 0.97 0.5C otherwise</p><formula xml:id="formula_12">R cage (c 1 , c 2 , c 3 , c 4 ) = 0.5(L( o ? h , 0, c 4 , o ? h i ) + T H0 (C, g)) C &gt; 0.97 0.5L( o ? h , 0, c 4 , o ? h i ) otherwise</formula><p>In each set of expressions given below, the arguments passed to R cage or R cage,dense correspond to [c 1 , c 2 , c 3 ...]. The caging reward also considers [h, h i , o, o i ] as described on the previous page, but these arguments are omitted for brevity.</p><p>If computation involves a parameter A, understand that A is non-zero if f the Sawyer successfully grasps the object. As such, A serves as a post-grasp guidance term.       Note: This technically uses a R cage,dense function with slightly different margin parameters than the one described above (they are constant rather than dynamic), but the behaviour is mostly the same. Note: This technically uses a R cage,dense function with slightly different margin parameters than the one described above (they are constant rather than dynamic), but the behaviour is mostly the same. In addition to the components described below, the assembly reward is weighted by how level the object is (tilted object quaternions are penalized). In addition to the components described below, the hammer reward is weighted by how level the object is (tilted object quaternions are penalized). Note: a is the second object in the environment, which in this case is a thermos. in is a condition involving lots of vector offsets from the object observations. It indicates whether the stick is inserted into the thermos' handle or not. The variable stick in place, and stick grabbed have also been defined so that the reward function fits on one page. In addition to the components described below, the shelf-place reward includes negative components that help avoid collision with the shelf. The pick-place-wall reward is essentially two pick-place rewards stacked on top of one another. The first pick-place reward incentivizes movement to a neutral midpoint above the wall (to avoid running into it). The second pick-place reward incentivizes movement to the target position. The math is such that there is no discontinuity between the two reward components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1 Basketball</head><formula xml:id="formula_13">A = I o?h &lt;0.035 &amp; g&gt;0 &amp; o (z) ?o i(z) &gt;0.01 ? (1 + L( 1, 1, 2 ? (t ? o) , 0, 0.08, 1, 1, 2 ? (t ? o i ) )) R = ? ? ? A + T H0 (R cage,dense (0.025, 0.06, 0.005), L( 1, 1, 2 ? (t ? o) , 0, 0.08, 1, 1, 2 ? (t ? o i ) )) 1, 1, 2 ? (t ? o) ? 0.08 10 otherwise E.1.2 Button Press Top Down R = 5T H0 (1 ? g, L( o ? h , 0, 0.01, o ? h i )) o ? h &gt; 0.03 5T H0 (1 ? g, L( o ? h , 0, 0.01, o ? h i )) + 5L(|t (z) ? o (z) |, 0, 0.005, |t (z) ? o i,(z) |)) otherwise E.1.3 Button Press Top Down Wall R = 5T H0 (1 ? g, L( o ? h , 0, 0.01, o ? h i )) o ? h &gt; 0.03 5T H0 (1 ? g, L( o ? h , 0, 0.01, o ? h i )) + 5L(|t (z) ? o (z) |, 0, 0.005, |t (z) ? o i,(z) |)) otherwise E.1.4 Button Press R = 2T H0 (g, L( o ? h , 0, 0.05, o ? h i )) o ? h &gt; 0.05 2T H0 (g, L( o ? h , 0, 0.05, o ? h i )) + 8L(|t (y) ? o (y) |, 0, 0.005, |t (y) ? o i,(y) |)) otherwise E.1.5 Button Press Wall R = 2T H0 (1 ? g, L( o ? h , 0, 0.01, o ? h i )) o ? h &gt; 0.07 4 + 2g + 4(L(|t (y) ? o (y) |, 0, 0.005, |t (y) ? o i,(y) |) 2 )) otherwise</formula><formula xml:id="formula_14">A = I o?h &lt;0.02 &amp; g&gt;0 ? (1 + 7L( t ? o , 0, 0.05, t ? o i )) R = A + T H0 (R cage,</formula><formula xml:id="formula_15">A =I o?h &lt;0.04 &amp; g&lt;0.33 &amp; o (z) ?o i,(z) &gt;0.02 ? 1 + 5T H0 L( t ? o , 0, 0.02, t ? o i ), max(I h (z) &gt;alt , L(alt ? h (z) , 0, 0.01, 0.02)) R = A + T H0 (R cage,</formula><formula xml:id="formula_16">A = I o?h &lt;0.07 &amp; h (z) ?0.03 R = A ? (2 + 7L( t ? o , 0, 0.05, t ? o i )) + (1 ? A) ? 1.5L( o ? h , 0, 0.05, o i ? h i ? 0.05) t ? o &gt; 0.05 10 otherwise E.1.22 Plate Slide Back A = I o?h &lt;0.07 &amp; h (z) ?0.03 R = A ? (2 + 7L( t ? o , 0, 0.05, t ? o i )) + (1 ? A) ? 1.5L( o ? h , 0, 0.05, o i ? h i ? 0.05) t ? o &gt; 0.</formula><formula xml:id="formula_17">R = 10T H0 (L(|t (z) ? o (z) |, 0, 0.05, |t (z) ? o i,(z) |), L( o ? h , 0, 0.02, o i ? h i ? 0.02)) t ? o &gt; 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.50 Push Wall</head><p>The push-wall reward is the same as the pick-place-wall reward, but without incentives to pick up the object. Additionally, the midpoint is configured to be next to the wall (so that policies push the object around the wall) rather than above the wall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Parametric/non-parametric variation: all "reach puck" tasks (left) can be parameterized by the puck position, while the difference between "reach puck" and "open window" (right) is non-parametric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Comparison on our simplest meta-RL evaluation, ML1 on 10 seeds. RL 2 shows the strongest performance in generalization. Pearl shows the weakest performance, though this could be attributed to difficulty in training its task encoder Performance of the tested MTRL algorithms on 10 seeds. MT-SAC performs the best on MT-10, exhibiting the greatest sample efficiency and performance. For detailed plots of these algorithm's learning curves, see appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Rotate the faucet counter-clockwise. Randomize faucet positions sweep Sweep a puck off the table. Randomize puck positions assemble nut Pick up a nut and place it onto a peg. Randomize nut and peg positions turn off faucet Rotate the faucet clockwise. Randomize faucet positions push Push the puck to a goal. Randomize puck and goal positions pull lever Pull a lever down 90 degrees. Randomize lever positions turn dial Rotate a dial 180 degrees. Randomize dial positions push with stick Grasp a stick and push a box using the stick. Randomize stick positions. get coffee Push a button on the coffee machine. Randomize the position of the coffee machine pull handle side Pull a handle up sideways. Randomize the handle positions basketball Dunk the basketball into the basket. Randomize basketball and basket positions pull with stick Grasp a stick and pull a box with the stick. Randomize stick positions sweep into hole Sweep a puck into a hole. Randomize puck positions disassemble nut pick a nut out of the a peg. Randomize the nut positions place onto shelf pick and place a puck onto a shelf. Randomize puck and shelf positions push mug Push a mug under a coffee machine. Randomize the mug and the machine positions press handle side Press a handle down sideways. Randomize the handle positions hammer Hammer a screw on the wall. Randomize the hammer and the screw positions slide plate Slide a plate into a cabinet. Randomize the plate and cabinet positions slide plate side Slide a plate into a cabinet sideways. Randomize the plate and cabinet positions press button wall Bypass a wall and press a button. Randomize the button positions press handle Press a handle down. Randomize the handle positions pull handle Pull a handle up. Randomize the handle positions soccer Kick a soccer into the goal. Randomize the soccer and goal positions retrieve plate side Get a plate from the cabinet sideways. Randomize plate and cabinet positions retrieve plate Get a plate from the cabinet. Randomize plate and cabinet positions close drawer Push and close a drawer. Randomize the drawer positions press button top Press a button from the top. Randomize button positions reach reach a goal position. Randomize the goal positions press button top wall Bypass a wall and press a button from the top. Randomize button positions reach with wall Bypass a wall and reach a goal. Randomize goal positions insert peg side Insert a peg sideways. Randomize peg and goal positions pull Pull a puck to a goal. Randomize puck and goal positions push with wall Bypass a wall and push a puck to a goal. Randomize puck and goal positions pick out of hole Pick up a puck from a hole. Randomize puck and goal positions pick&amp;place w/ wall Pick a puck, bypass a wall and place the puck. Randomize puck and goal positions press button Press a button. Randomize button positions pick&amp;place Pick and place a puck to a goal. Randomize puck and goal positions pull mug Pull a mug from a coffee machine. Randomize the mug and the machine positions unplug peg Unplug a peg sideways. Randomize peg positions close window Push and close a window. Randomize window positions open window Push and open a window. Randomize window positions open door Open a door with a revolving joint. Randomize door positions close door Close a door with a revolving joint. Randomize door positions open drawer Open a drawer. Randomize drawer positions insert hand Insert the gripper into a hole. close box Grasp the cover and close the box with it. Randomize the cover and box positions lock door Lock the door by rotating the lock clockwise. Randomize door positions unlock door Unlock the door by rotating the lock counter-clockwise. Randomize door positions pick bin Grasp the puck from one bin and place it into another bin. Randomize puck positions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :</head><label>1213141516</label><figDesc>Comparison of PEARL, MAML, and RL 2 learning curves on ML-1 reach. Comparison of PEARL, MAML, and RL 2 learning curves on ML-1 push. Comparison of PEARL, MAML, and RL 2 learning curves on the simplest evaluation, ML-1, where the methods need to adapt quickly to new object and goal positions within the one meta-training task. Comparison of MTRL algorithms on MT-10. MT-SAC vastly outperforms is on-policy counterparts in performance and sample efficiency.0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2Comparison of MTRL algorithms on MT-50. MT-SAC vastly outperforms is on-policy counterparts in sample efficiency. Its performance tapers off, and with more training, MT-PPO outperforms it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Performance of meta-RL algorithms on ML-10. RL 2 significantly outperforms other methods in terms of sample efficiency and performance on test tasks. MAML has better test performance early on, RL 2 outperforms it with more training. Learning curves of all methods on the ML-45 benchmark. Y-axis represents success rate averaged over tasks in percentage (%). The dashed lines represent asymptotic performances. PEARL underperforms MAML and RL 2 . RL 2 significantly outperforms other methods in terms of sample efficiency and performance on train tasks. RL 2 and MAML have similar performance on test tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>position h ? R 3 :</head><label>3</label><figDesc>hand/gripper position t ? R 3 : target/goal position h l ? R 3 : position of the left hand/gripper pad h r ? R 3 : position of the right hand/gripper pad O i ? R 3 : initial position of the object h i ? R 3 : initial position of the hand/gripper g ? R : gripper closed/open amount</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>patterns include A+T H0 (R cage , L(t?o, ...)), T H0 (1?g, L(o?h, ...)), and L(t?o, ...)+ L(o ? h, ...). As a general rule, rewards for simple tasks consist of summed tolerances, while more difficult tasks add complexity in the form of Hamacher Products. The Hamacher Products combine tolerances, grip effort, and/or R cage to produce a smooth, dense reward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>E. 1 . 6 AA 2 ? 2 ? 2 ,</head><label>16222</label><figDesc>Coffee Button R = 2T H0 (g, L( o ? h , 0, 0.05, o ? h i )) o ? h &gt; 0.05 2T H0 (g, L( o ? h , 0, 0.05, o ? h i )) + 8L(|t (y) ? o (y) |, 0, 0.005, |t (y) ? o i,(y) |)) otherwise E.1.7 Coffee PullA = I o?h &lt;0.04 &amp; g&gt;0 ? (1 + 5L( 2, 2, 1 ? (t ? o) , 0, 0.05, 2, 2, 1 ? (t ? o i ) )) + T H0 (R cage (0.02, 0.05, 0.05, 0.04), L( 2, 2, 1 ? (t ? o) , 0, 0.05, 2, 2, 1 ? (t ? o i ) )) 2, 2, 1 ? (t ? o) ? 0.05 10 otherwise E.1.8 Coffee Push A = I o?h &lt;0.04 &amp; g&gt;0 ? (1 + 5L( 2, 2, 1 ? (t ? o) , 0, 0.05, 2, 2, 1 ? (t ? o i ) )) + T H0 (R cage (0.02, 0.05, 0.05, 0.04), L( 2, 2, 1 ? (t ? o) , 0, 0.05, 2, 2, 1 ? (t ? o i ) )) 2, 2, 1 ? (t ? o) ? 0.05 10 otherwise E.1.9 Door Close R = 6L( t ? o , 0, 0.05, t ? o i )) + 3L( t ? h , 0, 0.012, 0.1 + h i ? o ) t ? o ? 0.05 10 otherwise E.1.10 Door Lock R = 2T H0 (g, L( 1, 4, 2 ? (o ? h) , 0, 0.01, 1, 4, 2 ? (o ? h i ) )) + 8L(|t (z) ? o i,(z) |, 0, 0.005, 0.1) (o ? h + 0,0.055, 0.07 ) , (o i ? h i + 0, 0.055, 0.07 ) )) + 8L(|t (x) ? o i,(x) |, 0, 0.005, 0.1) E.1.12 Door Open alt = I h (xy) ?o (xy) &gt;0.12 ? 0.4 + 0.04 log h (xy) ? o (xy) ?0.12 ready = T H0 L( h ? o ? 0.05, 0.03, ?0.01 , 0, 0.06, 0.5), L(alt ? h (z) , 0, 0.01, alt 2 ), h (z) &lt; alt L( h ? o ? 0.05, 0.03, ?0.01 , 0, 0.06, 0.5) otherwise R = 2T H0 (g, ready) + 8 0.2I o (?) &lt;0.03 + 0.8L(o (?) + 2? 3 , 0, 0.5, ? 3 ) |t (x) ? o (x) | &gt; 0.08 10 otherwise E.1.13 Box Close alt = I h (xy) ?o (xy) &gt;0.02 ? 0.4 + 0.04 log h (xy) ? o (xy) ?0.02 ready = T H0 L( h ? o , 0, 0.02, 0.5), L(alt ? h (z) , 0, 0.01, alt 2 ), h (z) &lt; alt L( h ? o , 0, 0.02, 0.5) ready + 8 0.2I o (z) &gt;0.04 + 0.8L( 1, 1, 3 t ? o , 0, 0.05, 0.25) |t ? o| ? 0.08 10 otherwise E.1.14 Drawer Open R = 5 (L ( t ? o , 0, 0.02, 0.2) + L ( (o ? h) ? 3, 3, 1 , 0, 0.01, (o i ? h i ) ? 3, 3, 1 )) E.1.15 Drawer Close R = T H0 (L( t ? o , 0, 0.05, t ? o i ? 0.05), T H0 (g, L( o ? h , 0, 0.005, o i ? h i ? 0.005))) t ? o &gt; 0.065 10 otherwise E.1.16 Faucet Close R = 4L( o ? h , 0, 0.01, o i ? h i ? 0.01) + 6L( t ? o , 0, 0.07, t ? o i ? 0.07) t ? o &gt; 0o ? h + ?.04, 0, .03 , 0, 0.01, o i ? h i ? 0.01) + 6L( t ? o + ?.04, 0, .03 , 0, 0.07, t ? o i ? 0.07)) t ? o + ?.04, 0, .03 &gt; 0.07 10 otherwise E.1.18 Hand Insert</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>05 10 otherwise E. 1 .</head><label>1</label><figDesc>23 Plate Slide SideA = I o?h &lt;0.07 &amp; h (z) ?0.03 R = A ? (2 + 7L( t ? o , 0, 0.05, t ? o i )) + (1 ? A) ? 1.5L( o ? h , 0, 0.05, o i ? h i ? 0.05) t ? o &gt; 0.05 10 otherwise E.1.24 Plate Slide R = 8T H0 (L( t ? o , 0, 0.05, t ? o i ), L( o ? h , 0, 0.05, o i ? h i )) t ? o ? 0.05 10 otherwise E.1.25 Handle Press Side R = 10T H0 (L(|t (z) ? o (z) |, 0, 0.05, |t (z) ? o i,(z) |), L( o ? h , 0, 0.02, o i ? h i ? 0.02)) t ? o &gt; 0.05 10 otherwise E.1.26 Handle Press</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>(</head><label></label><figDesc>2R cage,dense (0.02, 0.05, 0.01) + 2T H0 (R cage,dense (0.02, 0.05, 0.01), L( t ? o , 0, 0.05, t ? o i ))) t ? o &gt; 0.05 10 otherwise E.1.34 Push Back</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>A</head><label></label><figDesc>= I o?h &lt;0.01 &amp; 0&lt;g&lt;0.55 &amp; t?oi ? t?o &gt;0.01 ? (1 + 5L( t ? o , 0, 0.05, t ? o i ))R =A + T H0 (R cage,dense (0.01, 0.05, 0.01),L( t ? o , 0, 0.05, t ? o i )) t ? o &gt; 0.05 10 otherwise E.1.35 Window Open R = 10T H0 (L(|t (x) ? o (x) |, 0, 0.05, |t (x) ? o i,(x) |), L( o ? h , 0, 0.02, o i ? h i ? 0.02)) E.1.36 Window Close R = 10T H0 (L(|t (x) ? o (x) |, 0, 0.05, |t (x) ? o i,(x) |), L( o ? h , 0, 0.02, o i ? h i ? 0.02)) E.1.37 Dial Turn R = 10T H0 (L( t ? o , 0, 0.05, t ? o i ? 0.05),TH0 (g, L( o ? h + 0.05, 0.02, 0.09 , 0, 0.005, o i ? h i + 0.05, 0.02, 0.09 ? 0.005))) E.1.38 Bin Picking Two funnel-shaped surfaces guide the gripper as it seeks to carry the object between the two bins; this prevents the gripper from running into the side of the bins. The height (or "altitude") of this surface is given by alt since the variables h and z are already used. alt = min(I h (xy) ?o i,(xy) &gt;0.03 ? 0.2 + 0.02 log h (xy) ? o i,(xy) ?0.03 , I h (xy) ?t (xy) &gt;0.03 ? 0.2 + 0.02 log h (xy) ? t (xy) ?0.03 )A = I o?h &lt;0.04 &amp; g&lt;0.43 &amp; o (z) ?o i,(z) &gt;0.02 ? 1 + 5T H0 L( t ? o , 0, 0.05, t ? o i ), max(I h (z) &gt;alt , L(alt ? h (z) ,0, 0.01, 0.05)) R = A + T H0 (R cage,dense (0.015, 0.05, 0.01), L( t ? o , 0, 0.05, t ? o i )) t ? o &gt; 0.05 10 otherwise E.1.39 Assembly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>alt = I t (xy) ?o (xy) &gt;0.02 ? 0.4 + 0.04 log t (xy) ? o (xy) ?0.02 A = 0.1I o (z) &gt;0.02or t (xy) ?o (xy) &lt;0.02 + 0.9L( 1, 1, 3 t (x) ? o (x) , t (y) ? o (y) , alt ? o (z) , 0, 0.02, 0.4) R = 2R cage,dense (0.015, 0.02, 0.01) + 8A |t (x) ? o (x) | &gt; 0.02 10 otherwiseE.1.40 DisassembleIn addition to the components described below, the disassemble reward is weighted by how level the object is (tilted object quaternions are penalized).R = 2R cage,dense (0.015, 0.02, 0.01) + 6 0.1I o (z) &gt;0.02 + 0.9L( t ? o , 0, 0.02, 0.2) o (z) &gt; t (z) 10 otherwise E.1.41 Hammer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 + 38 E. 1 .</head><label>2381</label><figDesc>R =2R cage,dense (0.015, 0.02, 0.01)+ 6 0.1I o (z) &gt;0.02 + 0.9L( t ? o , 0, 0.02, 0.2) |o (y) ? o i,(y) | &gt; 0.09 10 otherwise E.1.42 Lever Pull R = 10T H0 (L ( t ? o , 0, 0.04, t ? o i ) , L ( 4, 1, 4 ? (h ? o + 0,0.055, 0.07 ), 0, 0.02, 4, 1, 4 ? (h i ? o i + 0, 0.055, 0.07 ))) E.1.43 Stick Push Note: a is the second object in the environment, which in this case is a thermos. 5L( t ? o , 0, 0.12, t ? o i ? 0.12) + 3L( t ? a , 0, 0.12, t ? a i ? 0.12) h ? o &lt; 0.02, g &gt; 0, o (z) ? o i,(z) &gt; 0.01, t ? a &gt; 0.12 10 h ? o &lt; 0.02, g &gt; 0, o (z) ? o i,(z) &gt; 0.01, t ? a ? 0.12 R cage,dense (0.04, 0.05, 0.01) otherwise 44 Stick Pull</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 + 6 ?</head><label>16</label><figDesc>stick in place = L( (o ? a) ? 1, 1, 2 , 0, 0.12, (o i ? a i ) ? 1, 1, 2 ) stick grabbed = h ? o &lt; 0.02, g &gt; 0, o (z) ? o i,(z) &gt; 0.01 stick in place stick grabbed, ?in, t ? a &gt; 0.12 6 + stick in place + 2L( t ? o , 0, 0.12, t ? o i ) + L( t ? a , 0, 0.12, t ? a i )stick grabbed, in, t ? a &gt; 0.12 10 stick grabbed, in, t ? a ? 0.12 T H0 (R cage,dense (0.014, 0.05, 0.01), stick in place) otherwise E.1.45 Shelf Place</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>AA?</head><label></label><figDesc>= I o?h &lt;0.025 &amp; g&gt;0 &amp; o (z) &gt;0.01 ? (1 + 5L( t ? o , 0, 0.05, t ? o i )) R = A + T H0 (R cage (0.01, 0.02, 0.05, 0.01), L( t ? o , 0, 0.05, t ? o i )) t ? o ? 0.05 10 otherwise E.1.46 Peg InsertIn addition to the components described below, the peg-insert reward includes negative components that help avoid collision with the hole/box into which the peg gets inserted.A = I o?h &lt;0.08 &amp; g&gt;0 &amp; o (z) &gt;0.01 ? (1 + 5L( 2, 2, 1 ? (t ? o) , 0, 0.05, 2, 2, 1 ? (t ? o i ) )) + TH0 (R cage (0.0075, 0.01, 0.03, 0.005), L( 2, 2, 1 ? (t ? o) , 0, 0.05, 2, 2, 1 ? (t ? o i ) )) 2, 2, 1 ? (t ? o) ? 0.07 10 otherwise E.1.47 Peg Unplug A = I o?h &lt;0.035 &amp; g&gt;0.5 &amp; o (x) ?o i,(x) &gt;0.015 ? (1 + 5L( t ? o , 0, 0.05, t ? o i )) R = A + 2R cage (0.01, 0.025, 0.05, 0.005) t ? o ? 0.07 10 otherwise E.1.48 SoccerIn addition to the components described below, the soccer reward function includes parameters to fine-tune movements near the goal line.3R cage (0.013, 0.023, 0.05, 0.005) + 6.5L( 3, 1, 1 (t ? o) , 0, 0.07, 3, 1, 1 (t ? o i ) ) 3, 1, 1 (t ? o) ? 0.07 10 otherwise E.1.49 Pick Place Wall</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, we find that the prior meta-RL methods, MAML and RL 2 reach 35% and 31% success on ML10 test tasks, while PEARL achieves only 13% on ML10. On ML45, MAML and RL 2 solve around 39.9% and 33.3% of the meta-test tasks. Note that, on both ML10 and ML45, the meta-training performance of all methods also has considerable room for improvement, suggesting that optimization challenges are generally more severe in the meta-learning setting. The fact that some methods nonetheless exhibit meaningful generalization suggests that the ML10 and ML45 benchmarks are solvable, but challenging for current methods, leaving considerable room for improvement in future work.</figDesc><table><row><cell></cell><cell cols="5">ML-1 Maximum Success Rates (N=10)</cell></row><row><cell>Train Environment</cell><cell>reach-v2 push-v2 pick-place-v2</cell><cell></cell><cell></cell><cell></cell><cell>MAML-TRPO RL2-PPO PEARL</cell></row><row><cell>Test Environment</cell><cell>reach-v2 push-v2 pick-place-v2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40 Success Rate 60</cell><cell>80</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The average maximum success rate over all tasks for MT10, MT50, ML10, and ML45 on 10 seeds.</figDesc><table><row><cell>RL2-PPO MAML-TRPO PEARL</cell></row><row><cell>Train Environment</cell></row></table><note>Figure 8: Average of maximum success rate for ML-45. Note that, even on the challenging ML-45 benchmark, current methods already exhibit some degree of generalization, but meta-training performance leaves consider- able room for improvement, suggesting that future work could attain better performance on these benchmarks. Though PEARL has week training performance, it has comparable performance on test tasks. RL 2 has the highest We also show the max average success rates for all benchmarks in Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>A list of all of the Meta-World tasks and a description of each task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters used for Garage experiments with Single Task SAC 24</figDesc><table><row><cell>D.2 Single Task PPO</cell><cell></cell><cell></cell></row><row><cell>Description</cell><cell>value</cell><cell>variable name</cell></row><row><cell>Normal Hyperparameters</cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell>5,000</cell><cell>batch size</cell></row><row><cell>Number of epochs</cell><cell>4,000</cell><cell>n epochs</cell></row><row><cell>Path length per roll-out</cell><cell>500</cell><cell>max path length</cell></row><row><cell>Discount factor</cell><cell>0.99</cell><cell>discount</cell></row><row><cell cols="2">Algorithm-Specific Hyperparameters</cell><cell></cell></row><row><cell>Policy mean hidden sizes</cell><cell>(128, 128)</cell><cell>hidden sizes</cell></row><row><cell>Policy minimum standard</cell><cell>0.5</cell><cell>min std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell></row><row><cell>Policy maximum standard</cell><cell>1.5</cell><cell>max std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell></row><row><cell>Policy share standard</cell><cell>True</cell><cell>std share network</cell></row><row><cell>deviation and mean</cell><cell></cell><cell></cell></row><row><cell>network</cell><cell></cell><cell></cell></row><row><cell>Activation function of</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>mean hidden layers</cell><cell></cell><cell></cell></row><row><cell>Optimizer learning rate</cell><cell>5 ? 10 ?4</cell><cell>learning rate</cell></row><row><cell cols="2">Likelihood ratio clip range 0.2</cell><cell>lr clip range</cell></row><row><cell>Advantage estimation ?</cell><cell>0.95</cell><cell>gae lambda</cell></row><row><cell>Use layer normalization</cell><cell>False</cell><cell>layer normalization</cell></row><row><cell>Entropy method</cell><cell>max</cell><cell>entropy method</cell></row><row><cell>Loss function</cell><cell>surrogate?clip</cell><cell>pg loss</cell></row><row><cell>Maximum number of</cell><cell>256</cell><cell>max epochs</cell></row><row><cell>epochs for update</cell><cell></cell><cell></cell></row><row><cell>Minibatch size for</cell><cell>32</cell><cell>batch size</cell></row><row><cell>optimization</cell><cell></cell><cell></cell></row><row><cell cols="2">Value Function Hyperparameters</cell><cell></cell></row><row><cell>Policy hidden sizes</cell><cell>(128, 128)</cell><cell>hidden sizes</cell></row><row><cell>Activation function of</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>hidden layers</cell><cell></cell><cell></cell></row><row><cell>Initial value for standard</cell><cell>1</cell><cell>init std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell></row><row><cell cols="2">Use trust region constraint False</cell><cell>use trust region</cell></row><row><cell>Normalize inputs</cell><cell>True</cell><cell>normalize inputs</cell></row><row><cell>Normalize outputs</cell><cell>True</cell><cell>normalize outputs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters used for Garage experiments with Single Task PPOBelow we summarize in as much detail as possible the hyperparameters used for each experiment in this chapter. Seed values were individually chosen at random for each experiment.</figDesc><table><row><cell>D.3 MT-PPO</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Description</cell><cell>MT10</cell><cell>MT50</cell><cell>variable name</cell></row><row><cell>Normal Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell>100,000</cell><cell>500,000</cell><cell>batch size</cell></row><row><cell>Number of epochs</cell><cell>10,000</cell><cell>10,000</cell><cell>n epochs</cell></row><row><cell>Path length per roll-out</cell><cell>500</cell><cell>500</cell><cell>max path length</cell></row><row><cell>Discount factor</cell><cell>0.99</cell><cell>0.99</cell><cell>discount</cell></row><row><cell cols="2">Algorithm-Specific Hyperparameters</cell><cell></cell><cell></cell></row><row><cell>Policy mean hidden sizes</cell><cell>(512, 512)</cell><cell>hidden sizes</cell><cell></cell></row><row><cell>Policy minimum standard</cell><cell>0.5</cell><cell>0.5</cell><cell>min std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy maximum standard</cell><cell>1.5</cell><cell>1.5</cell><cell>max std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy share standard</cell><cell>True</cell><cell>True</cell><cell>std share network</cell></row><row><cell>deviation and mean network</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Activation function of hidden</cell><cell>tanh</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>layers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimizer learning rate</cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell><cell>learning rate</cell></row><row><cell>Likelihood ratio clip range</cell><cell>0.2</cell><cell>0.2</cell><cell>lr clip range</cell></row><row><cell>Advantage estimation ?</cell><cell>0.97</cell><cell>0.97</cell><cell>gae lambda</cell></row><row><cell>Use layer normalization</cell><cell>False</cell><cell>False</cell><cell>layer normalization</cell></row><row><cell>Use trust region constraint</cell><cell>False</cell><cell>False</cell><cell>use trust region</cell></row><row><cell>Entropy method</cell><cell>max</cell><cell>max</cell><cell>entropy method</cell></row><row><cell>Policy entropy coefficient</cell><cell>5e ? 3</cell><cell>5e ? 3</cell><cell>policy ent coeff</cell></row><row><cell>Loss function</cell><cell>surrogate?clip</cell><cell>surrogate?clip</cell><cell>pg loss</cell></row><row><cell>Maximum number of epochs</cell><cell>16</cell><cell>16</cell><cell>max epochs</cell></row><row><cell>for update</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Minibatch size for</cell><cell>32</cell><cell>32</cell><cell>batch size</cell></row><row><cell>optimization</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Value Function Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Value Function hidden sizes</cell><cell>(512, 512)</cell><cell>(512, 512)</cell><cell>hidden sizes</cell></row><row><cell>Activation function of hidden</cell><cell>tanh</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>layers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Trainable standard deviation</cell><cell>True</cell><cell>True</cell><cell>learn std</cell></row><row><cell>Initial value for standard</cell><cell>1</cell><cell>1</cell><cell>init std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Use layer normalization</cell><cell>False</cell><cell>False</cell><cell>layer normalization</cell></row><row><cell>Use trust region constraint</cell><cell>False</cell><cell>False</cell><cell>use trust region</cell></row><row><cell>Normalize inputs</cell><cell>True</cell><cell>True</cell><cell>normalize inputs</cell></row><row><cell>Normalize outputs</cell><cell>True</cell><cell>True</cell><cell>normalize outputs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters used for Garage experiments with Multi-Task PPO</figDesc><table><row><cell>D.4 MT-TRPO</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Description</cell><cell>MT10</cell><cell>MT50</cell><cell>variable name</cell></row><row><cell>Normal Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell>100,000</cell><cell>500,000</cell><cell>batch size</cell></row><row><cell>Number of epochs</cell><cell>10,000</cell><cell>10,000</cell><cell>n epochs</cell></row><row><cell>Path length per roll-out</cell><cell>500</cell><cell>500</cell><cell>max path length</cell></row><row><cell>Discount factor</cell><cell>0.99</cell><cell>0.99</cell><cell>discount</cell></row><row><cell>Algorithm-Specific Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy mean hidden sizes</cell><cell>(512, 512)</cell><cell>hidden sizes</cell><cell></cell></row><row><cell cols="2">Policy minimum standard deviation 0.5</cell><cell>0.5</cell><cell>min std</cell></row><row><cell cols="2">Policy maximum standard deviation 1.5</cell><cell>1.5</cell><cell>max std</cell></row><row><cell>Policy share standard deviation and</cell><cell>True</cell><cell>True</cell><cell>std share network</cell></row><row><cell>mean network</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Activation function of hidden layers tanh</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>Advantage estimation ?</cell><cell>0.95</cell><cell>0.95</cell><cell>gae lambda</cell></row><row><cell>Maximum KL divergence</cell><cell>1 ? 10 ?2</cell><cell>1 ? 10 ?2</cell><cell>max kl step</cell></row><row><cell>Number of CG iterations</cell><cell>10</cell><cell>10</cell><cell>cg iters</cell></row><row><cell>Regularization coefficient</cell><cell>1 ? 10 ?5</cell><cell>1 ? 10 ?5</cell><cell>reg coeff</cell></row><row><cell>Use layer normalization</cell><cell>False</cell><cell>False</cell><cell>layer normalization</cell></row><row><cell>Use trust region constraint</cell><cell>False</cell><cell>False</cell><cell>use trust region</cell></row><row><cell>Entropy method</cell><cell>no?entropy</cell><cell>no?entropy</cell><cell>entropy method</cell></row><row><cell>Loss function</cell><cell>surrogate</cell><cell>surrogate</cell><cell>pg loss</cell></row><row><cell>Value Function Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hidden sizes</cell><cell>(512, 512)</cell><cell>(512, 512)</cell><cell>hidden sizes</cell></row><row><cell cols="2">Activation function of hidden layers tanh</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>Trainable standard deviation</cell><cell>True</cell><cell>True</cell><cell>learn std</cell></row><row><cell>Initial value for standard deviation</cell><cell>1</cell><cell>1</cell><cell>init std</cell></row><row><cell>Use layer normalization</cell><cell>False</cell><cell>False</cell><cell>layer normalization</cell></row><row><cell>Use trust region constraint</cell><cell>True</cell><cell>True</cell><cell>use trust region</cell></row><row><cell>Normalize inputs</cell><cell>True</cell><cell>True</cell><cell>normalize inputs</cell></row><row><cell>Normalize outputs</cell><cell>True</cell><cell>True</cell><cell>normalize outputs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters used for Garage experiments with Multi-Task TRPO</figDesc><table><row><cell>D.5 MT-SAC</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Description</cell><cell>MT10</cell><cell>MT50</cell><cell>variable name</cell></row><row><cell>General Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell>5,000</cell><cell>25,000</cell><cell>batch size</cell></row><row><cell>Number of epochs</cell><cell>500</cell><cell>500</cell><cell>epochs</cell></row><row><cell>Path length per roll-out</cell><cell>500</cell><cell>500</cell><cell>max path length</cell></row><row><cell>Discount factor</cell><cell>0.99</cell><cell>0.99</cell><cell>discount</cell></row><row><cell cols="2">Algorithm-Specific Hyperparameters</cell><cell></cell><cell></cell></row><row><cell>Policy hidden sizes</cell><cell>(400, 400)</cell><cell>(400, 400)</cell><cell>hidden sizes</cell></row><row><cell>Activation function of</cell><cell>ReLU</cell><cell>ReLU</cell><cell>hidden nonlinearity</cell></row><row><cell>hidden layers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy learning rate</cell><cell>3 ? 10 ?4</cell><cell>3 ? 10 ?4</cell><cell>policy lr</cell></row><row><cell>Q-function learning rate</cell><cell>3 ? 10 ?4</cell><cell>3 ? 10 ?4</cell><cell>qf lr</cell></row><row><cell>Policy minimum standard</cell><cell>e ?20</cell><cell>e ?20</cell><cell>min std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy maximum standard</cell><cell>e 2</cell><cell>e 2</cell><cell>max std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gradient steps per epoch</cell><cell>500</cell><cell>500</cell><cell>gradient steps per itr</cell></row><row><cell>Number of epoch cycles</cell><cell>200</cell><cell>40</cell><cell>epoch cycles</cell></row><row><cell>Soft target interpolation</cell><cell>5 ? 10 ?3</cell><cell>5 ? 10 ?3</cell><cell>target update tau</cell></row><row><cell>parameter</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Use automatic entropy</cell><cell>True</cell><cell>True</cell><cell>use automatic entropy tuning</cell></row><row><cell>Tuning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Minimum Buffer Batch</cell><cell>1500</cell><cell>7500</cell><cell>min buffer size</cell></row><row><cell>Size</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters used for Garage experiments with Multi-Task SAC</figDesc><table><row><cell>D.6 TE-PPO</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Description</cell><cell>MT10</cell><cell>MT50</cell><cell>argument name</cell></row><row><cell>General Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell>50,000</cell><cell>250,000</cell><cell>batch size</cell></row><row><cell>Number of epochs</cell><cell>4,000</cell><cell>2,000</cell><cell>n epochs</cell></row><row><cell>Algorithm-Specific Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy hidden sizes</cell><cell>(32, 16)</cell><cell>(32, 16)</cell><cell>hidden sizes</cell></row><row><cell>Activation function of hidden layers</cell><cell>tanh</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>Likelihood ratio clip range</cell><cell>0.2</cell><cell>0.2</cell><cell>lr clip range</cell></row><row><cell>Latent dimension</cell><cell>4</cell><cell>4</cell><cell>latent length</cell></row><row><cell>Inference window length</cell><cell>6</cell><cell>6</cell><cell>inference window</cell></row><row><cell cols="2">Embedding maximum standard deviation 0.2</cell><cell>0.2</cell><cell>embedding max std</cell></row><row><cell>Policy entropy coefficient</cell><cell>2e ? 2</cell><cell>2e ? 2</cell><cell>policy ent coeff</cell></row><row><cell>Value function</cell><cell cols="2">Gaussian MLP fit with</cell><cell>baseline</cell></row><row><cell></cell><cell cols="2">observations, latent variables</cell><cell></cell></row><row><cell></cell><cell>and returns</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters used for Garage experiments with Task Embeddings PPO 29</figDesc><table><row><cell>D.7 MAML</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Description</cell><cell>ML1</cell><cell>ML10</cell><cell>ML45</cell><cell>argument name</cell></row><row><cell cols="2">Meta-/Multi-Task Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Meta-batch size</cell><cell>20</cell><cell>20</cell><cell>45</cell><cell>meta batch size</cell></row><row><cell>Roll-outs per task</cell><cell>10</cell><cell>10</cell><cell>20</cell><cell>rollouts per task</cell></row><row><cell>General Hyperparameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Path length per roll-out</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>max path length</cell></row><row><cell>Discount factor</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>discount</cell></row><row><cell>Algorithm-specific</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hyperparameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy hidden sizes</cell><cell cols="4">(128, 128) (128, 128) (128, 128) hidden sizes</cell></row><row><cell>Activation function of</cell><cell>tanh</cell><cell>tanh</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>hidden layers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Activation function of</cell><cell>tanh</cell><cell>tanh</cell><cell>tanh</cell><cell>output nonlinearity</cell></row><row><cell>output layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inner algorithm learning</cell><cell>1 ? 10 ?4</cell><cell>1 ? 10 ?4</cell><cell>1 ? 10 ?4</cell><cell>inner lr</cell></row><row><cell>rate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimizer learning rate</cell><cell>1 ? 10 ?3</cell><cell>1 ? 10 ?3</cell><cell>1 ? 10 ?3</cell><cell>outer lr</cell></row><row><cell>Maximum KL divergence</cell><cell>1 ? 10 ?2</cell><cell>1 ? 10 ?2</cell><cell>1 ? 10 ?2</cell><cell>max kl step</cell></row><row><cell>Number of inner gradient</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>num grad update</cell></row><row><cell>updates</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy entropy coefficient</cell><cell>5 ? 10 ?5</cell><cell>5 ? 10 ?5</cell><cell>5 ? 10 ?5</cell><cell>policy ent coeff</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameters used for Garage experiments with MAML 30 D.8 RL 2</figDesc><table><row><cell>Description</cell><cell>ML1</cell><cell>ML10</cell><cell>ML45</cell><cell>argument name</cell></row><row><cell cols="2">Meta-/Multi-Task Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Meta-batch size</cell><cell>25</cell><cell>10</cell><cell>25</cell><cell>meta batch size</cell></row><row><cell>Roll-outs per task</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>rollouts per task</cell></row><row><cell>General Hyperparameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Path length per roll-out</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>max path length</cell></row><row><cell>Discount factor</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>discount</cell></row><row><cell cols="2">Algorithm-Specific Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Policy hidden sizes</cell><cell>(256, )</cell><cell>(256, )</cell><cell>(256, )</cell><cell>hidden sizes</cell></row><row><cell>Activation function of</cell><cell>tanh</cell><cell>tanh</cell><cell>tanh</cell><cell>hidden nonlinearity</cell></row><row><cell>hidden layers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Activation function of</cell><cell>sigmoid</cell><cell>sigmoid</cell><cell>sigmoid</cell><cell>recurrent nonlinearity</cell></row><row><cell>recurrent layers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimizer learning rate</cell><cell cols="3">5 ? 10 ?4 5 ? 10 ?4 5 ? 10 ?4</cell><cell>optimizer lr</cell></row><row><cell cols="2">Likelihood ratio clip range 0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>lr clip range</cell></row><row><cell>Advantage estimation ?</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell><cell>gae lambda</cell></row><row><cell>Optimizer maximum</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>optimizer max epochs</cell></row><row><cell>epochs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RNN cell type used in</cell><cell>GRU</cell><cell>GRU</cell><cell>GRU</cell><cell>cell type</cell></row><row><cell>Policy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Value function</cell><cell cols="3">Linear feature baseline</cell><cell>baseline</cell></row><row><cell>Policy entropy coefficient</cell><cell cols="3">5 ? 10 ?6 5 ? 10 ?6 5 ? 10 ?6</cell><cell>policy ent coeff</cell></row><row><cell>Minimum policy standard</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>min std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Maximum policy standard</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>max std</cell></row><row><cell>deviation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters used for Garage experiments with RL 2</figDesc><table><row><cell>31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters used for Garage experiments with PEARL 32 E Reward Functions and Single-Task Results</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>.20 Pick Out Of HoleA funnel-shaped surface guides the gripper as it seeks to grab and lift the object; this prevents the gripper from running into the side of the hole in the table. The height (or "altitude") of this surface is given by alt since the variables h and z are already used.alt = I h (xy) ?o i,(xy) &gt;0.03 ? 0.15 + 0.015 log h (xy) ? o i,(xy) ?0.03</figDesc><table><row><cell>E.1</cell><cell></cell></row><row><cell>dense (0.015, 0.05, 0.005), L( t ? o , 0, 0.05, t ? o i ))</cell><cell>t ? o &gt; 0.05</cell></row><row><cell>10</cell><cell>otherwise</cell></row><row><cell>E.1.19 Pick Place</cell><cell></cell></row><row><cell>)</cell><cell>t ? o &gt; 0.05</cell></row><row><cell>10</cell><cell>otherwise</cell></row><row><cell>35</cell><cell></cell></row></table><note>A = I o?h &lt;0.02 &amp; g&gt;0 &amp; o (z) &gt;0.01 ? (1 + 5L( t ? o , 0, 0.05, t ? o i )) R = A + T H0 (R cage,dense (0.015, 0.05, 0.005), L( t ? o , 0, 0.05, t ? o i )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>10L( t ? h , 0, 0.05, t ? h i ) E.1.30 Reach Wall R = 10L( t ? h , 0, 0.05, t ? h i )This technically uses a R cage,dense function with slightly different margin parameters than the one described above (they are constant rather than dynamic), but the behaviour is mostly the same.</figDesc><table><row><cell cols="2">E.1.29 Reach</cell><cell></cell></row><row><cell cols="2">R = E.1.31 Push</cell><cell></cell></row><row><cell cols="2">A = I o?h &lt;0.02 &amp; g&gt;0</cell><cell></cell></row><row><cell>R =</cell><cell cols="3">(A + 1) ? R cage,dense (0.015, 0.05, 0.005) + A ? (1 + 5L( t ? o , 0, 0.05, t ? o i )) t ? o &gt; 0.05 10 otherwise</cell></row><row><cell cols="2">E.1.32 Sweep Into Goal</cell><cell></cell></row><row><cell>Note: R = ? ?</cell><cell>(2R cage,dense (0.02, 0.05, 0.01) + 2T H0 (R cage,dense (0.02, 0.05, 0.01), L( t ? o , 0, 0.05, t ? o i )))</cell><cell cols="2">t ? o &gt; 0.05</cell></row><row><cell>?</cell><cell>10</cell><cell cols="2">otherwise</cell></row><row><cell cols="2">E.1.33 Sweep</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>05</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell>otherwise</cell></row><row><cell cols="2">E.1.27 Handle Pull</cell><cell></cell></row><row><cell cols="3">A = I o?h &lt;0.035 &amp; g&gt;0 &amp; o (z) ?o i,(z) &gt;0.01 ? (1 + 5L(|t (z) ? o (z) |, 0, 0.05, |t (z) ? o i,(z) |))</cell></row><row><cell>R =</cell><cell cols="2">A + T H0 R cage,dense (0.022, 0.05, 0.01), L(|t (z) ? o (z) |, 0, 0.05, |t (z) ? o i,(z) |) 10</cell><cell>t ? o &gt; 0.05 otherwise</cell></row><row><cell cols="2">E.1.28 Handle Pull Side</cell><cell></cell></row><row><cell></cell><cell></cell><cell>z) |)</cell><cell>t ? o &gt; 0.05</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell>otherwise</cell></row><row><cell></cell><cell>36</cell><cell></cell></row></table><note>A = I o?h &lt;0.035 &amp; g&gt;0 &amp; o (z) ?o i,(z) &gt;0.01 ? (1 + 5L(|t (z) ? o (z) |, 0, 0.05, |t (z) ? o i,(z) |)) R = A + T H0 R cage,dense (0.032, 0.06, 0.01), L(|t (z) ? o (z) |, 0, 0.05, |t (z) ? o i,(</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In practice, the policy must be able to read in the state for each of the tasks, which typically requires them to at least have the same dimensionality. In our benchmarks, some tasks have different numbers of objects, but the state dimensionality is always the same, meaning that some state coordinates are unused for some tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Videos are on the project webpage, at meta-world.github.io 7</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Suraj Nair for feedback on a draft of the paper. We thank K.R Zentner for her help in maintaining Meta-World. This research was supported in part by the National Science Foundation under IIS-1651843, IIS-1700697, and IIS-1700696, the Office of Naval Research, ARL DCIST CRA W911NF-17-2-0181, DARPA, Google, Amazon, and NVIDIA.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to select and generalize striking movements in robot table tennis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?lling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00177</idno>
		<title level="m">Learning dexterous in-hand manipulation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining model-based and model-free updates for trajectory-centric reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sukhatme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">kpam: Keypoint affordances for categorylevel robotic manipulation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<idno>abs/1809.04474</idno>
		<title level="m">Multi-task deep reinforcement learning with popart. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Actor-mimic: Deep multitask and transfer reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06342</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<title level="m">Openai gym</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Quantifying generalization in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02341</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">L</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefrancq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00690</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Deepmind control suite</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno>abs/1709.06009</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06295</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Policy distillation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01561</idno>
		<title level="m">Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Online multi-task learning using active sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rl$?2$: Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1611.02779</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to reinforcement learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05763</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rothfuss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Clavera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06784</idno>
		<title level="m">Promp: Proximal meta-policy search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient off-policy metareinforcement learning via probabilistic context variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning by the baldwin effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference Companion</title>
		<meeting>the Genetic and Evolutionary Computation Conference Companion</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Been there, done that: Meta-learning with episodic recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09692</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03720</idno>
		<title level="m">Gotta learn fast: A new benchmark for generalization in rl</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to adapt in dynamic, real-world environments through meta-reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Clavera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Fearing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11347</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saemundsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07551</idno>
		<title level="m">Meta reinforcement learning with latent variable gaussian processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03143</idno>
		<title level="m">Benchmarking in manipulation research: The ycb object and model set and benchmarking protocols</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">IKEA furniture assembly environment for long-horizon complex manipulation tasks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1911.07246" />
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rlbench: The robot learning benchmark and learning environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Arrojo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">More than a million ways to be pushed. a high-fidelity experimental dataset of planar pushing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fazeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bigs: Biotac grasp stability dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sukhatme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA 2016 Workshop on Grasping and Manipulation Datasets</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robot learning in homes: Improving generalization and reducing dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9112" to="9122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Orbay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02790</idno>
		<title level="m">A crowdsourcing platform for robotic skill learning through imitation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multiple interactions made easy (mime): Large scale demonstrations data for imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07121</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analysis and observations from the first amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Causo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Wurman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A list of household objects for robotic retrieval prioritized by people with als</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Rehabilitation Robotics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01201</idno>
		<title level="m">A platform for embodied ai research</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ai2-thor: An interactive 3d environment for visual ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05474</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Home: A household multimodal environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brodeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Celotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11017</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03931</idno>
		<title level="m">Multimodal indoor simulator for navigation in complex environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gibson env: Real-world perception for embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03938</idno>
		<title level="m">An open urban driving simulator</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Torcs, the open racing car simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Espi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guionneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dimitrakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sumner</surname></persName>
		</author>
		<ptr target="http://torcs.sourceforge.net" />
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2213" to="2222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Leveraging big data for grasp planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4304" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The kit object models database: An object model database for object recognition, localization and manipulation in service robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The columbia grasp database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goldfeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Surreal: Open-source reinforcement learning framework and robot manipulation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Creus-Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual reinforcement learning with imagined goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning an embedding space for transferable robot skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Garage: A toolkit for reproducible reinforcement learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/rlworkgroup/garage" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ray interference: a source of plateaus in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11455</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Online meta-learning. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
