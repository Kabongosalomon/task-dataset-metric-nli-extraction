<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Pavlichenko</surname></persName>
							<email>pavlichenko@yandex-team.ru</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandex</forename><surname>Moscow</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russia</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Stelmakh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
							<email>dustalov@yandex-team.ru</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yandex Saint Petersburg</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.5281/zenodo.5574585</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. In simple problems such as image classification, crowdsourcing has become one of the standard tools for cheap and time-efficient data collection: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing aggregation methods for more advanced applications is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CROWDSPEECH -the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing and novel aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of developing the methodology for reliable data collection via crowdsourcing. In that, we design a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VOXDIY -a counterpart of CROWDSPEECH for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing. 1 1 The code and data are released at https://github.com/Toloka/CrowdSpeech and https://doi. org/10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech recognition is an important research problem that has found its applications in various areas from voice assistants such as Siri or Alexa <ref type="bibr" target="#b19">[20]</ref> to call centers <ref type="bibr" target="#b34">[35]</ref> and accessibility tools <ref type="bibr" target="#b3">[4]</ref>. The research community has been actively developing tools for automated speech recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52</ref>, and many other works]. As a result, the state-of-the-art methods achieve near-perfect performance <ref type="bibr" target="#b54">[54]</ref> on LIBRISPEECH <ref type="bibr" target="#b37">[37]</ref> -a famous benchmark to compare speech recognition systems.</p><p>While the technical performance on curated benchmarks is almost perfect, it does not necessarily result in reliable practical performance <ref type="bibr" target="#b45">[45]</ref>. Indeed, in real applications, people may use some specific vocabulary or dialects underrepresented in the conventional training data. Thus, blind application of methods trained on the standard benchmarks may result in low accuracy or, perhaps more concerning, discrimination of some subpopulations. For example, a recent study of YouTube's Automatic Captions reveals a difference in accuracy across gender and dialect of the speaker <ref type="bibr" target="#b46">[46]</ref>.</p><p>One approach towards improving the practical performance of speech-recognition systems is to fine-tune the models in these systems on domain-specific ground truth data. Fine-tuning is very important and efficient for the speech-recognition task <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b53">53]</ref>, but the main problem with this approach is the lack of data. Indeed, even datasets that are considered to be very small in the area (e.g., CHiME-6 <ref type="bibr" target="#b49">[49]</ref>) contain hours of annotated audios. While getting such an amount of unlabeled data in a speech-focused application may be feasible, annotating this data with the help of expert annotators may be prohibitively expensive or slow.</p><p>Recently, crowdsourcing has become an appealing alternative to the conventional way of labeling data by a small number of experts. Platforms like Mechanical Turk (https://www.mturk.com/) and Toloka (https://toloka.ai/) significantly reduce the time and cost of data labeling by providing on-demand access to a large crowd of workers. Of course, this flexibility comes at some expense, and the main challenge with the crowdsourcing paradigm is that individual workers are noisy and may produce low-quality results. A long line of work <ref type="bibr">[12, 21, 42, 43, 50, 55,</ref> and others] has designed various methods to estimate true answers from noisy workers' responses to address this issue in multiclass classification. As a result, crowdsourcing has become an industry standard for image labeling with small and large technology companies using it to improve their services <ref type="bibr" target="#b12">[13]</ref>.</p><p>In speech recognition, however, the annotations obtained from crowd workers are sentences and not discrete labels, which makes the aforementioned classification methods impractical. Unfortunately, the problem of learning from noisy textual responses and other non-conventional modalities is much less studied in Machine Learning and Computer Science communities. One of the obstacles towards solving this problem in a principled manner is the lack of training data: in contrast to the classification setup, worker answers in the speech recognition tasks are high-dimensional, and researchers need a large amount of data to build and evaluate new methods. Therefore, we focus our work on bridging this gap by constructing and analyzing a large-scale dataset of crowdsourced audio transcriptions.</p><p>At a higher level, this work also considers the more general challenge of developing the methodology for reliable data collection via crowdsourcing. In many areas, data is the key resource for research and development. When the costs of getting data are high, it becomes available to only a privileged population of researchers and practitioners, contributing to the overall inequity in the community. Crowdsourcing offers an appealing opportunity to make data collection affordable. However, to take the full benefits of crowdsourcing, the research community needs to develop procedures and practices to take reliable control over the quality of collected data. In this work, we build on our long experience of industrial data collection at Yandex <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> and share resources as well as insights that may benefit researchers and engineers who want to collect reliable data on crowdsourcing platforms.</p><p>Our Contributions Overall, in this work we make several contributions:</p><p>First, we collect and release CROWDSPEECH -the first publicly available large-scale dataset of crowdsourced audio annotations. In that, we obtain annotations for more than 60 hours of English speech from 3,994 crowd workers.</p><p>Second, we propose a fully automated pipeline to construct semi-synthetic datasets of crowdsourced audio annotations in under-resourced domains. Using this procedure, we construct VOXDIY -a counterpart of CROWDSPEECH for Russian language.</p><p>Third, we evaluate the performance of several existing and novel methods for aggregation of noisy transcriptions on collected datasets. Our comparisons indicate room for improvement, suggesting that our data may entail progress in designing better algorithms for crowdsourcing speech annotation.</p><p>Fourth and finally, we release the code to fully replicate the data preparation and data collection processes we execute in this work. Additionally, we share various actionable insights that researchers and practitioners can use to fulfill their data collection needs.</p><p>The remainder of this paper is organized as follows. We begin with a survey of related work in Section 2. We then construct a pool of speech recordings for annotation in Section 3 and describe the annotation pipeline in Section 4. We provide an exploratory analysis of our datasets in Section 5 and evaluate existing and novel methods in Section 6. A short discussion of the results is provided in Section 7. Finally, we note that while the discussion in the present paper is centered around speech recognition, this work extends to other applications where textual sequences may be crowdsourced (e.g., optical character recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b48">48]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several past works investigated the use of crowdsourcing platforms to obtain training data for speech recognition systems. We now discuss the most relevant studies.</p><p>Aggregation Methods Despite the problem of aggregation of textual responses has been receiving much less attention than aggregation in the context of classification, there are several works in this direction. The first study dates back to 1997 when Fiscus <ref type="bibr" target="#b17">[18]</ref> proposed a method called ROVER to combine outputs of multiple speech-recognition systems. Several subsequent works <ref type="bibr">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> demonstrated the usefulness of this method in the crowdsourcing setup. More recently, two novel methods, RASA and HRRASA, were proposed to aggregate multiple translations of a sentence from one language to another <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Despite the fact that these methods were designed in the context of machine translation, they are positioned as general methods for text aggregation and hence apply to our problem. Overall, the three methods mentioned above constitute a pool of available baselines for our problem. In Section 6, we return to these methods for additional discussion and evaluation. <ref type="bibr" target="#b35">[36]</ref> annotated about thirty hours of audio recordings on MTurk to evaluate the quality of crowdsourced transcriptions by measuring the accuracy of a speech recognition model trained on this data. They concluded that quantity outweighs quality, that is, a larger number of recordings annotated once each led to a higher accuracy than a smaller number of recordings annotated multiple times with subsequent aggregation. We note, however, that this conclusion may itself be influenced by an absence of a principled aggregation algorithm. Additionally, it is not always the case that one can obtain more data for annotation. Thus, in our work, we complement that study by investigating an orthogonal problem of developing better aggregation algorithms. For this, in our data-collection procedure, we obtain more annotations for each recording (seven vs. three) to give algorithm designers more freedom in using our data. Finally, to the best of our knowledge, the dataset collected by Novotney and Callison-Burch is not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Collection Novotney and Callison-Burch</head><p>Another relevant contribution is a small dataset of translations from Japanese to English constructed by Li <ref type="bibr" target="#b27">[28]</ref>. Each sentence in this dataset is associated with ten crowdsourced translations and a ground truth translation. We treat this data as a baseline dataset and return to it in Sections 5 and 6.</p><p>Several other works construct benchmark datasets for automated speech recognition without relying on crowdsourced annotation of audios. Specifically, LIBRISPEECH <ref type="bibr" target="#b37">[37]</ref> (discussed in detail below) and GIGASPEECH <ref type="bibr" target="#b8">[9]</ref> build on audios with known transcriptions (e.g., audio books or videos with human-generated captions). Starting from annotated long audios, they split the recordings into smaller segments and carefully align these segments with the ground truth texts. While this approach may result in high fidelity datasets, its applicability is limited to domains with pre-existing annotated recordings. Another clever approach is used in the COMMONVOICE dataset <ref type="bibr">[2]</ref>. COMMONVOICE is constructed by starting from short ground truth texts and then crowdsourcing speech recordings of these texts. We note that this approach is complementary to ours (start from audios and then crowdsource transcriptions) and the choice between the two approaches may be application-dependent.</p><p>Other Approaches Several papers propose procedures to crowdsource high-quality annotations while avoiding automated aggregation of texts. One approach <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref> it to develop a multi-stage process in which initial transcriptions are improved in several rounds of post-processing. While this pipeline offers an appealing alternative to automated aggregation, it is much more complicated and may provide unstable quality due to variations in workers' accuracy. Another approach <ref type="bibr" target="#b4">[5]</ref> is to reduce the aggregation of texts to the conventional classification problem. Specifically, given noisy annotations obtained in the first stage, a requester can hire an additional pool of workers to listen to original recordings and vote for the best annotation from the given set, thereby avoiding the challenging step of learning from noisy texts. However, this approach is associated with increased costs, and its accuracy is fundamentally limited by the accuracy of the best annotation produced in the first stage. In contrast, aggregation-based approach that we focus on in this work can in principle result in transcriptions that are better than all initial transcriptions of the corresponding recordings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Source</head><p>In this section, we begin the description of our data collection procedure by introducing the pool of speech recordings that we annotate in Section 4. <ref type="table" target="#tab_0">Table 1</ref> gives an overview of our data sources.</p><p>LibriSpeech Benchmark (cf. Contribution 1) LIBRISPEECH is a famous benchmark for comparing speech recognition systems that consists of approximately 1,000 hours of read English speech derived from audiobooks and split into small segments (https://www.openslr.org/12). Specifically, LIBRISPEECH consists of two subsets -"clean" and "other". The clean subset contains recordings of higher quality with accents of the speaker being closer to the US English, while the other subset contains recordings that are more challenging for recognition. To achieve a better diversity of our data, we use both the other and clean parts of LIBRISPEECH: our initial pool of recordings comprises full dev and test sets as well as a part of the clean train set (11,000 recordings selected uniformaly at random from the train-clean-100 subset of LIBRISPEECH). An important feature of the LIBRISPEECH dataset is its gender balance -approximately half of the recordings are made by female speakers. Thus, the recordings we use in this work are also gender-balanced.</p><p>Under-Resourced Domains (cf. Contribution 2) LIBRISPEECH is a rich source of recordings, but it is focused on a specific domain of audiobooks and contains recordings of English speech only. Thus, the aggregation algorithms trained on the annotations of LIBRISPEECH we collect in Section 4 may not generalize to other domains and languages due to potential differences in workers' behaviour.</p><p>To alleviate this issue, we propose the following pipeline to obtain domain-specific datasets for fine-tuning or evaluating aggregation methods:</p><p>1. Obtain texts from the target under-resourced domain. 2. Use speech synthesis tools to construct recordings of texts collected in the previous step.</p><p>3. Obtain annotations of these recordings using crowdsourcing. <ref type="bibr">2</ref> The dataset constructed in this procedure can be used to fine-tune (evaluate) aggregation methods on data from novel domains. In this work, we demonstrate this pipeline by collecting a dataset of crowdsourced annotations of Russian speech recordings. Let us now describe the first two steps of the pipeline and introduce the second pool of synthetic recordings that we will annotate in Section 4.</p><p>Texts from a Target Domain For the sake of the example, we use the domain of news as our target domain. For this, we take sentences in Russian from the test set of the machine translation shared task executed as a part of the Eights and Ninth Workshops on Statistical Machine Translation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. To support reliable evaluation, we additionally filter these texts to align their formatting with that used in the LIBRISPEECH dataset. A detailed description of this stage is given in Appendix C.</p><p>Recording Synthetic recording is the crux of our approach as it allows us to obtain recordings with known ground truth transcriptions without involving costly human speakers. In this example, we rely on Yandex SpeechKit 3 -an industry-level tool for speech synthesis -to obtain recordings of the ground truth texts. Importantly, Yandex SpeechKit gives access to both "male" and "female" voices, as well as to different intonations (neutral and evil). Thus, in the recording stage, we choose the "gender" and intonation for each recording uniformly at random, ensuring the diversity of our synthetic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entrance Exam</head><p>? Test annotation of 10 audios</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High quality</head><p>Main Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low quality</head><p>Not accepted to the main task  Following the procedure outlined above, we obtain 3,091 recordings of Russian speech that we title RUSNEWS. <ref type="table" target="#tab_0">Table 1</ref> gives summary statistics for two pools of recordings used in this work. In the next section, we will use audios from LIBRISPEECH and RUSNEWS to construct datasets CROWDSPEECH (based on LIBRISPEECH) and VOXDIY (based on RUSNEWS) of crowdsourced audio annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Annotation</head><p>With datasets of speech recordings prepared in Section 3, we now proceed to the annotation stage in which we build our CROWDSPEECH and VOXDIY datasets. In that, we introduce the pipeline <ref type="figure" target="#fig_1">(Figure 1</ref>) that we used to gather reliable transcriptions on the Toloka crowdsourcing platform. <ref type="bibr" target="#b3">4</ref> Additionally, throughout this section, we reflect on our data collection experience and give practical advice that may be useful for researchers and practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Design</head><p>We begin the exposition of our pipeline with a discussion of instructions, interface, and compensations.</p><p>Instructions and Interface Despite the task of audio transcription may sound natural to workers, there are important nuances that should be captured in the instructions and the interface. First, as our ground truth annotations have some specific formatting, we put a strong emphasis on conveying the transcription rules to the workers in the instructions. Next, throughout the task, workers may experience technical difficulties with some recordings, and we design the interface with that in mind. Specifically, at the beginning of the task, workers are asked whether the given audio plays well in their browser. The positive answer to this question triggers the text field, and the negative answer allows workers to report the technical issue without contaminating our data with an arbitrary transcription. The full version of instructions and a screenshot of the interface are given in Appendices A and B.</p><p>Compensation The recordings we annotate in this work can roughly be grouped by the level of difficulty: RUSNEWS and the clean subset of LIBRISPEECH are relatively easy while the other subset of LIBRISPEECH is harder to annotate. Thus, we issue a compensation of one cent (respectively, three cents) per annotation for recordings in the first (respectively, second) group. This amount of compensation was selected for the following reasons: (i) a typical amount of compensation for similar tasks on Toloka is one cent per recording; (ii) several past speech recognition studies that employed crowdsourcing <ref type="bibr">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref> were issuing a compensation ranging from 0.5 cents to 5 cents for annotation of a recording of comparable length; (iii) a large fraction of workers on Toloka and other crowdsourcing platforms are residents of countries with a low minimum hourly wage.</p><p>Workers Well-Being Throughout the experiment, we were monitoring various quantities related to workers well-being. Specifically, the hourly compensation for active workers was close to or even exceeded the minimum hourly wage in Russia -the country of residence for primary investigators of this study. Additionally, our projects received mean quality ratings of 4.5 and above (out of 5) in anonymous surveys of workers, suggesting that workers deemed the work conditions reasonable.</p><p>Practical Comment. In preliminary trials, we experimented with issuing compensations to workers even when they were unable to play the audio due to self-reported technical difficulties. Unfortunately, this setup resulted in workers reporting technical difficulties for a huge share of the tasks. Once we switched to the compensation for annotated recordings only, the amount of self-reported technical problems reduced drastically without affecting the quality of annotations. This observation suggests a spamming behavior in the original setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Worker Selection and Quality Control</head><p>Another key aspect of crowdsourcing data collection is to recruit the right population of workers for the task. For this, we make our task available to only those workers who self-report the knowledge of the language of the task: English for CROWDSPEECH and Russian for VOXDIY. Additionally, we implement an Entrance Exam. For this, we ask all incoming eligible workers to annotate ten audio recordings. We then compute our target metric -Word Error Rate (WER) -on these recordings and accept to the main task all workers who achieve WER of 40% or less (the smaller the value of the metric, the higher the quality of annotation). In total, the acceptance rate of our exam was 64%.</p><p>Importantly, to achieve a consistent level of annotation quality, it is crucial to control the ability of workers not only at the beginning but also throughout the task. For this, we implement the following rules to detect spammers and workers who consistently provide erroneous annotations:</p><p>? Spam-detection rules. Spammers often try to complete as many tasks as possible before getting detected and removed from the platform. To mitigate this behavior, we use a rule that automatically blocks workers from our projects if they complete two or more tasks in less than ten seconds. ? Golden set questions. We use golden set questions to continuously monitor the quality of annotations supplied by workers. If the mean value of the WER metric over the last five completed golden set questions was reaching 35%, we were blocking the worker from taking more tasks. <ref type="bibr" target="#b4">5</ref> Practical Comment. Workers may be hesitant to participate in the tasks if there is a risk that all their work is rejected without compensation. To avoid the additional burden on workers, we follow best practices of (i) compensating the exam for all workers who attempted it, irrespective of whether they passed the bar for the main task or not; (ii) issuing compensations to the workers for the tasks they have completed before being flagged by our quality control rules (these tasks are also included in the final datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Running the Task</head><p>Having the pipeline prepared, we annotate each of the six sets of recordings described in <ref type="table" target="#tab_0">Table 1</ref> to construct our CROWDSPEECH and VOXDIY datasets. In that, we annotate each set of recordings in a separate pool (five pools for CROWDSPEECH with a common entrance exam and one pool for VOXDIY with a separate exam), keeping the task setup identical modulo the use of instructions in Russian for the VOXDIY dataset. Each individual recording was annotated by seven workers. If a worker reported technical issues on any recording, that recording was reassigned to another worker.</p><p>As a result of this procedure, we obtained six pools of annotated recordings -first five of these pools comprise the CROWDSPEECH dataset and the last pool of Russian recordings comprises the VOXDIY dataset. We release annotated data as well as the Python code to replicate our pipeline in the GitHub repository referenced on the first page of this manuscript.</p><p>Privacy Remark. The Toloka crowdsourcing platform associates workers with unique identifiers and returns these identifiers to the requester. To further protect the data, we additionally encode each identifier with an integer that is eventually reported in our released datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Exploratory Analysis of Collected Datasets</head><p>Having constructed the CROWDSPEECH and VOXDIY datasets, we now proceed to the analysis of collected data on various dimensions, specifically focusing on the reliability of annotators. Before we delve into details, let us make some important remarks. First, for the sake of analysis, we slightly post-process the annotations obtained in our datasets by removing punctuation marks and making all sentences lowercased. This post-processing step is only needed to ensure consistency with the ground truth data but does not conceptually affect the quality of collected data. Second, when possible, we compare our datasets with CROWDWSA2019 -a dataset of crowdsourced translations (supplied with ground truth translations) constructed by Li et al. <ref type="bibr" target="#b27">[28]</ref>. While this dataset is constructed in a different application, it is the largest publicly available dataset for the problem of noisy text aggregation. Hence, it is interesting to juxtapose it to our data. With these preliminaries, we are now ready to present the exploratory analysis of collected datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview of Annotated Datasets</head><p>A general overview of the collected datasets is presented in <ref type="table" target="#tab_1">Table 2</ref>. First, observe that in total, we have collected 176,519 annotations of 25,217 recordings made by 4,386 unique workers. Thus, our datasets are several orders of magnitude larger than CROWDWSA2019. To the best of our knowledge, our data is also the largest publicly available data of crowdsourced texts.</p><p>Second, it appears that in all datasets, the mean length of the crowdsourced annotations (translations) is slightly smaller than the mean length of the ground truth texts. This observation suggests that workers tend to skip some words in both the annotation and translation tasks.</p><p>Finally, <ref type="figure" target="#fig_2">Figure 2</ref> shows the distribution of the number of tasks completed by a worker for data collected in this study. Observe that these distributions differ significantly between projects, likely being dependent on the task difficulty. It would be interesting to see if the aggregation algorithms can adapt for the changing distribution to provide a consistent improvement on different kinds of projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inter-Rater Agreement</head><p>To evaluate whether the workers understood our task correctly, we compute Krippendorff's ? <ref type="bibr" target="#b22">[23]</ref>, a chance-corrected inter-rater agreement measure that handles missing values and allows an arbitrary  distance function. In this work, we compute the value of Krippendorff's ? using the Levenshtein distance (i.e., edit distance). Since the computation of ? is time-consuming as it iterates over all the possible co-occurring item pairs, we obtain and report the sampling estimate of this value as follows. For each sample in the set of 10,000 samples, we randomly select 100 different audio recordings with replacement and compute ? for all the transcriptions obtained for these recordings. We then take the mean of these values across all iterations and report it in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Following recommendations of Krippendorff <ref type="bibr" target="#b22">[23]</ref>, we note that values of ? 0.8 suggest that annotations obtained for our dataset are reliable. Thus, we conclude that workers on Toloka successfully understood and performed our audio transcription task. Interestingly, the CROWDWSA2019 dataset demonstrates a much lower agreement between raters. We hypothesize that this discrepancy is due to the different natures of the tasks. Indeed, in the case of translations, there may be multiple equally good translations, and even ideal translators may have some disagreement. In contrast, the audio transcription task has unique underlying ground truth (ideal annotators can be in perfect agreement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we evaluate the existing and novel methods for aggregation of noisy texts on our data. Specifically, in Sections 6.1 and 6.2 we analyze several existing methods. Next, in Section 6.3 we introduce a novel method developed on our data and compare it against the best baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline Methods</head><p>In our evaluations, we consider the following baseline methods, using implementations from the Crowd-Kit library (https://github.com/Toloka/crowd-kit) when available.</p><p>? Random A naive baseline that uniformly at random picks one of the annotations to be the answer.</p><p>? ROVER Recognizer Output Voting Error Reduction <ref type="bibr" target="#b17">[18]</ref> was originally designed to combine the output of several different automatic speech recognition systems but was also demonstrated to work well on crowdsourced sequences <ref type="bibr">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. Under the hood, it aligns given sequences using dynamic programming and then computes the majority vote on each token.</p><p>? RASA Reliability Aware Sequence Aggregation <ref type="bibr" target="#b27">[28]</ref> employs large-scale language models for aggregating texts. It encodes all worker responses using RoBERTa <ref type="bibr" target="#b29">[30]</ref> (RuBERT 6 for the Russian language) and iteratively updates the mean weighted embedding of workers' answers together with estimates of workers' reliabilities. Finally, the method defines the final answer to be the response closest to the aggregated embedding based on the notion of cosine distance.</p><p>? HRRASA We also use a modification of RASA called HRRASA <ref type="bibr" target="#b26">[27]</ref> that, besides the global reliabilities, uses local reliabilities represented by the distance from a particular response to other responses for the task. In the original paper <ref type="bibr" target="#b26">[27]</ref>, GLEU <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> metric was suggested to calculate the distance between sequences, and we resort to this choice in our experiments as well. In addition to comparing the baselines, we want to make a rough conclusion on whether any of them demonstrate the optimal performance on our data. Note that it may be infeasible to uncover all transcriptions with absolute accuracy as for some recordings, the noise in annotations could vanish out all the signal. To obtain a more reasonable estimate of achievable performance, we introduce the Oracle aggregation algorithm to the comparison. For each recording, it enjoys the knowledge of the ground truth and selects the best transcription provided by the workers as its answer.</p><p>The Oracle method achieves the maximum accuracy that can be reached by an aggregation algorithm restricted to the set of transcriptions provided by the workers. Thus, Oracle gives a weak estimate of the achievable quality as its accuracy could be improved by an algorithm that is allowed to modify transcriptions provided by the workers. Nevertheless, in the analysis below, we focus on the gap between the baselines and the Oracle to estimate if there is some room for improvement on our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance of Baseline Methods</head><p>To evaluate baseline methods, we run them on each of the datasets under consideration excluding the train set of CROWDSPEECH as its main purpose is model training. We then compute the mean value of WER (Word Error Rate) over all recordings in each dataset and report it in <ref type="table" target="#tab_3">Table 4</ref>. First, we note that when the quality of recordings is good, as in the case of the semi-synthetic VOXDIY dataset, non-trivial baseline methods achieve a near-perfect performance. This observation suggests that when the level of noise in collected annotations is relatively low, existing baselines satisfy the needs of practitioners.</p><p>However, observe that on the more challenging CROWDSPEECH dataset, there is a consistent gap between all baselines and Oracle, with the gap being larger for more difficult subsets (dev-other and test-other). This observation indicates that there is room for the development of better aggregation methods that keep up with, or even exceed, the performance of Oracle on more difficult tasks.</p><p>Finally, we note that the performance of all aggregation methods, including Oracle, is much weaker on the CROWDWSA2019 dataset. This effect is likely an artifact of the subjective nature of the machine translation task, which, in contrast to the speech recognition task, does not have a unique ground truth answer. Thus, CROWDWSA2019 may not be the best choice to design aggregation methods for objective tasks such as speech recognition. The same observation applies to the methods developed for that dataset (RASA and HRRASA): <ref type="table" target="#tab_3">Table 4</ref> indicates that on our data a simple ROVER baseline is always superior to these more advanced algorithms. Of course, a symmetric observation applies to our CROWDSPEECH and VOXDIY which may also be suboptimal for the machine translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Novel Methods Developed on Our Data</head><p>In parallel with preparing this paper, we have designed and executed a shared task on developing aggregation methods for crowdsourced audio transcriptions <ref type="bibr" target="#b47">[47]</ref>. The competitive nature of the task does not allow us to use LIBRISPEECH audios as it would result in the test data leakage. To avoid this problem, in the shared task we relied on the pipeline used to construct the VOXDIY dataset in the present paper. Specifically, we crowdsourced annotations of synthetic recordings of passages from Wikipedia and books <ref type="bibr" target="#b21">[22]</ref>. One of the best results <ref type="bibr" target="#b39">[39]</ref> in this shared task was demonstrated by a carefully fine-tuned T5 model <ref type="bibr" target="#b41">[41]</ref>. With permission of the author, we evaluate their approach on the test sets of the CROWDSPEECH dataset collected in this paper. For this, we introduce three additional models (see details in Appendix D):</p><p>? First, we consider T5 model trained on the shared task data [T5 (ST)]</p><p>? Second, we fine-tune T5 (ST) on the development sets of CROWDSPEECH [T5 (ST+FT)]</p><p>? Third, we fine-tune the original T5 <ref type="bibr" target="#b41">[41]</ref> on the train-clean subset of CROWDSPEECH [T5 (FT)] <ref type="table" target="#tab_4">Table 5</ref> juxtaposes these models to the best of the available baselines (ROVER) on the test sets of CROWDSPEECH. First, we note that all T5-based models significantly outperform ROVER -the baseline that remained unchallenged for more than twenty years -setting new state-of-the-art results. Second, we observe that fine-tuning on the domain-specific data is crucial for our task as T5 (ST+FT) model is superior to T5 (ST) on both test sets. Similarly, T5 (FT) that got fine-tuned on the train-clean subset of CROWDSPEECH (which is much larger than dev-clean) outperforms other models on the test-clean set. Not surprisingly, however, it demonstrates lower accuracy on the test-other set because, in contrast to T5 (ST + FT), it did not get to see data from the other subset of CROWDSPEECH.</p><p>With these observation, we make the following conclusions:</p><p>? First, there is initial evidence that the data we release in this paper is instrumental in developing novel principled methods for aggregation of crowdsourced annotations. ? Second, observe that T5-based models do not use the fact that each worker provides multiple annotations in the dataset and do not estimate workers' expertise. In contrast, this information is known to be crucial for aggregation of categorical data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">50]</ref>. Thus, we believe that it is possible to further improve the results by coupling T5 with some way to estimate worker skills. ? Third, note that the T5-based method is designed on the data collected through the semi-synthetic procedure used to construct VOXDIY. Given that the strong performance of thes method carried over to the realistic CROWDSPEECH dataset, we conclude that the semi-synthetic data may be useful to quickly explore new domains in which no annotated recordings of human voice exist.</p><p>Finally, we refer the reader to Appendix E where we provide additional comparison of models in terms of types of errors they make.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we collected and released CROWDSPEECH -the first publicly available large-scale dataset of crowdsourced audio transcriptions. Based on evaluations of existing and novel methods on our data, we believe that our work will enable researchers to develop principled algorithms for learning from noisy texts in the crowdsourcing setting. Additionally, we proposed an automated pipeline for collecting semi-synthetic datasets of crowdsourced audio transcriptions in under-resourced domains. We demonstrated this pipeline by constructing VOXDIY -a Russian counterpart of CROWDSPEECH.</p><p>In the end, we should mention some limitations of our work. First, we admit that the use of speech synthesis techniques could affect the distribution of errors people make when annotating audios, thereby affecting the generalization ability of aggregation tools trained on VOXDIY. Second, in this work, we annotated our datasets in an industry-level pipeline, which resulted in annotations of high quality. It would be interesting to additionally collect datasets under less stringent quality control rules or for more challenging data to make our data even more diverse in terms of complexity.</p><p>Better understanding and addressing these limitations is an interesting direction for future research. With these caveats, we encourage researchers and practitioners to use our data judiciously and to carefully evaluate all the risks and benefits in their specific application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exam</head><p>To start working on the main task, you have to pass a qualification exam. In the exam, you need to transcribe 10 audios and you will be accepted to the main task if the quality of your transcriptions is high enough.</p><p>FAQ Q: Do I get paid for the tasks I had technical difficulties with? A: Unfortunately, no. We will reject these tasks, but it won't hurt your status or payment for other tasks.   <ref type="figure">Figure 4</ref> demonstrates the interface of the task available to workers. In order to activate the text field to enter the annotation, a worker needs to positively answer the first question. The negative answer to the first question indicates a technical issue and enables the worker to skip the question. <ref type="figure">Figure 4</ref>: Task interface. There was a single audio-annotation task on each page, and a worker could complete as many tasks as they like (subject to availability of tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Task Interface</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Filtering of the RUSNEWS Dataset</head><p>In order to align the format of sentences in the RUSNEWS dataset to the format of the LIBRISPEECH ground truth, we perform the following filtering steps on the RUSNEWS dataset.</p><p>? Remove sentences that contain digits. From the workers' standpoint, numerals and spelledout numbers in the original texts do not make a difference because they are pronounced in the same way. However, to compute the accuracy of annotations (both crowdsourced and aggregated), we need to compare a target text against the ground truth text. Thus, it is crucial to ensure that numbers in the ground-truth texts are written in a consistent manner. To ensure this consistency, we remove sentences that contain digits. ? Remove sentences that contain specific abbreviations. Some texts in the source contain abbreviations that are special for the Russian written language (e.g., a Russian equivalent of the word "doctor" is sometimes shortened to "d-r"). Such abbreviations are not used in the spoken language so we remove sentences that contains them from the pool. ? Remove sentences that contain letters from non-Russian alphabets. Finally, some sentences in the initial pool contain words from other languages (e.g., names of companies). We also remove such sentences from the pool because these cannot be properly annotated using Russian alphabet.</p><p>In practice, instead of removing sentences that fall in the aforementioned categories, one could alternatively pre-process sentences to convert numerals to the spelled-out form and adjust instructions to account for abbreviations and non-Cyrillic letters. However, careful implementation of such changes requires a non-trivial amount of work and we do not take this route in this paper as it is orthogonal to our main contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D T5, a Transformer-based Model</head><p>In this section, we give a high-level overview of the T5 model -a workhorse for comparisons we described in Section 6.3. Additionally, we provide details on the fine-tuning procedure.</p><p>T5 Model. Our evaluations in Section 6.3 rely on a pre-trained T5-large model <ref type="bibr" target="#b41">[41]</ref> -a Transformer-based model that is designed to solve various text-to-text tasks. By using T5, we reduce the problem of aggregation of crowdsourced transcriptions to a more studied text summarization problem. In our case, the input to the model consists of concatenated transcriptions provided by crowd workers for a particular recording, and the T5 model outputs a single final transcription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Fine-Tuning</head><p>The main power of the T5 model is its ability to quickly adjust to new tasks: starting from initial weights provided by Google (https://huggingface.co/t5-large), one can utilize the knowledge of pre-trained T5 while letting the model to pick up the new task. In this work, both T5 (ST + FT) and T5 (FT) models are fine-tuned on the corresponding data for 8 epochs with 10% of data set aside for the validation purposes. Specifically, to train the T5 (FT) model, we use the original weights provided by Google as the initialization. For T5 (FT + ST), we started with the weights reported by Pletenev <ref type="bibr" target="#b39">[39]</ref>. The fine-tuning procedure directly follows the summarization training approach by HuggingFace. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Error Analysis</head><p>In order to further understand the difference between the performance of aggregation algorithms, we now conduct an analysis of errors made by human annotators and aggregation algorithms discussed in Section 6. Specifically, we rely on automated and manual analysis and evaluate (i) the impact of errors in human annotations on the performance of the algorithms and (ii) the causes of errors made by crowd workers.</p><p>Automated Analysis We begin from the automated analysis. For each example in the CROWD-SPEECH test-clean dataset, we compute the WER scores of each human annotation and the output of each algorithm under comparison. We then treat transcriptions with zero WER as correct (+) and those with non-zero WER as incorrect (?). To study the effect of the quality of human transcriptions on the accuracy of aggregation algorithms, we now compare the performance of the algorithms with breakdown by three different settings:</p><p>? All Correct All workers provided the correct transcription ? Has Correct At least one worker provided the correct transcription ? All Incorrect All the workers provided an incorrect transcription Similar to the experiments conducted in Section 6, we compare the performance of the baseline and novel methods with Oracle that always produces the best transcription submitted by the workers.</p><p>Results of the comparisons are summarized in <ref type="table" target="#tab_6">Table 6</ref>. Let us now make several important observations:</p><p>? All Correct First, we observe that none of the methods introduced errors if the crowd unanimously agreed in their correct response. ? Has Correct Next, as we increase the difficulty of the task, methods start to behave differently with T5 demonstrating better performance than other methods. ? All Incorrect Finally, in the most challenging category, RASA, HRRASA, and Oracle always produce an incorrect result with non-zero WER. In contrast, ROVER and T5 perform better and sometimes are able to recover correct transcriptions even when all transcriptions contain mistakes.</p><p>These observations corroborate the results presented in Section 6 ( <ref type="table" target="#tab_3">Table 4</ref> and <ref type="table" target="#tab_4">Table 5</ref>) and demonstrate promise of methods that are not limited to crowd-generated transcriptions (ROVER and especially T5).</p><p>As a side remark, we note that RASA and HRRASA showed identical performance in this experiment, while the former outperforms the latter in terms of the WER metric ( <ref type="table" target="#tab_3">Table 4</ref>). Inspection of the data reveals that HRRASA tends to choose transcriptions with a higher WER than RASA when they both make errors, thereby being scored worse on the WER metric. Manual Analysis Next, we proceed to the manual analysis with a goal of understanding the causes of human errors. In that, we condition on a subset of hard recordings: those, that caused non-zero WER in outputs of all algorithms under consideration and sample 100 transcriptions from this subset for manual analysis. In the analysis, the authors of this paper independently classified human errors into three predefined categories:</p><p>? Task Difficulty Some recordings were lengthy or contained unexpectedly difficult lexemes, such as rare words or proper nouns. Thus, the first cause of mistakes is the task difficulty. ? Violation of Instructions The second category corresponds to various violations of instructions:</p><p>incorrect punctuation marks, wrong formatting of numbers, incomplete transcriptions -all these mistakes were classified in this category. ? Homophones Finally, a transcription could be grammatically correct and meaningful, but contain homophones or verbs in wrong tenses that impact the WER score. We classified such cases in a separate category.</p><p>In the sample of 100 transcriptions that we annotated, homophones caused 55 errors, difficult or lengthy recordings caused 28 errors, and instruction breaks caused the remaining 17 errors. We evaluated the inter-rater agreement of our analysis using Krippendorff's alpha for nominal scale and found that our annotation is reliable in classifying error causes (? = 0.81).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>? Each audio is annotated by 7 workers?</head><label>7</label><figDesc>Spam-detection rules to identify malicious workers ? Golden set questions to detect quality decay</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Schematic representation of the data annotation pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of the number of tasks completed by a worker. For brevity, we use only dev and test subsets of CROWDSPEECH and combine dev-clean and test-clean subsets (similarly, dev-other and test-other subsets) together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Task instruction presented to the workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary statistics for the source data used in this work. "Spkrs" stands for "speakers", letters M and F stand for male and female, respectively.</figDesc><table><row><cell>Source Dataset</cell><cell>Version</cell><cell>Language</cell><cell>Nature</cell><cell cols="4">Total Length, hrs # Recordings # F Spkrs # M Spkrs</cell></row><row><cell></cell><cell>train-clean</cell><cell></cell><cell></cell><cell>38.8</cell><cell>11,000</cell><cell>52</cell><cell>46</cell></row><row><cell>LIBRISPEECH</cell><cell>dev-clean dev-other</cell><cell>English</cell><cell>Real</cell><cell>5.4 5.3</cell><cell>2, 703 2, 864</cell><cell>20 16</cell><cell>20 17</cell></row><row><cell></cell><cell>test-clean</cell><cell></cell><cell></cell><cell>5.4</cell><cell>2, 620</cell><cell>20</cell><cell>20</cell></row><row><cell></cell><cell>test-other</cell><cell></cell><cell></cell><cell>5.1</cell><cell>2, 939</cell><cell>17</cell><cell>16</cell></row><row><cell>RUSNEWS</cell><cell>Ru</cell><cell>Russian</cell><cell>Synthetic</cell><cell>4.8</cell><cell>3, 091</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overview of datasets collected in this work and comparison with CROWDWSA2019.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Mean Sentence Length, words</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="6">Version Ground Truth Crowdsourced # Recordings # Workers # Answers</cell></row><row><cell></cell><cell>train-clean</cell><cell>34.6</cell><cell>32.8</cell><cell>11, 000</cell><cell>2, 166</cell><cell>77, 000</cell></row><row><cell></cell><cell>dev-clean</cell><cell>20.1</cell><cell>19.5</cell><cell>2, 703</cell><cell>748</cell><cell>18, 921</cell></row><row><cell>CROWDSPEECH</cell><cell>dev-other</cell><cell>17.8</cell><cell>16.8</cell><cell>2, 864</cell><cell>1, 353</cell><cell>20, 048</cell></row><row><cell></cell><cell>test-clean</cell><cell>20.1</cell><cell>19.2</cell><cell>2, 620</cell><cell>769</cell><cell>18, 340</cell></row><row><cell></cell><cell>test-other</cell><cell>17.8</cell><cell>16.8</cell><cell>2, 939</cell><cell>1, 441</cell><cell>20, 573</cell></row><row><cell>VOXDIY</cell><cell>RU</cell><cell>13.8</cell><cell>13.6</cell><cell>3, 091</cell><cell>457</cell><cell>21, 637</cell></row><row><cell></cell><cell>J1</cell><cell>9.5</cell><cell>9.3</cell><cell>250</cell><cell>70</cell><cell>2, 490</cell></row><row><cell>CROWDWSA2019</cell><cell>T1</cell><cell>11.9</cell><cell>9.1</cell><cell>100</cell><cell>42</cell><cell>1, 000</cell></row><row><cell></cell><cell>T2</cell><cell>11.8</cell><cell>8.6</cell><cell>100</cell><cell>43</cell><cell>1, 000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Inter-rater agreement according to the Krippendorff's ? with Levenshtein distance. Higher values indicate higher reliability.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Version Overlap Krippendorff's ?</cell></row><row><cell></cell><cell>train-clean</cell><cell>7</cell><cell>0.81</cell></row><row><cell></cell><cell>dev-clean</cell><cell>7</cell><cell>0.86</cell></row><row><cell>CROWDSPEECH</cell><cell>dev-other</cell><cell>7</cell><cell>0.77</cell></row><row><cell></cell><cell>test-clean</cell><cell>7</cell><cell>0.84</cell></row><row><cell></cell><cell>test-other</cell><cell>7</cell><cell>0.78</cell></row><row><cell>VOXDIY</cell><cell>RU</cell><cell>7</cell><cell>0.96</cell></row><row><cell></cell><cell>J1</cell><cell>10</cell><cell>0.37</cell></row><row><cell>CROWDWSA2019</cell><cell>T1</cell><cell>10</cell><cell>0.42</cell></row><row><cell></cell><cell>T2</cell><cell>10</cell><cell>0.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the baselines and the oracle performance. Evaluation criterion is the average word error rate (WER) and lower values are better.</figDesc><table><row><cell>Dataset</cell><cell cols="6">Version Oracle Random ROVER RASA HRRASA</cell></row><row><cell></cell><cell>dev-clean</cell><cell>3.81</cell><cell>17.39</cell><cell>6.76</cell><cell>7.50</cell><cell>7.45</cell></row><row><cell>CROWDSPEECH</cell><cell>dev-other test-clean</cell><cell>8.26 4.32</cell><cell>27.73 18.89</cell><cell>13.19 7.29</cell><cell>14.21 8.60</cell><cell>14.20 8.59</cell></row><row><cell></cell><cell>test-other</cell><cell>8.50</cell><cell>27.28</cell><cell>13.41</cell><cell>15.67</cell><cell>15.66</cell></row><row><cell>VOXDIY</cell><cell>RU</cell><cell>0.70</cell><cell>7.09</cell><cell>1.92</cell><cell>2.22</cell><cell>2.20</cell></row><row><cell></cell><cell>J1</cell><cell>36.50</cell><cell>76.64</cell><cell>61.16</cell><cell>65.86</cell><cell>67.57</cell></row><row><cell>CROWDWSA2019</cell><cell>T1</cell><cell>28.07</cell><cell>63.08</cell><cell>51.35</cell><cell>48.29</cell><cell>49.99</cell></row><row><cell></cell><cell>T2</cell><cell>30.46</cell><cell>63.69</cell><cell>52.44</cell><cell>49.82</cell><cell>52.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the T5-based method developed on our shared task with ROVER -the strongest of the existing baselines. Models are compared on WER and lower values are better.</figDesc><table><row><cell>Dataset</cell><cell cols="6">Version Oracle ROVER T5 (ST) T5 (ST+FT) T5 (FT)</cell></row><row><cell>CROWDSPEECH</cell><cell>test-clean test-other</cell><cell>4.32 8.50</cell><cell>7.29 13.41</cell><cell>6.21 11.80</cell><cell>5.32 10.46</cell><cell>5.22 11.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Some audios are hard to transcribe, and it is ok. Listen to it once more and write your best guess if you do not know for sure.</figDesc><table><row><cell>Q: I have technical difficulties with all audios; what to do?</cell></row><row><cell>A: Try to use another browser (we recommend Google Chrome).</cell></row><row><cell>Q: I do not understand some words in the audio; what to do?</cell></row><row><cell>A: Q: When will I get paid?</cell></row><row><cell>A: We try to review the tasks within several hours, but occasionally it may take up to a day.</cell></row><row><cell>Q: All my tasks are rejected. Why?</cell></row></table><note>A: We have some spam detection rules in place and reject users who spam. Additionally, we have some ground truth transcriptions and may block a worker if they consistently supply clearly inadequate transcriptions. If you believe that we made a mistake, shoot us a note.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the automated algorithms with breakdown by the human performance. Specifically, for each of the three regimes (All Correct, Has Correct, All Incorrect), we compare how often aggregation methods are able to come up with perfect transcriptions. Comparison is executed on the test-clean subset of CROWDSPEECH.</figDesc><table><row><cell>Crowd</cell><cell>ROVER +</cell><cell>?</cell><cell>RASA +</cell><cell>?</cell><cell cols="2">HRRASA +</cell><cell>?</cell><cell>T5 (ST) +</cell><cell>?</cell><cell>Oracle +</cell><cell>?</cell></row><row><cell>All Correct</cell><cell>46</cell><cell>0</cell><cell>46</cell><cell>0</cell><cell>46</cell><cell></cell><cell>0</cell><cell>46</cell><cell>0</cell><cell>46</cell><cell>0</cell></row><row><cell>Has Correct</cell><cell cols="3">1, 055 482 1, 085</cell><cell cols="2">452 1, 085</cell><cell cols="5">452 1, 146 391 1, 537</cell><cell>0</cell></row><row><cell>All Incorrect</cell><cell cols="2">63 974</cell><cell cols="2">0 1, 037</cell><cell cols="3">0 1, 037</cell><cell cols="2">157 880</cell><cell cols="2">0 1, 037</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that this data captures the behavior of real workers in the target domain modulo potential differences induced by the use of a synthetic speech generator.3 https://cloud.yandex.com/en-ru/services/speechkit</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code to fully reproduce the pipeline is available at https://github.com/Toloka/CrowdSpeech.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For the sake of quality control, in this work we treat all recordings as golden set questions. In practice, one could annotate a handful of examples manually and use them as golden set questions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://huggingface.co/DeepPavlov/rubert-base-cased</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/huggingface/transformers/tree/master/examples/pytorch/ summarization</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements &amp; Author Contributions</head><p>I.S. and N.P. designed the setup of the problem and the data-collection pipeline. N.P. collected data on Toloka. N.P. and D.U. conducted data analysis. All authors contributed to the writeup.</p><p>The work of I.S. was supported in part by NSF grants CIF 1763734 and CAREER: CIF 1942124.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We now provide supplementary materials. In Appendix A, we provide the text of instructions given to workers on the Toloka crowdsourcing platform. Appendix B provides a screenshot of the interface; details of the filtering procedure mentioned in Section 3 are given in Appendix C. In Appendix D, we provide additional details on the T5-based models. Finally, Appendix E reports the analysis of errors made by humans and algorithms on our data. <ref type="figure">Figure 3</ref> displays the task instruction presented to the workers. HTML template of the instruction is also available on our GitHub data release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Task Instruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transcription Rules</head><p>? All recordings have a clearly identifiable speaker -you need to transcribe their speech only. If there is some background speech, ignore it.</p><p>? Your transcription must use only letters and apostrophes ('). Do not use digits and any punctuation marks (including the question mark "?") except the apostrophe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Important Details:</head><p>Checklist </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Common Voice: A Massively-Multilingual Speech Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosana</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020</title>
		<meeting>The 12th Language Resources and Evaluation Conference, LREC 2020<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4218" to="4222" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate transcription of broadcast news speech using multiple noisy transcribers and unsupervised reliability metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="4980" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accessibility, transcription, and access everywhere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Basson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Faisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kanevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="589" to="603" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soylent: A Word Processor with a Crowd Inside</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrina</forename><surname>Panovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology, UIST &apos;10</title>
		<meeting>the 23Nd Annual ACM Symposium on User Interface Software and Technology, UIST &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<title level="m">Findings of the 2013 Workshop on Statistical Machine Translation</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02133</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gigaspeech: An evolving, multi-domain asr corpus with 10</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>000 hours of transcribed audio</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Digitalkoot: Making old archives accessible using crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otto</forename><surname>Chrons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Sundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HCOMP</title>
		<meeting>HCOMP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crowdsourcing an OCR gold standard for a German and French heritage corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenz</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Volk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Philip Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Practice of efficient data collection via crowdsourcing at large-scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Drutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriya</forename><surname>Farafonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Megorskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evfrosiniya</forename><surname>Zerminova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Zhilinskaya</surname></persName>
		</author>
		<idno>abs/1912.04444</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crowdsourcing practice for efficient data labeling: Aggregation, incremental relabeling, and pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Drutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Megorskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evfrosiniya</forename><surname>Zerminova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daria</forename><surname>Baidakova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;20</title>
		<meeting>the 2020 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2623" to="2627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practice of efficient data collection via crowdsourcing: Aggregation, incremental relabelling, and pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Drutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Megorskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evfrosiniya</forename><surname>Zerminova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daria</forename><surname>Baidakova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM &apos;20</title>
		<meeting>the 13th International Conference on Web Search and Data Mining, WSDM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="873" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crowdsourcing natural language data at scale: A hands-on tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Drutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Megorskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daria</forename><surname>Baidakova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Amazon Mechanical Turk for Transcription of Non-Native Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="53" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings</title>
		<imprint>
			<biblScope unit="page" from="347" to="354" />
			<date type="published" when="1997" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Introduction to Voice Assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cortana</forename><surname>Siri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">More</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Reference Services Quarterly</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Homemade BookCorpus</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Content Analysis: An Introduction to Its Methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>SAGE Publications, Inc</publisher>
			<pubPlace>Thousand Oaks, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>4th edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The cmu sphinx-4 speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evandro</forename><surname>Gouv?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recent Development of Open-Source Speech Recognition Engine Julius. em Proceedings of the 2009 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinobu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Transcription Task for Crowdsourcing with Automatic Quality Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2011</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3041" to="3044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowdsourced Text Sequence Aggregation Based on Hybrid Reliability and Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1761" to="1764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Dataset of Crowdsourced Word Sequences: Collections and Answer Aggregation for Ground Truth Creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP, AnnoNLP &apos;19</title>
		<meeting>the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP, AnnoNLP &apos;19<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Letter-Based Speech Recognition with Gated ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using the Amazon Mechanical Turk for transcription of spoken language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5270" to="5273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automating Crowd-supervised Learning for Spoken Language Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2012</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2474" to="2477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ground Truth for Grammatical Error Correction Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="588" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02592</idno>
		<title level="m">GLEU Without Tuning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Advances in Speech Recognition: Mobile Environments, Call Centers and Clinics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Neustein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer US</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic Speech Recognition with Non-Expert Transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Novotney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Good</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>South Brisbane, QLD, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Noisy Text Sequences Aggregation as a Summarization Subtask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Pletenev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Crowd Science Workshop: Trust, Ethics, and Excellence in Crowdsourced Data Management at Scale</title>
		<meeting>the 2nd Crowd Science Workshop: Trust, Ethics, and Excellence in Crowdsourced Data Management at Scale<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Permutation-Based Model for Crowd Labeling: Optimal Estimation and Robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nihar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaraman</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4162" to="4184" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><forename type="middle">G</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;08</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;08<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-End ASR: from Supervised to Semi-Supervised Learning with Modern Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Self-Supervision in Audio and Speech at the 37th International Conference on Machine Learning</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">WER we are and WER we think we are</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Szyma?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolaj</forename><surname>Piotr?elasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Morzy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szymczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Marzena?y?a-Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Banaszczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Augustyniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mizgajski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carmiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3290" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gender and Dialect Bias in YouTube&apos;s Automatic Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachael</forename><surname>Tatman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
		<meeting>the First ACL Workshop on Ethics in Natural Language Processing<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">VLDB 2021 Crowd Science Challenge on Aggregating Crowdsourced Audio Transcriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Pavlichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Kuznetsov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Crowd Science Workshop: Trust, Ethics, and Excellence in Crowdsourced Data Management at Scale</title>
		<meeting>the 2nd Crowd Science Workshop: Trust, Ethics, and Excellence in Crowdsourced Data Management at Scale<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">re-CAPTCHA: Human-Based Character Recognition via Web Security Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Luis Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcmillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="issue">5895</biblScope>
			<biblScope unit="page" from="1465" to="1468" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">CHiME-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09249</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ting-Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">R</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural Network Language Modeling with Letter-Based Features and Importance Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6109" to="6113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Valtchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<title level="m">The HTK Book Version 3.4</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<meeting>NIPS Workshop on Deep Learning and Unsupervised Feature Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10504</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>eess.AS</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral Methods Meet EM: A Provably Optimal Algorithm for Crowdsourcing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Even when the grammar rules and speaker&apos;s intonation suggest the punctuation mark, omit it (except the apostrophe). We need to obtain texts without punctuation marks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">constructions like I&apos;m, don&apos;t, shouldn&apos;t). Importantly: listen carefully to what the speaker says. If they use a full form (e.g., do not), you must also write the full form</title>
	</analytic>
	<monogr>
		<title level="m">Use the apostrophe according to the grammar rules: ? To show omissions of letters</title>
		<imprint/>
	</monogr>
	<note>If they use a shortform (e.g., I&apos;m), you should also write the short form. ? To show possession (e.g., Ivan&apos;s pen</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">She got two A&apos;s in the biology exam) the task: 1. Play the audio and carefully listen to the speech. Important: You must listen to the complete audio record to submit your response. Technical Difficulties: If the audio does not play, or there is no voice in the recording</title>
		<imprint/>
	</monogr>
	<note>or any other technical difficulty arises, answer &quot;No&quot; to the &quot;Does the audio play properly?&quot; question and proceed to the next task (skip all steps below. Hint: It is best to use headphones to perform this task -you will hear the speech better</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Transcribe the audio and type the transcription into the text field Important: You must follow the transcription rules outlined above. Hint: You can play the audio multiple times and pause it at any point</title>
		<imprint/>
	</monogr>
	<note>Please do your best to produce a high-quality transcription</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Carefully check your transcription for typos and mistakes Important: Speech in the recordings should be grammatically correct</title>
		<imprint/>
	</monogr>
	<note>But if you are sure that the speaker makes a mistake, do not fix it. Your goal is to provide accurate transcriptions. Hint: Listen to the audio in full one more time to double-check your transcription</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
