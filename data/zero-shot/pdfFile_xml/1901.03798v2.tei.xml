<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Machines with Self-supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019">2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhan</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
						</author>
						<title level="a" type="main">3D Human Pose Machines with Self-supervised Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2019">2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Driven by recent computer vision and robotic applications, recovering 3D human poses has become increasingly important and attracted growing interests. In fact, completing this task is quite challenging due to the diverse appearances, viewpoints, occlusions and inherently geometric ambiguities inside monocular images. Most of the existing methods focus on designing some elaborate priors /constraints to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions. However, due to the insufficient 3D pose data for training and the domain gap between 2D space and 3D space, these methods have limited scalabilities for all practical scenarios (e.g., outdoor scene). Attempt to address this issue, this paper proposes a simple yet effective self-supervised correction mechanism to learn all intrinsic structures of human poses from abundant images. Specifically, the proposed mechanism involves two dual learning tasks, i.e., the 2D-to-3D pose transformation and 3D-to-2D pose projection, to serve as a bridge between 3D and 2D human poses in a type of "free" self-supervision for accurate 3D human pose estimation. The 2D-to-3D pose implies to sequentially regress intermediate 3D poses by transforming the pose representation from the 2D domain to the 3D domain under the sequence-dependent temporal context, while the 3D-to-2D pose projection contributes to refining the intermediate 3D poses by maintaining geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. Therefore, these two dual learning tasks enable our model to adaptively learn from 3D human pose data and external large-scale 2D human pose data. We further apply our self-supervised correction mechanism to develop a 3D human pose machine, which jointly integrates the 2D spatial relationship, temporal smoothness of predictions and 3D geometric knowledge. Extensive evaluations on the Human3.6M and HumanEva-I benchmarks demonstrate the superior performance and efficiency of our framework over all the compared competing methods. Please find the code of this project at: http://www.sysu-hcp.net/3d pose ssl/ Index Terms-human pose estimation, convolutional neural networks, spatio-temporal modeling, self-supervised learning, geometric deep learning. ! K. Wang is with the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ECENTLY, estimating 3D full-body human poses from monocular RGB imagery has attracted substantial academic interests for its vast potential on human-centric applications, including human-computer interactions <ref type="bibr" target="#b0">[1]</ref>, surveillance <ref type="bibr" target="#b1">[2]</ref>, and virtual reality <ref type="bibr" target="#b2">[3]</ref>. In fact, estimating human pose from images is quite challenging with respect to large variances in human appearances, arbitrary viewpoints, invisibilities of body parts. Besides, the 3D articulated pose recovery from monocular imagery is considerably more difficult since 3D poses are inherently ambiguous from a geometric perspective <ref type="bibr" target="#b3">[4]</ref>, as shown in <ref type="figure">Fig. 1</ref>.</p><p>Recently, notable successes have been achieved for 2D pose estimation based on 2D part models coupled with 2D deformation priors <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and the deep learning techniques <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Driven by these successes, some 3D pose estimation works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> attempt to leverage the state-of-the-art 2D pose network architectures (e.g., Convolutional Pose Machines (CPM) <ref type="bibr" target="#b9">[10]</ref> and Stacked Hourglass Networks <ref type="bibr" target="#b17">[18]</ref>) by combing the image-based 2D part detectors, 3D geometric pose priors and temporal models. These attempts mainly follow three types of pipelines. The first type <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> focuses on <ref type="figure">Fig. 1</ref>: Some visual results of our approach on the Hu-man3.6M benchmark <ref type="bibr" target="#b4">[5]</ref>. (a) illustrates the intermediate 3D poses estimated by the 2D-to-3D pose transformer module, (b) denotes the final 3D poses refined by the 3D-to-2D pose projector module, and (c) denotes the ground-truth. The estimated 3D joints are reprojected into the images and shown by themselves from the side view (next to the images). As shown, the predicted 3D poses in (b) have been significantly corrected, compared with (a). Best viewed in color. Note that, red and green indicate left and right, respectively. directly recovering 3D human poses from 2D input images by utilizing the state-of-the-art 2D pose network architecture to extract 2D pose-aware features with separate techniques and prior knowledge. In this way, these methods can em-arXiv:1901.03798v2 [cs.CV] 15 Jan 2019 ploy sufficient 2D pose annotations to improve the shared feature representation of the 3D pose and 2D pose estimation tasks. The second type <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> concentrates on learning a 2D-to-3D pose mapping function. Specifically, the methodsbelonging to this kind first extract 2D poses from 2D input images and further perform 3D pose reconstruction/regression based on these 2D pose predictions. The third type <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> aims at integrating the Skinned Multi-Person Linear (SMPL) model <ref type="bibr" target="#b26">[27]</ref> within a deep network to reconstruct 3D human pose and shape in a full 3D mesh of human bodies. Although having achieved a promising performance, all of these kinds suffer from the heavy computational cost by using the time-consuming network architecture (e.g., ResNet-50 <ref type="bibr" target="#b27">[28]</ref>) and limited scalability for all scenarios due to the insufficient 3D pose data.</p><p>To address the above-mentioned issues and utilize the sufficient 2D pose data for training, we propose an effective yet efficient 3D human pose estimation framework, which implicitly learns to integrate the 2D spatial relationship, temporal coherency and 3D geometry knowledge by utilizing the advantages afforded by Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b9">[10]</ref> (i.e., the ability to learn feature representations for both image and spatial context directly from data), recurrent neural networks (RNNs) <ref type="bibr" target="#b28">[29]</ref> (i.e., the ability to model the temporal dependency and prediction smoothness) and the self-supervised correction (i.e., the ability to implicitly retain 3D geometric consistency between the 2D projections of 3D poses and the predicted 2D poses). Concretely, our model employs a sequential training to capture long-range temporal coherency among multiple human body parts, and it is further enhanced via a novel selfsupervised correction mechanism, which involves two dual learning tasks, i.e., 2D-to-3D pose transformation and 3D-to-2D pose projection, to generate geometrically consistent 3D pose predictions under a self-supervised correction mechanism, i.e., forcing the 2D projections of the generated 3D poses to be identical to the estimated 2D poses.</p><p>As illustrated in <ref type="figure">Fig. 1</ref>, our model enables the gradual refinement of the 3D pose prediction for each frame according to the coherency of sequentially predicted 2D poses and 3D poses, contributing to seamlessly learning the pose-dependent constraints among multiple body parts and sequence-dependent context from the previous frames. Specifically, taking each frame as input, our model first extracts the 2D pose representations and predicts the 2D poses. Then, the 2D-to-3D pose transformer module is injected to transform the learned pose representations from the 2D domain to the 3D domain, and it further regresses the intermediate 3D poses via two stacked long shortterm memory (LSTM) layers by combining the following two lines of information, i.e., the transformed 2D pose representations and the learned states from past frames. Intuitively, the 2D pose representations are conditioned on the monocular image, which captures the spatial appearance and context information. Then, temporal contextual dependency is captured by the hidden states of LSTM units, which effectively improves the robustness of the 3D pose estimations over time. Finally, the 3D joint prediction implicitly encodes the 3D geometric structural information by the 3D-to-2D pose projector module under the introduced selfsupervised correction mechanism. In specific, considering that the 2D projections of 3D poses and the predicted 2D poses should be identical, the minimization of their dissimilarities is regarded as a learning objective for the 3Dto-2D pose projector module to bidirectionally correct (or refine) the intermediate 3D pose predictions. Through this self-supervised correction mechanism, our model is capable of effectively achieving geometrically coherent 3D human pose predictions without requesting additional 3D pose annotations. Therefore, our introduced correction mechanism is self-supervised, and can enhance our model by adding the external large-scale 2D human pose data into the training process to cost-effectively increase the 3D pose estimation performance.</p><p>The main contributions of this work are three-fold. i) We present a novel model that learns to integrate rich spatial and temporal long-range dependencies as well as 3D geometric constraints, rather than relying on specific manually defined body smoothness or kinematic constraints; ii) Developing a simple yet effective self-supervised correction mechanism to incorporate 3D pose geometric structural information is innovative in literature, and may also inspire other 3D vision tasks; iii) The proposed self-supervised correction mechanism enables our model to significantly improve 3D human pose estimation via sufficient 2D human pose data. Extensive evaluations on the public challenging Human3.6M <ref type="bibr" target="#b4">[5]</ref> and HumanEva-I <ref type="bibr" target="#b29">[30]</ref> benchmarks demonstrate the superiority of our framework over all the compared competing methods.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews the existing 3D human pose estimation approaches that motivate this work. Section 3 presents the details of the proposed model, with a thorough analysis of every component. Section 4 presents the experimental results on two public benchmarks with comprehensive evaluation protocols, as well as comparisons with competing alternatives. Finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Considerable research has addressed the challenge of 3D human pose estimation. Early research on 3D monocular pose estimation from videos involved frame-to-frame pose tracking and dynamic models that rely on Markov dependencies among previous frames, e.g., <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The main drawbacks of these approaches are the requirement of the initialization pose and the inability to recover from tracking failure. To overcome these drawbacks, more recent approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b32">[33]</ref> focus on detecting candidate poses in each individual frame, and a post-processing step attempts to establish temporally consistent poses. Yasin et al. <ref type="bibr" target="#b21">[22]</ref> proposed a dualsource approach for 3D pose estimation from a single image. They combined the 3D pose data from a motion capture system with an image source annotated with 2D poses. They transformed the estimation into a 3D pose retrieval problem. One major limitation of this approach is its time efficiency. Processing an image requires more than 20 seconds. Sanzari et al. <ref type="bibr" target="#b33">[34]</ref> proposed a hierarchical Bayesian non-parametric model, which relies on a representation of the idiosyncratic motion of human skeleton joint groups, and the consistency of the connected group poses is considered during the pose reconstruction.</p><p>Deep learning has recently demonstrated its capabilities in many computer vision tasks, such as 3D human pose estimation. Li and Chan <ref type="bibr" target="#b34">[35]</ref> first used CNNs to regress the 3D human pose from monocular images and proposed two training strategies to optimize the network. Li et al. <ref type="bibr" target="#b35">[36]</ref> proposed integrating the structure learning into a deep learning framework, which consists of a convolutional neural network to extract image features and two following subnetworks to transform the image features and poses into a joint embedding. Tekin et al. <ref type="bibr" target="#b14">[15]</ref> proposed exploiting motion information from consecutive frames and applied a deep learning network to regress the 3D pose. Zhou et al. <ref type="bibr" target="#b13">[14]</ref> proposed a 3D pose estimation framework from videos that consists of a novel synthesis among a deep-learningbased 2D part detector, a sparsity-driven 3D reconstruction approach and a 3D temporal smoothness prior. Zhou et al. <ref type="bibr" target="#b3">[4]</ref> proposed directly embedding a kinematic object model into the deep learning network. Du et al. <ref type="bibr" target="#b36">[37]</ref> introduced additional built-in knowledge for reconstructing the 2D pose and formulated a new objective function to estimate the 3D pose from the detected 2D pose. More recently, Zhou et al. <ref type="bibr" target="#b18">[19]</ref> presented a coarse-to-fine prediction scheme to cast 3D human pose estimation as a 3D keypoint localization problem in a voxel space in an end-to-end manner. Moreno-Noguer et al. <ref type="bibr" target="#b37">[38]</ref> formulated the 3D human pose estimation problem as a regression between matrices encoding 2D and 3D joint distances. Chen et al. <ref type="bibr" target="#b15">[16]</ref> proposed a simple approach to 3D human pose estimation by performing 2D pose estimation followed by 3D exemplar matching. Tome et al. <ref type="bibr" target="#b19">[20]</ref> proposed a multi-task framework to jointly integrate 2D joint estimation and 3D pose reconstruction to improve both tasks. To leverage the well-annotated large-scale 2D pose datasets, Zhou et al. <ref type="bibr" target="#b22">[23]</ref> proposed a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep two-stage cascaded structure network. However, these methods oversimplify the 3D geometric knowledge. In contrast to all these aforementioned methods, our model can leverage a lightweight network architecture to implicitly learn to integrate the 2D spatial relationship, temporal coherency and 3D geometry knowledge in a fully differential manner.</p><p>Instead of directly computing 2D and 3D joint locations, several works concentrate on producing a 3D mesh body representation by using a CNN to predict Skinned Multi-Person Linear model <ref type="bibr" target="#b26">[27]</ref>. For instance, Omran et al. <ref type="bibr" target="#b24">[25]</ref> proposed to integrate a statistical body model within a CNN, leveraging reliable bottom-up semantic body part segmentation and robust top-down body model constraints. Kanazawa et al. <ref type="bibr" target="#b25">[26]</ref> presented an end-to-end adversarial learning framework for recovering a full 3D mesh model of a human body by parameterizing the mesh in terms of 3D joint angles and a low dimensional linear shape space. Furthermore, this method employs the weak-perspective camera model to project the 3D joints onto the annotated 2D joints via an iterative error feedback loop <ref type="bibr" target="#b38">[39]</ref>. Similar to our proposed method, these approaches also regard the in-the-wild images with 2D ground-truth as the supervision to improve the model performance. The main difference is that our self-supervised learning method is more flexible and robust without relying on the assumption of the weakperspective camera model.</p><p>Our approach is close to <ref type="bibr" target="#b19">[20]</ref>, which also used the projection from the 3D space to the 2D space to improve the 3D pose estimation performance. However, there are two main differences between <ref type="bibr" target="#b19">[20]</ref> and our model: i) The definition of the 3D-to-2D projection function and the optimization strategy. Rather than explicitly defining a concrete model, our 3D-to-2D projection is implicitly learned in a completely data-driven manner. However, the projection of 3D poses in <ref type="bibr" target="#b19">[20]</ref> is explicitly modeled by using a weak perspective model, which consists of the orthographic projection matrix, a known external camera calibration matrix and an unknown rotation matrix. As claimed in <ref type="bibr" target="#b19">[20]</ref>, this explicit model is prone to sticking in local minima during the training. Thus, the authors have to quantize over the space of possible rotations. Through this approximation, their model performance may suffer from the fixed choices of rotations;</p><p>ii) The way of utilizing the projected 2D pose. In contrast to <ref type="bibr" target="#b19">[20]</ref> which learns to weightily fuse the projected 2D and the estimated 2D poses for further regressing the final 3D pose, our model exploits the 3D geometric consistency between the projected 2D and the estimated 2D poses to bidirectionally refine the intermediate 3D pose predictions.</p><p>Self-supervised Learning. Aiming at training the feature representation without relying on manual data annotation, self-supervised learning (SSL) has first been introduced in <ref type="bibr" target="#b39">[40]</ref> for vowel class recognition, and further extended for object extraction in <ref type="bibr" target="#b40">[41]</ref>. Recently, plenty of SSL methods (e.g., <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>) have been proposed. For instance, <ref type="bibr" target="#b41">[42]</ref> investigated multiple self-supervised methods to encourage the network to factorize the information in its representation. In contrast to these methods that focus on learning an optimal visual representation, our work considers the self-supervision as an optimization guidance for 3D pose estimation.</p><p>Note that a preliminary version of this work was published in <ref type="bibr" target="#b20">[21]</ref>, which uses multiple stages to gradually refine the predicted 3D poses. The network parameters in the multiple stages are recurrently trained in a fully end-to-end manner. However, the multi-stage mechanism results in a heavy computational cost, and the stage-bystage improvement is less significant as the number of stages increases. In this paper, we inherit its idea of integrating the 2D spatial relationship, temporal coherency as well as 3D geometry knowledge, and we further impose a novel selfsupervised correction mechanism to further enhance our model by bridging the domain gap between the 3D and 2D human poses. Specifically, we develop a 3D-to-2D pose projector module to replace the multi-stage refinement to correct the intermediate 3D pose predictions by retaining the 3D geometric consistency between their 2D projections and the predicted 2D poses. Therefore, the imposed correction mechanism enables us to leverage the external large-scale 2D human pose data to boost 3D human pose estimation. Moreover, more comparisons with competing approaches and more detailed analyses of the proposed modules are included to further verify our statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D HUMAN POSE MACHINE</head><p>We propose a 3D human pose machine to resolve 3D pose sequence generation for monocular frames, and we intro- Our model predicts the 3D human poses for the given monocular image frames, and it progressively refines its predictions with the proposed self-supervised correction. Specifically, the estimated 2D pose p 2d t with the corresponding pose representation f 2d t for each frame of the input sequence is first obtained and further passed into two neural network modules: i) a 2D-to-3D pose transformer module for transforming the pose representations from the 2D domain to the 3D domain to intermediately predict the human joints p 3d t in the 3D coordinates, and ii) a 3D-to-2D pose projector module to obtain the projected 2D posep 2d t after regressing p 3d t intop 3d t . Through minimizing the difference between p 2d t andp 2d t , our model is capable of bidirectionally refining the regressed 3D posesp 3d t via the proposed self-supervised correction mechanism. Note that the parameters of the 2D-to-3D pose transformer module for all frames are shared to preserve the temporal motion coherence. 3K and 2K denotes the dimension of the vector for representing the 3D and 2D human pose formed by K skeleton joints, respectively. duce a concise self-supervised correction mechanism to enhance our model by retaining the 3D geometric consistency. After extracting the 2D pose representation and estimating the 2D poses for each frame via a common 2D pose subnetwork, our model employs two consecutive modules. The first module is the 2D-to-3D pose transformer module for transforming the 2D pose-aware features from the 2D domain to the 3D domain. This module is designed to estimate intermediate 3D poses for each frame by incorporating temporal dependency in the image sequence. The second module is the 3D-to-2D pose projector module for bidirectionally refining the intermediate 3D pose prediction via our introduced selfsupervised correction mechanism. These two modules are combined in a unified framework to be optimized in a fully end-to-end manner.</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, our model performs the sequential refinement with self-supervised correction to generate the 3D pose sequence. Specifically, the t-th frame I t is passed into the 2D pose sub-network ? R , the 2D-to-3D pose transformer module ? T , and the 3D-to-2D projector module {? C , ? P } to predict the final 3D poses. The 2D pose subnetwork is stacked by convolutional and fully connected layers, and the 2D-to-3D pose transformer module contains two LSTM layers to capture the temporal dependency over frames. Specifically, given the input image sequence with N frames, the 2D pose sub-network ? R is first employed to extract the 2D pose-aware features f 2d t and predict the 2D pose p 2d t for the t-th frame of the input sequence. Then, the extracted 2D pose-aware features f 2d t are further fed into the 2D-to-3D pose transformer module ? T to obtain the intermediate 3D pose p 3d t , where ? T is composed of the hidden states H t?1 learned from the past frames. Finally, the predicted 2D poses p 2d t and intermediate 3D pose p 3d t are fed into the 3D-to-2D projector module with two functions, i.e., ? C and ? P , to obtain the final 3D posesp * 3d t . Considering that most existing 2D human pose data are still images without temporal orders, we additionally introduce a simple yet effective regression function ? C to transform the intermediate 3D pose vector p 3d t into a changeable prediction p 3d  as an optimization objective to obtain the optimalp * 3d t for the t-th frame.</p><formula xml:id="formula_0">p 3d t p 3d (gt) t p 2d (gt) t p 3d t p 2d t p *3 d t p 3d tp 3d t</formula><p>In the following, we will introduce more details of our model and provide comprehensive clarifications to make the work easier to understand. The corresponding algorithm for jointly training these modules will also be discussed at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">2D Pose Sub-network</head><p>The objective of the 2D pose sub-network is to encode each frame in a given monocular sequence with a compact representation of the pose information, e.g., the body shape of the human. The shallow convolution layers often extract the common low-level information, which is a very basic representation of the human image. We build our 2D pose sub-network by borrowing the architecture of the convolutional pose machines <ref type="bibr" target="#b9">[10]</ref>. Please see <ref type="table" target="#tab_1">Table 1</ref> for more details. Note that other state-of-the-art architectures for 2D pose estimation can be also utilized. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, the 2D pose sub-network takes the 368?368 image as input, and it outputs the 2D pose-aware feature maps with a size of 128 ? 46 ? 46 and the predicted 2D pose vectors with 2K entries being the argmax positions of these feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">2D-to-3D Pose Transformer Module</head><p>Based on the features extracted by the 2D pose sub-network, the 3D pose transformer module is employed to adapt the 2D pose-aware features in an adapted feature space for the later 3D pose prediction. As depicted in <ref type="figure" target="#fig_0">Fig. 2 (a)</ref>, two convolutional layers and one fully connected layer are leveraged. Each convolutional layer contains 128 different kernels with a size of 5 ? 5 and a stride of 2, and a max pooling layer with a 2 ? 2 kernel size and a stride of 2 is appended on the convolutional layers. Finally, the convolution features are fed to a fully connected layer with 1024 units to produce the adapted feature vector. In this way, the 2D pose-aware features are transformed into the 1024-dimensional adapted feature vector.</p><p>Given the adapted features for all frames, we employ LSTM to sequentially predict the 3D pose sequence by incorporating rich temporal motion patterns among frames as <ref type="bibr" target="#b20">[21]</ref>. Note that, LSTM <ref type="bibr" target="#b28">[29]</ref> has been proven to achieve better performance in exploiting temporal correlations than a vanilla recurrent neural network in many tasks, e.g., speech recognition <ref type="bibr" target="#b43">[44]</ref> and video description <ref type="bibr" target="#b44">[45]</ref>. In our model, we use the LSTM layers to capture the temporal dependency in the monocular sequence for refining the 3D pose prediction for each frame. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> (b), our model employs two LSTM layers with 1024 hidden cells and an output layer that predicts the locations of K joint points of the human. In particular, the hidden states learned by the LSTM layers are capable of implicitly encoding the temporal dependency across different frames of the input sequence. As formulated in Eq. (1), incorporating the previous hidden states imparts our model with the ability to sequentially refine the pose predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">3D-to-2D Projector Module</head><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref> (a), this module consists of six fully connected (FC) layers containing ReLU and batch normalization operations. As one can see from left to right in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>, the first two FC layers (denoted in blue) define the regression function ? C in which the intermediate 3D pose predictions are regressed into the pose predictionp 3d t , and the remaining four FC layers (denoted in yellow) with 1024 units represent the projection function ? P that project? p 3d t into the image plane to obtain the projected 2D pos? p 2d t . Moreover, an identical mapping as ResNet <ref type="bibr" target="#b27">[28]</ref> is used inside ? P to make the information pass through quickly to avoid overfitting. Therefore, our 3D-to-2D projector module is simple yet powerful for both regression and projection tasks. Considering the self-corrected 3D pose may need to be discarded sometimes, we regard the regression function ? C as a copy to be corrected for the intermediate 3D poses.</p><p>In the training phase, we first initialize the module parameters {? C , ? P } for ? C and ? P via the supervision of the 3D and 2D ground-truth poses from 3D human pose data as illustrated in <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>, respectively. The optimization function is:</p><formula xml:id="formula_1">min {? C ,? P } N t=1 p 3d t ? p 3d(gt) t 2 2 + ?P (p 3d t ; ?P ) ? p 2d(gt) t 2 2 ,<label>(2)</label></formula><p>wherep 3d t is the regressed 3D pose via ? C in Eq. (1), and its 2D projection isp 2d</p><formula xml:id="formula_2">t = ? P (p 3d t ; ? P ). Eq. (2) forces ? C to regressp 3d t from intermediate 3D poses p 3d t to the 3D pose ground-truth p 3d(gt) t</formula><p>, and it further forces the output of ? P , i.e., the projected 2D poses, to be similar to the 2D . In this way, the 3D-to-2D pose projector module can learn the geometric consistency to correct intermediate 3D pose predictions. After initialization, we substitute the predicted 2D poses and 3D poses for the 2D and 3D ground-truth to optimize ? C and ? P in a self-supervised fashion. Considering that the predictions for certain body joints (e.g., hand lef t , hand right , foot lef t and foot right defined in the Human3.6M dataset) may not be accurate and reliable due to the challenging nature of the rich flexibilities and occlusions of body joints, we employ the dropout trick <ref type="bibr" target="#b45">[46]</ref> in the intermediate 3D pose estimations p 3d t and the predicted 2D pose p 2d t , i.e., the position for each body joint has a probability ? to be zero. This trick enables the regression function ? C and the project function ? P to be insensitive to the outliers inside p 3d t and p 2d t . As reported in <ref type="bibr" target="#b45">[46]</ref>, the dropout trick can significantly contribute to alleviating the overfitting of the fully connected layers inside ? C and ? P . Meanwhile, we also employ the 3D pose ground-truth to encourage the regression function ? C to learn to regress the 3D pose estimationp 3d t . In our experiments, ? is empirically set to be 0.3.</p><p>The inference phase of this module is also selfsupervised. Specifically, given the predicted 2D pose p 2d t , we can obtain the initialp 3d t and the corresponding projected 2D posep 2d t via forward propagation, as indicated in <ref type="figure" target="#fig_1">Fig. 3  (b)</ref>. According to the 3D geometric consistency that the projected 2D posep 2d t should be identical to the predicted 2D pose p 2d t , we propose minimizing the dissimilarity between p 2d t and p 2d t by optimizing its specific ? t P and ? t C as follows:</p><formula xml:id="formula_3">{? * t P , ? * t C } = arg min {p 3d t ,? t P } p 2d t ?p 2d t 2 2 = arg min {p 3d t ,? t P } p 2d t ? ? P (p 3d t ; ? t P ) 2 2 = arg min {? t P ,? t C } p 2d t ? ? P (? C (p 3d t ; ? t C ); ? t P ) 2 2 ,<label>(3)</label></formula><p>where the parameters {? t P , ? t C } are initialized from the well-optimized {? P , ? C } from the training phase. Note that, ? * t C and ? * t P are disposable and only valid for I t . Since p 2d t and p 3d t are fixed, we first perform forward propagation to obtain the initial prediction, and further employ the standard back-propagation algorithm <ref type="bibr" target="#b46">[47]</ref> to obtain ? * t C and ? * t P via Eq. (3). Thus, the output 3D pose regressionp 3d t is bidirectionally refined to be the final 3D pose prediction during the the optimizing of ? t C and ? t P according to the proposed self-supervised correction mechanism. At the end, the final 3D posep * 3d t is obtained according to Eq. (3) as follows:p * 3d t</p><formula xml:id="formula_4">= ? C (p 3d t ; ? * t C ).<label>(4)</label></formula><p>The hyperparameters (i.e., the iteration number and learning rate) for ? * t P and ? * t C play a crucial role in effectively and efficiently refining the 3D pose estimation. In fact, a large iteration number and small learning rate can ensure that the model is capable of converging to a satisfactor? p * 3d t . However, this setting results in a heavy computational cost. Therefore, a small iteration number with large learning rate is preferred to achieve a trade-off between efficiency and accuracy. Moreover, although we can achieve high accuracy on 2D pose estimation, the predicted 2D poses may contain errors due to the heavy occlusion of human body parts. Treating these inaccurate 2D poses as optimization objectives to bidirectionally refine the 3D pose prediction is prone to a decrease in performance. To address this issue, we utilize a heuristic strategy to determine the optimal hyperparameters used for each frame in our implementation. Specifically, we can check the convergence of some robust skeleton joints (i.e., Pelvis, Shoulder lef t , Shoulder right , Hip lef t and Hip right defined in the Human3.6M dataset) in each iteration. In practice, we find that the predictions of these reliable joints are generally less flexible and have lower probabilities of being occluded than other joints. If the predicted 2D pose contains small errors, then these joints of the refined 3D posep * 3d t will have large and inconsistent changes within the self-supervised correction. Hence, we terminate the further refinement when the positions of these joints are converged (i.e., average changes &lt; mm), and discard the self-supervised correction when the average change in these joints are not within an empirical threshold ? mm.</p><p>In our experiments, we empirically set {?, } = {20, 5} and employ two back-propagation operations to update ? P and ? C before outputting the final 3D pose predictionp * 3d t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>In the training phase, the optimization of our proposed model occurs in a fully end-to-end manner, and we have defined several types of loss functions to fine-tune the network parameters: ? R , ? T and {? C , ? P }, respectively. For the 2D pose sub-network, we build an extra FC layer upon the convolutional layers of the 2D pose sub-network to generate 2K joint location coordinates. We leverage the Euclidean distances between the predictions for all K body joints and the corresponding ground-truth to train ? R . Formally, we have:</p><formula xml:id="formula_5">min ? R N t=1 p 2d t (gt) ? ? R (I t ; ? R ) 2 2 ,<label>(5)</label></formula><p>where p 2d t (gt) denotes the 2D pose ground-truth for the t-th frame I t .</p><p>For the 2D-to-3D pose transformer module, our model enforces the 3D pose sequence prediction loss for all frames, which is also defined as follows: </p><formula xml:id="formula_6">min ? T N t=1 p 3d t ? p 3d(gt) t 2 2 = N t=1 ? T (f 2d t ; ? T , H t?1 ) ? p 3d(gt) t 2 2 ,<label>(6)</label></formula><formula xml:id="formula_7">, ? T , ? C , ? P } on {I 3d t } N t=1 and {I 2d i } M i=1 via Eq. (7). 5: return {? R , ? T , ? C , ? P }.</formula><p>where p 3d(gt) t is the 3D pose ground-truth for the t-th frame I t . According to Eq. (6), we integrally fine-tune the parameters of the 2D-to-3D pose transformer module and the convolutional layers of the 2D pose sub-network in an end-to-end optimization manner. Note that, to obtain sufficient samples to train the 3D pose transformer module, we propose decomposing one long monocular image sequence into several small equal clips with N frames. In our experiments, we jointly feed our model with 2D and 3D pose data after all the network parameters are well initialized. For the 2D human pose data, this module regards their 3D pose sequence prediction loss as zero.</p><p>After initializing the 3D-to-2D projector module via Eq. (2), we fine-tune the whole network to jointly optimize the network parameters {? R , ? T , ? C , ? P } in a fully end-toend manner as follows:</p><formula xml:id="formula_8">min {? R ,? T ,? C ,? P } p 2d t ?p 2d t 2 2 .<label>(7)</label></formula><p>Since our model consists of two cascaded modules, the training phase can be divided into the following steps: (i) Initialize the 2D pose representation via pre-training. To obtain a satisfactory feature representation, the 2D pose sub-network is first pre-trained with the MPII Human Pose dataset <ref type="bibr" target="#b47">[48]</ref>, which includes a larger variety of 2D pose data. (ii) Initialize the 2D-to-3D pose transformer module. We fix the parameters of the 2D pose sub-network and optimize the network parameter ? T . (iii) Initialize the 3Dto-2D pose projector module. We fix the above optimized parameters and optimize the network parameter {? C , ? P }. (iv) Fine-tune the whole model jointly to further update the network parameters {? R , ? T , ? C , ? P } with the 2D pose and 3D pose training data. For each of the abovementioned steps, the ADAM <ref type="bibr" target="#b48">[49]</ref> strategy is employed for parameter optimization. The entire algorithm can then be summarized as Algorithm 1. Obviously, this algorithm is in a good agreement with the pipeline of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Inference</head><p>In the testing phase, every frame of the input image sequence is sequentially processed via Eq. (1). Note that each frame I t has its own {? t C , ? t P } in the 3D-to-2D projector module. {? t C , ? t P } are initialized from the well trained {? C , ? P }, and they will be updated by minimizing the difference between the predicted 2D poses p 2d t and projected 2D poses ? P (p 3d t ; ? P ) via Eq. (3). During the inference, the 3D pose estimation is bidirectionally refined until convergence is achieved according to the hyperparameter settings. Finally, we output the final 3D pose estimation via Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We perform extensive evaluations on two publicly available benchmarks: Human3.6M <ref type="bibr" target="#b4">[5]</ref> and HumanEva-I <ref type="bibr" target="#b29">[30]</ref>.</p><p>Human3.6M dataset. The Human3.6M dataset is a recently published dataset that provides 3.6 million 3D human pose images and corresponding annotations from a controlled laboratory environment. This dataset captures 11 professional actors performing in 15 scenarios under 4 different viewpoints. Moreover, there are three popular data partition protocols for this benchmark in the literature.</p><p>? Protocol #1: The data from five subjects (S1, S5, S6, S7, and S8) are for training, and the data from two subjects (S9 and S11) are for testing. To increase the number of training samples, the sequences from different viewpoints of the same subject are treated as distinct sequences. By downsampling the frame rate from 50 FPS to 2 FPS, 62,437 human pose images (104 images per sequence) are obtained for training and 21,911 images are obtained for testing (91 images per sequence). This is the widely used evaluation protocol on Human3.6M, and it was followed by several works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>. To be more general and make a fair comparison, our model is trained both on training samples from all 15 actions as previous works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref> and by exploiting individual actions as <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>? Protocol #2: This protocol only differs from Protocol #1 in that only the frontal view is considered for testing, i.e., testing is performed on every 5-th frame of the sequences from the frontal camera (cam-3) from trial 1 of each activity with ground-truth cropping. The training data contain all actions and viewpoints.</p><p>? Protocol #3: Six subjects (S1, S5, S6, S7, S8 and S9) are used for training, and every 64-th frame of S11's video clips is used for testing. The training data contain all actions and viewpoints.</p><p>HumanEva-I dataset. The HumanEva-I dataset contains video sequences of four subjects performing six common actions (e.g., walking, jogging, boxing, etc.), and it also provides 3D pose annotation for each frame in the video sequences. We train our model on the training sequences of subjects 1, 2 and 3 and test on the 'validation' sequence under the same protocol as <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Similar to Protocol #1 of the Human3.6M dataset, the data from different camera viewpoints are also regarded as different training samples. Note that we did not downsample the video sequences to obtain more samples for training.</p><p>Implementation Details: For We follow <ref type="bibr" target="#b43">[44]</ref> to build the LSTM memory cells, except that the peephole connections between cells and gates are omitted. Following <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b35">[36]</ref>, the input image is cropped around the human. To maintain the human width / height ratio, we crop a square image of the subject from the image according to the bounding   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Images h3m_vis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Ground-truth Lin et al. <ref type="bibr" target="#b20">[21]</ref> Tome et al. <ref type="bibr" target="#b19">[20]</ref> Pavlakos et al. <ref type="bibr" target="#b18">[19]</ref> Zhou et al. <ref type="bibr" target="#b13">[14]</ref> Zhou et al. <ref type="bibr" target="#b22">[23]</ref>   box provided by the dataset. Then, we resize the image region inside the bounding box to 368?368 before feeding it into our model. Moreover, we augment the training data by simply performing random scaling with factors in [0.9,1.1].</p><p>To transform the absolute locations of joint points into the [0,1] range, a max-min normalization strategy is applied. In the testing phase, the predicted 3D pose is transformed to the origin scale according to the maximum and minimum values of the pose from the training frames. During the training, the Xavier initialization method <ref type="bibr" target="#b60">[61]</ref> is used to initialize the weights of our model. A learning rate of 1e ?5 is employed for training. The training phase requires approximately 2 days on a single NVIDIA GeForce GTX 1080. Evaluation metric. Following <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b36">[37]</ref>, we employ the popular 3D pose error metric <ref type="bibr" target="#b54">[55]</ref>, which calculates the Euclidean errors on all joints and all frames up to translation. In the following section, we report the 3D pose error metric for all the experimental comparisons and analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with Existing Methods</head><p>Comparison on Human3.6M: We compare our model with the various competing methods on the Human3.6M <ref type="bibr" target="#b4">[5]</ref> and HumanEva-I <ref type="bibr" target="#b29">[30]</ref> datasets. For the fair comparison, we only consider the competing methods that do not need the intrinsic parameters of cameras for inference. This is reasonable for practical use under various scenarios. These methods are LinKDE <ref type="bibr" target="#b4">[5]</ref>, Tekin et al. <ref type="bibr" target="#b14">[15]</ref>, Li et al. <ref type="bibr" target="#b35">[36]</ref>, Zhou et al. <ref type="bibr" target="#b13">[14]</ref>, Zhou et al. <ref type="bibr" target="#b3">[4]</ref>, Du et al. <ref type="bibr" target="#b36">[37]</ref>, Sanzari et al. <ref type="bibr" target="#b33">[34]</ref>, Yasin et al. <ref type="bibr" target="#b16">[17]</ref>, and Bogo et al. <ref type="bibr" target="#b23">[24]</ref>. Moreover, we compare other competing methods, i.e., Moreno-Noguer et al. <ref type="bibr" target="#b37">[38]</ref>, Tome et al. <ref type="bibr" target="#b19">[20]</ref>, Chen et al. <ref type="bibr" target="#b15">[16]</ref>, Pavlakos et al. <ref type="bibr" target="#b18">[19]</ref>, Zhou et al. <ref type="bibr" target="#b22">[23]</ref>, Bruce et al. <ref type="bibr" target="#b50">[51]</ref>, Tekin et al. <ref type="bibr" target="#b49">[50]</ref> and our conference version, i.e., Lin et al. <ref type="bibr" target="#b20">[21]</ref>. For those compared methods (i.e., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b50">[51]</ref>) whose source codes are not publicly available, we directly obtain their results from their published papers. For the other methods (i.e., <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b49">[50]</ref>), we directly use their official implementations for comparisons.</p><p>The results under three different protocols are summarized in <ref type="table" target="#tab_3">Table 2</ref>. Clearly, our model outperforms all the competing methods (including those trained from the individual action as in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref> and on all 15 actions) under Protocol #1. Specifically, under the training from individual action setting, our model achieves superior performance on all the action types, and it outperforms the best competing methods with the joint mean error reduced by approximately 8% compared with Tekin et al. <ref type="bibr" target="#b49">[50]</ref> (63.79mm vs 69.73mm). Under the training from all 15 actions, our model still consistently performs better than the compared approaches and obtains better accuracy. Notably, our model In summary, our proposed model significantly outperforms all compared methods under all protocols with the mean error reduced by a clear margin. Note that some com-pared methods, e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, also employ deep learning techniques. In particular, Zhou et al. <ref type="bibr" target="#b3">[4]</ref>'s method used the residual network <ref type="bibr" target="#b27">[28]</ref>. Note that, the recently proposed methods all employ very deep network architectures (i.e., <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b18">[19]</ref> proposed using Stacked Hourglass <ref type="bibr" target="#b17">[18]</ref>, while <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b19">[20]</ref> employed CPM <ref type="bibr" target="#b9">[10]</ref>) to obtain satisfactory accuracies. This makes these methods time-consuming. In contrast to these methods, our model achieves a more lightweight architecture by replacing the multi-stage refinement in <ref type="bibr" target="#b20">[21]</ref> with the 3D-to-2D pose projector module. The superior performance achieved by our model demonstrates that our model is simple yet power-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Input Image Zhou et al. <ref type="bibr" target="#b22">[23]</ref> Zhou et al. <ref type="bibr">[</ref> ful in capturing complex contextual features within images, learning temporal dependencies within image sequences and preserving the geometric consistency within 3D pose predictions, which are critical for estimating 3D pose sequences. Some visual comparison results are presented in <ref type="figure" target="#fig_3">Fig. 4</ref>. Comparison on HumanEva-I: We compare our model against competing methods, including discriminative regressions <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, 2D pose detector-based methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, CNN-based approaches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b49">[50]</ref> and our preliminary version Lin <ref type="bibr" target="#b20">[21]</ref>, on the HumanEva-I dataset. For a fair comparison, our model also predicts the 3D pose consisting of 14 joints, i.e., left/right shoulder, elbow, wrist, left/right hip knee, ankle, head top and neck, as <ref type="bibr" target="#b21">[22]</ref>. <ref type="table" target="#tab_4">Table 3</ref> presents the performance comparisons of our model with all compared methods. Clearly, our model obtains substantially lower 3D pose errors than the compared methods on all the walking, jogging and boxing sequences. This result demonstrates the high generalization capability of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Running Time</head><p>To compare the efficiencies of our model and of the compared methods, we have conducted all the experiments on a desktop with an Intel 3.4GHz CPU and a single NVIDIA GeForce GTX 1080 GPU. In terms of time efficiency, compared with <ref type="bibr" target="#b13">[14]</ref> (880 ms per image), <ref type="bibr" target="#b18">[19]</ref> (170 ms per image), <ref type="bibr" target="#b22">[23]</ref> (311 ms per image), and <ref type="bibr" target="#b19">[20]</ref> (444 ms per image), our model model only requires 51 ms per image. The detailed time analysis is presented in <ref type="table" target="#tab_5">Table 4</ref>. Our model performs approximately 3 times faster than <ref type="bibr" target="#b18">[19]</ref>, which is the fastest of the compared methods. Moreover, although only performing slightly better than the best of the compared methods <ref type="bibr" target="#b22">[23]</ref> under Protocol #1, our model runs nearly 6 times faster, thanks to the 3D-to-2D pose projector module enabling a lightweight architecture. This result validates the efficiency of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To perform a detailed component analysis, we conducted the experiments on the Human3.6M benchmark under the Protocol #1 and our proposed model was trained on all actions for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Self-supervised Correction Mechanism</head><p>To demonstrate the superiority of the proposed selfsupervised correction (SSC) mechanism, we conduct the following experiment: disabling this module in both training and inference phase by directly regarding the intermediate 3D pose predictions as the final output (denoted as "ours w/o SSC train+test"). Moreover, we have also disabled the self-supervised correction mechanism during the inference phase and denote this version as ours w/o SSC test".</p><p>The results in <ref type="table" target="#tab_6">Table 5</ref>  These observations justify the contribution of the proposed SSC mechanism. This result demonstrates that the selfsupervised correction mechanism is highly beneficial for improving the performance both in the training and testing phase. Qualitative comparison results on the Human3.6M dataset are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. As depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>, the intermediate 3D pose predictions (i.e., ours w/o self-correction) contain several inaccurate joint locations compared with the ground truth because of the self-occlusion of body parts. However, the predicted 2D poses are of high accuracy because of the powerful CNN, which is trained from a large variety of 2D pose data. Because the estimated 2D poses are more reliable, our proposed 3D-to-2D pose projector module can utilize them as optimization objectives to bidirectionally refine the 3D pose predictions. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the joint predictions are effectively corrected by our model. This experiment clearly demonstrates that our 3D-to-2D pose projector, utilizing these predicted 2D poses as guidance, can contribute to enhancing 3D human pose estimation by correcting the 3D pose predictions without additional 3D pose annotations.</p><p>To further upper bound the performance of the proposed self-supervised correction mechanism in the testing phase, we directly employ the 2D pose ground-truth to correct the intermediate predicted 3D poses. We denote this version of our model as "ours w/ 2D GT". <ref type="table" target="#tab_6">Table 5</ref> demonstrates that ours w/ 2D GT achieves significantly better results than our model by reducing the mean joint error by approximately 6% (59.41mm vs 63.67mm). Moreover, we have also implemented another 2D pose prediction model from the hourglass network with two stages for self-supervised correction (denoted as "ours w/ HG features"). Our method w/ HG features performs slightly better than ours (62.85mm vs 63.67mm). This result evidences the effectiveness of our designed 3D-to-2D pose projector module in bidirectionally refining the intermediate 3D pose estimation.</p><p>We have further compared our method with a multi-task approach that simultaneously estimates 3D and 2D pose without re-projection (denoted as "ours w/o projection"). Specifically, ours w/o projection shares the same network architecture as our full model. The only difference is that ours w/o projection directly estimates both 2D and 3D poses during the training phase. Since it is quite difficult to directly train the network into convergence with huge 2D pose data and relatively small 3D pose data, we first optimize the network parameters by using the 2D pose data only from the MPII dataset, and further perform training with the 2D pose data and 3D pose data from the Human3.6M dataset.</p><p>As illustrated in <ref type="table" target="#tab_6">Table 5</ref>, our full model performs substantially better than ours w/o projection (63.67mm vs 86.80mm). The reason may be that directly regressing the 2D pose may mislead the main learning task of the model, i.e., the network concentrates on improving the overall performance of both 2D and 3D pose prediction. This demonstrates the effectiveness of the proposed 3D-to-2D pose projector module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Temporal Dependency</head><p>The model performance without temporal dependency in the training phase is also compared in <ref type="table" target="#tab_6">Table 5</ref> (denoted as "ours w/o temporal"). Note that the input of ours w/o temporal is only a single image rather than a sequence in the training and testing phases. Hence, the temporal information is ignored. Thus, the LSTM layer for the 3D pose errors is replaced by a fully connected layer with the same units as the LSTM layer. As illustrated in <ref type="table" target="#tab_6">Table 5</ref>, ours w/o temporal has suffered considerably higher 3D pose errors than ours (90.83mm vs 63.67mm). Moreover, we have also analyzed the contribution of the temporal dependency in the testing phase. To discard the temporal dependency during the inference phase, we regarded a single frame as input to evaluate the performance of our model, and we denote it as "ours w/ single frame". <ref type="table" target="#tab_6">Table 5</ref> demonstrates that this variant performs worse compared to ours by increasing the mean joint error by approximately 6% (67.70mm vs 63.67mm). This result validates the contribution of temporal information for 3D human pose estimation during the training and testing phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">External 2D Human Pose Data</head><p>To evaluate the performance without external 2D human pose data, we have only employed 3D pose data with 3D and 2D annotations from Human3.6M for training our model. We denote this version of our model as "ours w/o external". As shown in <ref type="table" target="#tab_6">Table 5</ref>, the ours w/o external version performs quite worse than our model (63.67mm vs 111.30mm). This reason may be that the training samples from Human3.6M, compared with the MPII dataset <ref type="bibr" target="#b47">[48]</ref>, are less challenging, therein having fewer variations for our model to learn a rich and powerful 2D pose presentation. Thanks to the proposed self-supervised correction mechanism, our model can effectively leverage a large variety of 2D human pose data to improve the performance of the 3D human pose estimation.</p><p>Moreover, in terms of estimating 3D human pose in the wild, our model advances the existing method <ref type="bibr" target="#b22">[23]</ref> in leveraging more abundant 3D geometric knowledge for mining in-the-wild 2D human pose data. Instead of oversimplifying the 3D geometric constraint as the relative bone length in <ref type="bibr" target="#b22">[23]</ref>, our model introduces a self-supervised correction mechanism to retain the 3D geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. Therefore, our model can bidirectionally refine the 3D pose predictions in a self-supervised manner. <ref type="figure">Fig. 6</ref> presents qualitative comparisons for images taken from the KTH Football II <ref type="bibr" target="#b59">[60]</ref> and MPII dataset <ref type="bibr" target="#b47">[48]</ref>, respectively. As shown in <ref type="figure">Fig. 6</ref>, our model achieves 3D pose predictions of superior accuracy compared to the competing method <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper presented a 3D human pose machine that can learn to integrate rich spatio-temporal long-range dependencies and 3D geometry knowledge in an implicit and comprehensive manner. We further enhanced our model by developing a novel self-supervised correction mechanism, which involves two dual learning tasks, i.e., 2D-to-3D pose transformation and 3D-to-2D pose projection, under a selfsupervised correction mechanism. This mechanism retains the geometric consistency between the 2D projections of 3D poses and the estimated 2D poses, and it enables our model to utilize the estimated 2D human pose to bidirectionally refine the intermediate 3D pose estimation. Therefore, our proposed self-supervised correction mechanism can bridge the domain gap between 3D and 2D human poses to leverage the external 2D human pose data without requiring additional 3D annotations. Extensive evaluations on two public 3D human pose datasets validate the effectiveness and superiority of our proposed model. In future work, focusing on sequence-based human centric analyses (e.g., human action and activity recognition), we will extend our proposed self-supervised correction mechanism for temporal relationship modeling, and design new self-supervision objectives to incorporating abundant 3D geometric knowledge for training models in a cost-effective manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>An overview of the proposed 3D human pose machine framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Detailed sub-network architecture of our proposed 3D-to-2D pose projector module in the (a) training phase and (b) testing phase. The Fully Connected (FC) layers for the regression function are in blue, while those for the projection function are in yellow. The black arrows represent the forward data flow, while the dashed arrows denote the backward propagation used to update the network parameters and perform gradual pose refinement in (a) and (b), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative comparisons on the Human3.6M dataset. The 3D poses are visualized from the side view, and the cameras are depicted. The results from Zhou et al. [14], Pavlakos et al. [19], Lin et al. [21], Zhou et al. [23], Tome et al. [20], our model and the ground truth are illustrated from left to right. Our model achieves considerably more accurate estimations than all the compared methods. Best viewed in color. Red and green indicate left and right, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparisons of ours and ours w/o self-correction on the Human3.6M dataset. The input image, estimated 2D pose, ours w/o self-correction, ours and ground truth are listed from left to right, respectively. With the ground truth as reference, one can easily observe that the inaccurately predicted human 3D joints in ours w/o self-correction are effectively corrected in ours. Best viewed in color. Red and green indicate left and right, respectively. achieves a performance gain of nearly 12% compared with our conference version (63.67mm vs 73.10 mm). The similar superior performance of our model over all the compared methods can also be observed under Protocol #2 and Protocol #3. Specifically, our model outperforms the best of the competing methods with the joint mean error reduced by approximately 19% (63.74mm vs 79.6mm) under Protocol #2 and 16% (54.14mm vs 70.70mm) under Protocol #3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Details of the convolutional layers in the 2D pose sub-network.</figDesc><table><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>Layer Name</cell><cell>conv1 1</cell><cell>conv1 2</cell><cell>max 1</cell><cell>conv2 1</cell><cell>conv2 2</cell><cell>max 2</cell><cell>conv3 1</cell><cell>conv3 2</cell><cell>conv3 3</cell></row><row><cell>Channel (kernel-stride)</cell><cell>64(3-1)</cell><cell>64(3-1)</cell><cell>64(2-2)</cell><cell>128(3-1)</cell><cell>128(3-1)</cell><cell>128(2-2)</cell><cell>256(3-1)</cell><cell>256(3-1)</cell><cell>256(3-1)</cell></row><row><cell></cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell></row><row><cell>Layer Name</cell><cell>conv3 4</cell><cell>max 3</cell><cell>conv4 1</cell><cell>conv4 2</cell><cell>conv4 3</cell><cell>conv4 4</cell><cell>conv4 5</cell><cell>conv4 6</cell><cell>conv4 7</cell></row><row><cell>Channel (kernel-stride)</cell><cell>256(3-1)</cell><cell>256(2-2)</cell><cell>512(3-1)</cell><cell>512(3-1)</cell><cell>256(3-1)</cell><cell>256(3-1)</cell><cell>256(3-1)</cell><cell>256(3-1)</cell><cell>128(3-1)</cell></row><row><cell>2d(gt) pose ground-truth p t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>3D human pose data {I 3d t } N t=1 and 2D human posedata {I 2d i } M i=1 1: Pre-train the 2D pose sub-network with {I 2d i } M i=1to initialize ? R via Eq. (5);2:  Fixing ? R , initialize ? T with hidden variables H with {I 3d t } N t=1 via Eq. (6);3:  Fixing ? R and ? T , initialize {? C , ? P } with {I 3d Fine-tune the whole model to further update {? R</figDesc><table><row><cell>t } N t=1 via</cell></row><row><cell>Eq. (2);</cell></row><row><cell>4:</cell></row></table><note>Algorithm 1 The Proposed Training Algorithm Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 : Quantitative comparisons on the Human3.6M dataset usingOurs 48.54 59.71 56.12 56.12 67.68 57.31 55.57 78.26 115.85 69.99 71.47 61.29 44.63 62.22 51.42 63.7454.77 48.92 54.65 47.49 47.17 64.73 94.30 56.84 78.85 49.29 33.07 58.71 38.96 54.14</head><label>2</label><figDesc>3D pose errors (in millimeters). The entries with the smallest 3D pose errors for each category are bold-faced. A method with "*" denotes that it is individually trained on each action category. Our model achieves a significant improvement over all compared approaches.Protocol #1 Method Direction Discuss Eat Greet Phone Pose Purchase Sit SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg. Ionescu PAMI'14 [5]* 132.71 183.55 132.37 164.39 162.12 150.61 171.31 151.57 243.03 162.14 205.94 170.69 96.60 177.13 127.88 162.14 67.80 76.50 92.10 73.00 75.30 100.30 137.30 83.40 77.00 77.30 86.80 79.70 81.70 82.03 Moreno-Noguer CVPR'17 [38] 66.07 77.94 72.58 84.66 99.71 74.78 65.29 93.40 103.14 85.03 98.52 98.78 78.74.74 79.09 70.05 67.56 89.30 90.74 195.62 83.46 93.26 71.15 55.74 85.86 62.51 82.72</figDesc><table><row><cell>Li ICCV'15 [36]*</cell><cell>-</cell><cell cols="4">136.88 96.94 124.74 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">168.68 -69.97 132.17</cell><cell>-</cell><cell>-</cell></row><row><cell>Tekin CVPR'16 [15]*</cell><cell cols="14">102.41 147.72 88.83 125.28 118.02 112.38 129.17 138.89 224.9 118.42 182.73 138.75 55.07 126.29</cell><cell cols="2">65.76 124.97</cell></row><row><cell>Zhou CVPR'16 [14]*</cell><cell cols="14">87.36 109.31 87.05 103.16 116.18 106.88 99.78 124.52 199.23 107.42 143.32 118.09 79.39 114.23</cell><cell cols="2">97.70 113.01</cell></row><row><cell>Zhou ECCVW'16 [4]*</cell><cell cols="14">91.83 102.41 96.95 98.75 113.35 90.04 93.84 132.16 158.97 106.91 125.22 94.41 79.02 126.04</cell><cell cols="2">98.96 107.26</cell></row><row><cell>Du ECCV'16 [37]*</cell><cell cols="14">85.07 112.68 104.90 122.05 139.08 105.93 166.16 117.49 226.94 120.02 135.91 117.65 99.26 137.36</cell><cell cols="2">106.54 126.47</cell></row><row><cell>Chen CVPR'17 [16]*</cell><cell>89.87</cell><cell cols="13">97.57 89.98 107.87 107.31 93.56 136.09 133.14 240.12 106.65 139.17 106.21 87.03 114.05</cell><cell cols="2">90.55 114.18</cell></row><row><cell>Tekin ICCV'17 [50]*</cell><cell>54.23</cell><cell cols="13">61.41 60.17 61.23 79.41 63.14 81.63 70.14 107.31 69.29 78.31 70.27 51.79 74.28</cell><cell cols="2">63.24 69.73</cell></row><row><cell>Ours*</cell><cell>50.36</cell><cell cols="13">59.74 54.86 57.12 66.30 53.24 54.73 84.58 118.49 63.10 78.61 59.47 41.96 64.88</cell><cell cols="2">49.48 63.79</cell></row><row><cell>Sanzari ECCV'16 [34]</cell><cell>48.82</cell><cell cols="13">56.31 95.98 84.78 96.47 66.30 107.41 116.89 129.63 97.84 105.58 65.94 92.58 130.46</cell><cell cols="2">102.21 93.15</cell></row><row><cell>Tome CVPR'17 [20]</cell><cell>64.98</cell><cell cols="13">73.47 76.82 86.43 86.28 68.93 74.79 110.19 173.91 84.95 110.67 85.78 86.26 71.36</cell><cell cols="2">73.14 88.39</cell></row><row><cell cols="2">Moreno-Noguer CVPR'17 [38] 69.54</cell><cell cols="13">80.15 78.20 87.01 100.75 76.01 69.65 104.71 113.91 89.68 102.71 98.49 79.18 82.40</cell><cell cols="2">77.17 87.30</cell></row><row><cell>Lin CVPR'17 [21]</cell><cell>58.02</cell><cell cols="13">68.16 63.25 65.77 75.26 61.16 65.71 98.65 127.68 70.37 93.05 68.17 50.63 72.94</cell><cell cols="2">57.74 73.10</cell></row><row><cell>Pavlakos CVPR'17 [19]</cell><cell>67.38</cell><cell cols="13">71.95 66.70 69.07 71.95 65.03 68.30 83.66 96.51 71.74 76.97 65.83 59.11 74.89</cell><cell cols="2">63.24 71.90</cell></row><row><cell>Bruce ICCV'17 [51]</cell><cell>90.1</cell><cell>88.2</cell><cell cols="4">85.7 95.6 103.9 90.4</cell><cell cols="2">117.9 136.4</cell><cell>98.5</cell><cell cols="4">103.0 92.4 94.4 90.6</cell><cell>86.0</cell><cell>89.5</cell><cell>97.5</cell></row><row><cell>Tekin ICCV'17 [50]</cell><cell>53.91</cell><cell cols="13">62.19 61.51 66.18 80.12 64.61 83.17 70.93 107.92 70.44 79.45 68.01 52.81 77.81</cell><cell cols="2">63.11 70.81</cell></row><row><cell>Zhou ICCV'17 [23]</cell><cell>54.82</cell><cell cols="13">60.70 58.22 71.41 62.03 53.83 55.58 75.20 111.59 64.15 65.53 66.05 63.22 51.43</cell><cell cols="2">55.33 64.90</cell></row><row><cell>Ours</cell><cell>50.03</cell><cell cols="6">59.96 Protocol #2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="16">Direction Discuss Eat Greet Phone Pose Purchase Sit SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg.</cell></row><row><cell>Akhter CVPR'15 [52]</cell><cell cols="14">1199.20 177.60 161.80 197.80 176.20 195.40 167.30 160.70 173.70 177.80 186.50 181.90 198.60 176.20</cell><cell cols="2">192.70 181.56</cell></row><row><cell>Zhou PAMI'16 [53]</cell><cell>99.70</cell><cell cols="13">95.80 87.90 116.80 108.30 93.50 95.30 109.10 137.50 106.00 107.30 102.20 110.40 106.50</cell><cell cols="2">115.20 106.10</cell></row><row><cell>Bogo ECCV'16 [24]</cell><cell>62.00</cell><cell cols="13">60.20 12 80.05</cell><cell cols="2">74.77 83.52</cell></row><row><cell>Tome CVPR'17 [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.60</cell></row><row><cell>Chen CVPR'17 [16]</cell><cell>71.63</cell><cell cols="6">66.60 Protocol #3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="16">Direction Discuss Eat Greet Phone Pose Purchase Sit SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg.</cell></row><row><cell>Yasin CVPR'16 [17]</cell><cell>88.40</cell><cell cols="13">72.50 108.50 110.20 97.10 81.60 107.20 119.00 170.80 108.20 142.50 86.90 92.10 165.70</cell><cell cols="2">102.00 110.18</cell></row><row><cell cols="2">Moreno-Noguer CVPR'17 [38] 67.44</cell><cell cols="13">63.76 87.15 73.91 71.48 69.88 65.08 71.69 98.63 81.33 93.25 74.62 76.51 77.72</cell><cell cols="2">74.63 76.47</cell></row><row><cell>Tome CVPR'17 [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.7</cell></row><row><cell>Bruce ICCV'17 [51]</cell><cell>62.8</cell><cell>69.2</cell><cell cols="4">79.6 78.8 80.8 72.5</cell><cell>73.9</cell><cell>96.1</cell><cell>106.9</cell><cell cols="4">88.0 86.9 70.7 71.9</cell><cell>76.5</cell><cell>73.2</cell><cell>79.5</cell></row><row><cell>Sun ICCV'17 [54]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.3</cell></row><row><cell>Ours</cell><cell>38.69</cell><cell>45.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>54.66 56.55 65.65 52.74 54.81 85.85 117.98 62.48 79.63 59.55 41.48 65.21 48.52 63.67</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Quantitative comparisons on the HumanEva-I dataset using 3D pose errors (in millimeters) for the "walking", "jogging" and "boxing" sequences. '-' indicates that the author of the corresponding method did not report the accuracy on that action. The entries with the smallest 3D pose errors for each category are bold-faced. Our model outperforms all the compared methods by a clear margin.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Walking</cell><cell></cell><cell></cell><cell cols="2">Jogging</cell><cell></cell><cell></cell><cell cols="2">Boxing</cell><cell></cell></row><row><cell>Methods</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg.</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg.</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg.</cell></row><row><cell>Simo-Serra CVPR'12 [55]</cell><cell>99.6</cell><cell>108.3</cell><cell>127.4</cell><cell>111.8</cell><cell>109.2</cell><cell>93.1</cell><cell>115.8</cell><cell>108.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Radwan ICCV'13 [59]</cell><cell>75.1</cell><cell>99.8</cell><cell>93.8</cell><cell>89.6</cell><cell>79.2</cell><cell>89.8</cell><cell>99.4</cell><cell>89.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang CVPR'14 [31]</cell><cell>71.9</cell><cell>75.7</cell><cell>85.3</cell><cell>77.6</cell><cell>62.6</cell><cell>77.7</cell><cell>54.4</cell><cell>71.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Du ECCV'16 [37]</cell><cell>62.2</cell><cell>61.9</cell><cell>69.2</cell><cell>64.4</cell><cell>56.3</cell><cell>59.3</cell><cell>59.3</cell><cell>58.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Simo-Serra CVPR'13 [56]</cell><cell>65.1</cell><cell>48.6</cell><cell>73.5</cell><cell>62.4</cell><cell>74.2</cell><cell>46.6</cell><cell>32.2</cell><cell>56.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Bo IJCV'10 [58]</cell><cell>45.4</cell><cell>28.3</cell><cell>62.3</cell><cell>45.3</cell><cell>55.1</cell><cell>43.2</cell><cell>37.4</cell><cell>45.2</cell><cell>42.5</cell><cell>64.0</cell><cell>69.3</cell><cell>58.6</cell></row><row><cell>Kostrikov BMVC'14 [57]</cell><cell>44.0</cell><cell>30.9</cell><cell>41.7</cell><cell>38.9</cell><cell>57.2</cell><cell>35.0</cell><cell>33.3</cell><cell>40.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tekin CVPR'16 [15]</cell><cell>37.5</cell><cell>25.1</cell><cell>49.2</cell><cell>37.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.5</cell><cell>61.7</cell><cell>57.5</cell><cell>56.6</cell></row><row><cell>Yasin CVPR'16 [22]</cell><cell>35.8</cell><cell>32.4</cell><cell>41.6</cell><cell>36.6</cell><cell>46.6</cell><cell>41.4</cell><cell>35.4</cell><cell>38.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Lin CVPR'17 [21]</cell><cell>26.5</cell><cell>20.7</cell><cell>38.0</cell><cell>28.4</cell><cell>41.0</cell><cell>29.7</cell><cell>29.1</cell><cell>33.2</cell><cell>39.4</cell><cell>57.8</cell><cell>61.2</cell><cell>52.8</cell></row><row><cell>Pavlakos CVPR'17 [19]</cell><cell>22.3</cell><cell>19.5</cell><cell>29.7</cell><cell>23.8</cell><cell>28.9</cell><cell>21.9</cell><cell>23.8</cell><cell>24.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Moreno-Noguer CVPR'17 [38]</cell><cell>19.7</cell><cell>13.0</cell><cell>24.9</cell><cell>19.2</cell><cell>39.7</cell><cell>20.0</cell><cell>21.0</cell><cell>26.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tekin ICCV'17 [50]</cell><cell>27.2</cell><cell>14.3</cell><cell>31.7</cell><cell>24.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>17.2</cell><cell>13.4</cell><cell>20.5</cell><cell>17.0</cell><cell>27.9</cell><cell>19.5</cell><cell>20.9</cell><cell>22.8</cell><cell>29.7</cell><cell>44.0</cell><cell>47.2</cell><cell>40.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison of the average running time (milliseconds per image) on the Human3.6M benchmark. As shown in this table, our model performs nearly three times faster than the fastest of the compared methods. Specifically, the 2D pose sub-network costs 19ms, the 2D-to-3D pose transformer module costs 19 ms, and the 3D-to-2D pose projector module costs 13 ms.</figDesc><table><row><cell cols="4">Method Zhou Pavlakos Zhou</cell><cell cols="2">Tome Ours</cell></row><row><cell></cell><cell cols="4">et al. [14] et al. [19] et al. [23] et al. [20]</cell><cell></cell></row><row><cell>Time</cell><cell>880</cell><cell>174</cell><cell>311</cell><cell>444</cell><cell>51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>Empirical comparisons under different settings for ablation study using Protocol #1. The entries with the smallest 3D pose errors on the Human3.6M dataset for each category are bold-faced.Method Direction Discuss Eating Greet Phone Pose Purchase Sitting SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg. Ours w/o SSC train+test 62.89 74.74 67.86 73.33 79.76 67.48 76.19 100.21 148.03 75.95 100.26 75.82 58.03 78.74 62.93 80.15 Ours w/o SSC test 52.95 63.82 57.15 59.42 68.83 55.81 57.75 95.44 125.70 66.23 82.91 64.22 44.24 69.49 50.54 67.63 Ours w/o projection 69.46 81.86 74.46 78.46 85.14 72.50 85.39 112.12 158.14 80.37 115.46 77.10 58.38 87.60 65.54 86.80 Ours w/o temporal 70.46 83.36 76.46 80.96 88.14 76.00 92.39 116.62 163.14 85.87 111.46 83.60 65.38 95.10 73.54 90.83 Ours w/ single frame 51.76 63.46 56.93 59.93 69.58 54.02 59.55 90.83 129.6 65.49 84.08 61.63 45.07 70.33 53.21 67.70 Ours w/o external 91.58 109.35 93.28 98.52 102.16 93.87 118.15 134.94 190.6 109.39 121.49 101.82 88.69 110.14 105.56 111.3</figDesc><table><row><cell>Ours</cell><cell>50.03</cell><cell>59.96 54.66 56.55 65.65 52.74 54.81</cell><cell cols="3">85.85 117.98 62.48 79.63 59.55 41.48 65.21</cell><cell>48.52 63.67</cell></row><row><cell>Ours w/ HG features</cell><cell>49.34</cell><cell>59.09 54.08 56.26 64.48 51.89 54.09</cell><cell cols="3">83.85 116.55 61.47 78.52 58.68 41.48 64.49</cell><cell>48.48 62.85</cell></row><row><cell>Ours w/ 2D GT</cell><cell>48.37</cell><cell>57.10 49.81 54.84 57.23 50.88 51.62</cell><cell>76.00</cell><cell>109.8</cell><cell>55.28 74.52 56.98 40.16 61.29</cell><cell>47.15 59.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Some qualitative comparisons of our model and Zhou et al.<ref type="bibr" target="#b22">[23]</ref> on two representative datasets in the wild, i.e., KTH Football II<ref type="bibr" target="#b59">[60]</ref> (first row) and MPII datasets<ref type="bibr" target="#b47">[48]</ref> (the remaining rows). For each image, the original viewpoint and a better viewpoint are illustrated. Best viewed in color. Red and green indicate left and right, respectively.</figDesc><table><row><cell>Ours</cell><cell>23]</cell><cell>Ours</cell></row><row><cell>Fig. 6:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>demonstrate that our w/o SSC test significantly outperforms ours w/o SSC train+test (67.63mm vs 80.15mm), and our model surpasses ours w/o SSC test by a clear margin (63.67mm vs. 67.63mm).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t . The projection function ? P implies projecting the 3D coordinatep 3d t into the image plane to obtain the projected 2D posep 2d t . Formally, f 2d t , p 3d t ,p 3d t , andp 2d t are formulated as follows:{f 2d t , p 2d t } = ? R (I t ; ? R ), p 3d t = ? T (f 2d t ; ? T , H t?1 ), p 3d t = ? C (p 3d t ; ? C ), p 2d t = ? P (p 3d t ; ? P ),(1)where ? R , ? T , ? C and ? P are parameters of ? R , ? T , ? C and ? P , respectively. Note that, H 0 is initially set to be a vector of zeros. After obtaining the predicted 2D pose p 2d t via ? R , and the projected 2D posep 2d t via ? P in Eq. (1), we consider minimizing the dissimilarity between p 2d t andp 2d t</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An Introduction to Cyberpsychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Errity</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
	<note>Human-computer interaction</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intelligent video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Markel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Schenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="83" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Virtual Reality: Exploring the Brave New Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rheingold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Simon &amp; Schuster Adult Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops: 2016, Proceedings, Part III</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatio-temporal matching for human detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bugra Popaand Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference for Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coarseto-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Recurrent 3d pose sequence machines,&quot; in CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="248" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Looselimbed people: Estimating 3d human pose and motion using nonparametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Merging pose estimates across space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Computer recognition of vowel sounds using a self-supervised learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASI</title>
		<imprint>
			<biblScope unit="volume">VI</biblScope>
			<biblScope unit="page" from="117" to="123" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-organization for object extraction using a multilayer neural network and fuzziness measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Fuzzy Systems</title>
		<imprint>
			<biblScope unit="page" from="54" to="68" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle lim-its for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Single image 3d human pose estimation from noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aleny?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-view body part recognition with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">He is currently pursuing his dual Ph.D. degree at Sun Yat-Sen University and Hong Kong Polytechnic University, advised by Prof. Liang Lin and Lei Zhang. His current research interests include computer vision and machine learning</title>
		<ptr target="http://kezewang.com" />
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Guangzhou, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Keze Wang received his B.S. degree in software engineering from Sun Yat-Sen University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
