<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Instance Query Network for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Instance Query Network for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition (NER) is a fundamental task in natural language processing. Recent works treat named entity recognition as a reading comprehension task, constructing type-specific queries manually to extract entities. This paradigm suffers from three issues. First, type-specific queries can only extract one type of entities per inference, which is inefficient. Second, the extraction for different types of entities is isolated, ignoring the dependencies between them. Third, query construction relies on external knowledge and is difficult to apply to realistic scenarios with hundreds of entity types. To deal with them, we propose Parallel Instance Query Network (PIQN), which sets up global and learnable instance queries to extract entities from a sentence in a parallel manner. Each instance query predicts one entity, and by feeding all instance queries simultaneously, we can query all entities in parallel. Instead of being constructed from external knowledge, instance queries can learn their different query semantics during training. For training the model, we treat label assignment as a one-to-many Linear Assignment Problem (LAP) and dynamically assign gold entities to instance queries with minimal assignment cost. Experiments on both nested and flat NER datasets demonstrate that our proposed method outperforms previous state-ofthe-art models 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) aims to identify text spans to specific entity types such as Person, Location, Organization. It has been widely used in many downstream applications such as entity linking <ref type="bibr">(Ganea and Hofmann, 2017;</ref><ref type="bibr">Le and Titov, 2018)</ref> and relation extraction <ref type="bibr">(Li and Ji, 2014;</ref><ref type="bibr"></ref> * This work was conducted when Yongliang Shen was interning at Alibaba DAMO Academy. ? Corresponding author. 1 Our code is available at https://github.com/ tricktreat/piqn.  <ref type="figure">Figure 1</ref>: (a) For a sentence, type-specific queries can only extract entities of one type per inference, so the model needs to be run multiple times. (b) In contrast, instance-based queries can be input into the model simultaneously, and all entities can be extracted in parallel. Furthermore, the parallel manner can model the interactions between entities of different types. <ref type="bibr" target="#b11">Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b19">Shen et al., 2021b)</ref>. Traditional approaches for NER are based on sequence labeling, assigning a single tag to each word in a sentence. However, the words of nested entities have more than one tag, thus these methods lack the ability to identify nested entities. <ref type="bibr">Recently, Ju et al. (2018)</ref>; <ref type="bibr" target="#b21">Strakov? et al. (2019)</ref>; <ref type="bibr" target="#b28">Wang et al. (2020a)</ref> redesign sequence labeling models to support nested structures using different strategies. Instead of labeling each word, <ref type="bibr" target="#b7">Luan et al. (2019)</ref>; <ref type="bibr" target="#b23">Tan et al. (2020)</ref>; <ref type="bibr">Li et al. (2021)</ref>; <ref type="bibr" target="#b18">Shen et al. (2021a)</ref> perform a classification task on the text span, and <ref type="bibr" target="#b21">Strakov? et al. (2019)</ref>; <ref type="bibr">Paolini et al. (2021)</ref>; <ref type="bibr" target="#b34">Yan et al. (2021)</ref>; <ref type="bibr" target="#b24">Tan et al. (2021)</ref> treat NER as a sequence generation or set prediction task and design encoder-decoder models to generate entities. Recently, ; <ref type="bibr" target="#b10">Mengge et al. (2020)</ref>;  reformulate the NER task as a machine reading task and achieve a promising performance on both flat and nested datasets. As shown in <ref type="figure">Figure 1(a)</ref>, they treat the sentence as context and construct typespecific queries from external knowledge to extract entities. For example, for the sentence "U.S. President Barack Obama and his wife spent eight years arXiv:2203.10545v1 [cs.CL] 20 Mar 2022 in the White House",  constructs the PER-specific query in natural language form -"Find person entity in the text, including a single individual or a group" to extract the PER entities, such as "U.S. President", "Barack Obama". However, since the queries are type-specific, only one type of entities can be extracted for each inference. This manner not only leads to inefficient prediction but also ignores the intrinsic connections between different types of entities, such as "U.S." and "U.S. President". In addition, type-specific queries rely on external knowledge for manual construction, which makes it difficult to fit realistic scenarios with hundreds of entity types.</p><p>In this paper, we propose the Parallel Instance Query Network (PIQN), where global and learnable instance queries replace type-specific ones to extract entities in parallel. As shown in <ref type="figure">Figure 1(b)</ref>, each instance query predicts one entity, and multiple instance queries can be fed simultaneously to predict all entities. Different from previous methods, we do not need external knowledge to construct the query into natural language form. The instance query can learn different query semantics during training, such as position-related or typerelated semantics. Since the semantics of instance queries are implicit, we cannot assign gold entities as their labels in advance. To tackle this, we treat label assignment as a one-to-many Linear Assignment Problem (LAP) <ref type="bibr" target="#b2">(Burkard and ?ela, 1999)</ref>, and design a dynamic label assignment mechanism to assign gold entities for instance queries.</p><p>Our main contributions are as follow:</p><p>? Different from type-specific queries that require multiple rounds of query, our model employs instance queries that can extract all entities in parallel. Furthermore, the style of parallel query can model the interactions between entities of different types.</p><p>? Instead of relying on external knowledge to construct queries in natural language form, instance queries learn their query semantics related to entity location and entity type during training.</p><p>? To train the model, we design a dynamic oneto-many label assignment mechanism, where the entities are dynamically assigned as labels for the instance queries during training. The one-to-many manner allows multiple queries to predict the same entity, which can further improve the model performance.</p><p>? Experiments show that our model achieves state-of-the-art performance consistently on several nested and flat NER datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional approaches for NER can be divided into three categories, including tagging-based, hypergraph-based and span-based approaches. The typical sequence labeling approach <ref type="bibr">(Huang et al., 2015)</ref> predicts labels for each token, and struggles to address nested NER. Some works <ref type="bibr" target="#b1">(Alex et al., 2007;</ref><ref type="bibr" target="#b28">Wang et al., 2020a)</ref> adapt the sequence labeling model to nested entity structures by designing a special tagging scheme. Different from the decoding on the linear sequence, the hypergraphbased approaches <ref type="bibr" target="#b6">(Lu and Roth, 2015;</ref><ref type="bibr" target="#b12">Muis and Lu, 2017;</ref><ref type="bibr">Katiyar and Cardie, 2018)</ref> construct hypergraphs based on the entity nesting structure and decode entities on the hypergraph. Span-based methods first extract spans by enumeration <ref type="bibr" target="#b20">(Sohrab and Miwa, 2018;</ref><ref type="bibr" target="#b7">Luan et al., 2019)</ref> or boundary identification <ref type="bibr" target="#b37">(Zheng et al., 2019;</ref><ref type="bibr" target="#b23">Tan et al., 2020)</ref>, and then classify the spans. Based on these, <ref type="bibr" target="#b18">Shen et al. (2021a)</ref> treats NER as a joint task of boundary regression and span classification and proposes a two-stage identifier of locating entities first and labeling them later. Three novel paradigms for NER have recently been proposed, reformulating named entity recognition as sequence generation, set prediction, and reading comprehension tasks, respectively. <ref type="bibr" target="#b34">Yan et al. (2021)</ref> formulates NER as an entity span sequence generation problem and uses a BART (Lewis et al., 2020) model with the pointer mechanism to tackle NER tasks. <ref type="bibr" target="#b24">Tan et al. (2021)</ref> formulates NER as an entity set prediction task. Different from <ref type="bibr" target="#b21">Strakov? et al. (2019)</ref>, they utilize a non-autoregressive decoder to predict entity set. ; <ref type="bibr" target="#b10">Mengge et al. (2020)</ref> reformulate the NER task as an MRC question answering task. They construct type-specific queries using semantic prior information for entity categories.</p><p>Different from <ref type="bibr" target="#b30">Jiang et al. (2021)</ref>, our method attempts to query at the entity level, where it adaptively learns query semantics for instance queries and extracts all types of entities in parallel. It is worth noting that Seq2Set <ref type="bibr" target="#b24">(Tan et al., 2021)</ref> is quite different from ours: (1) Seq2Set attempts to eliminate the incorrect bias introduced by specified entity decoding order in the seq2seq framework, and proposes an entity set predictor, while we follow the MRC paradigm and focus on extracting entities using instance queries. (2) Seq2Set is an encoder-decoder architecture, while our model throws away the decoder and keeps only the encoder as in <ref type="bibr" target="#b29">Wang et al. (2022a)</ref>, which speeds up inference and allows full interaction between query and context. (3) Seq2Set uses bipartite graph matching to compute the entity-set level loss, while we focus on the label assignment for each instance query and propose a one-to-many dynamic label assignment mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first introduce the task formulation in ? 3.1, and then describe our method. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, our method consists of three components: the Encoder ( ? 3.2), the Entity Prediction ( ? 3.3) and the Dynamic Label Assignment ( ? 3.4). The encoder encodes both the sentence and instance queries. Then for each instance query, we perform entity localization and entity classification using Entity Pointer and Entity Classifier respectively. For training the model, we introduce a dynamic label assignment mechanism to assign gold entities to the instance queries in ? 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formulation</head><p>We use (X, Y ) to denote a training sample, where X is a sentence consisting of N words labeled by a set of triples</p><formula xml:id="formula_0">Y = {&lt; Y l k , Y r k , Y t k &gt;} G?1 k=0 . Y l k ? [0, N ? 1], Y r k ? [0, N ? 1]</formula><p>and Y t k ? E are the indices for the left boundary, right boundary and entity type of the k-th entity, where E is a finite set of entity types. In our approach, We set up M (M &gt; G) global and learnable instance queries I = R M ?h , each of which (denoted as a vector of size h) extracts one entity from the sentence. They are randomly initialized and can learn the query semantics automatically during training. Thus we define the task as follows: given an input sentence X, the aim is to extract the entities Y based on the learnable instance queries I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>Model input consists of two sequences, the sentence X of length N and the instance queries I of length M . The encoder concatenates them into one sequence and encodes them simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Embedding</head><p>We calculate the token embeddings E tok , position embeddings E pos and type embeddings E typ of the input from two sequences as follows (E tok , E pos , E typ ? R (N +M )?h ):</p><formula xml:id="formula_1">E tok = Concat(V, I) E pos = Concat(P w , P q ) E typ = Concat([U w ] N , [U q ] M )<label>(1)</label></formula><p>where V ? R N ?h are token embeddings of the word sequence, I ? R M ?h are the vectors of instance queries, P w ? R N ?h and P q ? R M ?h are separate learnable position embeddings. U w and U q are type embeddings and [?] N means repeating N times. Then the input can be represented as</p><formula xml:id="formula_2">H 0 = E tok + E pos + E typ ? R (N +M )?h .</formula><p>One-Way Self-Attention Normal self-attention would let the sentence interact with all instance queries. In such a way, randomly initialized instance queries can affect the sentence encoding and break the semantics of the sentence. To keep the sentence semantics isolated from the instance queries, we replace the self-attention in BERT <ref type="bibr">(Devlin et al., 2019)</ref> with the one-way version:</p><formula xml:id="formula_3">OW-SA(H) = ?HW v (2) ? = softmax HW q (HW k ) T ? h + M<label>(3)</label></formula><p>where W q , W k , W v ? R h?h are parameter matrices and M ? {0, ? inf} (N +M )?(N +M ) is a mask matrix for the attention score where elements in M set to 0 for kept units and ? inf for removed ones. In our formula, the upper right sub-matrix of M is a full ? inf matrix of size (N ? M ) and other elements are zero, which can prevent the sentence encoding from attending on the instance queries. In addition, the self-attention among instance queries can model the connections between each other, and then enhance their query semantics. After BERT encoding, we further encode the sequence at word-level by two bidirectional LSTM layers and L extra transformer layers. Finally we split H ? R (N +M )?h into two parts: the sentence encoding H w ? R N ?h and the instance query encoding H q ? R M ?h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity Prediction</head><p>Each instance query can predict one entity from the sentence, and with M instance queries, we can predict at most M entities in parallel. Entity prediction  can be viewed as a joint task of boundary prediction and category prediction. We design Entity Pointer and Entity Classifier for them respectively.</p><formula xml:id="formula_4">[2] &amp; R[2] L[1] R[1] L[M-1] R[M-1] T[1] T[M-1] T[2] P t iY t k P l iY l k P r iY r k Cost i k = + + ( ( - M-2 M-3 M-4 V[CLS] P0 U w IM-1 U q PM-1 IM-2 U PM-2 q IM-3 U PM-3 q IM-4 U PM-4 q ??? ??? ??? I2 U P2 q I1 U P1 q I0 U P0 q V[SEP] PN-1 U w VHouse PN-2 U w VWhite PN-3 U w Vthe PN-4 U w ??? ??? ??? VObama P4 U w VBarack P3 U w VPresident P2 U w VU.S. P1 U w Token Position Type Assignment Matrix ( A )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Label Assignment</head><formula xml:id="formula_5">YG-2 YG-1 Y0 Cost Matrix ( Cost ) Y1 Assignable Quantity ( q ) -1.7 -0.6 -1.0 -1.2 -2.7 -0.8 -0.9 -1.2 -2.8 -1.1 -1.0 -1.2 -1.9 -0.9 -0.7 ? ? ? ? ? ? -2.9 -0.8 -1.0 -0.7 -2.8 1 2 ? ? ? 1 1 ? ? ? ? ? ? G ground-truth entities Y = ( Yk , Yk , Yk ) l r t M labeled instance queries 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 ? ? ? ? ? ? 1 0 0 0 1 0 0 0 1 0 YG-2 YG-1 Y0 Y1 None ? ? ? ? ? ? P t P l P r M unlabeled instance queries w w w w w w w w w q q q q q q q M-1</formula><p>Entity Pointer For the i-th instance query H q i , we first interact the query with each word of the sentence by two linear layers. The fusion representation of the i-th instance query and j-th word is computed as:</p><formula xml:id="formula_6">S ? ij = ReLU(H q i W q ? + H w j W w ? )<label>(4)</label></formula><p>where ? ? {l, r} denotes the left or right boundary and W q ? , W w ? ? R h?h are trainable projection parameters. Then we calculate the probability that the j-th word of the sentence is a left or right boundary:</p><formula xml:id="formula_7">P ? ij = sigmoid(S ? ij W ? + b ? )<label>(5)</label></formula><p>where W ? ? R h and b ? are learnable parameters.</p><p>Entity Classifier Entity boundary information are useful for entity typing. We use P ? i = [P ? i0 , P ? i1 , ? ? ? , P ? iN ?1 ], ? ? {l, r} to weigh all words and then concatenate them with instance queries. The boundary-aware representation of the i-th instance query can be calculated as:</p><formula xml:id="formula_8">S t i = ReLU H q i W q t ; P l i H w ; P r i H w<label>(6)</label></formula><p>where W q t ? R h?h is a learnable parameter. Then we can get the probability of the entity queried by the i-th instance query belonging to category c:</p><formula xml:id="formula_9">P t ic = exp(S t i W c t + b c t ) c ?E exp(S t i W c t + b c t )<label>(7)</label></formula><p>where W c t ? R h and b c t are learnable parameters. Finally, the entity predicted by the i-th instance query is</p><formula xml:id="formula_10">T i = T l i , T r i , T t i . T l i = arg max j (P l ij ) and T r i = arg max j (P r ij )</formula><p>are the left and right boundary, T t i = arg max c (P t ic ) is the entity type. We perform entity localization and entity classification on all instance queries to extract entities in parallel. If multiple instance queries locate the same entity but predict different entity types, we keep only the prediction with the highest classification probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dynamic Label Assignment for Training</head><p>Dynamic Label Assignment Since instance queries are implicit (not in natural language form), we cannot assign gold entities to them in advance. To tackle this, we dynamically assign labels for the instance queries during training. Specifically, we treat label assignment as a Linear Assignment Problem. Any entity can be assigned to any instance query, incurring some cost that may vary depending on the entity-query assignment. We define the cost of assigning the k-th entity (</p><formula xml:id="formula_11">Y k = &lt; Y l k , Y r k , Y t k &gt;)</formula><p>to the i-th instance query as:</p><formula xml:id="formula_12">Cost ik = ? P t iY t k + P l iY l k + P r iY r k (8)</formula><p>where Y t k , Y l k and Y r k denote the indices for the entity type, left boundary and right boundary of the k-th entity. It is required to allocate as many entities as possible by assigning at most one entity to each query and at most one query to each entity, in such a way that the total cost of the assignment is minimized. However, the one-to-one manner does not fully utilize instance queries, and many instance queries are not assigned to gold entities. Thus we extend the traditional LAP to one-to-many one, where each entity can be assigned to multiple instance queries. The optimization objective of this one-to-many LAP is defined as:</p><formula xml:id="formula_13">min M ?1 i=0 G?1 k=0 A ik Cost ik s.t. k A ik ? 1 i A ik = q k ?i, k, A ik ? {0, 1} .<label>(9)</label></formula><p>where A ? {0, 1} M ?G is the assignment matrix, G denotes the number of the entities and A ik = 1 indicates the k-th entity assigned to the i-th instance query. q k denotes the assignable quantity of the k-th gold entity and Q = k q k denotes the total assignable quantity for all entities. In our experiments, the assignable quantities of different entities are balanced.</p><p>We then use the Hungarian (Kuhn, 1955) algorithm to solve Equation 9, which yields the label assignment matrix with the minimum total cost. However, the number of instance queries is greater than the total assignable quantity of entity labels (M &gt; Q), so some of them will not be assigned to any entity label. We assign None label to them by extending a column for the assignment matrix. The new column vector a is set as follows:</p><formula xml:id="formula_14">a i = 0, k A ik = 1 1, k A ik = 0 (10)</formula><p>Based on the new assignment matrix? ? {0, 1} M ?(G+1) , we can further get the labels? = Y. indexby(? * ) for M instance queries, where ? * = arg max dim=1 (?) is the label index vector for instance queries under the optimal assignment.</p><p>Training Objective We have computed the entity predictions for M instance queries in ? 3.3 and got their labels? with the minimum total assignment cost in ? 3.4. To train the model, we define boundary loss and classification loss. For left and right boundary prediction, we use binary cross entropy function as a loss:</p><formula xml:id="formula_15">L b = ? ??{l,r} M ?1 i=0 N ?1 j=0 1[? ? i = j] log P ? ij + 1[? ? i = j] log 1 ? P ? ij<label>(11)</label></formula><p>and for entity classification we use cross entropy function as a loss:</p><formula xml:id="formula_16">L t = ? M ?1 i=0 c?E 1[? t i = c] log P t ic<label>(12)</label></formula><p>where 1[?] denotes indicator function that takes 1 when ? is true and 0 otherwise. Follow Al-Rfou et al. <ref type="formula" target="#formula_1">(2019)</ref> and Carion et al. (2020), we add Entity Pointer and Entity Classifier after each word-level transformer layer, and we can get the two losses at each layer. Thus, the total loss on the train set D can be defined as:</p><formula xml:id="formula_17">L = D L ? =1 L ? t + L ? b<label>(13)</label></formula><p>where L ? t , L ? b are classification loss and boundary loss at the ? -th layer. For prediction, we just perform entity prediction at the final layer.   <ref type="bibr" target="#b26">(Walker et al., 2006)</ref>, <ref type="bibr">KBP17 (Ji et al., 2017)</ref>, GENIA <ref type="bibr" target="#b13">(Ohta et al., 2002)</ref>, NNE <ref type="bibr" target="#b17">(Ringland et al., 2019)</ref> and three flat NER dataset: <ref type="bibr">FewNERD (Ding et al., 2021)</ref>, CoNLL03 <ref type="bibr" target="#b25">(Tjong Kim Sang and De Meulder, 2003)</ref>, <ref type="bibr">OntoNotes (Pradhan et al., 2013)</ref>, and one Chinese flat NER dataset: MSRA <ref type="bibr">(Levow, 2006)</ref>. FewNERD and NNE are two datasets with large entity type inventories, containing 66 and 114 fine-grained entity types. Please refer to Appendix A for statistical information about the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In our experiments, we use pretrained BERT (Devlin et al., 2019) in our encoder. For a fair comparison, we use bert-large on ACE04, ACE05, NNE, CoNLL03 and OntoNotes, bert-base on KBP17 and FewN-ERD, biobert-large (Chiu et al., 2016) on GENIA and chinese-bert-wwm (Cui et al., 2020) on Chinese MSRA. For all datasets, we train our model for 30-60 epochs and use the Adam Optimizer (Kingma and Ba, 2015) with a linear warmup-decay learning rate schedule. We initialize all instance queries using the normal distribution N (0.0, 0.02). See Appendix B for more detailed parameter settings and Appendix C for all baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We use strict evaluation metrics that an entity is confirmed correct when the entity boundary and the entity type are correct simultaneously. We employ precision, recall and F1-score to evaluate the performance. We also report the F1-scores on the entity localization and entity classification subtasks in ? 5.2 and Appendix D.2. We consider the localization as correct when the left and right boundaries are predicted correctly. Based on the accurately localized entities, we then evaluate the performance of entity classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance</head><p>Overall Performance <ref type="table" target="#tab_1">Table 1</ref> illustrates the performance of the proposed model as well as baselines on the nested NER datasets. We observe significant performance boosts on the nested NER datasets over previous state-of-the-art models, achieving F1-scores of 81.77%, 88.14%, 87.42% and 84.50% on GENIA, ACE04, ACE05, KBP17 and NNE datasets with +1.23%, +0.73%, +0.37%, +0.45% and +0.96% improvements. Our model can be applied to flat NER. As shown in <ref type="table">Table 2</ref>, our model achieves state-of-the-art performance on the FewNERD and Chinese MSRA datasets with +1.44% and +0.88% improvements. On the CoNLL03 and OntoNotes datasets, our model also achieves comparable results. Compared with the type-specific query-based method , our model improves by +2.85%, +2.16%, +0.54%, +3.53% on the GENIA, ACE04, ACE05 and KBP17 datasets. We believe there are three reasons: (1) Rather than relying on external knowledge to inject semantics, instance queries can learn query semantics adaptively, avoiding the sensitivity to hand-constructed queries of varying quality. (2) Each query no longer predicts a group of entities of a specific type, but only one entity. This manner refines the query to the entity level with more precise query semantics. (3) Instance queries are fed into the model in parallel for encoding and prediction, and different instance queries can exploit the intrinsic connections between entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference Speed</head><p>We compare the inference speed on ACE04 and NNE, as shown in <ref type="table" target="#tab_5">Table 4</ref>. Compared to the type-specific query method , our model not only improves the performance, but also gains significant inference speedup. In particular, on the NNE dataset with 114 entity types, our model speeds up by 30.46? and improves performance by +39.2%. This is because  requires one inference for each type-specific query, while our approach performs parallel inference for all instance queries and only needs to be run once. We also compare previous state-of-the-art models <ref type="bibr" target="#b24">(Tan et al., 2021;</ref><ref type="bibr" target="#b18">Shen et al., 2021a</ref>) and our method is still faster and performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>In this section, we analyze the effects of different components in PIQN. As shown in <ref type="table" target="#tab_4">Table 3</ref>, we have the following observations: (1) Compared to the static label assignment in order of occurrence, the dynamic label assignment shows significant improvement on localization, classification, and NER F1-score, which improves NER F1-score by +5.71% on ACE04 and +8.84% on GENIA. This shows that modeling label assignment as a LAP  <ref type="table">Table 2</ref>: Results for flat NER task. * means the result reproduced by <ref type="bibr" target="#b34">(Yan et al., 2021)</ref>, ? means the reproduction on the same preprocessed dataset and ? means that we run the code on the unreported dataset.</p><p>problem enables dynamic assignment of optimal labels to instance queries during training, eliminating the incorrect bias when pre-specifying labels. Furthermore, one-to-many for label assignment is more effective than one-to-one, improving the F1score by +3.86% on ACE04 and +0.51% on GE-NIA.</p><p>(2) The one-way self-attention blocks the attention of sentence encoding on instance queries, which improves the F1-score by +0.98% on ACE04 and +0.57% on GENIA. It illustrates the importance of keeping the semantics of the sentence independent of the query. In contrast, semantic interactions between queries are effective, which improves the F1-score by +0.92% on ACE04 and +0.67% on GENIA. The major reason is that entities in the same sentence are closely related and the interaction between instance queries can capture the relation between them.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>In order to analyze the query semantics learned by the instance query in the training, we randomly selected several instance queries and analyzed the locations and types of entities they predicted. Entity Location We normalize the predicted central locations of the entities and use kernel density estimation to draw the distribution of the predicted entity locations for different queries, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We observe that different instance queries focus on entities at different positions, which means that the instance queries can learn the query semantics related to entity position.</p><p>For example, instance queries #28 and #39 prefer to predict entities at the beginning of sentences, while #11 and #53 prefer entities at the end.</p><p>Entity Type We count the co-occurrence of different instance queries and different entity types they predicted. To eliminate the imbalance of entity types, we normalize the co-occurrence matrix on the entity type axis. As shown in <ref type="figure">Figure 4</ref>, different instance queries have preferences for different entity types. For example, instance queries #11 and #13 prefer to predict PER entities, #30 and #43 prefer VEH entities, #25 and #49 prefer WEA entities, #12 prefers FAC entities, and #35 prefers LOC entities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Co-occurrence statistics between instance queries and different entity types</head><p>We also analyze the auxiliary loss, the dynamic label assignment mechanism, and the performance on entity localization and classification, please see the Appendix D.</p><p>6 Case Study  <ref type="table" target="#tab_6">Table 5</ref>: Cases Study. In the left column, the label in the lower right corner indicates the type of entity, and the superscripts indicate the positions of the left and right boundary words. In the right column, we show the correspondence between the instance queries and the predicted entities. long entities well. In case 1, the entities of length 31 or with the three-level nested structure are predicted accurately. And thanks to the one-to-many dynamic label assignment mechanism, each entity can be predicted by multiple instance queries, which guarantees a high coverage of entity prediction. However, the model's ability to understand sentences is still insufficient, mainly in the following ways: (1) There is a deficiency in the understanding of special phrases. Yahoo ! Communications Services in case 2 is misclassified as ORG, but in fact Yahoo ! is ORG.</p><p>(2) Over-focus on local semantics. In case 3, the model misclassifies Venezuelan consumer as PER, ignoring the full semantics of the long phrase the Venezuelan consumer protection agency, which should be ORG.</p><p>(3) Insensitivity to morphological variation. The model confused Venezuelan and Venezuela, and misidentified the former as GPE in case 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose Parallel Instance Query Network for nested NER, where a collection of instance queries are fed into the model simultaneously and can predict all entities in parallel. The instance queries can automatically learn query semantics related to entity types or entity locations during training, avoiding manual constructions that rely on external knowledge. To train the model, we design a dynamic label assignment mechanism to assign gold entities for these instance queries. Experiments on both nested and flat NER datasets demonstrate that the proposed model achieves stateof-the-art performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>GENIA <ref type="bibr" target="#b13">(Ohta et al., 2002)</ref> is an English biology nested named entity dataset and contains 5 entity types, including DNA, RNA, protein, cell line, and cell type categories. Follow <ref type="bibr" target="#b22">Yu et al. (2020)</ref>, we use 90%/10% train/test split and evaluate the model on the last epoch. <ref type="bibr">(Doddington et al., 2004;</ref><ref type="bibr" target="#b26">Walker et al., 2006)</ref> are two English nested datasets, each of them contains 7 entity categories. We follow the same setup as previous work Katiyar and Cardie (2018); <ref type="bibr" target="#b5">Lin et al. (2019)</ref>. In <ref type="table" target="#tab_9">Table 6</ref> and <ref type="table">Table 7</ref>, we report the number of sentences, the number of sentences containing nested entities, the average sentence length, the total number of entities, the number of nested entities, the nesting ratio, the maximum and the average number of entities in a sentence on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE04 and ACE05</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>In default setting, we set the number of instance queries M = 60, and the total assignable quantity Q = M ?0.75 = 45. To ensure that the assignable quantities of different entities are balanced, we randomly divide Q to different entities and adjust each division to be larger than Q/G, where G is the number of the ground-truth entities. When the number of entities is more than the total assignable quantity, we specify Q = G. We have also tried other configurations that will be discussed in Appendix D.3. We set L word-level transformer layers after BERT and set auxiliary losses in each layer. In the default setting L equals 5. We compare the effect of different auxiliary layers on the model performance, which will be discussed in Appendix D.1. Since the instance queries are randomly initialized and do not have query semantics at the initial stage of training, we first fix the parameters of BERT and train the model for 5 epochs, allowing the instance queries to initially learn the query semantics. When decoding entities, we filter out the predictions with localization probability and classification probability less than the threshold 0.6 and 0.8, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baselines</head><p>We compare PIQN with the following baselines:</p><p>? ARN (Lin et al., 2019) designs a sequence-tonuggets architecture for nested mention detection, which first identifies anchor words and then recognizes the mention boundaries.</p><p>? HIT <ref type="bibr" target="#b32">(Wang et al., 2020b</ref>) designs a head-tail detector and a token interaction tagger, which can leverage the head-tail pair and token interaction to express the nested structure.</p><p>? Pyramid <ref type="bibr" target="#b28">(Wang et al., 2020a</ref>) presents a layered neural model for nested entity recognition, consisting of a stack of inter-connected layers.</p><p>? Biaffine  formulates NER as a structured prediction task and adopts a dependency parsing approach for NER.</p><p>? BiFlaG (Luo and Zhao, 2020) designs a bipartite flat-graph network with two subgraph modules for outermost and inner entities.</p><p>? BERT-MRC  formulates the NER task as a question answering task. They construct type-specific queries using semantic prior information for entity categories.  <ref type="table">Table 7</ref>: Statistics of the flat datasets used in the experiments. #S: the number of sentences, #E: the total number of entities, AL: the average sentence length, #ME: the maximum number of entities in a sentence, #AE: the average number of entities in a sentence</p><p>? BARTNER <ref type="bibr" target="#b34">(Yan et al., 2021)</ref> formulates NER as an entity span sequence generation problem and uses a unified Seq2Seq model with the pointer mechanism to tackle flat, nested, and discontinuous NER tasks.</p><p>? Seq2Set <ref type="bibr" target="#b24">(Tan et al., 2021)</ref> formulates NER as an entity set prediction task. Different from <ref type="bibr" target="#b21">Strakov? et al. (2019)</ref>, they utilize a nonautoregressive decoder to predict entity set.</p><p>? Locate&amp;Label (Shen et al., 2021a) treats NER as a joint task of boundary regression and span classification and proposed a twostage identifier of locating entities first and labeling them later.</p><p>For a fair comparison, we did not compare with <ref type="bibr" target="#b22">Sun et al. (2020)</ref>; <ref type="bibr" target="#b3">Li et al. (2020a)</ref>; <ref type="bibr" target="#b9">Meng et al. (2019)</ref> on Chinese MSRA because they either used glyphs or an external lexicon or a larger pre-trained language model. In addition, some works <ref type="bibr" target="#b30">(Wang et al., 2021</ref><ref type="bibr" target="#b31">(Wang et al., , 2022b</ref> used search engines to retrieve input-related contexts to introduce external information, and we did not compare with them as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Analysis of Auxiliary Loss</head><p>Many works <ref type="bibr" target="#b0">(Al-Rfou et al., 2019;</ref><ref type="bibr">Carion et al., 2020)</ref> have demonstrated that the auxiliary loss in the middle layer introduces supervised signals in advance and can improve model performance. We compared the effect of the different number of auxiliary-loss layers on the model performance (F1-score on ACE04). Overall, the model performs better as the number of auxiliary-loss layers increases. The model achieves the best results when the number of layers equals 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Analysis of Two Subtasks</head><p>We compare the model performance on entity localization and entity classification subtasks on the ACE04 dataset, as shown in <ref type="table" target="#tab_11">Table 8</ref>. Compared with the previous state-of-the-art models <ref type="bibr" target="#b24">(Tan et al., 2021;</ref><ref type="bibr" target="#b18">Shen et al., 2021a)</ref>, our model achieves better performance on both entity localization and entity classification subtasks. This illustrates that the instance queries can automatically learn their query semantics about location and type of entities, which is consistent with our analysis in 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Localization</head><p>Pr.</p><p>Rec. F1   We analyze the impact of dynamic label assignment on model performance for different combinations of the number M of instance queries and the total assignable quantity Q of labels. From <ref type="table" target="#tab_12">Table 9</ref>, we observe that (1) there is a tradeoff between M and Q, and the model achieves the best performance with a ratio of 4:3. With this setting, the ratio of positive to negative instances of instance queries is 3:1. (2) The number of instance queries and the total assignable quantity is not as large as possible, and an excessive number may de-grade the model performance. In our experiments (M, Q) = (60, 45) is the best combination.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The overall architecture of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Kernel density estimation of entity distribution at different locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Analysis of Auxiliary Loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Results for nested NER task. ? means the reproduction on the same preprocessed dataset and ? means that we run the code on the unreported dataset.on eight English datasets, including five nested NER datasets: ACE04 (Doddington et al., 2004) , ACE05</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study.(1) w/o Dynamic LA: replace dynamic label assignment to static label assignment, i.e., assign labels to instance queries in the order of the entities' occurrence in the sentence. (2) w/o OvM LA: replace the one-to-many label assignment to one-to-one, i.e., set the number of queries to which each entity can be assigned to be 1. (3) w/o One Way SA: encode sentences and instance queries using the original BERT. (4) w/o Query Interaction: eliminate interactions between instance queries by masking the attention weights between them.</figDesc><table><row><cell>Model</cell><cell>ACE04</cell><cell>NNE</cell></row><row><cell></cell><cell>Speedup F1</cell><cell>Speedup F1</cell></row><row><cell>Li et al. (2020b)</cell><cell cols="2">1.00? 85.98 1.00? 54.84</cell></row><row><cell>Tan et al. (2021)</cell><cell cols="2">1.40? 87.26 22.18? 91.07</cell></row><row><cell cols="3">Shen et al. (2021a) 0.96? 87.41 11.41? 91.98</cell></row><row><cell>PIQN</cell><cell cols="2">2.16? 88.14 30.46? 94.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Inference Speed on ACE04 and NNE. All experiments are conducted on a single NVIDIA RTX A6000 Graphical Card with 48G graphical memory.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>A number of powerful international companies and commercial agencies , such as [ 12 Ito Bureau of [ 15 Japan 15 ]GPE 15 ]ORG , [ 17 Han Hua Group of [ 21 South Korea 22 ]GPE 22 ]ORG , [ 24 Jeffrey Group of [ 27 the US 28 ]GPE 28 ]ORG , [ 30 etc 30 ]ORG 30 ]ORG . participated in this Urumchi Negotiation Meeting .</figDesc><table><row><cell>shows a case study about model predic-</cell></row><row><cell>tions. Our model can recognize nested entities and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In ECCV 2020, pages 213-229, Cham. Springer International Publishing.</figDesc><table><row><cell></cell><cell>Xiaobo Jiang, Kun He, Jiajun He, and Guangyu Yan.</cell></row><row><cell></cell><cell>2021. A new entity extraction method based on ma-</cell></row><row><cell></cell><cell>chine reading comprehension.</cell></row><row><cell></cell><cell>Meizhi Ju, Makoto Miwa, and Sophia Ananiadou.</cell></row><row><cell></cell><cell>2018. A neural layered model for nested named en-</cell></row><row><cell>Billy Chiu, Gamal Crichton, Anna Korhonen, and</cell><cell>tity recognition. In Proceedings of the 2018 Con-</cell></row><row><cell>Sampo Pyysalo. 2016. How to train good word em-</cell><cell>ference of the North American Chapter of the Asso-</cell></row><row><cell>beddings for biomedical NLP. In Proceedings of</cell><cell>ciation for Computational Linguistics, pages 1446-</cell></row><row><cell>the 15th Workshop on Biomedical Natural Language</cell><cell>1459, New Orleans, Louisiana. Association for Com-</cell></row><row><cell>Processing, pages 166-174, Berlin, Germany. Asso-</cell><cell>putational Linguistics.</cell></row><row><cell>ciation for Computational Linguistics.</cell><cell>Arzoo Katiyar and Claire Cardie. 2018. Nested named</cell></row><row><cell>Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-</cell><cell>entity recognition revisited. In Proceedings of the</cell></row><row><cell>jin Wang, and Guoping Hu. 2020. Revisiting pre-</cell><cell>2018 Conference of the North American Chapter</cell></row><row><cell>trained models for Chinese natural language process-</cell><cell>of the Association for Computational Linguistics,</cell></row><row><cell>ing. In Proceedings of the 2020 Conference on Em-</cell><cell>pages 861-871, New Orleans, Louisiana. Associa-</cell></row><row><cell>pirical Methods in Natural Language Processing:</cell><cell>tion for Computational Linguistics.</cell></row><row><cell>Findings, pages 657-668, Online. Association for Computational Linguistics.</cell><cell>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3th Inter-</cell></row><row><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of</cell><cell>national Conference on Learning Representations, ICLR 2021.</cell></row><row><cell>deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171-4186, Min-</cell><cell>Harold W Kuhn. 1955. The hungarian method for the assignment problem. Naval research logistics quar-terly, 2(1-2):83-97.</cell></row><row><cell>neapolis, Minnesota. Association for Computational Linguistics.</cell><cell>Phong Le and Ivan Titov. 2018. Improving entity link-ing by modeling latent relations between mentions.</cell></row><row><cell>Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. 2021. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of the 59th An-</cell><cell>In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1595-1604, Melbourne, Australia. Association for Computational Linguistics.</cell></row><row><cell>nual Meeting of the Association for Computational</cell><cell>Gina-Anne Levow. 2006. The third international Chi-</cell></row><row><cell>Linguistics, pages 3198-3213, Online. Association</cell><cell>nese language processing bakeoff: Word segmen-</cell></row><row><cell>for Computational Linguistics.</cell><cell>tation and named entity recognition. In Proceed-</cell></row><row><cell>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction</cell><cell>ings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108-117, Sydney, Aus-tralia. Association for Computational Linguistics.</cell></row><row><cell>(ACE) program -tasks, data, and evaluation. In</cell><cell>Mike Lewis, Yinhan Liu, Naman Goyal, Mar-</cell></row><row><cell>Proceedings of the Fourth International Conference</cell><cell>jan Ghazvininejad, Abdelrahman Mohamed, Omer</cell></row><row><cell>on Language Resources and Evaluation (LREC'04),</cell><cell>Levy, Veselin Stoyanov, and Luke Zettlemoyer.</cell></row><row><cell>Lisbon, Portugal. European Language Resources As-</cell><cell>2020. BART: Denoising sequence-to-sequence pre-</cell></row><row><cell>sociation (ELRA).</cell><cell>training for natural language generation, translation,</cell></row><row><cell></cell><cell>and comprehension. In Proceedings of the 58th An-</cell></row><row><cell>Octavian-Eugen Ganea and Thomas Hofmann. 2017.</cell><cell>nual Meeting of the Association for Computational</cell></row><row><cell>Deep joint entity disambiguation with local neural</cell><cell>Linguistics, pages 7871-7880, Online. Association</cell></row><row><cell>attention. In Proceedings of the 2017 Conference on</cell><cell>for Computational Linguistics.</cell></row><row><cell>Empirical Methods in Natural Language Processing,</cell><cell></cell></row><row><cell>pages 2619-2629, Copenhagen, Denmark. Associa-</cell><cell>Fei Li, ZhiChao Lin, Meishan Zhang, and Donghong</cell></row><row><cell>tion for Computational Linguistics.</cell><cell>Ji. 2021. A span-based model for joint overlapped</cell></row><row><cell></cell><cell>and discontinuous named entity recognition. In Pro-</cell></row><row><cell>Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-</cell><cell>ceedings of the 59th Annual Meeting of the Asso-</cell></row><row><cell>tional lstm-crf models for sequence tagging. arXiv</cell><cell>ciation for Computational Linguistics, pages 4814-</cell></row><row><cell>preprint arXiv:1508.01991.</cell><cell>4828, Online. Association for Computational Lin-</cell></row><row><cell>Heng Ji, Xiaoman Pan, Boliang Zhang, Joel Noth-</cell><cell>guistics.</cell></row><row><cell>man, James Mayfield, Paul McNamee, and Cash</cell><cell>Qi Li and Heng Ji. 2014. Incremental joint extraction</cell></row><row><cell>Costello. 2017. Overview of TAC-KBP2017 13 lan-</cell><cell>of entity mentions and relations. In Proceedings of</cell></row><row><cell>guages entity discovery and linking. In Proceed-</cell><cell>the 52nd Annual Meeting of the Association for Com-</cell></row><row><cell>ings of the 2017 Text Analysis Conference, TAC</cell><cell>putational Linguistics, pages 402-412, Baltimore,</cell></row><row><cell>2017, Gaithersburg, Maryland, USA, November 13-</cell><cell>Maryland. Association for Computational Linguis-</cell></row><row><cell>14, 2017. NIST.</cell><cell>tics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>LOC, ORG, PER and MISC. Follow Yan et al. (2021); Yu et al. (2020), we train our model on the train and development sets.</figDesc><table><row><cell>KBP17 (Ji et al., 2017) has 5 entity categories,</cell></row><row><cell>including GPE, ORG, PER, LOC, and FAC. We fol-</cell></row><row><cell>low Lin et al. (2019) to split all documents into</cell></row><row><cell>866/20/167 documents for train/dev/test set.</cell></row><row><cell>NNE (Ringland et al., 2019) is a English nested</cell></row><row><cell>NER dataset with 114 fine-grained entity types.</cell></row><row><cell>Follow Wang et al. (2020a), we keep the original</cell></row><row><cell>dataset split and pre-processing.</cell></row><row><cell>FewNERD (Ding et al., 2021) is a large-scale</cell></row><row><cell>English flat NER dataset with 66 fine-grained en-</cell></row><row><cell>tity types. Follow Ding et al. (2021), we adopt a</cell></row><row><cell>standard supervised setting.</cell></row><row><cell>CoNLL03 (Tjong Kim Sang and De Meulder,</cell></row><row><cell>2003) is an English dataset with 4 types of named</cell></row><row><cell>entities: OntoNotes (Pradhan et al., 2013) is an English</cell></row><row><cell>dataset with 18 types of named entity, consisting</cell></row><row><cell>of 11 types and 7 values. We use the same train,</cell></row><row><cell>development, test splits as Li et al. (2020b).</cell></row><row><cell>Chinese MSRA (Levow, 2006) is a Chinese</cell></row><row><cell>dataset with 3 named entity types, including ORG,</cell></row><row><cell>PER, LOC. We keep the original dataset split and</cell></row><row><cell>pre-processing.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Statistics of the nested datasets used in the experiments. #S: the number of sentences, #NS: the number of sentences containing nested entities, #E: the total number of entities, #NE: the number of nested entities, NR: the nesting ratio (%), AL: the average sentence length, #ME: the maximum number of entities in a sentence, #AE: the average number of entities in a sentence</figDesc><table><row><cell></cell><cell></cell><cell>CoNLL03</cell><cell></cell><cell></cell><cell>OntoNotes</cell><cell></cell><cell></cell><cell>FewNERD</cell><cell></cell><cell cols="3">Chinese MSRA</cell></row><row><cell></cell><cell cols="2">Train Dev</cell><cell cols="3">Test Train Dev</cell><cell cols="3">Test Train Dev</cell><cell cols="3">Test Train Dev</cell><cell>Test</cell></row><row><cell>#S</cell><cell cols="12">14041 3250 3453 49706 13900 10348 131965 18824 37648 41728 4636 4365</cell></row><row><cell>#E</cell><cell cols="12">23499 5942 5648 128738 20354 12586 340247 48770 96902 70446 4257 6181</cell></row><row><cell>AL</cell><cell cols="12">14.50 15.80 13.45 24.94 20.11 19.74 24.49 24.61 24.47 46.87 46.17 39.54</cell></row><row><cell>#ME</cell><cell>20</cell><cell>20</cell><cell>31</cell><cell>32</cell><cell>71</cell><cell>21</cell><cell>50</cell><cell>35</cell><cell>49</cell><cell>125</cell><cell>18</cell><cell>461</cell></row><row><cell cols="12">#AE 1.67 1.83 1.64 2.59 1.46 1.22 2.58 2.59 2.57 1.69 0.92</cell><cell>1.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="6">: Localization and Classification Performance</cell></row><row><cell>on ACE04</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">D.3 Analysis of Label Assignment</cell></row><row><cell>(M, Q)</cell><cell cols="2">Loc. F1 Cls. F1</cell><cell>Pr.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell>(60, 15)</cell><cell>91.05</cell><cell>90.15</cell><cell cols="3">87.57 85.67 86.61</cell></row><row><cell>(60, 30)</cell><cell>91.76</cell><cell>90.37</cell><cell cols="3">88.23 86.16 87.18</cell></row><row><cell>(60, 45)</cell><cell>92.23</cell><cell>91.53</cell><cell cols="3">88.48 87.81 88.14</cell></row><row><cell>(60, 50)</cell><cell>92.01</cell><cell>90.81</cell><cell cols="3">87.38 87.12 87.25</cell></row><row><cell>(30, 15)</cell><cell>91.26</cell><cell>89.66</cell><cell cols="3">88.61 84.88 86.70</cell></row><row><cell>(60, 30)</cell><cell>91.76</cell><cell>90.37</cell><cell cols="3">88.23 86.16 87.18</cell></row><row><cell>(90, 45)</cell><cell>91.88</cell><cell>90.56</cell><cell cols="3">88.23 86.46 87.34</cell></row><row><cell>(120, 60)</cell><cell>91.75</cell><cell>90.45</cell><cell cols="3">87.19 86.56 86.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Analysis on Dynamic Label Assignment for different combinations of the number M of instance queries and the total assignable quantity Q of labels.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33013159</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3159" to="3166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognising nested named entities in biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biological, translational, and clinical language processing</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Linear assignment problems and extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>?ela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Combinatorial Optimization</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FLAT: Chinese NER using flatlattice transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.611</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6836" to="6842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequence-to-nuggets: Nested entity mention detection via anchor-region networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1511</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5182" to="5192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bipartite flat-graph network for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.571</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6408" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glyce: Glyph-vectors for chinese character representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Pre-training for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Mengge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6345" to="6354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Labeling gaps between words: Recognizing overlapping mentions with mention separators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldrian</forename><surname>Obaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1276</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2608" to="2618" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The genia corpus: An annotated research abstract corpus in molecular biology domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.5555/1289189.1289260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Language Technology Research</title>
		<meeting>the Second International Conference on Human Language Technology Research<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="82" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
	<note>ICLR 2021</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NNE: A dataset for nested named entity recognition in English newswire</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5176" to="5181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Locate and label: A two-stage identifier for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.216</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2782" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A trigger-sense memory flow framework for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yechun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3449895</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021, WWW &apos;21</title>
		<meeting>the Web Conference 2021, WWW &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1704" to="1715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep exhaustive model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golam</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miwa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6428</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boundary enhanced neural span classification for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9016" to="9023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A sequence-to-set network for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 30th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ace 2005 multilingual training corpus. linguistic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural segmental hypergraphs for overlapping mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramid: A layered model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5918" to="5928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fp-detr: Detection transformer advanced by fully pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving named entity recognition by external context retrieving and cooperative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1800" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HIT: Nested named entity recognition via head-tail pair and token interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziye</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6027" to="6036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Better feature integration for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.271</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3457" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A unified generative framework for various NER subtasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5808" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bottom-up constituency parsing and nested named entity recognition with pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A boundary-aware neural model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandong</forename><surname>Ho-Fung Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chinese medical named entity recognition using crf-mt-adapt and ner-mrc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Bin Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CDS52072.2021.00068</idno>
		<idno>28606 1292 2489 #E 22204 2514 3035 24441 3200 2993 31236 1879 12601 50509 5506 248136 10463 21196 #NE 10149 1092 1417 9389 1112 1118 8773 605 3707 9064 1199 206618 8487 17670 NR 45.71 46.69 45.61 38.41 34.75 37.35 28.09 32.20 29.42 17.95 21.78 83.27 81.11 83.36 AL 22.50 23.02 23.05 19.21 18.93 17.2 19.62 20.61 19.26 25.35 25.99 23.84 24.20 23.80 #ME 28</idno>
	</analytic>
	<monogr>
		<title level="m">2th International Conference on Computing and Data Science</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">446</biblScope>
			<biblScope unit="page" from="362" to="365" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
