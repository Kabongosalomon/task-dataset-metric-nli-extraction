<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Pre-training with Hard Examples Improves Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
							<email>bapeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<email>mingyuan.zhou@mccombs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Pre-training with Hard Examples Improves Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised pre-training (SSP) employs random image transformations to generate training data for visual representation learning. In this paper, we first present a modeling framework that unifies existing SSP methods as learning to predict pseudo-labels. Then, we propose new data augmentation methods of generating training examples whose pseudo-labels are harder to predict than those generated via random image transformations. Specifically, we use adversarial training and CutMix to create hard examples (HEXA) to be used as augmented views for MoCo-v2 and DeepCluster-v2, leading to two variants HEXA MoCo and HEXA DCluster , respectively. In our experiments, we pre-train models on ImageNet and evaluate them on multiple public benchmarks. Our evaluation shows that the two new algorithm variants outperform their original counterparts, and achieve new state-of-the-art on a wide range of tasks where limited task supervision is available for fine-tuning. These results verify that hard examples are instrumental in improving the generalization of the pre-trained models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-supervised visual representation learning aims to learn image features from raw pixels without relying on manual supervisions. Recent results show that self-supervised pre-training (SSP) outperforms state-of-the-art (SoTA) fullysupervised pre-training methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>, and is becoming the building block in many computer vision applications. The pre-trained model produces general-purpose features and serve as the backbone of various downstream tasks such as classification, detection and segmentation, improving the generalization of those task-specific models that are often trained on limited amounts of task labels.</p><p>Most state-of-the-art SSP methods focus on designing novel pretext objectives, ranging from the traditional prototype learning <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b55">56]</ref>, to a recently popular concept known as contrastive learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21]</ref>, and a combination of both <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. Apart from the improved efficiency, all these methods heavily rely on data augmentation to create different views of an image using image transformations, such as random crop (with flip and resize), color distortion, and Gaussian blur. Recent studies show that SSP performance can be further improved by using more aggressively transformed views, such as increasing the number of views <ref type="bibr" target="#b3">[4]</ref>, and more distinctive views via minimizing mutual information <ref type="bibr" target="#b44">[45]</ref>.</p><p>However, image transformations are agnostic to the pretext objectives, and it remains unknown how to augment views specifically based on the pre-training tasks themselves, and how different augmentation methods affect the generalization of the learned models. To tailor data augmentation to pre-training tasks, we explicitly formulate SSP as a problem of predicting pseudo-labels, based on which we propose to generate hard examples (HEXA), a family of augmented views whose pseudo-labels are difficult to predict. Specifically, two schemes are considered. (i) Adversarial examples are created with the intention to cause an SSP model to make prediction mistakes and thus improve the generalization of the model <ref type="bibr" target="#b49">[50]</ref>. (ii) Cut-mixed examples are created via cutting and pasting patches among different images <ref type="bibr" target="#b53">[54]</ref>, so that its content is a mixture of multiple images.</p><p>Our contributions include: (i) A pseudo-label perspective is formulated to motivate the concept of hard examples in self-supervised learning. (ii) Two novel algorithms are proposed, through applying our framework to two distinctly different existing approaches. (iii) Experiment are conducted on a wide range of tasks in self-supervised benchmarks, showing that HEXA consistently improves their original counterparts, and achieves SoTA performance under the same settings. It demonstrates the genericity and effectiveness of proposed framework in constructing hard examples for improving the visual representations using SSP.</p><p>pseudo-labels, and maximizing agreement between pseudolabels and the learned representations. This framework comprises the following four major components <ref type="bibr" target="#b5">[6]</ref>. (i) Data augmentation that randomly transforms any given image x, resulting in multiple correlated views of the same example, denoted as {x i }. (ii) A backbone network h = f ?0 (x) parameterized by ? 0 that extracts a feature representation h ? R d from an augmented viewx. (iii) A projection head z = f ?1 (h) parameterized by ? 1 that maps the feature representation h to a latent representation z, on which self-supervised loss is applied. (iv) A self-supervised loss function aims to predict the pseudo-label y based on z. Different self-supervised learning methods differ in their exploited weak signals, based on which different kinds of pseudo-labels are constructed. We cast a broad family of SSP methods as a pseudo-label classification task, where {? 0 , ? 1 } are the classifier parameters; After training, ? 0 provides generic visual representations. Following this point of view, we revisit two types of methods.</p><p>Type I: Contrastive Learning. Contrastive learning is a framework that learns representations by maximizing agreement between differently augmented views of the same image via a contrastive loss in the latent space. For a given query z q , we identify its positive samples z k + from a set of keys {z k } = {z k + , z k ? }, where positive samples are indexed by k + and negative samples are indexed by k ? . The pseudolabels in contrastive learning are defined by feature pairwise comparisons: y = 1 for the pair (z q , z k + ) and y = 0 for the pair (z q , z k ? ). For a query with K pairs, its pseudo-label vector is y ? {0, 1} K .</p><p>The contrastive prediction task can be formulated as a dictionary look-up problem. By mapping a viewx into a latent representation z using the function composition z = f ? (x) = f ?1 ? f ?0 (x), an effective contrastive loss function, called InfoNCE, can be derived as:</p><formula xml:id="formula_0">min ? L Std C (x q ,x k , y) = min ? ? k y k log exp(f ? (x q )?f ? (x k ) ? ) ? k ? exp(f ? (x q )?f ? (x k ? ) ? ) (1) = min ? ? log exp(f ? (x q )?f ? (x k + ) ? ) ? k ? exp(f ? (x q )?f ? (x k ? ) ? )<label>(2)</label></formula><p>where ? = {? 0 , ? 1 } denotes the set of trainable parameters and ? is a temperature hyper-parameter. From (1) to (2), only the loss term indexed with k + remains, while the ones indexed with k ? are excluded, because their corresponding pseudo-label y = 0.</p><p>In the instance discrimination pretext task (used by MoCo and SimCLR), a query and a key form a positive pair if they are data-augmented versions of the same image, and otherwise form a negative pair. The contrastive loss (2) can be minimized by various mechanisms that differ in how the keys (or negative samples) are maintained <ref type="bibr" target="#b7">[8]</ref>.</p><p>? SimCLR <ref type="bibr" target="#b5">[6]</ref> The negative keys are from the same batch and updated end-to-end by back-propagation. SimCLR is based on this mechanism and requires a large batch to provide a large set of negatives.</p><p>? MoCo <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref> In the MoCo mechanism, the negative keys are maintained in a queue Q, and only the queries and positive keys are encoded in each training batch. A momentum encoder is adopted to improve the representation consistency between the current and earlier keys. MoCo decouples the batch size from the number of negatives. MoCo-v2 <ref type="bibr" target="#b7">[8]</ref> is an improved version using strong augmentation (i.e. more aggressive image transformations) and MLP projection proposed in SimCLR.</p><p>Type II: Prototype Learning. The prototype learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref> introduce a "prototype" as the centroid for a cluster formed by similar image views. The latent representations are fed into a clustering algorithm to produce the prototype/cluster assignments, which are subsequently used as "pseudo-labels" to supervise representation learning.</p><p>DeepCluster is a representative prototype learning work. It employs K-means as the clustering algorithm, which takes a set of latent vectors z = f ? (x) as input, clusters them into K distinct groups with prototypes C, and simultaneously output the optimal cluster assignment y ? ? K as a one-hot probability simplex. The model is trained to predict the optimal assignment:</p><formula xml:id="formula_1">min ? L Std P (x, y) = min ? ? K j=1 y j log exp(f ? (x j ) ? c j ? ) ? j ? exp(f ? (x j ? ) ? c j ? ? ) (3) = min ? ? log exp(f ? (x j * ) ? c j * ? ) ? j ? exp(f ? (x j ? ) ? c j ? ? )<label>(4)</label></formula><p>where c j is jth prototype/cluster centroid, and the j * is the index of the assigned cluster forx. DeepCluster alternates between two steps: feature clustering using K-means and feature learning by predicting these pseudo-labels. The prototype learning improves limitations of the contrastive instance discrimination methods via allowing views with similar semantics but from different source images to be pushed together. Note that (2) and (4) represent a traditional view for selfsupervised learning formulations, while (1) and (3) are our derived pseudo-label view, where y is explicitly involved in the learning objectives. It opens opportunities to study new data augmentationx based on y, in improving the robustness and generalization of ? 0 for visual representations.    The adversarial examplex Adv ("?") fools the model to make a prediction mistake, and the cut-mixed examplex Cmx ("?") is created between two standard augmentations.</p><formula xml:id="formula_2">V x T X u Y d w h / l Z M C i K n l 1 x q s 4 Y 9 C 9 x J 6 R C J q j 1 7 A 8 v S H g W Q Y x c M q 0 7 r p N i N 2 c K B Z d Q l L 1 M Q 8 r 4 L e t D x 9 C Y R a C 7 + f i L g u 4 Z J a B h o k z F S M f q z 4 m c R V o P I 9 9 0 R g x v 9 L Q 3 E v / z O h m G J 9 1 c x G m G E P P v R W E m K S Z 0 F A k N h A K O c m g I 4 0 q Y W y m / Y Y p x N M G V T Q j u 9 M t / S f O g 6 h 5 W D 6 6 O K q f n k z h K Z J v</formula><formula xml:id="formula_3">X Z 0 = " &gt; A A A C B X i c b V C 7 S g N B F J 3 1 G e M r a q n F Y B C s w m 4 U t A y m s Y y Q F 2 R j m J 3 c T Y b M P p i 5 K w l L G h t / x c Z C E V v / w c 6 / c f I o N P H A h T P n 3 M v c e 7 x Y C o 2 2 / W 2 t r K 6 t b 2 x m t r L b O 7 t 7 + 7 m D w 7 q O E s W h x i M Z q a b H N E g R Q g 0 F S m j G C l j g S W h 4 g / L E b z y A 0 i I K q z i K o R 2 w X i h 8 w R k a q Z M 7 c a t C d i F N X c + n w / H 4 P n U R h p i W A / P o 5 P J 2 w Z 6 C L h N n T v J k j k o n 9 + V 2 I 5 4 E E C K X T O u W Y 8 f Y T p l C w S W M s 2 6 i I W Z 8 w H r Q M j R k A</formula><formula xml:id="formula_4">b W E D K E o v C C i t 2 V 5 X X g I t 1 i c Y 1 i W X b v q 1 J w x 6 F / i T k i V T N D s 2 u 9 e m P I 8 h g S 5 Z F p 3 X C d D v 2 A K B Z d Q V r x c Q 8 b 4 g P W g Y 2 j C Y t B + M f 6 i p L t G C W m U K l M J 0 r H 6 f a J g s d b D O D C d M c O + / u 2 N x P + 8 T o 7 R k V + I J M s R E v 6 1 K M o l x Z S O I q G h U M B R D g 1 h X A l z K + V 9 p h h H E 1 z F h O D + f v k v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pre-training with Hard Examples</head><p>Augmented viewsx play a vital role in SSP. Most existing methods synthesize views through random image transformations, without carefully considering their feasibility in completing the self-supervised learning task: predicting pseudo-labels. By contrast, we focus on studying hard examples, which are defined as augmented viewsx whose pseudo-labels y are difficult to be predicted. Specifically, we consider two schemes: adversarial examples and cut-mixed examples. We visually illustrate in <ref type="figure" target="#fig_3">Figure 1</ref> how hard examples are constructed from image transformations, detail the derivation process as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adversarial Examples</head><p>Adversarial robustness refers to a model's invariance to small (often imperceptible) perturbations of its inputs (i.e. clean examples). The adversarial examples are produced by adding perturbations on clean examples to fool the predictions of a trained model the most. In self-supervised learning, we propose to add perturbations on the augmented viewsx to fool their predicted pseudo-labels.</p><p>Adversarial Contrastive Learning. For the instance contrastive discrimination methods, we focus on the MoCo algorithm. Specifically, we propose to generate adversarial examples for queryx q only. Since both keyx k and pseudolabel y are fixed, it is feasible to compute the gradient on the queryx q , leading to the adversarial training objective:</p><formula xml:id="formula_5">min ? L Adv C (x q ,x k , y) = min ? max ? 2? ? log exp(f ? (x q + ?)?f ? (x k + ) ? ) ? k ? exp(f ? (x q + ?)?f ? (x k ? ) ? )<label>(5)</label></formula><p>where is a hyper-parameter governing how invariant the resulting model should be to adversarial attacks, and ? is the perturbation. In practice, (5) is updated using two steps: (i) By applying Projected Gradient Descent (PGD) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>, we obtain adversarial examples on-the-fly:</p><p>x Adv</p><formula xml:id="formula_6">q =x q + ? =x q + ? sign(?x q L Std C (x q ,x k , y)),<label>(6)</label></formula><p>where ? the step size for PGD. The generatedx Adv q can be viewed as a new data augmentation on queryx q , and (ii) we then feedx Adv q into the model to update parameters ?. Not? x Adv q differs from traditional random augmentations in that it takes into consideration of the relationship between the positive examplex k and all negative examples within the memory bank, and tends to be a "harder" query thanx q for the dictionary look-up problem. The adversarial examples for SimCLR is easier to construct, and can be viewed as a special case when all negative examples are from current batch, rather than from memory bank.</p><p>Adversarial Prototype Learning. The adversarial training for prototype-based methods are similar to supervised settings, after the cluster assignments y are learned. We treat these pseudo-labels as targets to fool the model:</p><formula xml:id="formula_7">min ? L Adv P (x, y) = min ? max ? 2? ? log exp(f ? (x j * + ?) ? c j * ? ) ? j ? exp(f ? (x j ? + ?) ? c j ? ) ? )<label>(7)</label></formula><p>Similarly, <ref type="bibr" target="#b6">(7)</ref> is also updated in two steps: starting with adversarial example generation, followed by model update.</p><p>The adversarial examplex Adv j * =x j * + ? is "harder" thanx j * to be correctly aligned into clusters.</p><p>Implementations. It is shown in AdvProp <ref type="bibr" target="#b49">[50]</ref>  # Produce a minibatch of query and key samples <ref type="bibr">3:</ref> Sample a batch of image x from the full dataset; <ref type="bibr">4:</ref> Clean queryx q = T (x) and keyx k = T ? (x);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p># Adversarial example generation <ref type="bibr">6:</ref> Forwardz</p><formula xml:id="formula_8">q = f ? (x q ) andz k = f ? ? (x k ); 7:</formula><p>Synthesize adversarial queryx Adv q using (6); <ref type="bibr">8:</ref> # Cutmixed example generation <ref type="bibr">9:</ref> Synthesize (x Cmx , y Cmx ) using (8); <ref type="bibr">10:</ref> # Update query network <ref type="bibr">11:</ref> Forwardx q ,x Adv q andx Cmx using (10); <ref type="bibr">12:</ref> Compute ?L HEXA C ?? of (10) and update ? ; <ref type="bibr">13:</ref> # Update key network with momentum 14:</p><formula xml:id="formula_9">? ? ? ?? ? + (1 ? ?)?; 15:</formula><p># Update memory bank <ref type="bibr">16:</ref> Queuingz k in Q and dequeuing oldest elements; 17: end for divergence. Thus, we adopt the AdvProp training scheme, where two separate sets of batch normalization (BN) <ref type="bibr" target="#b24">[25]</ref> parameters are considered, summarizing the statistic for clean examples and that for adversarial examples, respectively. In <ref type="figure" target="#fig_3">Figure 1</ref>, we use the dog image as input and visualize the perturbations as noisy grey maps, which are added o? x Std . Thoughx Adv look indistinguishable withx Std visually, their corresponding pseudo-labels have been revised significantly, depending on how much they move across the decision boundary (i.e. how many PGD steps are applied). We study the impact of hyper-parameters in PGD in Appendix, and we choose PGD step as 1 for computational efficiency, perturbation threshold = 1 and step size ? = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cut-Mixed Examples</head><p>Cutmix <ref type="bibr" target="#b53">[54]</ref> is a recent image augmentation technique for supervised learning. Patches are cut and pasted among images to create a new example, where the ground truth labels are also mixed proportionally to the area of the patches. Specifically, for randomly selected two images, we consider an augmented view from (x a , y a ) and (x b , y b ), where y a and y b are the corresponding pseudo-labels (i.e. instance identity in contrast learning or cluster index in prototype learning). The cutmixed examplex Cmx and its pseudo-labels y Cmx are generated using the combining operation as: for a number of training iteration do 3:</p><formula xml:id="formula_10">x Cmx = M ?x a + (1 ? M) ?x b y Cmx = ?y a + (1 ? ?)y b<label>(8)</label></formula><p># Produce a minibatch of samples <ref type="bibr">4:</ref> Sample image batch x from the full dataset; <ref type="bibr">5:</ref> Augmented viewsx = T (x) ; 6:</p><p># Adversarial example generation <ref type="bibr" target="#b6">7</ref>:</p><formula xml:id="formula_11">Forwardz = f ? (x); 8:</formula><p>Synthesizex Adv j * using (7); 9:</p><p># Cutmixed example generation <ref type="bibr">10:</ref> Synthesize (x Cmx , y Cmx ) using (8); <ref type="bibr">11:</ref> # Update network <ref type="bibr">12:</ref> Forwardx,x Adv j * andx Cmx using (10); <ref type="bibr">13:</ref> Compute ?L HEXA P ?? of (10) and update ?; <ref type="bibr">14:</ref> end for 15:</p><p># Update assignment pseudolabel for each image <ref type="bibr">16:</ref> Collectz in above inner-loop; <ref type="bibr">17:</ref> Solve K-means to update C and y for each x; 18: end for where M ? {0, 1} W ?H (width W and height H) denotes a binary mask indicating where to drop out and fill in from two images, 1 is a binary mask filled with ones, ? is elementwise multiplication, ? is the combination ratio between two views. Following <ref type="bibr" target="#b53">[54]</ref>, ? is initially sampled from the beta distribution Beta(?, ?), and is finally set as the area percentage that viewx a occupies inx Cmx . The beta distribution controls how much the two views are mixed. We empirically study hyper-parameters of (?, ?) in Appendix, and use Beta <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b2">3)</ref> in our experiments, which leads to the mean E[?] = 0.62 and standard deviation Std[?] = 0.16.</p><p>Sincex Cmx has mixed contents from two different source images, it tends to be a hard example in predicting either of its labels. The added patches further enhance the localization ability by requiring the model to identify the object from a partial view. To train the model, an objective can be written with the standard loss function as:</p><formula xml:id="formula_12">min ? L Cmx (x Cmx , y Cmx ) = min ? ?L Std (x Cmx , y a ) + (1 ? ?)L Std (x Cmx , y b ) (9)</formula><p>Implementations. In each training iteration, we consider images in the original batch asx a , randomly permute images in a batch to createx b , and generate cut-mixed samples by combining selected examples from two batches with the same index, according to <ref type="bibr" target="#b7">(8)</ref>. For MoCo, we perform cutmix on queries, and leave keys unchanged. When multiple crops are considered for each image in DeepCluster, the same permutation index is shared among the crops. In <ref type="figure" target="#fig_3">Figure 1</ref>, transformations on dog image arex a , transformations on cat image arex b . The cut-mixed examplesx Cmx are "dogcat" images shown on the right side of <ref type="figure" target="#fig_3">Figure 1</ref>(a). One may imaginex Cmx often lies at the decision boundary, depending on how much content is mixed from each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Full HEXA Objective</head><p>The overall self-adversarial training objective considers both clean and hard examples constructed by adversarial and cutmix augmentations:</p><formula xml:id="formula_13">min ? L HEXA = L Std + ? 1 L Adv + ? 2 L Cmx<label>(10)</label></formula><p>where ? 1 and ? 2 are the weighting hyper-parameters to control the effect of adversarial examples and cutmixed examples, respectively. In our experiments, we set ? 1 = 1 and/or ? 2 = 1. Note that ? 1 = ? 2 = 0 reduces the objective to the standard self-supervised training algorithms. Concretely, we consider two novel algorithms:</p><p>? HEXA MoCo By plugging terms <ref type="formula" target="#formula_0">(2) (8)</ref> and <ref type="formula" target="#formula_5">(5)</ref> into <ref type="formula" target="#formula_13">(10)</ref>, it yields the full self-adversarial contrastive learning objective denoted as L HEXA C . The HEXA MoCo training procedure is detailed in Algorithm 1. We build HEXA MoCo on top of MoCo-v2. The two algorithms are distinguished from each other in Lines 5-9, where hard examples are computed on query and subsequently employed in model update for HEXA MoCo .</p><p>? HEXA DCluster The full self-adversarial prototype learning objective L HEXA P is obtained via plugging (4)(8) and <ref type="bibr" target="#b6">(7)</ref> into <ref type="bibr" target="#b9">(10)</ref>. We build HEXA DCluster based on DeepCluster-v2 <ref type="bibr" target="#b29">[30]</ref>, which improves DeepCluster <ref type="bibr" target="#b2">[3]</ref> to reach similar performance with recent state-of-theart methods. The HEXA DeepCluster training procedure is detailed in Algorithm 2. It differs from DeepCluster-v2 in Lines 6-10, where hard examples are computed to train the network in conjunction with clean examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-supervised Pre-training</head><p>Pretext task taxonomy. Self-supervised learning is a popular form of unsupervised learning, where labels annotated by humans are replaced by "pseudo-labels" directly extracted from the raw input data by leveraging its intrinsic structures. We broadly categorize existing self-supervised learning methods into three classes: (i) Handcrafted pretext tasks. This includes many traditional self-supervised methods such as relative position of patches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>, masked pixel/patch prediction <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref>, auto-regressive modeling <ref type="bibr" target="#b4">[5]</ref> , rotation prediction <ref type="bibr" target="#b17">[18]</ref>, image colorization <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b28">29]</ref>, cross-channel prediction <ref type="bibr" target="#b58">[59]</ref> and generative modeling <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">13]</ref>. These approaches typically exploit domain knowledge to carefully design a pretext task, with the learned features often focusing on one certain aspect of images, leading to a limited transfer ability. (ii) Contrastive learning. The instance-level classification task is considered <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b59">60]</ref>, where each image in a dataset is treated as a unique class, and various augmented views of an image are the examples to be classified. Some recent works in this line are CPC <ref type="bibr" target="#b37">[38]</ref>, deep Info-Max <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1]</ref>, MoCo <ref type="bibr" target="#b21">[22]</ref>, SimCLR <ref type="bibr" target="#b5">[6]</ref>, BYOL <ref type="bibr" target="#b20">[21]</ref> etc. (iii) Prototype learning. Clustering is employed for deep representation learning, including DeepCluster <ref type="bibr" target="#b2">[3]</ref>, SwAV <ref type="bibr" target="#b3">[4]</ref> and PCL <ref type="bibr" target="#b29">[30]</ref>, among many others <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b55">56]</ref>. The proposed HEXA can be generally applied to all three classes in principle, as long as the notation of pseudo-labels exists. In this paper, we focus on the latter two classes, as they have shown SoTA representation learning performance, surpassing the ImageNet-supervised counterpart in multiple downstream vision tasks.</p><p>The role of augmentations. Image data augmentations/transformations such as crop and blurring play a crucial role in modern self-supervised learning pipeline. It has been empirically shown that visual representations can be improved by employing stronger image transformations <ref type="bibr" target="#b7">[8]</ref> and increasing the number of augmented views of an image <ref type="bibr" target="#b3">[4]</ref>. InfoMin <ref type="bibr" target="#b44">[45]</ref> studied the principles of good views for contrastive learning, and suggested to select views with less mutual information. By definition, adversarial and cut-mixed examples tend to be harder examples than transformationaugmented ones for self-supervised problems, and are complementary to the above techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hard Examples</head><p>Robustness. A vast majority of works commonly view adversarial examples as a threat to models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>, and suggest training with adversarial examples leads to accuracy drop on clean data <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b33">34]</ref>. Adversarial training have been studied for self-supervised pre-training <ref type="bibr" target="#b6">[7]</ref>. Our work is significantly different from Chen et al. <ref type="bibr" target="#b6">[7]</ref> in two aspects: (i) Motivations -We aim to use adversarial examples to boost standard recognition accuracy on large-scale datasets such as Ima-geNet, while Chen et al. <ref type="bibr" target="#b6">[7]</ref> mainly study model robustness on small datasets such as CIFAR-10. (ii) Algorithms -We focus on the modern contrastive/prototype learning methods (last two categories of SSP methods in Section 4.1), while Chen et al. <ref type="bibr" target="#b6">[7]</ref> work on traditional handcrafted SSP methods (the first category).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improved standard accuracy. Hard examples have been</head><p>shown to be effective in improving recognition accuracy in supervised learning settings. For adversarial examples, one early attempt is virtual adversarial training (VAT) <ref type="bibr" target="#b35">[36]</ref>, a regularization method that improves semi-supervised learning tasks. The success was recently extended to natural language processing <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref> and vision-and-language tasks <ref type="bibr" target="#b16">[17]</ref>. In computer vision, AdvProp <ref type="bibr" target="#b49">[50]</ref> is a recent work showing that adversarial examples improve recognition accuracy on ImageNet in supervised settings. Hadi et al. further show that adversarially robust ImageNet models transfer better <ref type="bibr" target="#b42">[43]</ref>. For cut-mixed examples, it was first studied by Yun et al. <ref type="bibr" target="#b53">[54]</ref>. Similar augmentation schemes using a mixture of images include mix-up <ref type="bibr" target="#b56">[57]</ref>, cut-out <ref type="bibr" target="#b10">[11]</ref> etc. All above hard examples are constructed in the supervised settings, our HEXA is the first work to systematically study hard examples in large-scale self-supervised settings, due to the proposed pseudo-label formulation. We confirm that hard examples improve the model's transfer ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>All of our study for unsupervised pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset <ref type="bibr" target="#b9">[10]</ref>. We implement HEXA MoCo based on the pre-training scheldule of MoCo-v2, and implement HEXA Dcluster based on the pre-training scheldule of DeepCluster-v2. Both use the cosine learning rate and MLP projection head. Due to the limit of computational resource, all experiments are conducted with ResNet-50 and pre-trained in 200/800 epochs if not specifically mentioned. Once the model is pre-trained, we follow the same finetuning protocols/schedules with the baseline methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>. Following common practice in evaluating pre-trained visual representations, we test the model's transfer learning ability on a wide range of datasets/tasks in the self-supervised learning benchmark <ref type="bibr" target="#b19">[20]</ref>, based on the principle that a good representation should transfer with limited supervision and limited fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">On the impact of different hard examples</head><p>To understand different design choices in our framework, we compare different schemes to add hard examples into SSP. In what follows, we denote HEXA MoCo and HEXA Dcluster as two variants that are both constructed with 2 random crops at resolution 224 and adversarial examples. More specifically, HEXA MoCo follows MoCo-v2: one crop for query and the other for key; HEXA Dcluster is always compared with the DeepCluster-v2 variant with 2 crops. The current SoTA method is SwAV <ref type="bibr" target="#b3">[4]</ref>, which employs 8 random crops: 2 crops at resolution 224 and 6 crops at resolution 96. To compare with SoTA, we also increase the number of crops to 8 and consider two variants: HEXA Dcluster (8-crop) is with adversarial examples, and HEXA + Dcluster (8-crop) is constructed with both adversarial and cut-mixed examples. All 2-crop methods use a mini-batch size of B=256, and 8-crop methods use a mini-batch size of B=4096.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Linear classification</head><p>To evaluate the learned representations, we first follow the widely used linear evaluation protocol, where a linear classifier is trained on top of the frozen base network, and test accuracy is used as a proxy for representations. We follow previous setup <ref type="bibr" target="#b19">[20]</ref> and evaluate the performance of such linear classifiers on four datasets, including ImageNet <ref type="bibr" target="#b9">[10]</ref>, PASCAL VOC2007 (VOC07) <ref type="bibr" target="#b14">[15]</ref>, CIFAR10 (C10) and CIFAR100 (C100) <ref type="bibr" target="#b27">[28]</ref>. A softmax classifier is trained for ImageNet/CIFAR, while a linear SVM <ref type="bibr" target="#b15">[16]</ref> is trained for VOC07. We report 1-crop (224 ? 224), Top-1 validation accuracy for ImageNet/CIFAR and mAP for VOC07.  <ref type="table" target="#tab_2">Table 1</ref>: Linear classification performance on learned representations using ResNet-50. All numbers for baselines are from their corresponding papers or <ref type="bibr" target="#b29">[30]</ref> , except that we use the released pretrained model for SwAV. interesting to observe that DeepCluster-v2 is slightly better than MoCo-v2, indicating that the traditional prototype methods can be on par with the popular contrastive methods, with the same pre-training epochs and data augmentation strategies. We hope this result can inspire future research to more carefully select different pretext objectives. By contrast, HEXA variants consistently outperform theirs counterparts for both contrastive and prototype methods, demonstrating that the proposed hard examples can effectively improve  learned visual representations in SSP. We also pre-train HEXA MoCo with 800 epochs, a longer schedule used in MoCo-v2 <ref type="bibr" target="#b7">[8]</ref>. The learning curves are compared in <ref type="figure" target="#fig_5">Figure 3</ref>(a). HEXA is consistently better than MoCo-v2 and the gap is larger at the beginning. We hypothesize that the augmentation space is more efficiently explored with hard examples than with traditional image transformations, but this advantage is less reflected in improved recognition accuracy, when the augmentation space is gradually fully occupied at the end of training. When comparing with SoTA methods equipped with multi-crop <ref type="bibr" target="#b3">[4]</ref>, we see that HEXA Dcluster <ref type="bibr">(8-crop)</ref> achieves slightly better than SwAV on ImageNet, and even outperforms InfoMin with 800 pretraining steps. By plotting the training curves of their linear classifiers in <ref type="figure" target="#fig_5">Figure 3</ref>(b), we observe that HEXA Dcluster (8crop) clearly outperforms SwAV with limited fine-tuning (e.g. &lt;20 epochs training). The advantage of HEXA is more significantly than SwAV with limit supervision, this can be seen from a larger performance gap on VOC07 in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Low-shot classification. We evaluate the learned representation on image classification tasks with few training samples per-category. We follow the setup in Goyal et al. <ref type="bibr" target="#b19">[20]</ref> and train linear SVMs using fixed representations on VOC07 for object classification. We vary the number k of training samples per-class and report the average result across 5 independent runs. The results are shown in <ref type="table" target="#tab_4">Table 2</ref>. Hard examples help improve the performance for both contrastive and prototype learning, especially when k ? 8. This is probably because the performance is very sensitive to the choice of selected labelled samples when k ? 4, rendering the eval-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semi-supervised learning on ImageNet</head><p>We perform semi-supervised learning experiments to evaluate whether the learned representation can provide a good basis for fine-tuning. Following the setup from Chen et al. <ref type="bibr" target="#b5">[6]</ref>, we select a subset (1% or 10%) of ImageNet training data (the same labelled images with Chenet al. <ref type="bibr" target="#b5">[6]</ref>), and fine-tune the entire self-supervised trained model on these subsets. For the proposed HEXA, and we fine-tune the models using the same schedule. SwAV with 8 augmentation crops and 200 pre-training epochs is used a fair baseline. We also fine-tune over 100% of ImageNet labels for 20 epochs, and HEXA reaches 78.6% Top-1 accuracy, outperforming the supervised approach (76.5%) using the same ResNet-50 architecture by a large margin (2.1% absolute recognition accuracy). HEXA also achieves higher performance compared with all existing self-supervised learning methods in both 200 and 800 pre-training epochs settings. This shows that hard examples can effectively improve SSP, which can be viewed as a promising approach to further improve standard supervised learning such as Big Transfer <ref type="bibr" target="#b26">[27]</ref> in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Object detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Epoch AP AP50 AP75 Supervised -53.5 81. <ref type="bibr" target="#b2">3</ref>   It is standard practice in data-scarce object detection tasks to initialize earlier model layers with the weights from ImageNet-trained networks. We study the benefits of using hard-examples-trained networks to initialize object detection. On the VOC object detection task, a Faster R-CNN detector <ref type="bibr" target="#b41">[42]</ref> is fine-tuned end-to-end on the VOC 07+12 trainval set1 and evaluated on the VOC 07 test set using the COCO suite of metrics <ref type="bibr" target="#b30">[31]</ref>. The results are shown in Table 4. We find that HEXA consistently outperforms MoCo-v2 that is pre-trained with standard image transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a comprehensive study of utilizing hard examples to improve visual representations for image self-supervised learning. By treating SSP as a pseudo-label classification task, we introduce a general framework to generate harder augmented views to boost the discriminative power of self-supervised learned models. Two novel algorithmic variants are proposed: HEXA MoCo for contrastive learning and HEXA DCluster for prototype learning. Our HEXA variants outperform their counterparts, often by a notable margin, and achieve SoTA under the same settings. Future research directions include incorporating more advanced hard examples under this framework, and exploring their performance with larger networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyper-parameter Choice</head><p>Adversarial examples. We study the hyper-parameter choices attack perturbation threshold and PGD step size ? in adversarial images. For HEXA DCluster , we grid search over = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> and ? = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Each variant is pre-trained for 40 epochs. A linear classifier is added on the pre-trained checkpoint and trained for 100 epochs. The results are shown <ref type="figure" target="#fig_7">Figure 4</ref>. Adding too large perturbations ( = 3, ? = 3) can hurt model performance significantly. Otherwise, the model perform similarly with differnt ways of adding small perturbations, allowing either a large threshold with a small step size, or a large step size with a small threshold. We used ( = 1, ? = 1) for convenience.  Cut-mixed examples. We study the hyper-parameter choices in Beta(?, ?) in cut-mixed images. We consider 6 random crops for each image: 2 crops at resolution 160 and 4 crops at resolution 96. The model is pre-trained in 5 epoch, and a linear classification on the checkpoint is trained for 1 epoch. The results are shown in <ref type="figure" target="#fig_8">Figure 5</ref>. Cut-mixed examples in various settings improves performance. We used Beta <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b2">3)</ref> in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments details for transfer learning</head><p>Linear classification on ImageNet The main network is fixed, and global average pooling features (2048-D) of ResNet-50 are extracted. We train for 100 epochs. For HEXA MoCo , we follow the block-decay training schedule of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref> with an initial learning rate of 30.0 and step decay with a factor of 0.1 at <ref type="bibr" target="#b59">[60,</ref><ref type="bibr">80]</ref>. For HEXA DCluster , we follow the cosine-decay training schedule of <ref type="bibr" target="#b3">[4]</ref> with an initial learning rate of 0.3. The logistic regression classifier is trained using SGD with a momentum of 0.9.</p><p>Linear classification on VOC07 For training linear SVMs on VOC07, we follow the procedure in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref> and use the LIBLINEAR package <ref type="bibr" target="#b15">[16]</ref>. We pre-process all images by resizing to 256 pixels along the shorter side and taking a 224 ? 224 center crop. The linear SVMs are trained on the global average pooling features of ResNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear classification on Cifar10 and Cifar100</head><p>We trained a linear classifier on features extracted from the frozen pre-trained network. We used Adamax to optimize the softmax cross-entropy objective for 20 epochs, a batch size of 256, a learning rate [0.1,0.01,0.001] and decay at <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> with a factor of 0.1. All images were resized to 224 pixels (after which we took a 224 ? 224 center crop), and we did not apply data augmentation.</p><p>Semi-supervised learning on ImageNet We follow <ref type="bibr" target="#b3">[4]</ref> to finetune ResNet-50 with pretrained weights on a subset of ImageNet with labels. We optimize the model with SGD, using a batch size of 256, a momentum of 0.9, and a weight decay of 0.0005. We apply different learning rate to the ConvNet and the linear classifier. The learning rate for the ConvNet is 0.01, and the learning rate for the classifier is 0.1 (for 10% labels) or 1 (for 1% labels). We train for 20 epochs, and drop the learning rate by 0.2 at 12 and 16 epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " z U V H 7 0 2 O x b W s s T K s 4 3 M X 1 I N 0 X Z 0 = " &gt; A A A C B X i c b V C 7 S g N B F J 3 1 G e M r a q n F Y B C s w m 4 U t A y m s Y y Q F 2 R j m J 3 c T Y b M P p i 5 K w l L G h t / x c Z C E V v / w c 6 / c f I o N P H A h T P n 3 M v c e 7 x Y C o 2 2 / W 2 t r K 6 t b 2 x m t r L b O 7 t 7 + 7 m D w 7 q O E s W h x i M Z q a b H N E g R Q g 0 F S m j G C l j g S W h 4 g / L E b z y A 0 i I K q z i K o R 2 w X i h 8 w R k a q Z M 7 c a t C d i F N X c + n w / H 4 P n U R h p i W A / P o 5 P J 2 w Z 6 C L h N n T v J k j k o n 9 + V 2 I 5 4 E E C K X T O u W Y 8 f Y T p l C w S W M s 2 6 i I W Z 8 w H r Q M j R k A e h 2 O r 1 i T M + M 0 q V + p E y F S K f q 7 4 m U B V q P A s 9 0 B g z 7 e t G b i P 9 5 r Q T 9 6 3 Y q w j h B C P n s I z + R F C M 6 i Y R 2 h Q K O c m Q I 4 0 q Y X S n v M 8 U 4 m u C y J g R n 8 e R l U i 8 W n I t C 8 e 4 y X 7 q Z x 5 E h x + S U n B O H X J E S u S U V U i O c P J J n 8 k r e r C f r x X q 3 P m a t K 9 Z 8 5 o j 8 g f X 5 A 7 I i m V g = &lt; / l a t e x i t &gt; x Adv &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 9 A D h c X / c p X y s a / x K X L 0 J K 7 uT q A = " &gt; A A A C B X i c b V C 5 T s N A E F 1 z h n A Z K K F Y E S F R R T Y g Q c n R U A Y p l x S H a L 0 e h x X r Q 7 v j i M h y Q 8 O v 0 F C A E C 3 / Q M f f s A k p I P C k k Z 7 e m 9 H M P D + V Q q P j f F o z s 3 P z C 4 u l p f L y y u r a u r 2 x 2 d R J p j g 0 e C I T 1 f a Z B i l i a K B A C e 1 U A Y t 8 C S 3 / 9 m L k t w a g t E j i O g 5 T 6 E a s H 4 t Q c I Z G 6 t k 7 X l 3 I A P L c 8 0 N 6 V x T X u Y d w h / l Z M C i K n l 1 x q s 4 Y 9 C 9 x J 6 R C J q j 1 7 A 8 v S H g W Q Y x c M q 0 7 r p N i N 2 c K B Z d Q l L 1 M Q 8 r 4 L e t D x 9 C Y R a C 7 + f i L g u 4 Z J a B h o k z F S M f q z 4 m c R V o P I 9 9 0 R g x v 9 L Q 3 E v / z O h m G J 9 1 c x G m G E P P v R W E m K S Z 0 F A k N h A K O c m g I 4 0 q Y W y m / Y Y p x N M G V T Qj u 9 M t / S f O g 6 h 5 W D 6 6 O K q f n k z h K Z J v s k n 3 i k m N y S i 5 J j T Q I J / f k k T y T F + v B e r J e r b f v 1 h l r M r N F f s F 6 / w K e R 5 l L &lt; / l a t e x i t &gt; x Std &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 N h l Z i m o n i t e t Z 7 g W T F f A x J 7 g L g = " &gt; A A A C B X i c b V C 7 T s N A E D z z J r w M l F C c i J C o I j s g Q R l B Q x l E A k i x i c 7 n d X L K + a G 7 N S K y 3 N D w K z Q U I E T L P 9 D x N 1 x C C l 4 j r T S a 2 d X u T p B J o d F x P q y p 6 Z n Z u f m F x c r S 8 s r q m r 2 + c a H T X H F o 8 1 S m 6 i p g G q R I o I 0 C J V x l C l g c S L g M B i c j / / I G l B Z p 0 s J h B n 7 M e o m I B G d o p K 6 9 7 b W E D K E o v C C i t 2 V 5 X X g I t 1 i c Y 1 i W X b v q 1 J w x 6 F / i T k i V T N D s 2 u 9 e m P I 8 h g S 5 Z F p 3 X C d D v 2 A K B Z d Q V r x c Q 8 b 4 g P W g Y 2 j C Y t B + M f 6 i p L t G C W m U K l M J 0 r H 6 f a J g s d b D O D C d M c O + / u 2 N x P + 8 T o 7 R k V + I J M s R E v 6 1 K M o l x Z S O I q G h U M B R D g 1 h X A l z K + V 9 p h h H E 1 z F h O D + f v k v u a j X 3 P 1 a / e y g 2 j i e x L F A t s g O 2 S M u O S Q N c k q a p E 0 4 u S M P 5 I k 8 W / f W o / V i v X 6 1 T l m T m U 3 y A 9 b b J 7 b b m V s = &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y J N t r X U j R G c o i O + c f r N S r y W T G T k = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 2 I J / R F e P C j i 1 d / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c i 1 E b F 6 w E n C / Y g O l Q g F o 2 i l d t Y L Q v I 0 7 Z c r b t W d g 6 w S L y c V y N H o l 7 9 6 g 5 i l E V f I J D W m 6 7 k J + h n V K J j k 0 1 I v N T y h b E y H v G u p o h E 3 f j Y / d 0 r O r D I g Y a x t K S R z 9 f d E R i N j J l F g O y O K I 7 P s z c T / v G 6 K 4 b W f C Z W k y B V b L A p T S T A m s 9 / J Q G j O U E 4 s o U w L e y t h I 6 o p Q 5 t Q y Y b g L b + 8 S l q 1 q n d R r d 1 f V u o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w z O 8 w p u T O C / O u / O x a C 0 4 + c w x / I H z + Q M w W I 9 4 &lt; / l a t e x i t &gt;x Adv &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 9 A D h c X / c p X y s a / x K X L 0 J K 7 u T q A = " &gt; A A A C B X i c b V C 5 T s N A E F 1 z h n A Z K K F Y E S F R R T Y g Q c n R U A Y p l x S H a L 0 e h x X r Q 7 v j i M h y Q 8 O v 0 F C A E C 3 / Q M f f s A k p I P C k k Z 7 e m 9 H M P D + V Q q P j f F o z s 3 P z C 4 u l p f L y y u r a u r 2 x 2 d R J p j g 0 e C I T 1 f a Z B i l i a K B A C e 1 U A Y t 8 C S 3 / 9 m L k t w a g t E j i O g 5 T 6 E a s H 4 t Q c I Z G 6 t k 7 X l 3 I A P L c 8 0 N 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>s k n 3 i k m N y S i 5 J j T Q I J / f k k T y T F + v B e r J e r b f v 1 h l r M r N F f s F 6 / w K e R 5 l L &lt; / l a t e x i t &gt;x Cmx &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z U V H 7 0 2 O x b W s s T K s 4 3 M X 1 I N 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e h 2 O r 1 i T M + M 0 q V + p E y F S K f q 7 4 m U B V q P A s 9 0 B g z 7 e t G b i P 9 5 r Q T 9 6 3 Y q w j h B C P n s I z + R F C M 6 i Y R 2 h Q K O c m Q I 4 0 q Y X S n v M 8 U 4 m u C y J g R n 8 e R l U i 8 W n I t C 8 e 4 y X 7 q Z x 5 E h x + S U n B O H X J E S u S U V U i O c P J J n 8 k r e r C f r x X q 3 P m a t K 9 Z 8 5 o j 8 g f X 5 A 7 I i m V g = &lt; / l a t e x i t &gt; x Std &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 N h l Z i m o n i t e t Z 7 g W T F f A x J 7 g L g = " &gt; A A A C B X i c b V C 7 T s N A E D z z J r w M l F C c i J C o I j s g Q R l B Q x l E A k i x i c 7 n d X L K + a G 7 N S K y 3 N D w K z Q U I E T L P 9 D x N 1 x C C l 4 j r T S a 2 d X u T p B J o d F x P q y p 6 Z n Z u f m F x c r S 8 s r q m r 2 + c a H T X H F o 8 1 S m 6 i p g G q R I o I 0 C J V x l C l g c S L g M B i c j / / I G l B Z p 0 s J h B n 7 M e o m I B G d o p K 6 9 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>u a j X 3 P 1 a / e y g 2 j i e x L F A t s g O 2 S M u O S Q N c k q a p E 0 4 u S M P 5 I k 8 W / f W o / V i v X 6 1 T l m T m U 3 y A 9 b b J 7 b b m V s = &lt; / l a t e x i t &gt; (a) Image transformations v.s. Hard examples (b) Augmented view space Illustration of HEXA: (a) Hard examples. For the original dog image, existing SSP methods employ random transformations to generate augmented examplex Std , we propose two types of hard examples. Adversarial examplesx Adv add perturbations onx Std and cut-mixed examplesx Cmx cut and paste patches betweenx Std . (b) A visualization example of augmented view space. Each circle " u " indicates an augmented examplex Std .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>(i) Std baseline: Only standard random transformations are used, i.e., ? 1 = ? 2 = 0; (ii) Std + Adv: Adversarial examples are added into Std, i.e., ? 1 = 1 and ? 2 = 0; (iii) Std + Cmt: Cutmixed examples are computed onx Std and then added, i.e., ? 1 = 0 and ? 2 = 1; (iv) Std + Adv + Cmt: Both hard examples are added, i.e., ? 1 = ? 2 = 1; (v) Std + Adv + Cmt A : As an ablation choice, we consider computing cutmixed examples on adversarial viewsx Adv , denoted as Cmx A ; (vi) Std + Adv + Cmt + Cmt A : All types of hard examples are added.We conduct the comparison experiments with a small number of pre-training steps on ImageNet. For HEXA MoCo , we pre-train for 20 epochs. For HEXA Dcluster , we pre-train for 5 epochs, but with 6 crops per image: 2 crops at resolution 160 and 4 crops at resolution 96. The last checkpoint is employed to extract features, on which a linear classifier is trained for 1 epoch on ImageNet. The results are reported S t d B a s e l i n e S Impact of different hard example combination schemes in HEXA.inFigure 2. Interestingly, cut-mixed examples computed o? x Std are more effective than those onx Adv . This is expected, as the ground-truth label ofx Adv should be different from x Std , the mixed label of the latter can not reflect ground-truth label of the former. Further, both adversarial and cut-mixed examples can improve the baseline method, regardless of whether they are added separately or simultaneously, showing the effectiveness of the proposed methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Learning curves on ImageNet. (a) For 800 pretraining epochs of contrastive methods, the Top-1 accuracy is measured for checkpoints at every 200 epochs. (b) Training a linear classifier on the 200th checkpoint produced by prototype methods for 100 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Top-1 accuracy on ImageNet is measured for different adversarial attack settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>0, =1.0), mean=0.50 std=0.29 ( =5.0, =0.1), mean=0.98 std=0.06 ( =5.0, =0.5), mean=0.91 std=0.11 ( =5.0, =1.0), mean=0.83 std=0.14 ( =5.0, =2.0), mean=0.71 std=0.16 ( =5.0, =3.0), mean=0.62 std=0.16 ( =5.0, =4.0), mean=0.56 std=0.16 ( =5.0, =5.0), mean=0.50 std=0.15 (a) PDF of Beta distribution S t d B a s e l i n e B e t a ( 1 , 1 ) B e t a ( 5 , 0 . 1 ) B e t a ( 5 , 0 . 5 ) B e t a ( 5 , 1 ) B e t a ( 5 , 2 ) B e t a ( 5 , 3 ) B e t a ( 5 , 4 ) B e t a ( 5 The impact of hyper-parameters in mixing two images, measured by Top-1 accuracy on ImageNet. (a) The probability density function (PDF) of a Beta distribution. (b) Accuracy of HEXA DCluster checkpoints with 5 pre-training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>that clean examples and adversarial examples tend to have different batch statistics, due to their salient empirical distribution Algorithm 1 HEXA MoCo Require: Initializing network parameters for query ? and key ? ? ; Random image transformations T and T ? ; A queue Q for memory bank, with momentum decay coefficient ? = 0.99. 1: for a number of training iterations do</figDesc><table><row><cell>2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 HEXA DClusterRequire: Initializing network parameters ?; Random image transformations T ; Initializing a set of prototypes C and compute initial assignment y. 1: for a number of training epoch do</figDesc><table><row><cell>2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Epoch</cell><cell cols="4">ImageNet VOC07 C10 C100</cell></row><row><cell>Supervised</cell><cell>-</cell><cell>76.5</cell><cell>87.5</cell><cell cols="2">93.6 78.3</cell></row><row><cell>Instance D. [49]</cell><cell>200</cell><cell>54.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Jigsaw [37]</cell><cell>90</cell><cell>45.7</cell><cell>64.5</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBiGAN [13]</cell><cell>-</cell><cell>56.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CPC-v2 [23]</cell><cell>200</cell><cell>63.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CMC [44]</cell><cell>200</cell><cell>66.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SimCLR [6]</cell><cell>200</cell><cell>61.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SimCLR [6]</cell><cell>1000</cell><cell>69.3</cell><cell>80.5</cell><cell cols="2">90.6 71.6</cell></row><row><cell>MoCo [22]</cell><cell>200</cell><cell>60.6</cell><cell>79.2</cell><cell>-</cell><cell>-</cell></row><row><cell>PIRL [35]</cell><cell>800</cell><cell>63.6</cell><cell>81.1</cell><cell>-</cell><cell>-</cell></row><row><cell>PCL-v2 [30]</cell><cell>200</cell><cell>67.6</cell><cell>85.4</cell><cell>-</cell><cell></cell></row><row><cell>BYOL [21]</cell><cell>800</cell><cell>74.3</cell><cell>-</cell><cell cols="2">91.3 78.4</cell></row><row><cell>SwAV(B=256)[4]</cell><cell>200</cell><cell>72.7</cell><cell>87.5</cell><cell cols="2">91.8 74.2</cell></row><row><cell>SwAV(B=4096)[4]</cell><cell>200</cell><cell>73.9</cell><cell>87.9</cell><cell cols="2">92.0 76.0</cell></row><row><cell>SwAV(B=4096)[4]</cell><cell>800</cell><cell>75.3</cell><cell>88.1</cell><cell cols="2">93.1 77.0</cell></row><row><cell>InfoMin [45]</cell><cell>200</cell><cell>70.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>InfoMin [45]</cell><cell>800</cell><cell>73.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MoCo-v2</cell><cell>200</cell><cell>67.5</cell><cell>84.5</cell><cell cols="2">89.4 70.1</cell></row><row><cell>HEXA MoCo</cell><cell>200</cell><cell>68.9</cell><cell>85.0</cell><cell cols="2">90.4 71.5</cell></row><row><cell>MoCo-v2</cell><cell>800</cell><cell>71.1</cell><cell>86.8</cell><cell cols="2">90.6 71.8</cell></row><row><cell>HEXA MoCo</cell><cell>800</cell><cell>71.7</cell><cell>87.0</cell><cell cols="2">91.5 73.2</cell></row><row><cell>DeepCluster-v2</cell><cell>200</cell><cell>67.6</cell><cell>85.4</cell><cell cols="2">89.6 70.9</cell></row><row><cell>HEXA DCluster</cell><cell>200</cell><cell>68.1</cell><cell>85.9</cell><cell cols="2">90.7 71.5</cell></row><row><cell>HEXA DCluster (8-crop)</cell><cell>200</cell><cell>74.0</cell><cell>88.1</cell><cell cols="2">92.9 76.6</cell></row><row><cell>HEXA + DCluster (8-crop)</cell><cell>200</cell><cell>73.4</cell><cell>88.8</cell><cell cols="2">91.9 75.2</cell></row><row><cell>DeepCluster-v2 (8-crop)</cell><cell>800</cell><cell>75.2</cell><cell>87.6</cell><cell cols="2">93.2 77.3</cell></row><row><cell>HEXA DCluster (8-crop)</cell><cell>800</cell><cell>75.5</cell><cell>87.9</cell><cell cols="2">93.4 78.6</cell></row><row><cell>HEXA + DCluster (8-crop)</cell><cell>800</cell><cell>75.1</cell><cell>88.2</cell><cell cols="2">93.5 78.0</cell></row></table><note>the results of linear classification. It is</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Low-shot classification on VOC07 using linear SVMs trained on fixed representations. We vary the number of labeled examples k per class and report the mAP across 5 runs. All baseline numbers are from<ref type="bibr" target="#b29">[30]</ref> except that we use the released pretrained model for SwAV.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Semi-supervised classification on ImageNet. We use the released pretrained model for MoCo/SwAV. All other numbers are adopted from corresponding papers. uation less stable. Pre-training longer (MoCo-v2 with 800 epochs) helps reduce this issue, and the proposed hard examples can further boost the performance. When k ? 32 samples are considered, the proposed scheme surpasses the ImageNet-supervised pre-training approach. To the best of our knowledge, HEXA is the first work to surpasses the supervised baseline with such a small number of labelled samples on VOC07, showing high sample-efficiency of the learned representations. HEXA pre-trained at 200 epochs also outperforms SwAV (pre-trained at both 200 epochs and 800 epochs) by a large margin in all cases.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc>reports the Top-1 and Top-5 accuracy on Ima-geNet validation set. HEXA improves its counterparts MoCo-v2 and DeepCluster-v2 in all cases. By different variants of HEXA DCluster , we see that cut-mixed examples are important in boosting performance, especially with 1% labels. HEXA + DCluster (8-crop ) sets a new SoTA under 200 training epochs, outperforming all existing self-supervised learning methods. It even outperforms BYOL pre-trained at 800 epochs in both cases. For SwAV pre-trained at 200 epochs, it is significantly inferior to HEXA in the same setting. For SwAV pre-trained at 800 epochs, it achieves Top-1 53.9% and Top-5 78.5% with 1% labelled images, which is lower than our HEXA pre-trained at 200 epochs by a notable margin. This again shows the effectiveness of hard examples in improving visual representations in low-resource settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Object detection results</cell></row><row><cell>on VOC. The numbers for MoCo-</cell></row><row><cell>v2 are from [8].</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors gratefully acknowledge Bai Li for helpful discussion. Additional thanks go to the entire Project Philly team inside Microsoft, who provided us the computing platform for our research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Epoch C10 C100 Supervised <ref type="bibr" target="#b5">[6]</ref> -97. <ref type="bibr" target="#b4">5</ref>   <ref type="table">Table 5</ref>: Image classification performance on fine-tuning the entire ResNet-50 network. All numbers for baselines are from <ref type="bibr" target="#b5">[6]</ref> , except that we use the released pretrained model for SwAV. ? indicates the results based on our runs using the same training schedules.</p><p>Object detection on VOC We follow <ref type="bibr" target="#b7">[8]</ref> to use the R50-FPN backbone for the Faster R-CNN detector available in the Detectron2 codebase <ref type="bibr" target="#b47">[48]</ref>. We freeze all the conv layers and also fix the BatchNorm parameters. The model is optimized with SGD, using a batch size of 8, a momentum of 0.9, and a weight decay of 0.0001. The initial learning rate is set as 0.05. We finetune the models for 15 epochs, and drop the learning rate by 0.1 at 12 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Fine-tuning</head><p>We fine-tuned the entire network using the weights of the pre-trained network as initialization. We trained for 20 epochs at a batch size of 256 using Adamax, decayed at <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> with a factor of 0.1. We grid search learning rate over [0.0005, 0.001, 0.01]. The results are shown in <ref type="table">Table 5</ref>. Our HEXA consistently improves their original counterparts for both datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Bubeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4980</idno>
		<title level="m">Convex optimization: Algorithms and complexity</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial robustness: From self-supervised pre-training to fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Robust neural machine translation with doubly adversarial inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02443</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>T-PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (VOC) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Big transfer (BIT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08994</idno>
		<title level="m">Adversarial training for large neural language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The curious case of adversarially robust models: More data can help, double descend, or hurt generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11080</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adversarial training can hurt generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06032</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08489</idno>
		<title level="m">Do adversarially robust imagenet models transfer better</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Selfie: Self-supervised pretraining for image embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02940</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03805</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 12</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">S 4 L: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Yew-Soon Ong, and Chen Change Loy. Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Splitbrain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
