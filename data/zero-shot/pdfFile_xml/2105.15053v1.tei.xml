<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Factorising Meaning and Form for Intent-Preserving Paraphrasing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hosking</surname></persName>
							<email>tom.hosking@ed.ac.ukmlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Factorising Meaning and Form for Intent-Preserving Paraphrasing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form. Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form. We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces. We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time. Crucially, our method does not require access to an external source of target exemplars. Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A paraphrase of an utterance is "an alternative surface form in the same language expressing the same semantic content as the original form" <ref type="bibr" target="#b29">(Madnani and Dorr, 2010)</ref>. For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in <ref type="table">Table 1</ref>. Question paraphrases are of significant interest, with applications in data augmentation <ref type="bibr" target="#b19">(Iyyer et al., 2018)</ref>, query rewriting <ref type="bibr" target="#b12">(Dong et al., 2017)</ref> and duplicate question detection <ref type="bibr" target="#b37">(Shah et al., 2018)</ref>, as they allow a system to better identify the underlying intent of a user query.</p><p>Recent approaches to paraphrasing use information bottlenecks with <ref type="bibr">VAEs (Bowman et al., 2016)</ref> or pivot languages  to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have lit-How is a dialect different from a language? The differences between language and dialect? What is the difference between language and dialect?</p><p>What is the weight of an average moose? Average weight of the moose? How much do moose weigh? How heavy is a moose? What country do parrots live in? In what country do parrots live? Where do parrots naturally live? What part of the world do parrots live in? <ref type="table">Table 1</ref>: Examples of question paraphrase clusters, drawn from Paralex <ref type="bibr" target="#b13">(Fader et al., 2013)</ref>. Each member of the cluster has essentially the same semantic intent, but a different surface form. Each cluster exhibits variation in word choice, syntactic structure and even question type. Our task is to generate these different surface forms, using only a single example as input. tle to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated <ref type="bibr" target="#b19">(Iyyer et al., 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2019a;</ref><ref type="bibr" target="#b24">Kumar et al., 2020)</ref>, but has so far assumed that the set of valid surface forms is known a priori.</p><p>In this paper, we propose SEPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the original intent. Our key innovations are: (a) to train a model to reconstruct a target question from an input paraphrase with the same meaning, and an exemplar with the same surface form, and (b) to separately encode the form and meaning of questions as discrete and continuous latent variables respectively, enabling us to modify the output surface form while preserving the original question intent. Crucially, unlike prior work on syntax controlled paraphrasing, we show that we can generate diverse paraphrases of an input question at test time by inferring a different discrete syntactic encoding, without needing access to reference exemplars.</p><p>We limit our work to English questions for three reasons: (a) the concept of a paraphrase is more  <ref type="figure">Figure 1</ref>: Overview of our approach. The model is trained to reconstruct a target question from one input with the same meaning and another input with the same form. This induces separate latent encoding spaces for meaning and form, allowing us to vary the output form while keeping the meaning constant. Using a discretized space for the syntactic encoding makes it tractable to predict valid surface forms at test time.</p><p>clearly defined for questions compared to generic utterances, as question paraphrases should lead to the same answer; (b) the space of possible surface forms is smaller for questions, making the task more achievable, and (c) better dataset availability. However, our approach does not otherwise make any assumptions specific to questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>The task is to learn a mapping from an input question, represented as a sequence of tokens X, to paraphrase(s) Y which have different surface form to X, but convey the same intent.</p><p>Our proposed approach, which we call SEPARATOR, uses an encoder-decoder model to transform an input question into a latent encoding space, and then back to an output paraphrase. We hypothesize that a principled information bottleneck (Section 2.1) and a careful choice of training scheme (Section 2.2) lead to an encoding space that separately represents the intent and surface form. This separation enables us to paraphrase the input question, varying the surface form of the output by directly manipulating the syntactic encoding of the input and keeping the semantic encoding constant (Section 2.3). We assume access to reference paraphrase clusters during training (e.g., <ref type="table">Table 1</ref>), sets of questions with different surface forms that have been collated as having the same meaning or intent.</p><p>Our model is a variant of the standard encoderdecoder framework <ref type="bibr" target="#b9">(Cho et al., 2014)</ref>, and consists of: (a) a vanilla Transformer sentence encoder <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>, that maps an input question X to a multi-head sequence of encodings, e h,t = ENCODER(X); (b) a principled choice of information bottleneck, with a continuous variational path and a discrete vector-quantized path, that maps the encoding sequence to a pair of latent vectors, z sem , z syn = BOTTLENECK(e h,t ), represented in more detail in <ref type="figure">Figure 1</ref>; (c) a vanilla Transformer decoder, that attends over the latent vectors to generate a sequence of output tokens, Y = DECODER(z sem , z syn ). The separation between z sem and z syn is induced by our proposed training scheme, shown in <ref type="figure">Figure 1</ref> and described in detail in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>While the encoder and decoder used by the model are standard Transformer modules, our bottleneck is more complex and we now describe it in more detail.</p><p>Let the encoder output be {e h,1 , . . . , e h,|X| } = ENCODER(X), where e h,t ? R D/H T , h ? 1, ..., H T with H T the number of transformer heads, |X| the length of the input sequence and D the dimension of the transformer. We first pool this sequence of encodings to a single vector, using the multi-head pooling described in <ref type="bibr" target="#b26">Liu and Lapata (2019)</ref>. For each head h, we calculate a distribution over time indexes ? h,t using attention:</p><formula xml:id="formula_0">? h,t = exp a h,t t ?|X| exp a h,t ,<label>(1)</label></formula><formula xml:id="formula_1">a h,t = k T h e h,t ,<label>(2)</label></formula><p>with k h ? R D/H a learned parameter.</p><p>We then take a weighted average of a linear projection of the encodings, to give pooled output? h ,</p><formula xml:id="formula_2">e h = t ?|X| ? h,t V h e h,t ,<label>(3)</label></formula><p>with V h ? R D/H?D/H a learned parameter. Transformer heads are assigned either to a semantic group H sem , that will be trained to encode the intent of the input,? sem = [. . . ;? h ; . . .], h ? H sem , or to a syntactic group H syn , that will be trained to represent the surface form? syn = [. . . ;? h ; . . .], h ? H syn (see <ref type="figure">Figure 1)</ref>.</p><p>The space of possible question intents is extremely large and may be reasonably approximated by a continuous vector space. However, the possible surface forms are discrete and smaller in number. We therefore use a Vector-Quantized Variational Autoencoder (VQ-VAE, van den Oord et al., 2017) for the syntactic encoding z syn , and model the semantic encoding z sem as a continuous Gaussian latent variable, as shown in the upper and lower parts of <ref type="figure">Figure 1</ref>, respectively.</p><p>Vector Quantization Let q h be discrete latent variables corresponding to the syntactic quantizer heads, h ? H syn . 1 Each variable can be one of K possible latent codes, q h ? [0, K]. The heads use distinct codebooks, C h ? R K?D/H , which map each discrete code to a continuous embedding C h (q h ) ? R D/H . Given sentence X and its pooled encoding {? 1 , ...,? H }, we independently quantize the syntactic subset of the heads h ? H syn to their nearest codes from C h and concatenate, giving the syntactic encoding</p><formula xml:id="formula_3">z syn = [C 1 (q 1 ); . . . ; C |Hsyn| (q |Hsyn| )].<label>(4)</label></formula><p>The quantizer module is trained through backpropagation using straight-through estimation <ref type="bibr" target="#b5">(Bengio et al., 2013)</ref>, with an additional loss term to constrain the embedding space as described in van den Oord et al. <ref type="formula" target="#formula_0">(2017)</ref>,</p><formula xml:id="formula_4">L cstr = ? h?Hsyn ? h ? sg(C h (q h )) 2 ,<label>(5)</label></formula><p>where the stopgradient operator sg(?) is defined as identity during forward computation and zero on backpropagation, and ? is a weight that controls the strength of the constraint. We follow the soft 1 The number and dimensionality of the quantizer heads need not be the same as the number of transformer heads. EM and exponentially moving averages training approaches described in earlier work <ref type="bibr" target="#b1">Angelidis et al., 2021)</ref>, which we find improve training stability.</p><p>Variational Bottleneck For the semantic path, we introduce a learned Gaussian posterior, that represents the encodings as smooth distributions in space instead of point estimates <ref type="bibr" target="#b23">(Kingma and Welling, 2014)</ref></p><formula xml:id="formula_5">. Formally, ?(z h |e h ) ? N (?(e h ), ?(e h ))</formula><p>, where ?(?) and ?(?) are learned linear transformations. To avoid vanishingly small variance and to encourage a smooth distribution, a prior is introduced, p(z h ) ? N (0, 1). The VAE objective is the standard evidence lower bound (ELBO), given by</p><formula xml:id="formula_6">ELBO = ?KL[?(z h |e h )||p(z h )] + E ? [log p(e h |z h )]. (6)</formula><p>We use the usual Gaussian reparameterisation trick, and approximate the expectation in Equation (6) by sampling from the training set and updating via backpropagation <ref type="bibr" target="#b23">(Kingma and Welling, 2014)</ref>. The VAE component therefore only adds an additional KL term to the overall loss,</p><formula xml:id="formula_7">L KL = ?KL[?(z h |e h )||p(z h )].<label>(7)</label></formula><p>In sum, BOTTLENECK(e h,t ) maps a sequence of token encodings to a pair of vectors z sem , z syn , with z sem a continuous latent Gaussian, and z syn a combination of discrete code embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Factorised Reconstruction Objective</head><p>We now describe the training scheme that causes the model to learn separate encodings for meaning and form: z sem should encode only the intent of the input, while z syn should capture any information about the surface form of the input. Although we refer to z syn as the syntactic encoding, it will not necessarily correspond to any specific syntactic formalism. We also acknowledge that meaning and form are not completely independent of each other; arbitrarily changing the form of an utterance is likely to change its meaning. However, it is possible for the same intent to have multiple phrasings , and it is this 'local independence' that we intend to capture.</p><p>We create triples {X sem , X syn , Y}, where X sem has the same meaning but different form to Y (i.e., it is a paraphrase, as in <ref type="table">Table 1</ref>) and X syn is a question with the same form but different meaning  (i.e., it shares the same syntactic template as Y), which we refer to as an exemplar. We describe the method for retrieving these exemplars in Section 2.3. The model is then trained to generate a target paraphrase Y from the semantic encoding z sem of the input paraphrase X sem , and from the syntactic encoding z syn of the exemplar X syn , as demonstrated in <ref type="figure">Figure 1</ref>.</p><p>Recalling the additional losses from the variational and quantized bottlenecks, the final combined training objective is given by</p><formula xml:id="formula_8">L = L Y + L cstr + L KL ,<label>(8)</label></formula><p>where L Y (X sem , X syn ) is the cross-entropy loss of teacher-forcing the decoder to generate Y from z sem (X sem ) and z syn (X syn ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Exemplars</head><p>It is important to note that not all surface forms are valid or licensed for all question intents. As shown in <ref type="figure">Figure 1</ref>, our approach requires exemplars during training to induce the separation between latent spaces. We also need to specify the desired surface form at test time, either by supplying an exemplar as input or by directly predicting the latent codes. The output should have a different surface form to the input but remain fluent.</p><p>Exemplar Construction During training, we retrieve exemplars X syn from the training data following a process which first identifies the underlying syntax of Y, and finds a question with the same syntactic structure but a different, arbitrary meaning. We use a shallow approximation of syntax, to ensure the availability of equivalent exemplars in the training data. An example of the exemplar retrieval process is shown in <ref type="table" target="#tab_1">Table 2</ref>; we first apply a chunker (FlairNLP, <ref type="bibr" target="#b0">Akbik et al., 2018)</ref> to Y, then extract the chunk label for each tagged span, ignoring stopwords. This gives us the template that Y follows. We then select a question at random from the training data with the same template to give X syn . If no other questions in the dataset use this template, we create an exemplar by replacing each chunk with a random sample of the same type. We experimented with a range of approaches to determining question templates, including using part-of-speech tags and (truncated) constituency parses. We found that using chunks and preserving stopwords gave a reasonable level of granularity while still combining questions with a similar form. The templates (and corresponding exemplars) need to be granular enough that the model is forced to use them, but abstract enough that the task is not impossible to learn.</p><p>Prediction at Test Time In general, we do not assume access to reference exemplars at test time and yet the decoder must generate a paraphrase from semantic and syntactic encodings. Since our latent codes are separated, we can directly predict the syntactic encoding, without needing to retrieve or generate an exemplar. Furthermore, by using a discrete representation for the syntactic space, we reduce this prediction problem to a simple classification task. Formally, for an input question X, we learn a distribution over licensed discrete codes q h , h ?H syn . We assume that the heads are independent, so that p(q 1 , . . . , qH syn ) = i p(q i ). We use a small fully connected network with the semantic and syntactic encodings of X as inputs, giving p(q h |X) = MLP(z sem (X), z syn (X)).</p><p>The network is trained to maximize the likelihood of all other syntactic codes licensed by each input. We calculate the discrete syntactic codes for each question in a paraphrase cluster, and minimize the cross-entropy loss of the network with respect to these codes. At test time, we set</p><formula xml:id="formula_9">q h = argmax q h [p(q h |X test )].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Datasets We evaluate our approach on two datasets: Paralex <ref type="bibr" target="#b13">(Fader et al., 2013)</ref>, a dataset of question paraphrase clusters scraped from WikiAnswers; and Quora Question Pairs (QQP) 2 sourced from the community question answering forum Quora. We observed that a significant fraction of the questions in Paralex included typos or were ungrammatical. We therefore filter out any questions marked as non-English by a language detection script <ref type="bibr">(Lui and Baldwin, 2012)</ref>, then pass the questions through a simple spellchecker. While this destructively edited some named entities in the questions, it did so in a consistent way across the whole dataset. There is no canonical split for Paralex, so we group the questions into clusters of paraphrases, and split these clusters into train/dev/test partitions with weighting 80/10/10. Similarly, QQP does not have a public test set. We therefore partitioned the clusters in the validation set randomly in two, to give us our dev/test splits. Summary statistics of the resulting datasets are given in Appendix B. All scores reported are on our test split.</p><p>Model Configuration Following previous work <ref type="bibr" target="#b21">(Kaiser et al., 2018;</ref><ref type="bibr" target="#b1">Angelidis et al., 2021)</ref>, our quantizer uses multiple heads (H = 4) with distinct codebooks to represent the syntactic encoding as 4 discrete categorical variables q h , with z syn given by the concatenation of their codebook embeddings C h (q h ). We use a relatively small codebook size of K = 256, relying on the combinatoric power of the multiple heads to maintain the expressivity of the model. We argue that, assuming each head learns to capture a particular property of a template (see Section 4.3), the number of variations in each property is small, and it is only through combination that the space of possible templates becomes large.</p><p>We include a detailed list of hyperparameters in Appendix A. Our code is available at http:// github.com/tomhosking/separator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Systems</head><p>We compare SEPARATOR against several related systems. These include a model which reconstructs Y only from X sem , with no signal for the desired form of the output. In other words, we derive both z sem and z syn from X sem , and no separation between meaning and form is learned. This model uses a continuous Gaussian latent variable for both z syn and z sem , but is otherwise equivalent in architecture to SEPARATOR. We refer to this as the VAE baseline. We also experiment with a vanilla autoencoder or AE baseline by removing the variational component, such that z sem , z syn =? sem ,? syn .</p><p>We include our own implementation of the VQ-VAE model described in <ref type="bibr" target="#b34">Roy and Grangier (2019)</ref>. They use a quantized bottleneck for both z sem and z syn , with a large codebook K = 64, 000, H = 8 heads and a residual connection within the quantizer. For QQP, containing only 55,611 train-  ing clusters, the configuration in <ref type="bibr" target="#b34">Roy and Grangier (2019)</ref> leaves the model overparameterized and training did not converge; we instead report results for K = 1, 000.</p><p>ParaNMT  translates input sentences into a pivot language (Czech), then back into English. Although this system was trained on high volumes of data (including Common Crawl), the training data contains relatively few questions, and we would not expect it to perform well in the domain under consideration. 'Diverse Paraphraser using Submodularity' (DiPS; Kumar et al. 2019) uses submodular optimisation to increase the diversity of samples from a standard encode-decoder model. Latent bag-of-words (BoW; <ref type="bibr" target="#b14">Fu et al. 2019</ref>) uses an encoder-decoder model with a discrete bag-of-words as the latent encoding. SOW/REAP <ref type="bibr" target="#b16">(Goyal and Durrett, 2020)</ref> uses a two stage approach, deriving a set of feasible syntactic rearrangements that is used to guide a second encoder-decoder model. We additionally implement a simple tf-idf baseline <ref type="bibr" target="#b20">(Jones, 1972)</ref>, retrieving the question from the training set with the highest similarity to the input. Finally, we include a basic copy baseline as a lower bound, that simply uses the input question as the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our experiments were designed to answer three questions: (a) Does SEPARATOR effectively factorize meaning and form? (b) Does SEPARATOR  <ref type="table">Table 4</ref>: Generation results, without access to oracle exemplars. Our approach achieves the highest iBLEU scores, indicating the best tradeoff between output diversity and fidelity to the reference paraphrases.</p><p>manage to generate diverse paraphrases (while preserving the intent of the input)? (c) What does the underlying quantized space encode (i.e., can we identify any meaningful syntactic properties)? We address each of these questions in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Verification of Separation</head><p>Inspired by <ref type="bibr" target="#b8">Chen et al. (2019b)</ref> we use a semantic textual similarity task and a template detection task to confirm that SEPARATOR does indeed lead to encodings {z sem , z syn } in latent spaces that represent different types of information.</p><p>Using the test set, we construct clusters of questions that share the same meaning C sem , and clusters that share the same template C syn . For each cluster C q ? {C sem , C syn }, we extract one question at random X q ? C q , compute its encodings {z sem , z syn , z} 3 , and its cosine similarity to the encodings of all other questions in the test set. We take the question with maximum similarity to the query X r , r = argmax r (z q .z r ), and compare the cluster that it belongs to, C r , to the query cluster I(C q = C r ), giving a retrieval accuracy score for each encoding type and each clustering type. For the VAE, we set {z sem , z syn } to be the same heads of z as the separated model. <ref type="table" target="#tab_3">Table 3</ref> shows that our approach yields encodings that successfully factorise meaning and form, with negligible performance loss compared to the VAE baseline; paraphrase retrieval performance using z sem for the separated model is comparable to using z for the VAE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Paraphrase Generation</head><p>Automatic Evaluation While we have shown that our approach leads to disentangled representations, we are ultimately interested in generating diverse paraphrases for unseen data. That is, given some input question, we want to generate an output question with the same meaning but different form.</p><p>We use iBLEU <ref type="bibr" target="#b39">(Sun and Zhou, 2012)</ref> as our primary metric, a variant of BLEU <ref type="bibr" target="#b32">(Papineni et al., 2002;</ref><ref type="bibr" target="#b33">Post, 2018)</ref> that is penalized by the similarity between the output and the input,</p><formula xml:id="formula_10">iBLEU = ?BLEU(output, ref erences) ?(1 ? ?)BLEU(output, input),<label>(9)</label></formula><p>where ? = 0.7 is a constant that weights the tradeoff between fidelity to the references and variation from the input. We also report the usual BLEU(output, ref erences) as well as Self-BLEU(output, input). The latter allows us to examine whether the models are making trivial changes to the input. The Paralex test set contains 5.6 references on average per cluster, while QQP contains only 1.3. This leads to lower BLEU scores for QQP in general, since the models are evaluated on whether they generated the specific paraphrase(s) present in the dataset. <ref type="table">Table 4</ref> shows that the Copy, VAE and AE models display relatively high BLEU scores, but achieve this by 'parroting' the input; they are good at reconstructing the input, but introduce little variation in surface form, reflected in the high Self-BLEU scores. This highlights the importance of considering similarity to both the references and to the input. The tf-idf baseline performs surprisingly  well on Paralex; the large dataset size makes it more likely that a paraphrase cluster with a similar meaning to the query exists in the training set. The other comparison systems (in the second block in <ref type="table">Table 4</ref>) achieve lower Self-BLEU scores, indicating a higher degree of variation introduced, but this comes at the cost of much lower scores with respect to the references. SEPARATOR achieves the highest iBLEU scores, indicating the best balance between fidelity to the references and novelty compared to the input. We give some example output in <ref type="table" target="#tab_6">Table 5</ref>; while the other systems mostly introduce lexical variation, SEPARATOR is able to produce output with markedly different syntactic structure to the input, and can even change the question type while successfully preserving the original intent.</p><p>The last row in <ref type="table">Table 4</ref> (ORACLE) reports results when our model is given a valid exemplar to use directly for generation, thus bypassing the code prediction problem. For each paraphrase cluster, we select one question at random to use as input, and select another to use as the target. We retrieve a question from the training set with the same template as the target to use as an oracle exemplar. This represents an upper bound on our model's performance. While SEPARATOR outperforms existing methods, our approach to predicting syntactic codes (using a shallow fully-connected network) is relatively simple. SEPARATOR using oracle exemplars achieves by far the highest scores in <ref type="table">Table 4</ref>, demonstrating the potential expressivity of our approach when exemplars are guaranteed to be valid. A more powerful code prediction model could close the gap to this upper bound, as well as enabling the generation of multiple diverse paraphrases for a single input question. However, we leave this to future work.</p><p>Human Evaluation In addition to automatic evaluation we elicited judgements from crowdworkers on Amazon Mechanical Turk. Specifically, they were shown a question and two paraphrases thereof (corresponding to different systems) and asked to select which one was preferred along three dimensions: the dissimilarity of the paraphrase compared to the original question, how well the paraphrase reflected the meaning of the original, and the fluency of the paraphrase (see Appendix C). We evaluated a total of 200 questions sampled equally from both Paralex and QQP, and collected 3 ratings for each sample. We assigned each system a score of +1 when it was selected, ?1 when the other system was selected, and took the mean over all samples. Negative scores indicate that a system was selected less often than an alternative. We chose the four best performing models according to <ref type="table">Table 4</ref> for our evaluation: SEPARATOR, DiPS <ref type="bibr" target="#b25">(Kumar et al., 2019)</ref>, Latent BoW <ref type="bibr" target="#b14">(Fu et al., 2019)</ref> and VAE. <ref type="figure" target="#fig_2">Figure 2</ref> shows that although the VAE baseline is the best at preserving question meaning, it is also the worst at introducing variation to the output. SEPARATOR introduces more variation than the other systems evaluated and better preserves the original question intent, as well as generating significantly more fluent output (using a one-way ANOVA with post-hoc Tukey HSD test, p&lt;0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>When predicting latent codes at test time, we assume that the code for each head may be predicted independently of the others, as working with the full joint distribution would be intractable. We now examine this assumption as well as whether different encodings represent distinct syntactic proper- Although the VAE baseline is the best at preserving question meaning, it is the worst at introducing variation to the output. SEPARATOR offers the best balance between dissimilarity and meaning preservation, and is more fluent than both DiPS and Latent BoW.</p><p>ties. Following <ref type="bibr" target="#b1">Angelidis et al. (2021)</ref>, we compute the probability of a question property f 1 , f 2 , . . . taking a particular value a, conditioned by head h and quantized code k h as</p><formula xml:id="formula_11">P (f i |h, k h ) = x?X I(q h (x) = k h )I(f i (x) = a) x?X I(q h (x) = k h ) ,<label>(10)</label></formula><p>where I(?) is the indicator function, and examples of values a are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We then calculate the mean entropy of these distributions, to determine how property-specific each head is:</p><formula xml:id="formula_12">H h = 1 K k h a P (a|h, k h ) log P (a|h, k h ). (11)</formula><p>Heads with lower entropies are more predictive of a property, indicating specialisation and therefore independence. <ref type="figure" target="#fig_3">Figure 3</ref> shows our analysis for four syntactic properties: head #2 has learned to control the high level output structure, including the question type or wh-word, and whether the question word appears at the beginning or end of the question. Head #3 controls which type of prepositional phrase is used. The length of the output is not determined by any one head, implying that it results from other properties of the surface form. Future work could leverage this disentanglement to improve the exemplar prediction model, and could lead to more fine-grained control over the generated output form.</p><p>In summary, we find that SEPARATOR successfully learns separate encodings for meaning and form. SEPARATOR is able to generate question paraphrases with a better balance of diversity and intent preservation compared to prior work. Although we are able to identify some high-level properties encoded by each of the syntactic latent variables, further work is needed to learn interpretable syntactic encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Paraphrasing Prior work on generating paraphrases has looked at extracting sentences with similar meaning from large corpora <ref type="bibr" target="#b4">(Barzilay and McKeown, 2001;</ref><ref type="bibr" target="#b2">Bannard and Callison-Burch, 2005;</ref><ref type="bibr" target="#b15">Ganitkevitch et al., 2013)</ref>, or identifying paraphrases from sources that are weakly aligned <ref type="bibr" target="#b11">(Dolan et al., 2004;</ref><ref type="bibr" target="#b10">Coster and Kauchak, 2011)</ref>.</p><p>More recently, neural approaches to paraphrasing have shown promise. Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs <ref type="bibr" target="#b6">(Bowman et al., 2016)</ref>, <ref type="bibr">VQ-VAEs (van den Oord et al., 2017;</ref><ref type="bibr" target="#b34">Roy and Grangier, 2019)</ref>, and a latent bag-of-words model <ref type="bibr" target="#b14">(Fu et al., 2019)</ref>. Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English <ref type="bibr" target="#b17">Hu et al., 2019)</ref>. <ref type="bibr" target="#b25">Kumar et al. (2019)</ref> use submodular function maximisation to improve the diversity of paraphrases generated by an encoder-decoder model. <ref type="bibr" target="#b12">Dong et al. (2017)</ref> use an automatic paraphrasing system to rewrite inputs to a question answering system at inference time, reducing the sensitivity of the system to the specific phrasing of a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic Templates</head><p>The idea of generating paraphrases by controlling the structure of the output has seen recent interest, but most work so far has assumed access to a template oracle. <ref type="bibr" target="#b19">Iyyer et al. (2018)</ref> use linearized parse trees as a template, then sample paraphrases by using multiple templates and reranking the output. Chen et al. (2019a) use a multi task objective to train a model to generate output that follows an input template. Their approach is limited by their use of automatically generated paraphrases for training, and their reliance on the availability of oracle templates. <ref type="bibr" target="#b3">Bao et al. (2019)</ref> use a discriminator to separate spaces, but rely on noising the latent space to induce variation in the output form. Their results show good fidelity to the references, but low variation compared to the input. <ref type="bibr" target="#b16">Goyal and Durrett (2020)</ref> use the artifically generated dataset ParaNMT-50m  for their training and evaluation, which displays low output variation according to our results. <ref type="bibr" target="#b24">Kumar et al. (2020)</ref> show strong performance using full parse trees as templates, but focus on generating output with the correct parse and do not consider the problem of template prediction.</p><p>Huang and Chang (2021) independently and concurrently propose training a model with a similar 'split training' approach to ours, but using constituency parses instead of exemplars, and a 'bagof-words' instead of reference paraphrases. Their approach has the advantage of not requiring paraphrase clusters during training, but they do not attempt to solve the problem of template prediction and rely on the availability of oracle target templates. <ref type="bibr" target="#b36">Russin et al. (2020)</ref> modify the architecture of an encoder-decoder model, introducing an inductive bias to encode the structure of inputs separately from the lexical items to improve compositional generalisation on an artificial semantic parsing task. <ref type="bibr" target="#b8">Chen et al. (2019b)</ref> use a multi-task setup to generate separated encodings, but do not experiment with generation tasks. <ref type="bibr" target="#b38">Shu et al. (2019)</ref> learn discrete latent codes to introduce variation to the output of a machine translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present SEPARATOR, a method for generating paraphrases that balances high variation in surface form with strong intent preservation. Our approach consists of: (a) a training scheme that causes an encoder-decoder model to learn separated latent encodings, (b) a vector-quantized bottleneck that results in discrete variables for the syntactic encoding, and (c) a simple model to predict different yet valid surface forms for the output. Extensive experiments and a human evaluation show that our approach leads to separated encoding spaces with negligible loss of expressivity, and is able to generate paraphrases with a better balance of variation and semantic fidelity than prior methods.</p><p>In future, we would like to investigate the properties of the syntactic encoding space, and improve on the code prediction model. It would also be interesting to reduce the levels of supervision required to train the model, and induce the separation without an external syntactic model or reference paraphrases. <ref type="bibr">Quantizer</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Statistics</head><p>Summary statistics for our partitions of Paralex and QQP are shown in <ref type="table" target="#tab_10">Table 7</ref>. Questions in QQP were 9.7 tokens long on average, compared to 8.2 for Paralex. We also show the distribution of different question types in <ref type="figure" target="#fig_4">Figure 4</ref>; QQP contains a higher percentage of why questions, and we found that the questions tend to be more subjective compared to the predominantly factual questions in Paralex.   <ref type="bibr" target="#b13">(Fader et al., 2013)</ref>, and our partitioning of QQP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Human Evaluation</head><p>Annotators were asked to rate the outputs according to the following criteria: ? To what extent is the meaning expressed in the original question preserved in the rewritten version, with no additional information added? Which of the questions generated by a system is likely to have the same answer as the original?</p><p>? Does the rewritten version use different words or phrasing to the original? You should choose the system that uses the most different words or word order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Reproducibility Notes</head><p>All experiments were run on a single Nvidia RTX 2080 Ti GPU. Training time for SEPARATOR was approximately 2 days on Paralex, and 1 day for QQP. SEPARATOR contains a total of 69,139,744 trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Template Dropout</head><p>Early experiments showed that, while the model was able to separately encode meaning and form, the 'syntactic' encoding space showed little ordering. That is, local regions of the encoding space did not necessarily encode templates that co-occurred with each other in paraphrase clusters. We therefore propose template dropout, where exemplars X syn are replaced with probability p td = 0.3 by a question with a different template from the same paraphrase cluster. This is intended to provide the model with a signal about which templates are similar to each other, and thus reduce the distance between their encodings. valid forms for each cluster overlaps significantly. In other words, regions of licensed templates for each input are not contiguous, and naively perturbing a syntactic encoding for an input question is not guaranteed to lead to a valid template. Template dropout, described in Appendix E, seems to improve the arrangement of encoding space, but is not sufficient to allow us to 'navigate' encoding space directly. The ability to induce an ordered encoding space and introduce syntactic diversity by simply perturbing the encoding, would allow us to drop the template prediction network, and we hope that future work will build on this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ordering of the Encoding Space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Failure Cases</head><p>A downside of our approach is the use of an information bottleneck; the model must learn to compress a full question into a single, fixed-length vector. This can lead to loss of information or corruption, with the output occasionally repeating words or generating a number that is slightly different to the correct one, as shown in <ref type="table" target="#tab_12">Table 8</ref>.</p><p>We also occasionally observe instances of the  well documented posterior collapse phenomenon, where the decoder ignores the input encoding and generates a generic high probability sequence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " o k Z 9 k E 0 B / H / G U P A m Q m I g e X G j o g Y = " &gt; A A A B + X i c b V B N S 8 N A E N 3 4 W e t X 1 K O X Y B E 8 l U Q U P R a 9 e K x g P 6 A N Z b O d t E s 3 m 7 A 7 K d a Q f + L F g y J e / S f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u Y F i e A a X f f b W l l d W 9 / Y L G 2 V t 3 d 2 9 / b t g 8 O m j l P F o M F i E a t 2 Q D U I L q G B H A W 0 E w U 0 C g S 0 g t H t 1 G + N Q W k e y w e c J O B H d C B 5 y B l F I / V s u 4 v w i E G Y P e W 9 T E O U 9 + y K W 3 V n c J a J V 5 A K K V D v 2 V / d f s z S C C Q y Q b X u e G 6 C f k Y V c i Y g L 3 d T D Q l l I z q A j q G S R q D 9 b H Z 5 7 p w a p e + E s T I l 0 Z m p v y c y G m k 9 i Q L T G V E c 6 k V v K v 7 n d V I M r / 2 M y y R F k G y + K E y F g 7 E z j c H p c w U M x c Q Q y h Q 3 t z p s S B V l a M I q m x C 8 x Z e X S f O 8 6 l 1 W 3 f u L S u 2 m i K N E j s k J O S M e u S I 1 c k f q p E E Y G Z N n 8 k r e r M x 6 s d 6 t j 3 n r i l X M H J E / s D 5 / A J Y i l E Y = &lt; / l a t e x i t &gt; z sem &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h q 5 N c n P D o S A K x k W r I 4 8 l u X Z I F D Y = " &gt; A A A B + X i c b V B N S 8 N A E N 3 4 W e t X 1 K O X x S J 4 K o k o e i x 6 8 V j B f k A b w m a 7 a Z d u N m F 3 U o w h / 8 S L B 0 W 8 + k + 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s E 1 O M 6 3 t b K 6 t r 6 x W d m q b u / s 7 u 3 b B 4 d t H a e K s h a N R a y 6 A d F M c M l a w E G w b q I Y i Q L B O s H 4 d u p 3 J k x p H s s H y B L m R W Q o e c g p A S P 5 t t 0 H 9 g h B m D 8 V f q 4 z W f h 2 z a k 7 M + B l 4 p a k h k o 0 f f u r P 4 h p G j E J V B C t e 6 6 T g J c T B Z w K V l T 7 q W Y J o W M y Z D 1 D J Y m Y 9 v L Z 5 Q U + N c o A h 7 E y J Q H P 1 N 8 T O Y m 0 z q L A d E Y E R n r R m 4 r / e b 0 U w m s v 5 z J J g U k 6 X x S m A k O M p z H g A V e M g s g M I V R x c y u m I 6 I I B R N W 1 Y T g L r 6 8 T N r n d f e y 7 t x f 1B o 3 Z R w V d I x O 0 B l y 0 R V q o D v U R C 1 E 0 Q Q 9 o 1 f 0 Z u X W i / V u f c x b V 6 x y 5 g j 9 g f X 5 A 7 Y f l F s = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " E o E j B O f / Z w O 8 o 3 M x b 5 n G 7 l n k P Y k = " &gt; A A A C A X i c b V B N S 8 N A E N 3 U r 1 q / q l 4 E L 8 E i e C q J K H o s e v F Y w X 5 A U 8 p m M 2 m X b j Z h d y K W E C / + F S 8 e F P H q v / D m v 3 H 7 c d D W B w O P 9 2 a Y m e c n g m t 0 n G + r s L S 8 s r p W X C 9 t b G 5 t 7 5 R 3 9 5 o 6 T h W D B o t F r N o + 1 S C 4 h A Z y F N B O F N D I F 9 D y h 9 d j v 3 U P S v N Y 3 u E o g W 5 E + 5 K H n F E 0 U q 9 8 4 C E X A W Q e w g P 6 Y Q Z 5 3 s v 0 S O a 9 c s W p O h P Y i 8 S d k Q q Z o d 4 r f 3 l B z N I I J D J B t e 6 4 T o L d j C r k T E B e 8 l I N C W V D 2 o e O o Z J G o L v Z 5 I P c P j Z K Y I e x M i X R n q i / J z I a a T 2 K f N M Z U R z o e W 8 s / u d 1 U g w v u x m X S Y o g 2 X R R m A o b Y 3 s c h x 1 w B Q z F y B D K F D e 3 2 m x A F W V o Q i u Z E Nz 5 l x d J 8 7 T q n l e d 2 7 N K 7 W o W R 5 E c k i N y Q l x y Q W r k h t R J g z D y S J 7 J K 3 m z n q w X 6 9 3 6 m L Y W r N n M P v k D 6 / M H J y e X / A = = &lt; / l a t e x i t &gt; e syn &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A q O 6 J f 7 / k y s s X W 9 m h N O r 7 w l q d s s = " &gt; A A A C A X i c b V B N S 8 N A E N 3 U r 1 q / o l 4 E L 8 E i e C q J K H o s e v F Y w X 5 A W 8 p m O 2 m X 7 i Z h d y K W E C / + F S 8 e F P H q v / D m v 3 H b 5 q C t D w Y e 7 8 0 w M 8 + P B d f o u t 9 W Y W l 5 Z X W t u F 7 a 2 N z a 3 r F 3 9 x o 6 S h S D O o t E p F o + 1 S B 4 C H X k K K A V K 6 D S F 9 D 0 R 9 c T v 3 k P S v M o v M N x D F 1 J B y E P O K N o p J 5 9 0 E E u + p B 2 E B 7 Q D 1 L I s l 6 q Q W Y 9 u + x W 3 C m c R e L l p E x y 1 H r 2 V 6 c f s U R C i E x Q r d u e G 2 M 3 p Q o 5 E 5 C V O o m G m L I R H U D b 0 J B K 0 N 1 0 + k H m H B u l 7 w S R M h W i M 1 V / T 6 R U a j 2 W v u m U F I d 6 3 p u I / 3 n t B I P L b s r D O E E I 2 W x R k A g H I 2 c S h 9 P n C h i K s S G U K W 5 u d d i Q K s r Q h F Y y I X j z L y + S x m n F O 6 + 4 t 2 f l 6 l U e R 5 E c k i N y Q j x y Q a r k h t R I n T D y S J 7 J K 3 m z n q w X 6 9 3 6 m L U W r H x m n / y B 9 f k D B y q X 5 w = = &lt; / l a t e x i t &gt;? t e x i t s h a 1 _ b a s e 6 4 = " s s K M r t 0 V U O v 3 e I n 9 + i o n O u / 6 Q c U = " &gt; A A A B + X i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 B I v g q S S i 6 L H o x W M F + w F t K J v t p F 2 6 2 Y T d S b G E / B M v H h T x 6 j / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S A T X 6 L r f V m l t f W N z q 7 x d 2 d n d 2 z + w D 4 9 a O k 4 V g y a L R a w 6 A d U g u I Q m c h T Q S R T Q K B D Q D s Z 3 M 7 8 9 A a V 5 L B 9 x m o A f 0 a H k I W c U j d S 3 7 R 7 C E w Z h 1 s n 7 m Y Y o 7 9 t V t + b O 4 a w S r y B V U q D R t 7 9 6 g 5 i l E U h k g m r d 9 d w E / Y w q 5 E x A X u m l G h L K x n Q I X U M l j U D 7 2 f z y 3 D k z y s A J Y 2 V K o j N X f 0 9 k N N J 6 G g W m M 6 I 4 0 s v e T P z P 6 6 Y Y 3 v g Z l 0 m K I N l i U Z g K B 2 N n F o M z 4 A o Y i q k h l C l u b n X Y i C r K 0 I R V M S F 4 y y + v k t Z F z b u q u Q + X 1 f p t E U e Z n J B T c k 4 8 c k 3 q 5 J 4 0 S J M w M i H P 5 J W 8 W Z n 1 Y r 1 b H 4 v W k l X M H J M / s D 5 / A G G s l C Q = &lt; / l a t e x i t &gt; X sem &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i f Z M B Q K u U i U 4 E s E P y H H a V i m C X D o = " &gt; A A A B + X i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 W P R i 8 c K 9 g P a E D b b T b t 0 s w m 7 k 2 I I + S d e P C j i 1 X / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u w X G + r c r a + s b m V n W 7 t r O 7 t 3 9 g H x 5 1 d J w q y t o 0 F r H q B U Q z w S V r A w f B e o l i J A o E 6 w a T u 5 n f n T K l e S w f I U u Y F 5 G R 5 C G n B I z k 2 / Y A 2 B M E Y d 4 r / F x n s v D t u t N w 5 s C r x C 1 J H Z V o + f b X Y B j T N G I S q C B a 9 1 0 n A S 8 n C j g V r K g N U s 0 S Q i d k x P q G S h I x 7 e X z y w t 8 Z p Q h D m N l S g K e q 7 8 n c h J p n U W B 6 Y w I j P W y N x P / 8 / o p h D d e z m W S A p N 0 s S h M B Y Y Y z 2 L A Q 6 4 Y B Z E Z Q q j i 5 l Z M x 0 Q R C i a s m g n B X X 5 5 l X Q u G u 5 V w 3 m 4 r D d v y z i q 6 A S d o n P k o m v U R P e o h d q I o i l 6 R q / o z c q t F + v d + l i 0 V q x y 5 h j 9 g f X 5 A 4 G p l D k = &lt; / l a t e x i t &gt; X syn &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H w + q 1 / c b u W / w C c G h z l 0 I p T 0 + y / E = " &gt; A A A B 8 X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k o e i x 6 8 V j B f m g b y m Y 7 a Z d u N m F 3 I p b Q f + H F g y J e / T f e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u Y F i R Q G X f f b W V p e W V 1 b L 2 w U N 7 e 2 d 3 Z L e / s N E 6 e a Q 5 3 H M t a t g B m Q Q k E d B U p o J R p Y F E h o B s P r i d 9 8 B G 1 E r O 5 w l I A f s b 4 S o e A M r f T Q Q X j C I M z u x 9 1 S 2 a 2 4 U 9 B F 4 u W k T H L U u q W v T i / m a Q Q K u W T G t D 0 3 Q T 9 j G g W X M C 5 2 U g M J 4 0 P W h 7 a l i k V g / G x 6 8 Z g e W 6 V H w 1 j b U k i n 6 u + J j E X G j K L A d k Y M B 2 b e m 4 j / e e 0 U w 0 s / E y p J E R S f L Q p T S T G m k / d p T 2 j g K E e W M K 6 F v Z X y A d O M o w 2 p a E P w 5 l 9 e J I 3 T i n d e c W / P y t W r P I 4 C O S R H 5 I R 4 5 I J U y Q 2 p k T r h R J F n 8 k r e H O O 8 O O / O x 6 x 1 y c l n D s g f O J 8 / + Q S R H A = = &lt; / l a t e x i t &gt; Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3</head><label></label><figDesc>z refers to the combined encoding, i.e., [zsem; zsyn].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Results of our human evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Predictive entropy by head for various question properties -lower entropy indicates higher predictive power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?Figure 4 :</head><label>4</label><figDesc>Which system output is the most fluent and grammatical? Distribution of wh-words for the datasets used in our experiments. QQP contains a much higher percentage of why questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>shows that the semantic encodings z sem are tightly clustered by paraphrase, but the set of(a) Semantic encodings (b) Syntactic encodings Figure 5: Visualisations of z sem and z syn using t-SNE (van der Maaten and Hinton, 2008), coloured by paraphrase cluster. The semantic encodings are clustered by meaning, as expected, but there is little to no local ordering in the syntactic space; valid surface forms of a particular question do not necessarily have syntactic encodings near to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input How heavy is a moose? Chunker output How [heavy]ADVP is a [moose]NP ? Template How ADVP is a NP ? Exemplar How much is a surgeon's income? Input What country do parrots live in Chunker output What [country]NP do [parrots]NP [live]VP in ? Template What NP do NP VP in ? Exemplar What religion do Portuguese believe in?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Examples of the exemplar retrieval process for training. The input is tagged by a chunker, ignoring stopwords. An exemplar with the same template is then retrieved from a different paraphrase cluster.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Retrieval accuracies for each encoding for</cell></row><row><cell>each cluster type. The VAE baseline is trained only on</cell></row><row><cell>paraphrase pairs and receives no signal for the desired</cell></row><row><cell>form of the output. SEPARATOR is able to learn sepa-</cell></row><row><cell>rate encodings for meaning and form, with negligible</cell></row><row><cell>loss in semantic encoding performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Self-BLEU ? iBLEU ? BLEU ? Self-BLEU ? iBLEU ?</figDesc><table><row><cell></cell><cell></cell><cell>Paralex</cell><cell></cell><cell></cell><cell>QQP</cell><cell></cell></row><row><cell cols="2">Model BLEU ? Copy 37.10</cell><cell>100.00</cell><cell>?4.03</cell><cell>32.61</cell><cell>100.00</cell><cell>?7.17</cell></row><row><cell>VAE</cell><cell>40.26</cell><cell>66.12</cell><cell>8.35</cell><cell>19.36</cell><cell>35.29</cell><cell>2.96</cell></row><row><cell>AE</cell><cell>40.10</cell><cell>75.71</cell><cell>5.36</cell><cell>19.90</cell><cell>39.81</cell><cell>1.99</cell></row><row><cell>tf-idf</cell><cell>25.08</cell><cell>25.25</cell><cell>9.98</cell><cell>22.73</cell><cell>61.81</cell><cell>?2.63</cell></row><row><cell>VQ-VAE</cell><cell>40.26</cell><cell>65.71</cell><cell>8.47</cell><cell>16.19</cell><cell>26.15</cell><cell>3.43</cell></row><row><cell>ParaNMT</cell><cell>20.42</cell><cell>39.90</cell><cell>2.32</cell><cell>24.24</cell><cell>56.42</cell><cell>0.04</cell></row><row><cell>DiPS</cell><cell>24.90</cell><cell>29.58</cell><cell>8.56</cell><cell>18.47</cell><cell>32.45</cell><cell>3.19</cell></row><row><cell>SOW/REAP</cell><cell>33.09</cell><cell>37.07</cell><cell>12.04</cell><cell>12.64</cell><cell>24.19</cell><cell>1.59</cell></row><row><cell>LBoW</cell><cell>34.96</cell><cell>35.86</cell><cell>13.71</cell><cell>16.17</cell><cell>29.00</cell><cell>2.62</cell></row><row><cell>SEPARATOR</cell><cell>36.36</cell><cell>35.37</cell><cell>14.84</cell><cell>14.70</cell><cell>14.84</cell><cell>5.84</cell></row><row><cell>ORACLE</cell><cell>53.37</cell><cell>24.55</cell><cell>29.99</cell><cell>24.50</cell><cell>16.04</cell><cell>12.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Input What is the most known singer? VAE What is the most known singer? DiPS What was the most known famous singer? SOW/REAP What is the most famous singer? Latent BoW What is the most famous singer? SEPARATOR Who is the most famous singer in America?Input What is the income for a soccer player? VAE What is the salary for a soccer player?</figDesc><table><row><cell>DiPS What is the median income in soccer?</cell></row><row><cell>SOW/REAP What is US cer?</cell></row><row><cell>Latent BoW What is the salary of a soccer [UNK]?</cell></row><row><cell>SEPARATOR How much is a soccer players' salary?</cell></row><row><cell>Input What has been the economic impact from Brexit</cell></row><row><cell>referendum so far?</cell></row><row><cell>VAE What has been the economic impact of Brexit</cell></row><row><cell>referendum so far?</cell></row><row><cell>DiPS What will be a impact of Brexit referendum?</cell></row><row><cell>SOW/REAP How do I spend my virginity?</cell></row><row><cell>Latent BoW How did Brexit referendum impact the Brexit</cell></row><row><cell>referendum?</cell></row><row><cell>SEPARATOR How much will the Brexit referendum cost?</cell></row><row><cell>Input What are the basics I should know before learn-</cell></row><row><cell>ing Hadoop?</cell></row><row><cell>VAE What are the basics should I know before learn-</cell></row><row><cell>ing Hadoop?</cell></row><row><cell>DiPS How do I know before I want to learn Hadoop?</cell></row><row><cell>SOW/REAP How can I know before learning Hadoop?</cell></row><row><cell>Latent BoW What are the basics of learning Hadoop?</cell></row><row><cell>SEPARATOR How much should I know before learning</cell></row><row><cell>Hadoop?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Examples of output generated by various approaches for a given input, from Paralex and QQP. SEP-ARATOR is able to generate questions with a different syntactic form to the input.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter values used for our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Summary statistics for our cleaned version of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Numerical error Input Replace starter on a 1988 Ford via? Output How do you replace a starter on a 1992 Ford? Repetition Input What brought about the organization of the Republican political party? Output What is the political party of the Republican party? Ignoring encoding Input What do Hondurans do for a living? Output What do Hondurans eat?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Examples of failure modes.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/quora-question-pairs</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank our anonymous reviewers for their feedback. We are grateful to Stefanos Angelidis for many valuable discussions, and Hao Tang for their comments on the paper. This work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh. Lapata acknowledges the support of the European Research Council (award number 681760, "Translating Multiple Modalities into Text").</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>Hyperparameters were selected by manual tuning, based on a combination of: (a) validation encoding separation, (b) validation BLEU scores using oracle exemplars, and (c) validation iBLEU scores using predicted syntactic codes. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extractive opinion summarization in quantized transformer spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="277" to="293" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Paraphrasing with bilingual parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219914</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from disentangled syntactic and semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1602</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6008" to="6019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting paraphrases from a parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073012.1073020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Controllable paraphrase generation with a syntactic exemplar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1599</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5972" to="5984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-task approach for disentangling syntax and semantics in sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1254</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2453" to="2464" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-4012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple English Wikipedia: A new text simplification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="665" to="669" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Geneva, Switzerland. COLING</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="350" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to paraphrase for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1091</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="875" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Paraphrase generation with latent bag of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13645" to="13656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural syntactic preordering for controlled paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="238" to="252" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Parabank: Monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CoRR, abs/1901.03644</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating syntactically controlled paraphrases without using annotated parallel pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1022" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1875" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen Sp?rck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>abs/1803.03382</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Syntax-guided controlled generation of paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuram</forename><surname>Vadapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="330" to="345" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1363</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3609" to="3619" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2012. langid.py: An off-the-shelf language identification tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating phrasal and sentential paraphrases: A survey of data-driven methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonnie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorr</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00002</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="387" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Long Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Oriol Vinyals, and koray kavukcuoglu</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised paraphrasing without translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1605</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6033" to="6039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Theory and experiments on vector quantized autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno>abs/1805.11063</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compositional generalization by factorizing alignment and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Russin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-srw.42</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation for duplicate question detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1131</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1056" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating diverse translations with sentence codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1177</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1823" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint learning of a dual SMT system for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>L?vy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Data noising as smoothing in neural network language models</title>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
