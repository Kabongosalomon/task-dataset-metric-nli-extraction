<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Birkbeck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balu</forename><surname>Adsumilli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
						</author>
						<title level="a" type="main">UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video quality assessment</term>
					<term>image quality assess- ment</term>
					<term>no-reference/blind</term>
					<term>user-generated content</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have witnessed an explosion of usergenerated content (UGC) videos shared and streamed over the Internet, thanks to the evolution of affordable and reliable consumer capture devices, and the tremendous popularity of social media platforms. Accordingly, there is a great need for accurate video quality assessment (VQA) models for UGC/consumer videos to monitor, control, and optimize this vast content. Blind quality prediction of in-the-wild videos is quite challenging, since the quality degradations of UGC videos are unpredictable, complicated, and often commingled. Here we contribute to advancing the UGC-VQA problem by conducting a comprehensive evaluation of leading no-reference/blind VQA (BVQA) features and models on a fixed evaluation architecture, yielding new empirical insights on both subjective video quality studies and objective VQA model design. By employing a feature selection strategy on top of efficient BVQA models, we are able to extract 60 out of 763 statistical features used in existing methods to create a new fusion-based model, which we dub the VIDeo quality EVALuator (VIDEVAL), that effectively balances the trade-off between VQA performance and efficiency. Our experimental results show that VIDEVAL achieves state-of-theart performance at considerably lower computational cost than other leading models. Our study protocol also defines a reliable benchmark for the UGC-VQA problem, which we believe will facilitate further research on deep learning-based VQA modeling, as well as perceptually-optimized efficient UGC video processing, transcoding, and streaming. To promote reproducible research and public evaluation, an implementation of VIDEVAL has been made available online: https://github.com/vztu/VIDEVAL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO dominates the Internet. In North America, Netflix and YouTube alone account for more than fifty percent of downstream traffic, and there are many other significant video service providers. Improving the efficiency of video encoding, storage, and streaming over communication networks is a principle goal of video sharing and streaming platforms. One relevant and essential research direction is the perceptual optimization of rate-distortion tradeoffs in video encoding and streaming, where distortion (or quality) is usually modeled using video quality assessment (VQA) algorithms that can predict human judgements of video quality. This has motivated Z. <ref type="bibr">Tu</ref>  years of research on the topics of perceptual video and image quality assessment (VQA/IQA). VQA research can be divided into two closely related categories: subjective video quality studies and objective video quality modeling. Subjective video quality research usually requires substantial resources devoted to time-and laborconsuming human studies to obtain valuable and reliable subjective data. The datasets obtained from subjective studies are invaluable for the development, calibration, and benchmarking of objective video quality models that are consistent with subjective mean opinion scores (MOS).</p><p>Hence, researchers have devoted considerable efforts on the development of high-quality VQA datasets that benefit the video quality community. <ref type="table" target="#tab_1">Table I</ref> summarizes the ten-year evolution of popular public VQA databases. The first successful VQA database was the LIVE Video Quality Database <ref type="bibr" target="#b0">[1]</ref>, which was first made publicly available in 2008. It contains 10 pristine high-quality videos subjected to compression and transmission distortions. Other similar databases targeting simulated compression and transmission distortions have been released subsequently, including EPFL-PoliMI <ref type="bibr" target="#b1">[2]</ref>, VQEG-HDTV <ref type="bibr" target="#b2">[3]</ref>, IVP <ref type="bibr" target="#b3">[4]</ref>, TUM 1080p50 <ref type="bibr" target="#b5">[5]</ref>, CSIQ <ref type="bibr" target="#b6">[6]</ref>, MCL-V <ref type="bibr" target="#b7">[7]</ref>, and MCL-JCV <ref type="bibr" target="#b8">[8]</ref>. All of the above mentioned datasets are based on a small set of high-quality videos, dubbed "pristine" or "reference," then synthetically distorting them in a controlled manner. We will refer to these kinds of synthetically-distorted video sets as legacy VQA databases. Legacy databases are generally characterized by only a small number of unique contents, each simultaneously degraded by only one or at most two synthetic distortions. For most practical scenarios, these are too simple to represent the great variety of real-world videos, and hence, VQA models derived on these databases may be insufficiently generalizable to large-scale realistic commercial VQA applications.</p><p>Recently, there has been tremendous growth in social media, where huge volumes of user-generated content (UGC) is shared over the media platforms such as YouTube, Facebook, and TikTok. Advances in powerful and affordable mobile devices and cloud computing techniques, combined with significant advances in video streaming have made it easy for most consumers to create, share, and view UGC pictures/videos instantaneously across the globe. Indeed, the prevalence of UGC has started to shift the focus of video quality research from legacy synthetically-distorted databases to newer, largerscale authentic UGC datasets, which are being used to create solutions to what we call the UGC-VQA problem. UGC-VQA studies typically follow a new design paradigm whereby: 1) All the source content is consumer-generated instead of professional-grade, thus suffers from unknown and highly diverse impairments; 2) they are only suitable for testing and comparing no-reference models, since reference videos are unavailable; 3) the types of distortions are authentic and commonly intermixed, and include but are not limited to capture impairments, editing and processing artifacts, compression, transcoding, and transmission distortions. Moreover, compression artifacts are not necessarily the dominant factors affecting video quality, unlike legacy VQA datasets and algorithms. These unpredictable perceptual degradations make perceptual quality prediction of UGC consumer videos very challenging.</p><p>Here we seek to address and gain insights into this new challenge (UGC-VQA) by first, conducting a comprehensive benchmarking study of leading video quality models on several recently released large-scale UGC-VQA databases. We also propose a new fusion-based blind VQA (BVQA) algorithm, which we call the VIDeo quality EVALuator (VIDEVAL), which is created by the processes of feature selection from existing top-performing VQA models. The empirical results show that a simple aggregation of these known models can achieve state-of-the-art (SOTA) performance. We believe that our expansive study will inspire and drive future research on BVQA modeling for the challenging UGC-VQA problem, and also pave the way towards deep learning-based solutions.</p><p>The outline of this paper is as follows: Section II reviews and analyzes the three most recent large-scale UGC-VQA databases, while Section III briefly surveys the development of BVQA models. We introduce the proposed VIDEVAL model in Section IV, and provide experimental results in Section V. Finally, concluding remarks are given in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. UGC-VQA DATABASES</head><p>The first UGC-relevant VQA dataset containing authentic distortions was introduced as the Camera Video Database (CVD2014) <ref type="bibr" target="#b12">[12]</ref>, which consists of videos with in-the-wild distortions from 78 different video capture devices, followed by the similar LIVE-Qualcomm Mobile In-Capture Database <ref type="bibr" target="#b13">[13]</ref>. These two databases, however, only modeled (camera) capture distortions on small numbers of not very diverse unique contents. Inspired by the first successful massive online crowdsourcing study of UGC picture quality <ref type="bibr" target="#b14">[14]</ref>, the authors of <ref type="bibr" target="#b10">[10]</ref> created the KoNViD-1k video quality database, the first such resource for UGC videos. It consists of 1,200 public-domain videos sampled from the YFCC100M dataset <ref type="bibr" target="#b15">[15]</ref>, and was annotated by 642 crowd-workers. LIVE-VQC <ref type="bibr" target="#b9">[9]</ref> was another large-scale UGC-VQA database with 585 videos, crowdsourced on Amazon Mechanical Turk to collect human opinions from 4,776 unique participants. The most recently published UGC-VQA database is the YouTube-UGC Dataset <ref type="bibr" target="#b11">[11]</ref> comprising 1,380 20-second video clips sampled from millions of YouTube videos, which were rated by more than 8,000 human subjects. <ref type="table" target="#tab_1">Table II</ref> summarizes the main characteristics of the three large-scale UGC-VQA datasets studied, while <ref type="figure">Figure 1</ref> shows some representative snapshots of the source sequences for each database, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Content Diversity and MOS Distribution</head><p>As a way of characterizing the content diversity of the videos in each database, Winkler <ref type="bibr" target="#b16">[16]</ref> suggested three quantitative attributes related to spatial activity, temporal activity, and colorfulness. Here we expand the set of attributes to include six low-level features including brightness, contrast, colorfulness <ref type="bibr" target="#b17">[17]</ref>, sharpness, spatial information (SI), and temporal information (TI), thereby providing a larger visual space in which to plot and analyze content diversities of the three UGC-VQA databases. To reasonably limit the computational cost, each of these features was calculated on every 10th frame, then was averaged over frames to obtain an overall feature representation of each content. For simplicity, we denote the features as {C i }, i = 1, 2, ..., 6. <ref type="figure">Figure 2</ref> shows the fitted kernel distribution of each selected feature. We also plotted the convex hulls of paired features, to show the feature coverage of each database, in <ref type="figure">Figure 3</ref>. To quantify the coverage and uniformity of these databases over each defined feature space,</p><formula xml:id="formula_0">(a) LIVE-VQC (b) KoNViD-1k (c) YouTube-UGC Fig. 1.</formula><p>Sample frames of the video contents contained in the three large scale UGC-VQA databases: (a) LIVE-VQC <ref type="bibr" target="#b9">[9]</ref>, (b) KoNViD-1k <ref type="bibr" target="#b10">[10]</ref>, and (c) YouTube-UGC <ref type="bibr" target="#b11">[11]</ref>. LIVE-VQC includes only natural contents captured by mobile devices, while KoNViD-1k and YouTube-UGC comprise of both natural videos, animations, and gaming sources. Note that YouTube-UGC video set is categorized whereas the others are not. we computed the relative range and uniformity of coverage <ref type="bibr" target="#b16">[16]</ref>, where the relative range is given by:</p><formula xml:id="formula_1">R k i = max(C k i ) ? min(C k i ) max k (C k i ) ,<label>(1)</label></formula><p>where C k i denotes the feature distribution of database k for a given feature dimension i, and max k (C k i ) specifies the maximum value for that given dimension across all databases.</p><p>Uniformity of coverage measures how uniformly distributed the videos are in each feature dimension. We computed this as the entropy of the B-bin histogram of C k i over all sources for each database indexed k:</p><formula xml:id="formula_2">U k i = ? B b=1 p b log B p b ,<label>(2)</label></formula><p>where p b is the normalized number of sources in bin b at feature i for database k. The higher the uniformity the more uniform the database is. Relative range and uniformity of coverage are plotted in <ref type="figure" target="#fig_1">Figure 4</ref> and <ref type="figure" target="#fig_2">Figure 5</ref>, respectively, quantifying the intra-and inter-database differences in source content characteristics. We also extracted 4,096-dimensional VGG-19 <ref type="bibr" target="#b18">[18]</ref> deep features and embedded these features into 2D subspace using t-SNE <ref type="bibr" target="#b19">[19]</ref> to further compare content diversity, as shown in <ref type="figure">Figure 7</ref>. Apart from content diversity expressed in terms of visual features, the statistics of the subjective ratings are another important attribute of each video quality database. The main aspect considered in the analysis here is the distributions of mean opinion scores (MOS), as these are indicative of the quality range of the subjective judgements. The analysis of standard deviation of MOS is not presented here since it is not provided in LIVE-VQC. <ref type="figure">Figure 6</ref> displays the histogram of MOS distributions for the three UGC-VQA databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Observations</head><p>We make some observations from the above plots. As may be seen in <ref type="figure">Figures 2a and 2b</ref>, and the corresponding convex hulls in <ref type="figure">Figure 3</ref>, KoNViD-1k and YouTube-UGC exhibit similar coverage in terms of brightness and contrast, while LIVE-VQC adheres closer to middle values. Regarding colorfulness, KoNViD-1k shows a skew towards higher scores than the other two datasets, which is consistent with the observations that Flickr users self-characterize as either professional video/photographers or as dedicated amateurs. On the sharpness and SI histograms, YouTube-UGC is spread most widely, while KoNViD-1k is concentrated on lower values. Another interesting finding from the TI statistics: LIVE-VQC is distributed more towards higher values than YouTube-UGC and KoNViD-1k, consistent with our observation that videos in LIVE-VQC were captured in the presence of larger and more frequent camera motions. We will revisit this interesting aspect of TI when evaluating the BVQA models in Section V. The visual comparison in <ref type="figure">Figure 7</ref> shows that YouTube-UGC and KoNViD-1k span a wider range of VGG-19 feature space than does LIVE-VQC, indicating significant content diversity differences. <ref type="figure">Figure 6</ref> shows the MOS distributions: all three databases have right-skewed MOS distributions, with KoNViD-1k less so, and LIVE-VQC and YouTube-UGC more so. The overall ranges and uniformity comparisons in Figures 4, 5, and 7 suggest that constructing a database by crawling  and sampling from a large content repository is likely to yield a more content-diverse, uniformly-distributed dataset than one created from pictures or videos captured directly from a set of user cameras. Both cases may be argued to be realistic in some scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. UGC-VQA MODELS</head><p>The goal of subjective video quality studies is to motivate the development of automatic objective video quality models. Conventionally, objective video quality assessment can be classified into three main categories: full-reference (FR), reduced-reference (RR), and no-reference (NR) models. FR-VQA models require the availability of an entire pristine source video to measure visual differences between a target signal and a corresponding reference <ref type="bibr" target="#b20">[20]</ref>- <ref type="bibr" target="#b23">[23]</ref>, while RR-VQA models only make use of a limited amount of reference information <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>. Some popular FR-VQA models, including PSNR, SSIM <ref type="bibr" target="#b26">[26]</ref>, and VMAF <ref type="bibr" target="#b21">[21]</ref> have already been successfully and massively deployed to optimize streaming and shared/uploaded video encoding protocols by leading video service providers. NR-VQA or BVQA models, however, rely solely on analyzing the test stimuli without the benefit of any corresponding "ground truth" pristine signal. It is obvious that only BVQA models are appropriate for the UGC-VQA problem. Here we briefly review the evolution of BVQA models, from conventional handcrafted feature-based approaches, on to convolutional neural network-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conventional Feature-Based BVQA Models</head><p>Almost all of the earliest BVQA models have been 'distortion specific,' meaning they were designed to quantify a specific type of distortion such as blockiness <ref type="bibr" target="#b27">[27]</ref>, blur <ref type="bibr" target="#b28">[28]</ref>, ringing <ref type="bibr" target="#b29">[29]</ref>, banding <ref type="bibr" target="#b30">[30]</ref>- <ref type="bibr" target="#b32">[32]</ref>, or noise <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> in distorted videos, or to assess multiple specific coincident distortion types caused by compression or transmission impairments <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>. More recent top-performing BVQA models are almost exclusively learning-based, leveraging a set of generic quality-aware features, combined to conduct quality prediction by machine learning regression <ref type="bibr" target="#b37">[37]</ref>- <ref type="bibr" target="#b45">[45]</ref>. Learningbased BVQA models are more versatile and generalizable than 'distortion specific' models, in that the selected features are broadly perceptually relevant, while powerful regression models can adaptively map the features onto quality scores learned from the data in the context of a specific application.</p><p>The most popular BVQA algorithms deploy perceptually relevant, low-level features based on simple, yet highly regular parametric bandpass models of good-quality scene statistics <ref type="bibr" target="#b46">[46]</ref>. These natural scene statistics (NSS) models predictably deviate in the presence of distortions, thereby characterizing perceived quality degradations <ref type="bibr" target="#b47">[47]</ref>. Successful blind picture quality assessment (BIQA) models of this type have been developed in the wavelet (BIQI <ref type="bibr" target="#b48">[48]</ref>, DIIVINE <ref type="bibr" target="#b37">[37]</ref>, C-DIIVINE <ref type="bibr" target="#b49">[49]</ref>), discrete cosine transform (BLIINDS <ref type="bibr" target="#b50">[50]</ref>, BLIINDS-II [51]), curvelet <ref type="bibr" target="#b52">[52]</ref>, and spatial intensity domains (NIQE <ref type="bibr" target="#b53">[53]</ref>, BRISQUE <ref type="bibr" target="#b38">[38]</ref>), and have further been extended to video signals using natural bandpass space-time video statistics models <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b54">[54]</ref>- <ref type="bibr" target="#b56">[56]</ref>, among which the most wellknown model is the Video-BLIINDS <ref type="bibr" target="#b39">[39]</ref>. Other extensions to empirical NSS include the joint statistics of the gradient magnitude and Laplacian of Gaussian responses in the spatial domain (GM-LOG <ref type="bibr" target="#b57">[57]</ref>), in log-derivative and log-Gabor spaces (DESIQUE <ref type="bibr" target="#b58">[58]</ref>), as well as in the gradient domain of LAB color transforms (HIGRADE <ref type="bibr" target="#b40">[40]</ref>). The FRIQUEE model <ref type="bibr" target="#b41">[41]</ref> has been observed to achieve SOTA performance both on UGC/consumer video/picture databases like LIVE-Challenge <ref type="bibr" target="#b14">[14]</ref>, CVD2014 <ref type="bibr" target="#b12">[12]</ref>, and KoNViD-1k <ref type="bibr" target="#b10">[10]</ref> by leveraging a bag of NSS features drawn from diverse color spaces and perceptually motivated transform domains.</p><p>Instead of using NSS-inspired feature descriptors, methods like CORNIA <ref type="bibr" target="#b43">[43]</ref> employ unsupervised learning techniques to learn a dictionary (or codebook) of distortions from raw image patches, and was further extended to Video CORNIA <ref type="bibr" target="#b59">[59]</ref> by applying an additional temporal hysteresis pooling <ref type="bibr" target="#b60">[60]</ref> of learned frame-level quality scores. Similar to CORNIA, the authors of <ref type="bibr" target="#b61">[61]</ref> proposed another codebook-based generalpurpose BVQA method based on High Order Statistics Aggregation (HOSA), requiring only a small codebook, yet yielding promising performance.</p><p>A very recent handcrafted feature-based BVQA model is the "two level" video quality model (TLVQM) <ref type="bibr" target="#b42">[42]</ref>, wherein a two-level feature extraction mechanism is adopted to achieve efficient computation of a set of carefully-defined impairment/distortion-relevant features. Unlike NSS features, TLVQM selects a comprehensive feature set comprising of empirical motion statistics, specific artifacts, and aesthetics. TLVQM does require that a large set of parameters (around 30) be specified, which may affect performance on datasets or application scenarios it has not been exposed to. The model currently achieves SOTA performance on three UGC video quality databases, CVD2014 <ref type="bibr" target="#b12">[12]</ref>, KoNViD-1k <ref type="bibr" target="#b10">[10]</ref>, and LIVE-Qualcomm <ref type="bibr" target="#b13">[13]</ref>, at a reasonably low complexity, as reported by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Convolutional Neural Network-Based BVQA Models</head><p>Deep convolutional neural networks (CNNs or ConvNets) have been shown to deliver standout performance on a wide variety of low-level computer vision applications. Recently, the release of several "large-scale" (in the context of IQA/VQA research) subjective quality databases <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b14">[14]</ref> have sped the application of deep CNNs to perceptual quality modeling. For example, several deep learning picture-quality prediction </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEATURE NAME FEATURE INDEX #(F INIT ) #(VIDEVAL)</head><p>BRISQUEavg</p><formula xml:id="formula_3">f 1 ? f 36 36 3 BRISQUE std f 37 ? f 72 36 1 GM-LOGavg f 73 ? f 112 40 4 GM-LOG std f 113 ? f 152 40 5 HIGRADE-GRADavg f 153 ? f 188 36 8 HIGRADE-GRAD std f 189 ? f 224 36 1 FRIQUEE-LUMAavg f 225 ? f 298 74 4 FRIQUEE-LUMA std f 299 ? f 372 74 8 FRIQUEE-CHROMAavg f 373 ? f 452 80 10 FRIQUEE-CHROMA std f 453 ? f 532 80 1 FRIQUEE-LMSavg f 533 ? f 606 74 1 FRIQUEE-LMS std f 607 ? f 680 74 0 FRIQUEE-HSavg f 681 ? f 684 4 0 FRIQUEE-HS std f 685 ? f 688 4 0 TLVQM-LCFavg f 689 ? f 710 22 5 TLVQM-LCF std f 711 ? f 733 23 3 TLVQM-HCF f 734 ? f 763 30 6 F ALL f 1 ? f 763 763 60</formula><p>All the spatial features are calculated every two frames and aggregated into a single feature vector within 1-sec chunks. The overall feature vector for the whole video is then obtained by averaging all the chunk-wise feature vectors. Subscript avg means within-chunk average pooling, whereas subscript std means within-chunk standard deviation pooling. methods were proposed in <ref type="bibr" target="#b62">[62]</ref>- <ref type="bibr" target="#b65">[65]</ref>. To conquer the limits of data scale, they either propose to conduct patch-wise training <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b66">[66]</ref> using global scores, or by pretraining deep nets on ImageNet <ref type="bibr" target="#b67">[67]</ref>, then fine tuning. Several authors report SOTA performance on legacy synthetic distortion databases <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b69">[69]</ref> or on naturally distorted databases <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b71">[70]</ref>.</p><p>Among the applications of deep CNNs to blind video quality prediction, Kim <ref type="bibr" target="#b72">[71]</ref> proposed a deep video quality assessor (DeepVQA) to learn the spatio-temporal visual sensitivity maps via a deep ConvNet and a convolutional aggregation network. The V-MEON model <ref type="bibr" target="#b73">[72]</ref> used a multi-task CNN framework which jointly optimizes a 3D-CNN for feature extraction and a codec classifier using fully-connected layers to predict video quality. Zhang <ref type="bibr" target="#b74">[73]</ref> leveraged transfer learning to develop a general-purpose BVQA framework based on weakly supervised learning and a resampling strategy. In the VSFA model <ref type="bibr" target="#b75">[74]</ref>, the authors applied a pre-trained image classification CNN as a deep feature extractor and integrated the frame-wise deep features using a gated recurrent unit and a subjectively-inspired temporal pooling layer, and reported leading performance on several natural video databases <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. These SOTA deep CNN-based BVQA models <ref type="bibr" target="#b72">[71]</ref>- <ref type="bibr" target="#b75">[74]</ref> produce accurate quality predictions on legacy (single synthetic distortion) video datasets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[6]</ref>, but struggle on recent in-the-wild UGC databases <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FEATURE FUSED VIDEO QUALITY EVALUATOR (VIDEVAL)</head><p>We have just presented a diverse set of BVQA models designed from a variety of perspectives, each either based on scene statistics, or motivated by visual impairment heuristics. As might be expected, and as we shall show later,  the performances of these models differ, and also vary on different datasets. We assume that the features extracted from different models may represent statistics of the signal in different perceptual domains, and henceforce, a selected fusion of BVQA models may be expected to deliver better consistency against subjective assessment, and also to achieve more reliable performance across different databases and use cases. This inspired our new feature fused VIDeo quality EVALuator (VIDEVAL), as described next.</p><p>We begin by constructing an initial feature set on top of existing high-performing, compute-efficient BVQA models and features, distilled through a feature selection program. The goal of feature selection is to choose an optimcal or sub-optimal feature subset F k ? R k from the initial feature set F INIT ? R N (where k &lt; N ) that achieves nearly top performance but with many fewer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction</head><p>We construct an initial feature set by selecting features from existing top-performing BVQA models. For practical reasons, we ignore features with high computational cost, e.g., certain features from DIIVINE, BLIINDS, C-DIIVINE, and V-BLIINDS. We also avoid using duplicate features in different models, such as the BRISQUE-like features in HIGRADE, and the C-DIIVINE features in V-BLIINDS. This filtering process yields the initial feature candidates, which we denote as BRISQUE, GM-LOG, HIGRADE-GRAD, FRIQUEE- LUMA, FRIQUEE-CHROMA, FRIQUEE-LMS, FRIQUEE-HS, TLVQM-LCF, and TLVQM-HCF. Inspired by the efficacy of standard deviation pooling as first introduced in GMSD <ref type="bibr" target="#b76">[75]</ref> and later also used in TLVQM <ref type="bibr" target="#b42">[42]</ref>, we calculate these spatial features every second frame within each sequentially cut non-overlapping one-second chunk, then we enrich the feature set by applying average and standard deviation pooling of frame-level features within each chunk, based on the hypothesis that the variation of spatial NSS features also correlates with the temporal properties of the video. Finally, all the chunk-wise feature vectors are average pooled <ref type="bibr" target="#b77">[76]</ref> across all the chunks to derive the final set of features for the entire video. <ref type="table" target="#tab_1">Table III</ref> indexes and summarizes the selected features in the initial feature set, yielding an overall 763-dimensional feature vector, F INIT ? R 763 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Selection</head><p>We deploy two types of feature selection algorithms to distill the initial feature set. The first method is a modelbased feature selector that utilizes a machine learning model to suggest features that are important. We employed the popular random forest (RF) to fit a regression model and eliminate the least significant features sorted by permutation importance. We also trained a support vector machine (SVM) with the linear kernel to rank the features, as a second model selector. Another sub-optimal solution is to apply a greedy search approach to find a good feature subset. Here we employed Sequential Forward Floating Selection (SFFS), and used SVM as the target regressor with its corresponding mean squared error between the predictions and MOS as the cost function. The mean squared error is calculated by cross-validation measures of predictive accuracy to avoid overfitting.</p><p>One problem with feature selection is that we do not know a priori what k to select, i.e., how many features are needed. Therefore, we conducted a two-step feature selection procedure. First, we evaluated the feature selection methods as a function of k via 10 train-test iterations, to select the best algorithm with corresponding optimal k. <ref type="figure" target="#fig_4">Figure 8</ref> shows the median PLCC (defined in Section V-A) performance with respect to k for different feature selection models, based on which we finally chose the SVM importance method with k = 60 in our next experiments. In the second step, we applied the best feature selection algorithm with the fixed  best k over 100 random train-test splits. On each iteration, a subset is selected from the feature selector, based on which the frequency of each feature over the iterations is counted, and the j most frequently occurring features are included into the final feature set. <ref type="figure" target="#fig_5">Figure 9</ref> shows the frequency of each feature being selected over 100 random splits in the second step. This selection process is implemented on a combined dataset constructed from three independent databases, as described in Section V-A. <ref type="table" target="#tab_1">Table III</ref> summarizes the results of the feature selection procedure (SVR importance with k = 60), yielding the final proposed VIDEVAL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Protocol</head><p>UGC Dataset Benchmarks. To conduct BVQA performance evaluation, we used the three UGC-VQA databases: KoNViD-1K <ref type="bibr" target="#b10">[10]</ref>, LIVE-VQC <ref type="bibr" target="#b9">[9]</ref>, and YouTube-UGC <ref type="bibr" target="#b11">[11]</ref>. We found that the YouTube-UGC dataset contains 57 grayscale videos, which yield numerical errors when computing the color model FRIQUEE. Therefore, we extracted a subset of  1,323 color videos from YouTube-UGC, which we denote here as the YouTube-UGC c set, for the evaluation of color models. In order to study overall model performances on all the databases, we created a large composite benchmark, which is referred to here as All-Combined c , using the iterative nested least squares algorithm (INLSA) suggested in <ref type="bibr" target="#b78">[77]</ref>, wherein YouTube-UGC is selected as the anchor set, and the objective MOS from the other two sets, KoNViD-1k and LIVE-VQC, are linearly mapped onto a common scale ( <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">5]</ref>). <ref type="figure" target="#fig_6">Figure 10</ref> shows scatter plots of MOS versus NIQE scores before ( <ref type="figure" target="#fig_6">Figure  10a</ref> where <ref type="formula">(3)</ref> and <ref type="formula">(4)</ref> are for calibrating KoNViD-1k and LIVE-VQC, respectively. y adj denotes the adjusted scores, while y org is the original MOS.</p><p>BVQA Model Benchmarks. We include a number of representative BVQA/BIQA algorithms in our benchmarking evaluation as references to be compared against. These baseline models include NIQE <ref type="bibr" target="#b53">[53]</ref>, ILNIQE <ref type="bibr" target="#b79">[78]</ref>, VIIDEO <ref type="bibr" target="#b55">[55]</ref>, BRISQUE <ref type="bibr" target="#b38">[38]</ref>, GM-LOG <ref type="bibr" target="#b57">[57]</ref>, HIGRADE <ref type="bibr" target="#b40">[40]</ref>, FRIQUEE <ref type="bibr" target="#b41">[41]</ref>, CORNIA <ref type="bibr" target="#b43">[43]</ref>, HOSA <ref type="bibr" target="#b61">[61]</ref>, KonCept512 <ref type="bibr" target="#b71">[70]</ref>, PaQ-2-PiQ <ref type="bibr" target="#b64">[64]</ref>, V-BLIINDS <ref type="bibr" target="#b39">[39]</ref>, and TLVQM <ref type="bibr" target="#b42">[42]</ref>. Among these, NIQE, ILNIQE, and VIIDEO are "completely blind" (opinionunaware (OU)), since no training is required to build them. The rest of the models are all training-based (opinion-aware (OA)) and we re-train the models/features when evaluating on a given dataset. We also utilized the well-known deep CNN models VGG-19 <ref type="bibr" target="#b18">[18]</ref> and ResNet-50 <ref type="bibr" target="#b80">[79]</ref> as additional CNN-based baseline models, where each was pretrained on the ImageNet classification task. The fully-connected layer (4,096-dim) from VGG-19 and average-pooled layer (2,048-dim) from ResNet-50 served as deep feature descriptors, by operating on 25 227?227 random crops of each input frame, then averagepooled into a single feature vector representing the entire frame <ref type="bibr" target="#b63">[63]</ref>. Two SOTA deep BIQA models, KonCept512 <ref type="bibr" target="#b71">[70]</ref> and PaQ-2-PiQ <ref type="bibr" target="#b64">[64]</ref>, were also included in our evaluations. We implemented the feature extraction process for each evaluated BVQA model using its initial released implementation in MATLAB R2018b, except that VGG-19 and ResNet-50 were implemented in TensorFlow, while KonCept512 1 and PaQ-2-PiQ 2 were implemented in PyTorch. All the feature-based BIQA models extract features at a uniform sampling rate of one frame per second, then temporally average-pooled to obtain the overall video-level feature. Regression Models. We used a support vector regressor (SVR) as the back-end regression model to learn the featureto-score mappings, since it achieves excellent performance in most cases <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b59">[59]</ref>, <ref type="bibr" target="#b63">[63]</ref>. The effectiveness of SVR, however, largely depends on the selection of its hyperparameters. As recommended in <ref type="bibr" target="#b81">[80]</ref>, we optimized the SVR parameter values (C, ?) by a grid-search of 10 ? 10 exponentially growing sequences (in our experiments, we used a grid of C = 2 1 , 2 2 , ..., 2 10 , ? = 2 ?8 , 2 ?7 , ..., 2 1 ) using crossvalidation on the training set. The pair (C, ?) yielding the best cross-validation performance, as measured by the root mean squared error (RMSE) between the predicted scores and the MOS, is picked. Afterward, the selected model parameters are applied to re-train the model on the entire training set, and we report the evaluation results on the test set. This kind of crossvalidation procedure can prevent over-fitting, thus providing fair evaluation of the compared BVQA models. We chose the linear kernel for CORNIA, HOSA, VGG-19, and ResNet-50, considering their large feature dimension, and the radial basis function (RBF) kernel for all the other algorithms. We used Python 3.6.7 with the scikit-learn toolbox to train and test all the evaluated learning-based BVQA models.</p><p>Performance Metrics. Following convention, we randomly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance on Individual and Combined Datasets</head><p>Table IV shows the performance evaluation of the three "completely blind" BVQA models, NIQE, ILNIQE, and VI-IDEO on the four UGC-VQA benchmarks. None of these methods performed very well, meaning that we still have much room for developing OU "completely blind" UGC video quality models. <ref type="table" target="#tab_5">Table V</ref> shows the performance evaluation of all the learning-based BVQA models trained with SVR on the four datasets in our evaluation framework. For better visualization, we also show box plots of performances as well as scatter plots of predictions versus MOS on the All-Combined c set, in <ref type="figure" target="#fig_7">Figures 11 and 12</ref>  It is also worth mentioning that the deep CNN baseline methods (VGG-19 and ResNet-50), despite being trained as picture-only models, performed quite well on KoNViD-1k and All-Combined c . This suggests that transfer learning is a promising technique for the blind UGC-VQA problem, consistent with conclusions drawn for picture-quality prediction <ref type="bibr" target="#b63">[63]</ref>. Deep models will perform even better, no doubt, if trained on temporal content and distortions.</p><p>The two most recent deep learning picture quality models, PaQ-2-PiQ, and KonCept512, however, did not perform very well on the three evaluated video datasets. The most probable reason would be that these models were trained on picture quality datasets <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b71">[70]</ref>, which contain different types of (strictly spatial) distortions than UGC-VQA databases. Models trained on picture quality sets do not necessarily transfer very well to UGC video quality problems. In other words, whatever model should be either trained or fine-tuned on UGC-VQA datasets in order to obtain reasonable performance. Indeed, if temporal distortions (like judder) are present, they may severely underperform if the frame quality is high <ref type="bibr" target="#b82">[81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation on Categorical Subsets</head><p>We propose three new categorical evaluation methodologies -resolution, quality, and content-based category breakdown. These will allow us to study the compared BVQA models from additional and practical aspects in the context of realworld UGC scenarios, which have not been, nor can it be accounted in previous legacy VQA databases or studies.</p><p>For resolution-dependent evaluation, we divided the All-Combined c set into three subsets, based on video resolution: (1) 427 1080p-videos (110 from LIVE-VQC, 317 from YouTube-UGC), (2) 566 720p-videos (316 from LIVE-VQC, 250 from YouTube-UGC), and <ref type="table" target="#tab_1">(3) 448 videos with resolu-TABLE XI  PERFORMANCE COMPARISON OF A TOTAL OF ELEVEN TEMPORAL POOLING METHODS USING TLVQM AND VIDEVAL AS TESTBEDS ON KONVID-1K,  LIVE-VQC, AND YOUTUBE-UGC. THE THREE BEST RESULTS ALONG EACH COLUMN ARE BOLDFACED.   DATABASE  KoNViD-1k  LIVE-VQC  YouTube-UGC  MODEL  TLVQM  VIDEVAL  TLVQM  VIDEVAL  TLVQM  VIDEVAL  POOLING  SRCC PLCC  SRCC PLCC  SRCC PLCC  SRCC PLCC  SRCC</ref>  tion ?480p (29 from LIVE-VQC, 419 from YouTube-UGC), since we are also interested in performance on videos of different resolutions. We did not include 540p-videos, since those videos are almost exclusively from KoNViD-1k. <ref type="table" target="#tab_1">Table  VI</ref> shows the resolution-breakdown evaluation results. Generally speaking, learned features (CORNIA, HOSA, VGG-19, KonCept512, and ResNet-50) outperformed hand-designed features, among which ResNet-50 ranked first.</p><p>Here we make two arguments to try to explain the observations above: (1) video quality is intrinsically correlated with resolution; (2) NSS features are implicitly resolutionaware, while CNN features are not. The first point is almost self-explanatory, no matter to what degree one agrees. To further justify this, we trained an SVR only using resolution (height, width) as features to predict MOS on YouTube-UGC, which contains balanced samples across five different resolutions. This yielded surprisingly high values 0.576/0.571 for SRCC/PLCC, indicating the inherent correlation between video quality and resolution. Secondly, we selected one 2160p video from YouTube-UGC, namely 'Vlog2160P-408f.mkv,' and plotted, in <ref type="figure" target="#fig_11">Figure 13</ref>, the mean-subtracted contrastnormalized (MSCN) distributions of its downscaled versions: 2160p, 1440p, 1080p, 720p, 480p, and 360p. It may be observed that resolution can be well separated by MSCN statistics, based on which most feature-based methods are built. We may infer, from these two standpoints, that including various resolutions of videos is favorable to the training of NSS-based models, since NSS features are resolution-aware, and resolution is further well correlated with quality. In other words, the resolution-breakdown evaluation shown in <ref type="table" target="#tab_1">Table  VI</ref>, which removes this important implicit feature (resolution), would possibly reduce the performance of NSS-based models, such as FRIQUEE and VIDEVAL.</p><p>We also divided the All-Combined c into subsets based on content category: Screen Content (163), Animation (81), Gaming (209), and Natural (2,667) videos. We only reported the evaluation results on the first three subsets in <ref type="table" target="#tab_1">Table  VII</ref>, since we observed similar results on the Natural subset with the entire combined set. The proposed VIDEVAL model outperformed over all categories, followed by ResNet-50 and FRIQUEE, suggesting that VIDEVAL features are robust quality indicatives across different content categories.</p><p>The third categorical division is based on quality scores: we partitioned the combined set into Low Quality (1,558) and High Quality (1,550) halves, using the median quality value 3.5536 as the threshold, to see the model performance only on high/low quality videos. Performance results are shown in <ref type="table" target="#tab_1">Table VIII</ref>, wherein VIDEVAL still outperformed the other BVQA models on both low and high quality partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross Dataset Generalizability</head><p>We also performed a cross dataset evaluation to verify the generalizability of BVQA models, wherein LIVE-VQC, KoNViD-1k, and YouTube-UGC c were included. That is, we trained the regression model on one full database and report the performance on another. To retain label consistency, we linearly scaled the MOS values in LIVE-VQC from raw [0, 100] to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">5]</ref>, which is the scale for the other two datasets. We used SVR for regression and adopted k-fold cross validation using the same grid-search as in Section V-A for hyperparameter selection. The selected parameter pair were then applied to re-train the SVR model on the full training set, and the performance results on the test set were recorded. <ref type="table" target="#tab_1">Table IX</ref> and X show the best performing methods with cross domain performances in terms of SRCC and PLCC, respectively.</p><p>We may see that the cross domain BVQA algorithm generalization between LIVE-VQC and KoNViD-1k was surprisingly N : number of pixels per frame; T : number of frames computed for feature extraction. Note that for VIIDEO and V-BLIINDS, T is the total number of frames, whereas for IQA models, T equals the total number of frames sampled at 1 fr/sec. For TLVQM and VIDEVAL, T 1 is total number of frames divided by 2, while T 2 is the number of frames sampled at 1 fr/sec. good, and was well characterized by pre-trained ResNet-50 features. We also observed better algorithm generalization between KoNViD-1k and YouTube-UGC than LIVE-VQC, as indicated by the performances of the best model, VIDEVAL. This might be expected, since as <ref type="figure">Figure 7</ref> shows, YouTube-UGC and KoNViD-1k share overlapped coverage of content space, much larger than that of LIVE-VQC. Therefore, we may conclude that VIDEVAL and ResNet-50 were the most robust BVQA models among those compared in terms of cross domain generalization capacities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effects of Temporal Pooling</head><p>Temporal pooling is one of the most important, unresolved problems for video quality prediction <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b77">[76]</ref>, <ref type="bibr" target="#b83">[82]</ref>, <ref type="bibr" target="#b84">[83]</ref>. In our previous work <ref type="bibr" target="#b77">[76]</ref>, we have studied the efficacy of various pooling methods using scores predicted by BIQA models. Here we extend this to evaluate on SOTA BVQA models. For practical considerations, the high-performing TLVQM and VIDEVAL were selected as exemplar models. Since these two models independently extract features on each one-second block, we applied temporal pooling of chunk-wise quality predictions. A total of eleven pooling methods were tested: three Pythagorean means (arithmetic, geometric, and harmonic mean), median, Minkowski (p = 2) mean, percentile pooling (20%) <ref type="bibr" target="#b85">[84]</ref>, VQPooling <ref type="bibr" target="#b83">[82]</ref>, primacy and recency pooling <ref type="bibr" target="#b86">[85]</ref>, hysteresis pooling <ref type="bibr" target="#b60">[60]</ref>, and our previously proposed ensemble method, EPooling <ref type="bibr" target="#b77">[76]</ref>, which aggregates multiply pooled scores by training a second regressor on top of mean, Minkowski, percentile, VQPooling, variation, and hysteresis pooling. We refer the reader to <ref type="bibr" target="#b77">[76]</ref> for detailed algorithmic formulations and parameter settings thereof.</p><p>It is worth noting that the results in <ref type="table" target="#tab_1">Table XI</ref> are only self-consistent, meaning that they are not comparable to any prior experiments -since we employed chunk-wise instead of previously adopted video-wise quality prediction to be able to apply temporal quality pooling, which may affect the base performance. Here we observed yet slightly different results using BVQA testbeds as compared to what we observed on BIQA <ref type="bibr" target="#b77">[76]</ref>. Generally, we found the mean families and ensemble pooling to be the most reliable pooling methods. Traditional sample mean prediction may be adequate in many cases, due to its simplicity. Pooling strategies that more heavily weight low-quality parts, however, were not observed to perform very well on the tested BVQA, which might be attributed to the fact that not enough samples <ref type="bibr">(8 ? 20)</ref> can be extracted from each video to attain statistically meaningful results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complexity Analysis and Runtime Comparison</head><p>The efficiency of a video quality model is of vital importance in practical commercial deployments. Therefore, we also tabulated the computational complexity and runtime cost of the compared BVQA models, as shown in <ref type="table" target="#tab_1">Tables  XII, XIII</ref>  in <ref type="table" target="#tab_1">Table XII</ref>. The proposed VIDEVAL method achieves a reasonable complexity among the top-performing algorithms, TLVQM, and FRIQUEE. We also present theoretical time complexity in <ref type="table" target="#tab_1">Table XII</ref> for potential analytical purposes. We also provide in <ref type="table" target="#tab_1">Table XIII</ref> an additional runtime comparison between MATLAB models on CPU and deep learning models on CPU and GPU, respectively. It may be observed that top-performing BVQA models such as TLVQM and VIDEVAL are essentially slower than deep CNN models, but we expect orders-of-magnitude speedup if re-implemented in pure C/C++. Simpler NSS-based models such as BRISQUE and HIGRADE (which only involve several convolution operations) still show competitive efficiency relative to CNN models even when implemented in MATLAB. We have also seen a 5 ? 10 times speedup switching from CPU to GPU for the CNN models, among which KonCept512 with PyTorch-GPU was the fastest since it requires just a single pass to the CNN backbone, while the other three entail multiple passes for each input frame.</p><p>Note that the training/test time of the machine learning regressor is approximately proportional to the number of features. Thus, it is not negligible compared to feature computation given a large number of features, regardless of the regression model employed. The feature dimension of each model is listed in <ref type="table" target="#tab_1">Table XII</ref>. As may be seen, codebookbased algorithms (CORNIA (10k) and HOSA (14.7k)) require significantly larger numbers of features than other hand-crafted feature based models. Deep ConvNet features ranked second in dimension (VGG-19 (4,080) and ResNet-50 (2,048)). Our proposed VIDEVAL only uses 60 features, which is fairly compact, as compared to other top-performing BVQA models like FRIQUEE (560) and TLVQM <ref type="bibr" target="#b76">(75)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ensembling VIDEVAL with Deep Features</head><p>We also attempted a more sophisticated ensemble fusion of VIDEVAL and deep learning features to determine whether this could further boost its performance, which could give insights on the future direction of this field. Since PaQ-2-PiQ aimed for local quality prediction, we included the predicted 3 ? 5 local quality scores as well as a single global score, as additional features. For KonCept512, the feature vector (256dim) immediately before the last linear layer in the fullyconnected head was appended. Our own baseline CNN models, VGG-19 and ResNet-50, were also considered, because these are commonly used standards for downstream vision tasks. The overall results are summarized in <ref type="table" target="#tab_1">Table XIV</ref>. We may observe that ensembling VIDEVAL with certain deep learning models improved the performance by up to ? 4% compared to the vanilla VIDEVAL, which is very promising. Fusion with either ResNet-50 or KonCept512 yielded top performance. It should be noted that the number of fused features is also an essential aspect. For example, blending VIDEVAL (60dim) with VGG-19 (4,096-dim) may not be recommended, since the enormous number of VGG-19 features could possibly dominate the VIDEVAL features, as suggested by some performance drops in <ref type="table" target="#tab_1">Table XIV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Summarization and Takeaways</head><p>Finally, we briefly summarize the experimental results and make additional observations: 1) Generally, spatial distortions dominated quality prediction on Internet UGC videos like those from YouTube and Flickr, as revealed by the remarkable performances of picture-only models (e.g., HIGRADE, FRIQUEE, HOSA, ResNet-50) on them. Some motion-related features (as in TLVQM) may not apply as well in this scenario. 2) On videos captured with mobile devices (e.g., those in LIVE-VQC), which often present larger and more frequent camera motions, including temporal-or motionrelated features can be advantageous (e.g., V-BLIINDS, TLVQM, VIDEVAL). 3) Deep CNN feature descriptors (VGG-19, ResNet-50, etc.) pre-trained for other classical vision tasks (e.g. image classification) are transferable to UGC video quality predictions, achieving very good performance, suggesting that using transfer learning to address the general UGC-VQA problem is very promising. 4) It is still a very hard problem to predict UGC video quality on non-natural or computer-generated video contents: screen contents, animations, gaming, etc. Moreover, there are no sufficiently large UGC-VQA datasets designed for those kinds of contents. 5) A simple feature engineering and selection implementation built on top of current effective feature-based BVQA models is able to obtain excellent performance, as exemplified by the compact new model (VIDEVAL). 6) Simple temporal mean pooling of chunk-wise quality predictions by BVQA models yields decent and robust results. Furthermore, an ensemble pooling approach can noticeably improve the quality prediction performance, albeit with higher complexity. 7) Ensembling scene statistics-based BVQA models with additional deep learning features (e.g., VIDEVAL plus KonCept512) could further raise the performance upper bound, which may be a promising way of developing future BVQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have presented a comprehensive analysis and empirical study of blind video quality assessment for user-generated content (the UGC-VQA problem). We also proposed a new fusion-based BVQA model, called the VIDeo quality EVALuator (VIDEVAL), which uses a feature ensemble and selection procedure on top of existing efficient BVQA models. A systematic evaluation of prior leading video quality models was conducted within a unified and reproducible evaluation framework and accordingly, we concluded that a selected fusion of simple distortion-aware statistical video features, along with well-defined visual impairment features, is able to deliver state-of-the-art, robust performance at a very reasonable computational cost. The promising performances of baseline CNN models suggest the great potential of leveraging transfer learning techniques for the UGC-VQA problem. We believe that this benchmarking study will help facilitate UGC-VQA research by clarifying the current status of BVQA research and the relative efficacies of modern BVQA models. To promote reproducible research and public usage, an implementation of VIDEVAL has been made available online: https://github. com/vztu/VIDEVAL. In addition to the software, we are also maintaining an ongoing performance leaderboard on Github: https://github.com/vztu/BVQA Benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Feature distribution comparisons among the three considered UGC-VQA databases: KoNViD-1k, LIVE-VQC, and YouTube-UGC. Source content (blue 'x') distribution in paired feature space with corresponding convex hulls (orange boundaries). Left column: BR?CT, middle column: CF?SR, right column: SI?TI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Relative range R k i comparisons of the selected six features calculated on the three UGC-VQA databases: KoNViD-1k, LIVE-VQC, and YouTube-UGC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of coverage uniformity U k i of the selected six features computed on the three UGC-VQA databases: KoNViD-1k, LIVE-VQC, and YouTube-UGC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>MOS histograms and the fitted kernel distributions of the three UGC-VQA databases: KoNViD-1k, LIVE-VQC, and YouTube-UGC. VGG-19 deep feature embedding via t-SNE<ref type="bibr" target="#b19">[19]</ref> on KoNViD-1k, LIVE-VQC, and YouTube-UGC, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Feature selection performance (PLCC) of three selected algorithms as a function of k on the All-Combinedc dataset. The shaded error bar denotes the standard deviation of PLCC over 10 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Visualization of the second step in feature selection: frequency of each feature being selected over 100 iterations of train-test splits using SVR importance selection method with fixed k = 60.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Scatter plots of MOS versus NIQE scores (a) before, and (b) after INLSA calibration<ref type="bibr" target="#b78">[77]</ref> using YouTube-UGC as the reference set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Box plots of PLCC, SRCC, and KRCC of evaluated learning-based BVQA algorithms on the All-Combinedc dataset over 100 random splits. For each box, median is the central box, and the edges of the box represent 25th and 75th percentiles, while red circles denote outliers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>? MODEL \ METRIC SRCC? (STD) KRCC? (STD) PLCC? (STD) RMSE? (STD) SRCC? (STD) KRCC? (STD) PLCC? (STD) RMSE? (STD) BRISQUE (1 fr/sec) 0.3820 (.051) 0.2635 (.036) 0.3952 (.048) 0.5919 (.021) 0.5695 (.028) 0.4030 (.022) 0.5861 (.027) 0.5617 (.016) GM-LOG (1 fr/sec) 0.3678 (.058) 0.2517 (.041) 0.3920 (.054) 0.5896 (.022) 0.5650 (.029) 0.3995 (.022) 0.5942 (.030) 0.5588 (.014) HIGRADE (1 fr/sec) 0.7376 (.033) 0.5478 (.028) 0.7216 (.033) 0.4471 (.024) 0.7398 (.018) 0.5471 (.016) 0.7368 (.019) 0.4674 (.015) FRIQUEE (1 fr/sec) 0.7652 (.030) 0.5688 (.026) 0.7571 (.032) 0.4169 (.023) 0.7568 (.023) 0.5651 (.021) 0.7550 (.022) 0.4549 (.018) CORNIA (1 fr/sec) 0.5972 (.041) 0.4211 (.032) 0.6057 (.039) 0.5136 (.024) 0.6764 (.021) 0.4846 (.017) 0.6974 (.020) 0.4946 (.013) HOSA (1 fr/sec) 0.6025 (.034) 0.4257 (.026) 0.6047 (.034) 0.5132 (.021) 0.6957 (.018) 0.5038 (.015) 0.7082 (.016) 0.4893 (.013) VGG-19 (1 fr/sec) 0.7025 (.028) 0.5091 (.023) 0.6997 (.028) 0.4562 (.020) 0.7321 (.018) 0.5399 (.016) 0.7482 (.017) 0.4610 (.013) ResNet-50 (1 fr/sec) 0.7183 (.028) 0.5229 (.024) 0.7097 (.027) 0.4538 (.021) 0.7557 (.017) 0.5613 (.016) 0.7747 (.016) 0.4385 (.013) KonCept512 (1 fr/sec) 0.5872 (.039) 0.4101 (.030) 0.5940 (.041) 0.5135 (.022) 0.6608 (.022) 0.4759 (.018) 0.6763 (.022) 0.5091 (.014) PaQ-2-PiQ (1 fr/sec) 0.2658 (.047) 0.1778 (.032) 0.2935 (.049) 0.6153 (.019) 0.4727 (.029) 0.3242 (.021) 0.4828 (.029) 0.6081 (.015) V-BLIINDS 0.5590 (.049) 0.3899 (.036) 0.5551 (.046) 0.5356 (.022) 0.6545 (.023) 0.4739 (.019) 0.6599 (.023) 0.5200 (.016) TLVQM 0.6693 (.030) 0.4816 (.025) 0.6590 (.030) 0.4849 (.022) 0.7271 (.018) 0.5347 (.016) 0.7342 (.018) 0.4705 (.013) VIDEVAL 0.7787 (.025) 0.5830 (.023) 0.7733 (.025) 0.4049 (.021) 0.7960 (.015) 0.6032 (.014) 0.7939 (.015) 0.4268 (.015) FRIQUEE and VIDEVAL were evaluated on a subset of 1,323 color videos in YouTube-UGC, denoted YouTube-UGCc, since it yields numerical errors when calculating on the remaining 57 grayscale videos. For the other BVQA models evaluated, no significant difference was observed when evaluated on YouTube-UGCc versus YouTube-UGC, and hence we still report the results on YouTube-UGC. ? For a fair comparison, we only combined and calibrated (via INLSA [77]) all the color videos from these three databases to obtain the combined dataset, i.e., All-Combinedc (3,108) = KoNViD-1k (1,200) + LIVE-VQC (585) + YouTube-UGCc (1,323).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>) and after (Figure 10b) INLSA linear mapping, calibrated by NIQE [53] scores. The All-Combined c (3,108) dataset is simply the union of KoNViD-1k (1,200), LIVE-VQC (575), and YouTube-UGC c (1,323) after MOS calibration: y adj = 5 ? 4 ? [(5 ? y org )/4 ? 1.1241 ? 0.0993] (3) y adj = 5 ? 4 ? [(100 ? y org )/100 ? 0.7132 + 0.0253] (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Scatter plots and nonlinear logistic fitted curves of VQA models versus MOS trained with a grid-search SVR using k-fold cross-validation on the All-Combinedc set. (a) BRISQUE (1 fr/sec), (b) GM-LOG (1 fr/sec), (c) HIGRADE (1 fr/sec), (d) FRIQUEE (1 fr/sec), (e) CORNIA (1 fr/sec), (f) HOSA (1 fr/sec), (g) VGG-19 (1 fr/sec), (h) ResNet-50 (1 fr/sec), (i) KonCept512 (1 fr/sec), (j) PaQ-2-PiQ (1 fr/sec), (k) V-BLIINDS, (l) TLVQM, and (m) VIDEVAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>(a) An examplary 2160p video from YouTube-UGC and (b) the mean-subtracted contrast-normalized (MSCN) distributions of its downscaled versions: 2160p, 1440p, 1080p, 720p, 480p, and 360p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and A. C. Bovik are with Laboratory for Image and Video Engineering (LIVE), Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, 78712, USA (email: zhengzhong.tu@utexas.edu, bovik@utexas.edu). Y. Wang, N. Birkbeck, and B. Adsumilli are with YouTube Media Algorithms Team, Google LLC, Mountain View, CA, 94043, USA. (email: yilin@google.com, birkbeck@google.com, badsumilli@google.com) This work is supported by Google.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EVOLUTION</head><label>I</label><figDesc>OF POPULAR PUBLIC VIDEO QUALITY ASSESSMENT DATABASES: FROM LEGACY LAB STUDIES OF SYNTHETICALLY DISTORTED VIDEO SETS TO LARGE-SCALE CROWDSOURCED USER-GENERATED CONTENT (UGC) VIDEO DATASETS WITH AUTHENTIC DISTORTIONS</figDesc><table><row><cell>DATABASE</cell><cell cols="5">YEAR #CONT #TOTAL RESOLUTION FR</cell><cell cols="3">LEN FORMAT DISTORTION TYPE</cell><cell cols="3">#SUBJ #RATES DATA</cell><cell>ENV</cell></row><row><cell>LIVE-VQA</cell><cell>2008 10</cell><cell>160</cell><cell cols="2">768?432</cell><cell cols="2">25/50 10</cell><cell cols="2">YUV+264 Compression, transmission</cell><cell>38</cell><cell>29</cell><cell cols="2">DMOS+? In-lab</cell></row><row><cell>EPFL-PoliMI</cell><cell>2009 12</cell><cell>156</cell><cell cols="2">CIF/4CIF</cell><cell cols="2">25/30 10</cell><cell cols="2">YUV+264 Compression, transmission</cell><cell>40</cell><cell>34</cell><cell>MOS</cell><cell>In-lab</cell></row><row><cell>VQEG-HDTV</cell><cell>2010 49</cell><cell>740</cell><cell>1080i/p</cell><cell></cell><cell cols="2">25/30 10</cell><cell>AVI</cell><cell>Compression, transmission</cell><cell>120</cell><cell>24</cell><cell>RAW</cell><cell>In-lab</cell></row><row><cell>IVP</cell><cell>2011 10</cell><cell>138</cell><cell>1080p</cell><cell></cell><cell>25</cell><cell>10</cell><cell>YUV</cell><cell>Compression, transmission</cell><cell>42</cell><cell>35</cell><cell cols="2">DMOS+? In-lab</cell></row><row><cell>TUM 1080p50</cell><cell>2012 5</cell><cell>25</cell><cell>1080p</cell><cell></cell><cell>50</cell><cell>10</cell><cell>YUV</cell><cell>Compression</cell><cell>21</cell><cell>21</cell><cell>MOS</cell><cell>In-lab</cell></row><row><cell>CSIQ</cell><cell>2014 12</cell><cell>228</cell><cell cols="2">832?480</cell><cell cols="2">24-60 10</cell><cell>YUV</cell><cell>Compression, transmission</cell><cell>35</cell><cell>N/A</cell><cell cols="2">DMOS+? In-lab</cell></row><row><cell>CVD2014</cell><cell>2014 5</cell><cell>234</cell><cell cols="2">720p, 480p</cell><cell cols="3">9-30 10-25 AVI</cell><cell>Camera capture (authentic)</cell><cell>210</cell><cell>30</cell><cell>MOS</cell><cell>In-lab</cell></row><row><cell>MCL-V</cell><cell>2015 12</cell><cell>108</cell><cell>1080p</cell><cell></cell><cell cols="2">24-30 6</cell><cell>YUV</cell><cell>Compression, scaling</cell><cell>45</cell><cell>32</cell><cell>MOS</cell><cell>In-lab</cell></row><row><cell>MCL-JCV</cell><cell>2016 30</cell><cell>1560</cell><cell>1080p</cell><cell></cell><cell cols="2">24-30 5</cell><cell>MP4</cell><cell>Compression</cell><cell>150</cell><cell>50</cell><cell cols="2">RAW-JND In-lab</cell></row><row><cell>KoNViD-1k</cell><cell>2017 1200</cell><cell>1200</cell><cell>540p</cell><cell></cell><cell cols="2">24-30 8</cell><cell>MP4</cell><cell cols="2">Diverse distortions (authentic) 642</cell><cell>114</cell><cell>MOS+?</cell><cell>Crowd</cell></row><row><cell cols="2">LIVE-Qualcomm 2018 54</cell><cell>208</cell><cell>1080p</cell><cell></cell><cell>30</cell><cell>15</cell><cell>YUV</cell><cell>Camera capture (authentic)</cell><cell>39</cell><cell>39</cell><cell>MOS</cell><cell>In-lab</cell></row><row><cell>LIVE-VQC</cell><cell>2018 585</cell><cell>585</cell><cell cols="4">1080p-240p 19-30 10</cell><cell>MP4</cell><cell cols="3">Diverse distortions (authentic) 4776 240</cell><cell>MOS</cell><cell>Crowd</cell></row><row><cell>YouTube-UGC</cell><cell>2019 1380</cell><cell>1380</cell><cell>4k-360p</cell><cell></cell><cell cols="2">15-60 20</cell><cell>MKV</cell><cell cols="2">Diverse distortions (authentic) &gt;8k</cell><cell>123</cell><cell>MOS+?</cell><cell>Crowd</cell></row><row><cell cols="3">#CONT: Total number of unique contents.</cell><cell cols="8">#TOTAL: Total number of test sequences, including reference and distorted videos.</cell><cell></cell></row><row><cell cols="4">RESOLUTION: Video resolution (p: progressive).</cell><cell cols="3">FR: Framerate.</cell><cell cols="2">LEN: Video duration/length (in seconds).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FORMAT: Video container.</cell><cell cols="6">#SUBJ: Total number of subjects in the study.</cell><cell cols="4">#RATES: Average number of subjective ratings per video.</cell></row></table><note>ENV: Subjective testing environment. In-lab: study was conducted in a laboratory. Crowd: study was conducted by crowdsourcing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">PUBLIC LARGE-SCALE USER-GENERATED CONTENT VIDEO QUALITY ASSESSMENT (UGC-VQA) DATABASES COMPARED: KONVID-1K [10],</cell></row><row><cell></cell><cell cols="2">LIVE-VQC [9], AND YOUTUBE-UGC [11]</cell><cell></cell></row><row><cell>DATABASE ATTRIBUTE</cell><cell>KoNViD-1k</cell><cell>LIVE-VQC</cell><cell>YouTube-UGC</cell></row><row><cell>Number of contents</cell><cell>1200</cell><cell>585</cell><cell>1380</cell></row><row><cell>Video sources</cell><cell>YFCC100m (Flickr)</cell><cell>Captured (mobile devices)</cell><cell>YouTube</cell></row><row><cell>Video resolutions</cell><cell>540p</cell><cell>1080p,720p,480p,etc.</cell><cell>4k,1080p,720p,480p,360p</cell></row><row><cell>Video layouts</cell><cell>Landscape</cell><cell>Landscape,portrait</cell><cell>Landscape,portrait</cell></row><row><cell>Video framerates</cell><cell>24,25,30 fr/sec</cell><cell>20,24,25,30 fr/sec</cell><cell>15,20,24,25,30,50,60 fr/sec</cell></row><row><cell>Video lengths</cell><cell>8 seconds</cell><cell>10 seconds</cell><cell>20 seconds</cell></row><row><cell>Audio track included</cell><cell>Yes (97%)</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Testing methodology</cell><cell>Crowdsourcing (CrowdFlower)</cell><cell>Crowdsourcing (AMT)</cell><cell>Crowdsourcing (AMT)</cell></row><row><cell>Number of subjects</cell><cell>642</cell><cell>4,776</cell><cell>&gt;8,000</cell></row><row><cell>Number of ratings</cell><cell>136,800 (114 votes/video)</cell><cell>205,000 (240 votes/video)</cell><cell>170,159 (123 votes/video)</cell></row><row><cell>Rating scale</cell><cell>Absolute Category Rating 1-5</cell><cell>Continuous Rating 0-100</cell><cell>Continuous Rating 1-5</cell></row><row><cell>Content remarks</cell><cell>Videos sampled from YFCC100m via a</cell><cell>Videos manually captured by certain</cell><cell>Videos sampled from YouTube via a</cell></row><row><cell></cell><cell>feature space of blur, colorfulness,</cell><cell>people; Content including many camera</cell><cell>feature space of spatial, color,</cell></row><row><cell></cell><cell>contrast, SI, TI, and NIQE; Some</cell><cell>motions; Content including some night</cell><cell>temporal, and chunk variation;</cell></row><row><cell></cell><cell>contents irrelevant to quality research;</cell><cell>scenes that are prone to be outliers;</cell><cell>Contents categorized into 15 classes,</cell></row><row><cell></cell><cell>Content was clipped from the original</cell><cell>Resolutions not uniformly distributed.</cell><cell>including HDR, screen content,</cell></row><row><cell></cell><cell>and resized to 540p.</cell><cell></cell><cell>animations, and gaming videos.</cell></row><row><cell>Study remarks</cell><cell>Study did not account for or remove</cell><cell>Distribution of MOS values slightly</cell><cell>Distribution of MOS values slightly</cell></row><row><cell></cell><cell>videos on which stalling events</cell><cell>skewed towards higher scores; standard</cell><cell>skewed towards higher values; three</cell></row><row><cell></cell><cell>occurred when viewed; test</cell><cell>deviation statistics of MOS were not</cell><cell>additional chunk MOS scores with</cell></row><row><cell></cell><cell>methodology prone to unreliable</cell><cell>provided.</cell><cell>standard deviation were provided.</cell></row><row><cell></cell><cell>individual scores.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SUMMARY</head><label>III</label><figDesc>OF THE INITIAL FEATURE SET AND THE FINALIZED VIDEVAL SUBSET AFTER FEATURE SELECTION.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell>NIQE (1 fr/sec)</cell><cell>0.5417 0.3790 0.5530 0.5336</cell></row><row><cell>KoNViD</cell><cell>ILNIQE (1 fr/sec)</cell><cell>0.5264 0.3692 0.5400 0.5406</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.2988 0.2036 0.3002 0.6101</cell></row><row><cell></cell><cell>NIQE (1 fr/sec)</cell><cell>0.5957 0.4252 0.6286 13.110</cell></row><row><cell>LIVE-C</cell><cell>ILNIQE (1 fr/sec)</cell><cell>0.5037 0.3555 0.5437 14.148</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.0332 0.0231 0.2146 16.654</cell></row><row><cell></cell><cell>NIQE (1 fr/sec)</cell><cell>0.2379 0.1600 0.2776 0.6174</cell></row><row><cell>YT-UGC</cell><cell>ILNIQE (1 fr/sec)</cell><cell>0.2918 0.1980 0.3302 0.6052</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.0580 0.0389 0.1534 0.6339</cell></row><row><cell></cell><cell>NIQE (1 fr/sec)</cell><cell>0.4622 0.3222 0.4773 0.6112</cell></row><row><cell>All-Comb</cell><cell>ILNIQE (1 fr/sec)</cell><cell>0.4592 0.3213 0.4741 0.6119</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.1039 0.0688 0.1621 0.6804</cell></row></table><note>COMPARISON OF EVALUATED OPINION-UNAWARE "COMPLETELY BLIND" BVQA MODELS.DATASET MODEL \ METRIC SRCC? KRCC? PLCC? RMSE?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON OF EVALUATED BVQA MODELS ON THE FOUR BENCHMARK DATASETS. THE UNDERLINED AND BOLDFACED ENTRIES INDICATE THE BEST AND TOP THREE PERFORMERS ON EACH DATABASE FOR EACH PERFORMANCE METRIC, RESPECTIVELY. METRIC SRCC? (STD) KRCC? (STD) PLCC? (STD) RMSE? (STD) SRCC? (STD) KRCC? (STD) PLCC? (STD) RMSE? (STD) BRISQUE (1 fr/sec) 0.6567 (.035) 0.4761 (.029) 0.6576 (.034) 0.4813 (.022) 0.5925 (.068) 0.4162 (.052) 0.6380 (.063) 13.100 (.796) GM-LOG (1 fr/sec) 0.6578 (.032) 0.4770 (.026) 0.6636 (.031) 0.4818 (.022) 0.5881 (.068) 0.4180 (.052) 0.6212 (.063) 13.223 (.822) HIGRADE (1 fr/sec) 0.7206 (.030) 0.5319 (.026) 0.7269 (.028) 0.4391 (.</figDesc><table><row><cell>DATASET</cell><cell>KoNViD-1k</cell><cell>LIVE-VQC</cell></row><row><cell cols="2">MODEL \ 018)</cell><cell>0.6103 (.068) 0.4391 (.054) 0.6332 (.065) 13.027 (.904)</cell></row><row><cell>FRIQUEE (1 fr/sec)</cell><cell>0.7472 (.026) 0.5509 (.024) 0.7482 (.025) 0.4252 (.017)</cell><cell>0.6579 (.053) 0.4770 (.043) 0.7000 (.058) 12.198 (.914)</cell></row><row><cell>CORNIA (1 fr/sec)</cell><cell>0.7169 (.024) 0.5231 (.021) 0.7135 (.023) 0.4486 (.018)</cell><cell>0.6719 (.047) 0.4849 (.039) 0.7183 (.042) 11.832 (.700)</cell></row><row><cell>HOSA (1 fr/sec)</cell><cell>0.7654 (.022) 0.5690 (.021) 0.7664 (.020) 0.4142 (.016)</cell><cell>0.6873 (.046) 0.5033 (.039) 0.7414 (.041) 11.353 (.747)</cell></row><row><cell>VGG-19 (1 fr/sec)</cell><cell>0.7741 (.028) 0.5841 (.027) 0.7845 (.024) 0.3958 (.017)</cell><cell>0.6568 (.053) 0.4722 (.044) 0.7160 (.048) 11.783 (.696)</cell></row><row><cell>ResNet-50 (1 fr/sec)</cell><cell>0.8018 (.025) 0.6100 (.024) 0.8104 (.022) 0.3749 (.017)</cell><cell>0.6636 (.051) 0.4786 (.042) 0.7205 (.043) 11.591 (.733)</cell></row><row><cell cols="2">KonCept512 (1 fr/sec) 0.7349 (.025) 0.5425 (.023) 0.7489 (.024) 0.4260 (.016)</cell><cell>0.6645 (.052) 0.4793 (.045) 0.7278 (.046) 11.626 (.767)</cell></row><row><cell>PaQ-2-PiQ (1 fr/sec)</cell><cell>0.6130 (.032) 0.4334 (.026) 0.6014 (.033) 0.5148 (.019)</cell><cell>0.6436 (.045) 0.4568 (.035) 0.6683 (.044) 12.619 (.848)</cell></row><row><cell>V-BLIINDS</cell><cell>0.7101 (.031) 0.5188 (.026) 0.7037 (.030) 0.4595 (.023)</cell><cell>0.6939 (.050) 0.5078 (.042) 0.7178 (.050) 11.765 (.828)</cell></row><row><cell>TLVQM</cell><cell>0.7729 (.024) 0.5770 (.022) 0.7688 (.023) 0.4102 (.017)</cell><cell>0.7988 (.036) 0.6080 (.037) 0.8025 (.036) 10.145 (.818)</cell></row><row><cell>VIDEVAL</cell><cell>0.7832 (.021) 0.5845 (.021) 0.7803 (.022) 0.4026 (.017)</cell><cell>0.7522 (.039) 0.5639 (.036) 0.7514 (.042) 11.100 (.810)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI PERFORMANCES</head><label>VI</label><figDesc>ON DIFFERENT RESOLUTION SUBSETS: 1080P (427), 720P (566), AND ?480P (448).</figDesc><table><row><cell>SUBSET</cell><cell>1080p</cell><cell>720p</cell><cell>?480p</cell></row><row><cell>MODEL</cell><cell>SRCC PLCC</cell><cell>SRCC PLCC</cell><cell>SRCC PLCC</cell></row><row><cell>BRISQUE</cell><cell>0.4597 0.4637</cell><cell>0.5407 0.5585</cell><cell>0.3812 0.4065</cell></row><row><cell>GM-LOG</cell><cell>0.4796 0.4970</cell><cell>0.5098 0.5172</cell><cell>0.3685 0.4200</cell></row><row><cell>HIGRADE</cell><cell>0.5142 0.5543</cell><cell>0.5095 0.5324</cell><cell>0.4650 0.4642</cell></row><row><cell>FRIQUEE</cell><cell>0.5787 0.5797</cell><cell>0.5369 0.5652</cell><cell>0.5042 0.5363</cell></row><row><cell>CORNIA</cell><cell>0.5951 0.6358</cell><cell>0.6212 0.6551</cell><cell>0.5631 0.6118</cell></row><row><cell>HOSA</cell><cell>0.5924 0.6093</cell><cell>0.6651 0.6739</cell><cell>0.6514 0.6652</cell></row><row><cell>VGG-19</cell><cell>0.6440 0.6090</cell><cell>0.6158 0.6568</cell><cell>0.5845 0.6267</cell></row><row><cell>ResNet-50</cell><cell>0.6615 0.6644</cell><cell>0.6645 0.7076</cell><cell>0.6570 0.6997</cell></row><row><cell cols="2">KonCept512 0.6332 0.6336</cell><cell>0.6055 0.6514</cell><cell>0.4271 0.4612</cell></row><row><cell>PaQ-2-PiQ</cell><cell>0.5304 0.5176</cell><cell>0.5768 0.5802</cell><cell>0.3646 0.4748</cell></row><row><cell cols="2">V-BLIINDS 0.4449 0.4491</cell><cell>0.5546 0.5719</cell><cell>0.4484 0.4752</cell></row><row><cell>TLVQM</cell><cell>0.5638 0.6031</cell><cell>0.6300 0.6526</cell><cell>0.4318 0.4784</cell></row><row><cell>VIDEVAL</cell><cell>0.5805 0.6111</cell><cell>0.6296 0.6393</cell><cell>0.5014 0.5508</cell></row><row><cell></cell><cell cols="2">TABLE VII</cell><cell></cell></row><row><cell cols="4">PERFORMANCES ON DIFFERENT CONTENT SUBSETS: SCREEN CONTENT</cell></row><row><cell></cell><cell cols="3">(163), ANIMATION (81), AND GAMING (209).</cell></row><row><cell>SUBSET</cell><cell>Screen Content</cell><cell>Animation</cell><cell>Gaming</cell></row><row><cell>MODEL</cell><cell>SRCC PLCC</cell><cell>SRCC PLCC</cell><cell>SRCC PLCC</cell></row><row><cell>BRISQUE</cell><cell>0.2573 0.3954</cell><cell>0.0747 0.3857</cell><cell>0.2717 0.3307</cell></row><row><cell>GM-LOG</cell><cell>0.3004 0.4244</cell><cell>0.2009 0.4129</cell><cell>0.3371 0.4185</cell></row><row><cell>HIGRADE</cell><cell>0.4971 0.5652</cell><cell>0.1985 0.4140</cell><cell>0.6228 0.6832</cell></row><row><cell>FRIQUEE</cell><cell>0.5522 0.6160</cell><cell>0.2377 0.4574</cell><cell>0.6919 0.7193</cell></row><row><cell>CORNIA</cell><cell>0.5105 0.5667</cell><cell>0.1936 0.4627</cell><cell>0.5741 0.6502</cell></row><row><cell>HOSA</cell><cell>0.4667 0.5255</cell><cell>0.1048 0.4489</cell><cell>0.6019 0.6998</cell></row><row><cell>VGG-19</cell><cell>0.5472 0.6229</cell><cell>0.1973 0.4700</cell><cell>0.5765 0.6370</cell></row><row><cell>ResNet-50</cell><cell>0.6199 0.6676</cell><cell>0.2781 0.4871</cell><cell>0.6378 0.6779</cell></row><row><cell cols="2">KonCept512 0.4714 0.5119</cell><cell>0.2757 0.5229</cell><cell>0.4780 0.6240</cell></row><row><cell>PaQ-2-PiQ</cell><cell>0.3231 0.4312</cell><cell>0.0208 0.4630</cell><cell>0.2169 0.3874</cell></row><row><cell cols="2">V-BLIINDS 0.3064 0.4155</cell><cell>0.0379 0.3917</cell><cell>0.5473 0.6101</cell></row><row><cell>TLVQM</cell><cell>0.3843 0.4524</cell><cell>0.2708 0.4598</cell><cell>0.5749 0.6195</cell></row><row><cell>VIDEVAL</cell><cell>0.6033 0.6610</cell><cell>0.3492 0.5274</cell><cell>0.6954 0.7323</cell></row><row><cell cols="4">sets. On LIVE-VQC, however, TLVQM outperformed other</cell></row><row><cell cols="4">BVQA models by a notable margin, while it significantly</cell></row><row><cell cols="4">underperformed on the more recent YouTube-UGC database.</cell></row><row><cell cols="4">We observed in Section II-B that LIVE-VQC videos gen-</cell></row><row><cell cols="4">erally contain more (camera) motions than KoNViD-1k and</cell></row><row><cell cols="4">YouTube-UGC, and TLVQM computes multiple motion rele-</cell></row><row><cell cols="4">vant features. Moreover, the only three BVQA models contain-</cell></row><row><cell cols="4">ing temporal features (V-BLIINDS, TLVQM, and VIDEVAL)</cell></row><row><cell cols="4">excelled on LIVE-VQC, which suggests that it is potentially</cell></row><row><cell cols="4">valuable to integrate at least a few, if not many, motion-related</cell></row><row><cell cols="4">features into quality prediction models, when assessing on</cell></row><row><cell cols="3">videos with large (camera) motions.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII PERFORMANCES</head><label>VIII</label><figDesc>ON DIFFERENT QUALITY SUBSETS: LOW QUALITY (1558) AND HIGH QUALITY (1550).</figDesc><table><row><cell cols="2">SUBSET</cell><cell cols="2">Low Quality</cell><cell>High Quality</cell></row><row><cell cols="2">MODEL</cell><cell cols="2">SRCC PLCC</cell><cell>SRCC PLCC</cell></row><row><cell cols="2">BRISQUE</cell><cell cols="2">0.4312 0.4593</cell><cell>0.2813 0.2979</cell></row><row><cell cols="2">GM-LOG</cell><cell cols="2">0.4221 0.4715</cell><cell>0.2367 0.2621</cell></row><row><cell cols="2">HIGRADE</cell><cell cols="2">0.5057 0.5466</cell><cell>0.4714 0.4799</cell></row><row><cell cols="2">FRIQUEE</cell><cell cols="2">0.5460 0.5886</cell><cell>0.5061 0.5152</cell></row><row><cell cols="2">CORNIA</cell><cell cols="2">0.4931 0.5435</cell><cell>0.3610 0.3748</cell></row><row><cell>HOSA</cell><cell></cell><cell cols="2">0.5348 0.5789</cell><cell>0.4208 0.4323</cell></row><row><cell cols="2">VGG-19</cell><cell cols="2">0.3710 0.4181</cell><cell>0.3522 0.3614</cell></row><row><cell cols="2">ResNet-50</cell><cell cols="2">0.3881 0.4250</cell><cell>0.2791 0.3030</cell></row><row><cell cols="4">KonCept512 0.3428 0.4497</cell><cell>0.2245 0.2597</cell></row><row><cell cols="2">PaQ-2-PiQ</cell><cell cols="2">0.2438 0.2713</cell><cell>0.2013 0.2252</cell></row><row><cell cols="4">V-BLIINDS 0.4703 0.5060</cell><cell>0.3207 0.3444</cell></row><row><cell cols="2">TLVQM</cell><cell cols="2">0.4845 0.5386</cell><cell>0.4783 0.4860</cell></row><row><cell cols="2">VIDEVAL</cell><cell cols="2">0.5680 0.6056</cell><cell>0.5546 0.5657</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE IX</cell></row><row><cell cols="5">BEST MODEL IN TERMS OF SRCC FOR CROSS DATASET GENERALIZATION</cell></row><row><cell></cell><cell></cell><cell cols="2">EVALUATION.</cell></row><row><cell>TRAIN\TEST</cell><cell cols="2">LIVE-VQC</cell><cell cols="2">KoNViD-1k</cell><cell>YouTube-UGCc</cell></row><row><cell>LIVE-VQC</cell><cell></cell><cell>-</cell><cell cols="2">ResNet-50 (0.69) ResNet-50 (0.33)</cell></row><row><cell>KoNViD-1k</cell><cell cols="2">ResNet-50 (0.70)</cell><cell></cell><cell>-</cell><cell>VIDEVAL (0.37)</cell></row><row><cell>YouTube-UGCc</cell><cell cols="2">HOSA (0.49)</cell><cell cols="2">VIDEVAL (0.61)</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE X</cell></row><row><cell cols="5">BEST MODEL IN TERMS OF PLCC FOR CROSS DATASET GENERALIZATION</cell></row><row><cell></cell><cell></cell><cell cols="2">EVALUATION.</cell></row><row><cell>TRAIN\TEST</cell><cell cols="2">LIVE-VQC</cell><cell cols="2">KoNViD-1k</cell><cell>YouTube-UGCc</cell></row><row><cell>LIVE-VQC</cell><cell></cell><cell>-</cell><cell cols="2">ResNet-50 (0.70) VIDEVAL (0.35)</cell></row><row><cell>KoNViD-1k</cell><cell cols="2">ResNet-50 (0.75)</cell><cell></cell><cell>-</cell><cell>VIDEVAL (0.39)</cell></row><row><cell>YouTube-UGCc</cell><cell cols="2">HOSA (0.50)</cell><cell cols="2">VIDEVAL (0.62)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XII FEATURE</head><label>XII</label><figDesc>DESCRIPTION, DIMENSIONALITY, COMPUTATIONAL COMPLEXITY, AND AVERAGE RUNTIME COMPARISON (IN SECONDS EVALUATED ON TWENTY 1080p VIDEOS FROM LIVE-VQC) AMONG MATLAB-IMPLEMENTED BVQA MODELS.</figDesc><table><row><cell cols="2">CLASS MODEL</cell><cell>FEATURE DESCRIPTION</cell><cell cols="2">DIM COMPUTATIONAL COMPLEXITY</cell><cell>TIME (SEC)</cell></row><row><cell></cell><cell>NIQE (1 fr/sec)</cell><cell>Spatial NSS</cell><cell>1</cell><cell>O(d 2 N T ) d: window size</cell><cell>6.3</cell></row><row><cell></cell><cell>ILNIQE (1 fr/sec)</cell><cell>Spatial NSS, gradient, log-Gabor, and color</cell><cell>1</cell><cell>O((d 2 + h + gh)N T ) d: window size; h: filter</cell><cell>23.3</cell></row><row><cell></cell><cell></cell><cell>statistics</cell><cell></cell><cell>size; g: log-Gabor filter size</cell><cell></cell></row><row><cell></cell><cell cols="2">BRISQUE (1 fr/sec) Spatial NSS</cell><cell>36</cell><cell>O(d 2 N T ) d: window size</cell><cell>1.7</cell></row><row><cell></cell><cell cols="2">GM-LOG (1 fr/sec) Joint statistics of gradient magnitude and lapla-</cell><cell>40</cell><cell>O(((h + k)N T ) d: window size; k: probability</cell><cell>2.1</cell></row><row><cell></cell><cell></cell><cell>cian of gaussian coefficients</cell><cell></cell><cell>matrix size</cell><cell></cell></row><row><cell></cell><cell cols="2">HIGRADE (1 fr/sec) Spatial NSS, and gradient magnitude statistics</cell><cell cols="2">216 O(3(2d 2 + k)N T ) d: window size; k: gradient</cell><cell>11.6</cell></row><row><cell>IQA</cell><cell cols="2">in LAB color space FRIQUEE (1 fr/sec) Complex streerable pyramid wavelet, luminance,</cell><cell cols="2">kernel size 560 O((f d 2 N + 4N (log(N ) + m 2 ))T ) d: window</cell><cell>701.2</cell></row><row><cell></cell><cell></cell><cell>chroma, LMS, HSI, yellow channel, and their</cell><cell></cell><cell>size; f : number of color spaces; m: neighbor-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>transformed domain statistics</cell><cell></cell><cell>hood size in DNT</cell><cell></cell></row><row><cell></cell><cell>CORNIA (1 fr/sec)</cell><cell>Spatially normalized image patches and max</cell><cell cols="2">10k O(d 2 KN T ) d: window size K: codebook size</cell><cell>14.3</cell></row><row><cell></cell><cell></cell><cell>min pooling</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HOSA (1 fr/sec)</cell><cell>Local normalized image patches based on high</cell><cell cols="2">14.7k O(d 2 KN T ) d: window size K: codebook size</cell><cell>1.2</cell></row><row><cell></cell><cell></cell><cell>order statistics aggregation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>VIIDEO</cell><cell>Frame difference spatial statistics, inter sub-band</cell><cell>1</cell><cell>O(N log(N )T )</cell><cell>674.8</cell></row><row><cell></cell><cell></cell><cell>statistics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>V-BLIINDS</cell><cell>Spatial NSS, frame difference DCT coefficient</cell><cell>47</cell><cell>O((d 2 N + log(k)N + k 2 w 3 )T ) d: window</cell><cell>1989.9</cell></row><row><cell></cell><cell></cell><cell>statistics, motion coherency, and egomotion</cell><cell></cell><cell>size; k: block size; w: motion vector tensor size</cell><cell></cell></row><row><cell>VQA</cell><cell>TLVQM</cell><cell>Captures impairments computed at two compu-tation levels: low complexity and high complex-</cell><cell>75</cell><cell>O((h 2 1 N + k 2 K)T 1 + (log(N ) + h 2 h 1 , h 2 : filter size; k: motion estimation block 2 )N T 2 ))</cell><cell>183.8</cell></row><row><cell></cell><cell></cell><cell>ity features</cell><cell></cell><cell>size; K: number of key points</cell><cell></cell></row><row><cell></cell><cell>VIDEVAL</cell><cell>Selected combination of NSS features in multi-ple perceptual spaces and using visual impair-</cell><cell>60</cell><cell>O((f h 2 1 N + k 2 K)T 1 + h 2 size; f : number of color spaces; k: motion 2 N T 2 ) h 1 , h 2 : filter</cell><cell>305.8</cell></row><row><cell></cell><cell></cell><cell>ment features from TLVQM</cell><cell></cell><cell>estimation block size; K: number of key points</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>. The experiments were performed in MATLAB R2018b and Python 3.6.7 under Ubuntu 18.04.3 LTS system on a Dell OptiPlex 7080 Desktop with Intel Core i7-8700 CPU@3.2GHz, 32G RAM, and GeForce GTX 1050 Graphics Cards. The average feature computation time of MATLABimplemented BVQA models on 1080p videos are reported</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XIII RUN</head><label>XIII</label><figDesc>TIME COMPARISON OF FEATURE-BASED AND DEEP LEARNING BVQA MODELS (IN SECONDS EVALUATED ON TWENTY 1080p VIDEOS FROM LIVE-VQC). MODEL LOADING TIME FOR DEEP MODELS ARE EXCLUDED.</figDesc><table><row><cell>MODEL</cell><cell></cell><cell>TIME (SEC)</cell></row><row><cell>BRISQUE (1 fr/sec)</cell><cell>MATLAB-CPU</cell><cell>1.7</cell></row><row><cell>HOSA (1 fr/sec)</cell><cell>MATLAB-CPU</cell><cell>1.2</cell></row><row><cell>TLVQM</cell><cell>MATLAB-CPU</cell><cell>183.8</cell></row><row><cell>VIDEVAL</cell><cell>MATLAB-CPU</cell><cell>305.8</cell></row><row><cell>VGG-19 (1 fr/sec)</cell><cell>TensorFlow-CPU</cell><cell>27.8</cell></row><row><cell></cell><cell>TensorFlow-GPU</cell><cell>5.7</cell></row><row><cell>ResNet-50 (1 fr/sec)</cell><cell>TensorFlow-CPU</cell><cell>9.6</cell></row><row><cell></cell><cell>TensorFlow-GPU</cell><cell>1.9</cell></row><row><cell>KonCept512 (1 fr/sec)</cell><cell>PyTorch-CPU</cell><cell>2.8</cell></row><row><cell></cell><cell>PyTorch-GPU</cell><cell>0.3</cell></row><row><cell>PaQ-2-PiQ (1 fr/sec)</cell><cell>PyTorch-CPU</cell><cell>6.9</cell></row><row><cell></cell><cell>PyTorch-GPU</cell><cell>0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIV PERFORMANCE</head><label>XIV</label><figDesc>OF THE ENSEMBLE VIDEVAL MODELS FUSED WITH ADDITIONAL DEEP LEARNING FEATURES.</figDesc><table><row><cell>DATASET</cell><cell>MODEL \ METRIC</cell><cell>SRCC KRCC PLCC RMSE</cell></row><row><cell></cell><cell>VIDEVAL</cell><cell>0.7832 0.5845 0.7803 0.4024</cell></row><row><cell></cell><cell>VIDEVAL+VGG-19</cell><cell>0.7827 0.5928 0.7913 0.3897</cell></row><row><cell>KoNViD</cell><cell>VIDEVAL+ResNet-50</cell><cell>0.8129 0.6212 0.8200 0.3659</cell></row><row><cell></cell><cell cols="2">VIDEVAL+KonCept512 0.8149 0.6251 0.8169 0.3670</cell></row><row><cell></cell><cell>VIDEVAL+PaQ-2-PiQ</cell><cell>0.7844 0.5891 0.7793 0.4018</cell></row><row><cell></cell><cell>VIDEVAL</cell><cell>0.7522 0.5639 0.7514 11.100</cell></row><row><cell></cell><cell>VIDEVAL+VGG-19</cell><cell>0.7274 0.5375 0.7717 10.749</cell></row><row><cell>LIVE-VQC</cell><cell>VIDEVAL+ResNet-50</cell><cell>0.7456 0.5555 0.7810 10.385</cell></row><row><cell></cell><cell cols="2">VIDEVAL+KonCept512 0.7849 0.5953 0.8010 10.145</cell></row><row><cell></cell><cell>VIDEVAL+PaQ-2-PiQ</cell><cell>0.7677 0.5736 0.7686 10.787</cell></row><row><cell></cell><cell>VIDEVAL</cell><cell>0.7787 0.5830 0.7733 0.4049</cell></row><row><cell></cell><cell>VIDEVAL+VGG-19</cell><cell>0.7868 0.5930 0.7847 0.3993</cell></row><row><cell>YT-UGC</cell><cell>VIDEVAL+ResNet-50</cell><cell>0.8085 0.6128 0.8033 0.3837</cell></row><row><cell></cell><cell cols="2">VIDEVAL+KonCept512 0.8083 0.6139 0.8028 0.3859</cell></row><row><cell></cell><cell>VIDEVAL+PaQ-2-PiQ</cell><cell>0.7981 0.6015 0.7941 0.3959</cell></row><row><cell></cell><cell>VIDEVAL</cell><cell>0.7960 0.6032 0.7939 0.4268</cell></row><row><cell></cell><cell>VIDEVAL+VGG-19</cell><cell>0.7859 0.5912 0.7962 0.4202</cell></row><row><cell>All-Comb</cell><cell>VIDEVAL+ResNet-50</cell><cell>0.8115 0.6207 0.8286 0.3871</cell></row><row><cell></cell><cell cols="2">VIDEVAL+KonCept512 0.8123 0.6193 0.8168 0.4017</cell></row><row><cell></cell><cell>VIDEVAL+PaQ-2-PiQ</cell><cell>0.7962 0.5991 0.7934 0.4229</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A H.264/AVC video database for the evaluation of quality metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">De</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naccari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2430" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">VQEG HDTV phase I database</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">IVP subjective quality video database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://ivp.ee.cuhk.edu.hk/research/database/subjective/" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The TUM high definition video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Redl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Qual. Multimedia Exper. (QoMEX)</title>
		<meeting>4th Int. Conf. Qual. Multimedia Exper. (QoMEX)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ViS3: An algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13016</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MCL-V: A streaming video quality assessment database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MCL-JCV: a JND-based H.264/AVC video quality assessment dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1509" to="1513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Konstanz natural video database (KoNViD-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Qual. Multimedia Exper</title>
		<meeting>9th Int. Conf. Qual. Multimedia Exper</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">YouTube UGC dataset for video compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inguva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Multimedia Signal Process. (MMSP)</title>
		<meeting>IEEE Int. Workshop Multimedia Signal ess. (MMSP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CVD2014-a database for evaluating no-reference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3073" to="3086" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In-capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2061" to="2077" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">YFCC100M: The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analysis of public image and video databases for quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="616" to="625" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring colorfulness in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Suesstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vis</title>
		<meeting>SPIE Human Vis</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5007</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video quality assessment based on structural distortion measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Toward a practical perceptual video quality metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manohara</surname></persName>
		</author>
		<ptr target="https://medium.com/netflix-techblog/toward-a-practical-perceptual-video-quality-metric-653f208b9652" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">De</forename><surname>Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Perceptual video quality prediction emphasizing chroma distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11203</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reduced-reference image quality assessment using a wavelet-domain natural image statistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5666</biblScope>
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video quality assessment by reduced reference spatio-temporal entropic differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="684" to="694" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Blind measurement of blocking artifacts in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A no-reference perceptual blur metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>III-III</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measurement of ringing artifacts in JPEG images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digit. Pub</title>
		<imprint>
			<biblScope unit="volume">6076</biblScope>
			<biblScope unit="page">60760</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A perceptual visibility metric for banding artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-U</forename><surname>Kum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kokaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2067" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bband index: a no-reference banding artifact predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2712" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive debanding filter</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1715" to="1719" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and reliable structure-oriented video noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="118" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Film grain synthesis for AV1 video codec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compress. Conf. (DCC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">No-reference quality metric for degraded and enhanced video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Caviedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Oberti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digit. Video Image Qual. Perceptual Coding</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="305" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">No-reference video quality evaluation for high-definition video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oelbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1145" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of tone-mapped HDR pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2957" to="2971" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment using human visual DOG model fused with random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3282" to="3292" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">RAPIQUE: Rapid and accurate video quality prediction of user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10955</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Netw.: Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A two-step framework for constructing blind image quality indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="516" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">C-DIIVINE: No-reference image quality assessment based on local magnitude and phase statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="725" to="747" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A DCT statistics-based blind image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="586" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in curvelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="494" to="505" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spatio-temporal measures of naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1750" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using joint statistics of gradient magnitude and laplacian features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4850" to="4862" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment based on log-derivative statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43025</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment via feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="491" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal hysteresis model of time varying subjective video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1153" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on high order statistics aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Proxiqa: A proxy approach to perceptual optimization of learned image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="360" to="373" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A deep neural network for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3773" to="3777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Color image database TID2013: Peculiarities and preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th</title>
		<meeting>4th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Eur. Workshop Vis. Inf. Process. (EUVIP)</title>
		<imprint>
			<biblScope unit="page" from="106" to="111" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4041" to="4056" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">End-to-end blind quality assessment of compressed videos using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf. (MM)</title>
		<meeting>ACM Multimedia Conf. (MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Blind video quality assessment with weakly supervised learning and resampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2244" to="2255" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf. (MM)</title>
		<meeting>ACM Multimedia Conf. (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A comparative evaluation of temporal pooling methods for blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An objective method for combining multiple subjective data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Visual Commun. Image Process. Conf</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5150</biblScope>
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Subjective and objective quality assessment of high frame rate videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11634</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Video quality pooling adaptive to perceptual distortion severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="610" to="620" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Recurrent and dynamic models for predicting streaming video quality of experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3316" to="3331" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Visual importance pooling for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The serial position effect of free recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Murdock</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">482</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
