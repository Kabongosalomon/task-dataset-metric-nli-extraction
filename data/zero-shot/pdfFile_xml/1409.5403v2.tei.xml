<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deformable Part Models are Convolutional Neural Networks Tech report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
							<email>forresti@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deformable Part Models are Convolutional Neural Networks Tech report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deformable part models (DPMs) and convolutional neural networks (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are "black-box" non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a novel synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent (and at times novel) CNN layer. From this perspective, it becomes natural to replace the standard image features used in DPM with a learned feature extractor. We call the resulting model DeepPyramid DPM and experimentally validate it on PAS-CAL VOC. DeepPyramid DPM significantly outperforms DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running an order of magnitude faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Part-based representations are widely used for visual recognition tasks. In particular, deformable part models (DPMs) <ref type="bibr" target="#b6">[7]</ref> have been especially useful for generic object category detection. DPMs update pictorial structure models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> (which date back to the 1970s) with modern image features and machine learning algorithms. Convolutional neural networks (CNNs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> are another influential class of models for visual recognition. CNNs also have a long history, and have come back into popular use in the last two years due to good performance on image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> and object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> tasks.</p><p>These two models, DPMs and CNNs, are typically viewed as distinct approaches to visual recognition. DPMs are graphical models (Markov random fields), while CNNs are "black-box" non-linear classifiers. In this paper we describe how a DPM can be formulated as an equivalent CNN, providing a novel synthesis of these ideas. This formulation (DPM-CNN) relies on a new CNN layer, distance transform pooling, that generalizes max pooling. Another innovation of our approach is that rather than using histograms of oriented gradients (HOG) features <ref type="bibr" target="#b3">[4]</ref>, we apply DPM-CNN to a feature pyramid that is computed by another CNN. Since the end-to-end system is the function composition of two networks, it is equivalent to a single, unified CNN. We call this end-to-end model DeepPyramid DPM.</p><p>We also show that DeepPyramid DPM works well in practice. In terms of object detection mean average precision, DeepPyramid DPM slightly outperforms a comparable version of the recently proposed R-CNN <ref type="bibr" target="#b13">[14]</ref> (specifically, R-CNN on the same conv 5 features, without finetuning), while running about 20x faster. This experimental investigation also provides a greater understanding of the relative merits of region-based detection methods, such as R-CNN, and sliding-window methods like DPM. We find that regions and sliding windows are complementary methods that will likely benefit each other if used in an ensemble.</p><p>HOG-based detectors are currently used in a wide range of models and applications, especially those where regionbased methods are ill-suited (poselets <ref type="bibr" target="#b0">[1]</ref> being a prime example). Our results show that sliding-window detectors on deep feature pyramids significantly outperform equivalent models on HOG. Therefore, we believe that the model presented in this paper will be of great practical interest to the visual recognition community. An open-source implementation will be made available, which will allow researchers to easily build on our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DeepPyramid DPM</head><p>In this section we describe the DeepPyramid DPM architecture. DeepPyramid DPM is a convolutional neural network that takes as input an image pyramid and produces as output a pyramid of object detection scores. Although the model is a single CNN, for pedagogical reasons we describe it in terms of two smaller networks whose function composition yields the full network. A schematic diagram of the model is presented in <ref type="figure" target="#fig_2">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature pyramid front-end</head><p>Objects appear at all scales in images. A standard technique for coping with this fact is to run a detector at multiple scales using an image pyramid. In the context of CNNs, this method dates back to (at least) early work on face detection in <ref type="bibr" target="#b30">[31]</ref>, and has been used in contemporary work including OverFeat <ref type="bibr" target="#b27">[28]</ref>, DetectorNet <ref type="bibr" target="#b2">[3]</ref>, and DenseNet <ref type="bibr" target="#b18">[19]</ref>. We follow this approach and use as our first CNN a network that maps an image pyramid to a feature pyramid. We use a standard single-scale architecture (Krizhevsky et al. <ref type="bibr" target="#b21">[22]</ref>) and tie the network weights across all scales. Implementation details are given in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DPM-CNN: Constructing an equivalent CNN from a DPM</head><p>In the DPM formalism, an object class is modeled as a mixture of "components", each being responsible for modeling the appearance of an object sub-category (e.g., side views of cars, people doing handstands, bi-wing propeller planes). Each component, in turn, uses a low-resolution global appearance model of the sub-type (called a "root filter"), together with a small number (e.g., 8) of higher resolution "part filters" that capture the appearance of local regions of the sub-type. At test time, a DPM is run as a sliding-window detector over a feature pyramid, which is traditionally built using HOG features (alternatives have recently been explored in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>). DPM assigns a score to each sliding-window location by optimizing a score function that trades off part deformation costs with image match scores. A global maximum of the score function is computed efficiently at all locations by sharing computation between neighboring positions using a dynamic programming algorithm. This algorithm is illustrated with all of the steps "unrolled" in <ref type="figure">Figure 4</ref> of <ref type="bibr" target="#b6">[7]</ref>. The key observation in this section is that for any given DPM, its unrolled detection algorithm yields a specific network architecture with a fixed depth.</p><p>In order to realize DPM as a convolutional network, we first introduce the idea of distance transform (DT) pooling, which generalizes the familiar max-pooling operation used in CNNs. Given DT-pooling, a single-component DPM is constructed by composing part-filter convolution layers and DT-pooling layers with an "object geometry" layer that encodes the relative offsets of DPM parts. We call this architecture DPM-CNN; its construction is explained in <ref type="figure">Figure 2</ref>. To simplify the presentation, we consider the case where the DPM parts operate at the same resolution as the root filter. Multi-resolution models can be implemented by taking two scales as input and inserting a subsampling layer after each DT-pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Distance transform pooling</head><p>Here we show that distance transforms of sampled functions <ref type="bibr" target="#b8">[9]</ref> generalize max pooling.</p><p>First, we define max pooling. Consider a function f : G ? R defined on a regular grid G. The max pooling operation on f , with a window half-length of k, is also a function M f : G ? R that is defined by M f (p) = max ?p?{?k,...,k} f (p + ?p).</p><p>Following <ref type="bibr" target="#b8">[9]</ref>, the distance transform of f is a function</p><formula xml:id="formula_0">D f : G ? R defined by D f (p) = max q?G (f (q) ? d(p ? q)</formula><p>). In the case of DPM, d(r) is as a convex quadratic function d(r) = ar 2 + br, where a &gt; 0 and b are learnable parameters for each part. Intuitively, these parameters define the shape of the distance transform's pooling region.</p><p>Max pooling can be equivalently expressed as</p><formula xml:id="formula_1">M f (p) = max q?G (f (q) ? d max (p ? q)), where d max (r) is zero if r ? {?k, .</formula><p>. . , k} and ? otherwise. Expressing max pooling as the maximization of a function subject to a distance penalty d max makes the connection between distance transforms and max pooling clear. The distance transform generalizes max pooling and can introduce learnable parameters, as is the case in DPM. Note that unlike max pooling, the distance transform of f at p is taken over the entire domain G. Therefore, rather than specifying a fixed pooling window a priori, the shape of the pooling region can be learned from the data.</p><p>In the construction of DPM-CNN, DT-pooling layers are inserted after each part filter convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Object geometry filters</head><p>The score of a DPM component c at each root filter location s is given by adding the root filter score at s to the distance transformed part scores at "anchor" locations offset from s. Each part p has its own anchor offset that is specified by a 2D vector v p = (v px , v py ).</p><p>Computing component scores at all root locations can be rephrased as a convolution. The idea is to stack the root filter score map together with the P distance transformed part score maps to form a feature map with P + 1 channels, and then convolve that feature map with a specially constructed filter that we call an "object geometry" filter. This name comes from the fact that the filter combines a spatial configuration of parts into a whole. The coefficients in the object geometry filter are all zero, except for a single coefficient set to one in each of the filter's P + 1 channels. The first channel corresponds to the root filter, and has a one in its upper-left corner (the root's anchor is defined as v 0 = (0, 0)). Filter channel p (counting from zero), has a one at index v p (using matrix-style indexing, where indices grow down and to the right). To clarify this description, an example object geometry filter for a DPM component with one root and one part is shown in the   (1) An image pyramid is built from a color input image. (2) Each pyramid level is forward propagated through a truncated SuperVision CNN <ref type="bibr" target="#b21">[22]</ref> that ends at convolutional layer 5 (conv5). <ref type="formula">(3)</ref> The result is a pyramid of conv5 feature maps, each at 1/16th the spatial resolution of its corresponding image pyramid level. (4) Each conv5 level is then input into a DPM-CNN, which produces a pyramid of DPM detection scores <ref type="bibr" target="#b4">(5)</ref>. Since the whole system is simply the composition of two CNNs, it can be viewed as a single, unified CNN that takes a color image pyramid as input and outputs a DPM score pyramid. The part convolution maps are then processed with a distance transform, which we show is a generalization of max pooling. <ref type="formula">(3)</ref> The root convolution map and the DT transformed part convolution maps are stacked into a single feature map with P + 1 channels and then convolved with a sparse object geometry filter (see sidebar diagram and Section 2.2.2). The output is a single-channel score map for the DPM component.</p><p>DPM is usually thought of as a flat model, but making the object geometry filter explicit reveals that DPM actually has a second, implicit convolutional layer. This insight shows that in principle one could train this filter discriminatively, rather than heuristically setting it to a sparse binary pattern. We revisit this idea when discussing our experiments in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Combining mixture components with maxout</head><p>Each single-component DPM-CNN produces a score map for each pyramid level. Let z sc be the score for component c at location s in a pyramid level. In the DPM formalism, components compete with each other at each location. This competition is modeled as a max over component scores:</p><formula xml:id="formula_2">z s = max c z sc , where the overall DPM score at s is given by z s . In DPM-CNN, z sc = w c ? x s + b c , where w c is component c's object geometry filter (vectorized),</formula><p>x s is the sub-array of root and part scores at s (vectorized), and b c is the component's scalar bias. <ref type="figure" target="#fig_4">Figure 3</ref> shows the full multicomponent DPM-CNN including the max non-linearity that combines component scores.</p><p>It is interesting to note that the max non-linearity used in DPM is mathematically equivalent to the "maxout" unit recently described by Goodfellow et al. <ref type="bibr" target="#b16">[17]</ref>. In the case of DPM, the maxout unit has a direct interpretation as a switch over the choice of model components.   <ref type="figure">Figure 2</ref>) and a maxout <ref type="bibr" target="#b16">[17]</ref> layer that takes a max over component DPM-CNN outputs at each location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Generalizations</head><p>For clarity, we constructed a DPM-CNN for the case of mixtures of star models. However, the construction is general and naturally extends to a variety of models such as object detection grammars <ref type="bibr" target="#b15">[16]</ref> and recent DPM variants, such as <ref type="bibr" target="#b33">[34]</ref>.</p><p>The CNN construction for these more complex models is analogous to the DPM-CNN construction: take the exact inference algorithm used for detection and explicitly unroll it (this can be done given a fixed model instance); express the resulting network in terms of convolutional layers (for appearance and geometry filters), distance transform pooling layers, subsampling layers, and maxout layers.</p><p>We also note that our construction is limited to models for which exact inference is possible with a non-iterative algorithm. Models with loopy graphical structures, such as Wang et al.'s hierarchical poselets model <ref type="bibr" target="#b32">[33]</ref>, require iterative, approximate inference algorithms that cannot be converted to equivalent fixed-depth CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Related work</head><p>Our work is most closely related to the deep pedestrian detection model of Ouyang and Wang <ref type="bibr" target="#b24">[25]</ref>. Their CNN is structured like a DPM and includes a "deformation layer" that takes a distance penalized global max within a detection window. Their work reveals some connections between single-component DPMs and CNNs, however since that is not the focus of their paper a clear mapping of generic DPMs to CNNs is not given. We extend their work by: developing object geometry filters, showing that multicomponent models are implemented with maxout, describing how DPM inference over a whole image is efficiently computed in a CNN by distance transform pooling, and using a more powerful CNN front-end to generate the feature pyramid. Our DT-pooling layer differs from the "deformation layer" described in <ref type="bibr" target="#b24">[25]</ref> since it efficiently computes a distance transform over the entire input, rather than an independent global max for each window.</p><p>The idea of unrolling (or "time unfolding") an inference algorithm in order to construct a fixed-depth network was explored by Gregor and LeCun in application to sparse cod-ing <ref type="bibr" target="#b17">[18]</ref>. In sparse coding, inference algorithms are iterative and converge to a fixed point. Gregor and LeCun proposed to unroll an inference algorithm for a fixed number of iterations in order to define an approximator network. In the case of DPM (and similar low tree-width models), the inference algorithm is exact and non-iterative, making it possible to unroll it into a fixed-depth network without any approximations.</p><p>Boureau et al. <ref type="bibr" target="#b1">[2]</ref> study average and max pooling from theoretical and experimental perspectives. They discuss variants of pooling that parametrically transition from average to max. Distance transform pooling, unlike the pooling functions in <ref type="bibr" target="#b1">[2]</ref>, is interesting because it has a learnable pooling region. Jia et al. <ref type="bibr" target="#b20">[21]</ref> also address the problem of learning pooling regions by formulating it as a feature selection problem.</p><p>Our work is also related to several recent approaches to object detection using CNNs. OverFeat <ref type="bibr" target="#b27">[28]</ref> performs coarse sliding-window detection using a CNN. At each rough location, OverFeat uses a regressor to predict the bounding box of a nearby object. Another recent CNNbased object detector called DetectorNet <ref type="bibr" target="#b2">[3]</ref> also performs detection on a coarse set of sliding-window locations. At each location, DetectorNet predicts masks for the left, right, top, bottom, and whole object. These predicted masks are then clustered into object hypotheses. Currently, the most accurate object detection method for both ImageNet detection as well as PASCAL VOC is the Region-based CNN framework (R-CNN) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Unlike DetectorNet and OverFeat, R-CNN does not perform sliding-window detection; instead R-CNN begins by extracting a set of region proposals <ref type="bibr" target="#b29">[30]</ref> and classifies them with a linear SVM applied to CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation details</head><p>We implemented our experiments by modifying the DPM voc-release5 code <ref type="bibr" target="#b14">[15]</ref> and using Caffe <ref type="bibr" target="#b19">[20]</ref> for CNN computation. We describe the most important implementation details in this section. Source code for the complete system will be available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature pyramid construction</head><p>Our DeepPyramid DPM implementation starts by processing an input image with a truncated variant of the Su-perVision CNN <ref type="bibr" target="#b21">[22]</ref>. We use the publicly available network weights that are distributed with R-CNN <ref type="bibr" target="#b13">[14]</ref>, which allows for a direct comparison. These weights were trained on the ILSVRC 2012 classification training dataset using Caffe (we do not use the detection fine-tuned weights).</p><p>Starting from this network, we made two structural modifications. The first was to remove the last max pooling layer (pool 5 ) and all of the fully-connected layers (fc 6 , fc 7 , fc 8 , and softmax). The network's output is, therefore, the feature map computed by the fifth convolutional layer (conv 5 ), which has 256 feature channels. The second modification is that before each convolutional or max pooling layer, with kernel size k, we zero-pad the layer's input with k/2 zeros on all sides (top, bottom, left, and right). This padding implements "same" convolution (or pooling), where the input and output maps have the same spatial extent. With this padding, the mapping between image coordinates and CNN output coordinates is straightforward. A "pixel" (or cell) at zero-based index (x, y) in the CNN's conv 5 feature map has a receptive field centered on pixel (16x, 16y) in the input image. The conv 5 features, therefore, have a stride of 16 pixels in the input image with highly overlapping receptive fields of size 163 ? 163 pixels. Our experimental results show that even though the receptive fields are very large, the features are localized well enough for sliding-window detectors to precisely localize objects without regression (as in OverFeat and DetectorNet).</p><p>For simplicity, we process the image pyramid with a naive implementation in which each image pyramid level is embedded in the upper-left corner of a large (1713 ? 1713 pixel) image. For the first image pyramid level, the original image is resized such that its largest dimension is 1713 pixels. For PASCAL VOC images, this results in up-sampling images by a factor of 3.4 on average. The first conv 5 pyramid level has 108 cells on its longest side. We use a pyramid with 7 levels, where the scale factor between levels is 2 ?1/2 (the pyramid spans three octaves). The entire conv 5 pyramid has roughly 25k output cells (sliding-window locations). For comparison, this is considerably more than the roughly 1,500 sliding-window locations used in OverFeat, but many fewer than the 250k commonly used in HOG feature pyramids. Computing the conv 5 feature pyramid as described above is fast, taking only 0.5 seconds on an NVIDIA Tesla K20c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parameter learning</head><p>DeepPyramid DPM is a single CNN that takes an image pyramid as input and outputs a pyramid of DPM detection scores. In principle, the entire network can be trained endto-end using backpropagation by defining a loss function on the score pyramid. In this work we opt for a simpler approach in which DeepPyramid DPM is trained stage-wise, in two stages. The first stage trains the truncated Super-Vision CNN. For this we simply use the publicly available model distributed with R-CNN. This model was trained for image classification on ILSVRC 2012 and was not finetuned for detection. In the second stage, we keep the Su-perVision CNN parameters fixed and train a DPM on top of conv 5 feature pyramids using the standard latent SVM training algorithm used for training DPMs on HOG features. In the future, we plan to experiment with training the entire system with backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">DPM training and testing details</head><p>Compared to training DPM with HOG features, we found it necessary to make some changes to the standard DPM training procedure. First, we don't use left/right mirrored pairs of mixture components. These components are easy to implement with HOG because model parameters can be explicitly "flipped" allowing them to be tied between mirrored components. Second, R-CNN and DPM use different non-maximum suppression functions and we found that the one used in R-CNN, which is based on intersectionover-union (IoU) overlap, performs slightly better with conv 5 features (it is worse for the baseline HOG-DPM). Lastly, we found that it's very important to use poorly localized detections of ground-truth objects as negative examples. As in R-CNN, we define negative examples as all detections that have a max IoU overlap with a ground-truth object of less than 0.3. Using poorly localized positives as negative examples leads to substantially better results (Section 4.1.2) than just using negatives from negative images, which is the standard practice when training HOG-DPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on PASCAL VOC 2007</head><p>Deformable part models have been tuned for use with HOG features over several years. A priori, it's unclear if the same structural choices that work well with HOG (e.g., the number of mixture components, number of parts, multiresolution modeling, etc.) will also work well with very different features. We conducted several preliminary experiments on the PASCAL VOC 2007 dataset <ref type="bibr" target="#b5">[6]</ref> in order to find a DPM structure suited to conv 5 features.</p><p>In <ref type="table">Table 1</ref>, rows 1-3, we show the effect of adding parts to a three component DPM (three was selected through cross-validation). As in HOG-based DPMs, the dimensions of root filters vary across categories and components, influenced by the aspect ratio distribution for each class. Our root filter sizes typically range from 4 ? 12 to 12 ? 4 feature cells. We start with a "root-only" model (i.e., no parts) and then show results after adding 4 or 8 parts (of size 3 ? 3) to each component. With 4 parts, mAP increases by 0.9 percentage points, with an improvement in 16 out of 20 classes. The effect size is small, but appears to be statistically significant (p = 0.016) as judged by a paired-sample permutation test, a standard analysis tool used in the information retrieval community <ref type="bibr" target="#b28">[29]</ref>.</p><p>One significant difference between HOG and conv 5 features is that HOG describes scale-invariant local image statistics (intensity gradients), while conv 5 features describe large (163 ? 163 pixel) image patches. The top two rows of <ref type="figure">Figure 4</ref> illustrate this point. Each row shows a feature pyramid for an image of a face. The first is HOG and the second is the "face channel" of conv 5 . In the HOG representation, the person's face appears at all scales and one can imagine that for each level in the pyramid, it would be possible to define an appropriately sized template that would fire on the face. The conv 5 face channel is quite different. It only shows strong activity when the face is observed in a narrow range of scales. In the first several levels of the pyramid, the face feature responses are nearly all zero (black). The feature peaks in level 6 when the face is at the optimal scale.</p><p>Based on the previous experiments with parts and the feature visualizations, we decided to explore another hypothesis: that the convolution filters in conv 5 already act as a set of shared "parts" on top of the conv 4 features. This perspective suggests that one can spread the conv 5 features to introduce some local deformation invariance and then learn a root-only DPM to selectively combine them. This hypothesis is also supported by the features visualized in <ref type="figure">Figure 4</ref>. The heat maps are characteristic of part responses (cat head, person face, upper-left quarter of a circle) in that they select specific visual structures.</p><p>We implemented this idea by applying a 3 ? 3 max filter to conv 5 and then training a root-only DPM with three components. The max filter is run with a stride of one to prevent subsampling the feature map, which would increase the sliding-window stride to a multiple of 16. We refer to the max-filtered conv 5 features as "max 5 ". Note that this model does not have any explicit DPM parts and that the root filters can be thought of as a learned object geometry filters that combine the conv 5 "parts". This approach (Table 1 row 4) outperforms the DPM variants that operate directly on conv 5 in terms of mAP as well as training and testing speed (since each model only has three filters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison with other methods</head><p>We compare our approach to several other methods on the VOC 2007 dataset. The first notable comparison is to DPM on HOG features. We report results using the standard 6 component, 8 part configuration, which achieves a mAP of 33.7%. We also computed another HOG-DPM baseline using 6 components, but without parts. Removing parts de-creases HOG-DPM performance to 25.2%. The max 5 variant of DeepPyramid DP, which uses conv 5 implicitly as a set of shared parts, has a mAP of 45.2%.</p><p>We also compare our method to the recently proposed R-CNN <ref type="bibr" target="#b13">[14]</ref>. The directly comparable version of R-CNN uses pool 5 features and no fine-tuning (pool 5 is the same as max 5 , but with a stride of two instead of one). This comparison isolates differences to the use of a sliding-window method versus classifying warped selective search <ref type="bibr" target="#b29">[30]</ref> windows. We can see that for some classes where we expect segmentation to succeed, such as aeroplane and cat, R-CNN strongly outperforms DeepPyramid DPM. For classes where we expect segmentation might fail, such as bottle, chair, and person, DeepPyramid DPM strongly outperforms R-CNN. Performance is similar for most of the other categories, with DeepPyramid DPM edging out R-CNN pool 5 in terms of mAP. Of course, this represents the weakest variant of R-CNN, with significant improvements coming from fine-tuning for detection and incorporating a bounding-box (BB) regression stage.</p><p>We have shown that DeepPyramid DPM is competitive with R-CNN pool 5 without fine-tuning. The R-CNN results suggest that most of the gains from fine-tuning come in through the non-linear classifier (implemented via layers fc 6 and fc 7 ) applied to pool 5 features. This suggests that it might be possible to achieve similar levels of performance with DeepPyramid DPM through the use of a more powerful non-linear classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation study</head><p>To understand the effects of some of our design choices, we report mAP performance on VOC 2007 test using a few ablations of the DP-DPM max 5 model. First, we look at mAP versus the number of mixture components. Mean AP with {1, 2, 3} components is {39.9%, 45.1%, 45.2%}. For most classes, performance improves when going from 1 to 2 or 1 to 3 components because the variety of templates allows for more recall. We also looked at the effect of training with negative examples that come only from negative images (i.e., not using mislocalized positives as negative examples). Using negatives only from negative images decreases mAP by 6.3 percentage points to 38.8%. Using standard DPM non-maximum suppression decreases mAP by 1.3 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on PASCAL VOC 2010-2012</head><p>We used the VOC 2007 dataset for model and hyperparameter selection, and now we report results on VOC 2010-2012 obtained using the official evaluation server. formance against HOG-DPM is especially strong. When comparing to R-CNN FT fc 7 , without bounding-box regression (BB), DeepPyramid DPM manages better performance in two classes: bottle and person. This likely speaks to the weakness in the region proposals for those classes. The VOC 2011 and 2012 sets are the same and performance is similar to 2010, with a mAP of 41.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel synthesis of deformable part models and convolutional neural networks. In particular, we demonstrated that any DPM can be expressed as an equivalent CNN by using distance transform pooling layers and maxout units. Distance transform pooling generalizes max pooling and relates the idea of deformable parts to max pooling. We also showed that a DPM-CNN can run on top a feature pyramid constructed by another CNN. The resulting model-which we call DeepPyramid DPM-is a single CNN that performs multi-scale object detection by mapping an image pyramid to a detection score pyramid.</p><p>Our theoretical and experimental contributions bring new life to DPM and show the potential for replacing HOG templates in a wide range of visual recognition systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2sidebar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Schematic model overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )DTFigure 2 .</head><label>12</label><figDesc>Convolve feature pyramid level l with filters from DPM component c (2) Compute distance transforms (DT) of part convolution maps (3) Sum root and transformed part maps CNN equivalent to a single-component DPM. DPM can be written as an equivalent CNN by unrolling the DPM detection algorithm into a network. We present the construction for a single-component DPM-CNN here and then show how several of these CNNs can be composed into a multi-component DPM-CNN using a maxout layer (Figure 3). A single-component DPM-CNN operates on a feature pyramid level. (1) The pyramid level is convolved with the root filter and P part filters, yielding P + 1 convolution maps. (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>CNN equivalent to a multi-component DPM. A multi-component DPM-CNN is composed of one DPM-CNN per component (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 comparesTable 1 .</head><label>21</label><figDesc>DeepPyramid DPM with a variety of methods on VOC 2010. DeepPyramid DPM outperforms all recent methods other than the fine-tuned versions of R-CNN. Per-HOG versus conv5 feature pyramids. In contrast to HOG features, conv5 features are more part-like and scale selective. Each conv5 pyramid shows 1 of 256 feature channels. See Section 4.1 for details. Detection average precision (%) on VOC 2007 test. Column C shows the number of components and column P shows the number of parts per component. Our method is DP-DPM (DeepPyramid DPM). method C P aero bike bird boat botl bus car cat chair cow table dog horse mbike pers plant sheep sofa train tv mAP DP-DPM conv 5 3 0 41.2 64.1 30.5 23.9 35.6 51.8 54.5 37.2 25.8 46.1 38.8 39.1 58.5 54.8 51.6 25.8 48.3 33.1 49.1 56.1 43.3 DP-DPM conv 5 3 4 43.1 64.2 31.3 25.0 37.6 55.8 55.7 37.8 27.3 46.0 35.5 39.0 58.2 57.0 53.0 26.6 47.6 35.3 50.6 56.6 44.2 DP-DPM conv 5 3 8 42.3 65.1 32.2 24.4 36.7 56.8 55.7 38.0 28.2 47.3 37.1 39.2 61.0 56.4 52.2 26.6 47.0 35.0 51.2 56.1 44.4 DP-DPM max 5 3 0 44.6 65.3 32.7 24.7 35.1 54.3 56.5 40.4 26.3 49.4 43.2 41.0 61.0 55.7 53.7 25.5 47.0 39.8 47.9 59.2 45.2 /a 29.2 35.2 19.4 16.7 3.7 53.2 50.2 27.2 10.2 34.8 30.2 28.2 46.6 41.7 26.2 10.3 32.8 26.8 39.8 47.0 30.5 R-CNN [14] pool 5 n/a n/a 51.8 60.2 36.4 27.8 23.2 52.8 60.6 49.2 18.3 47.8 44.3 40.8 56.6 58.7 42.4 23.4 46.1 36.7 51.3 55.7 44.2 fine-tuned variants of R-CNN R-CNN FT pool 5 n/a n/a 58.2 63.3 37.9 27.6 26.1 54.1 66.9 51.4 26.7 55.5 43.4 43.1 57.7 59.0 45.8 28.1 50.8 40.6 53.1 56.4 47.3 R-CNN FT fc 7 n/a n/a 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2 R-CNN FT fc 7 BB n/a n/a 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5</figDesc><table><row><cell cols="2">image pyramid level 1 level 2</cell><cell>level 3</cell><cell>level 4</cell><cell>level 5</cell><cell>level 6</cell><cell>level 7</cell></row><row><cell cols="2">HOG</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">conv5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>conv5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>conv5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 4. HOG-DPM</cell><cell cols="6">6 0 23.8 51.3 5.1 11.5 19.2 41.3 46.3 8.5 15.8 20.8 8.6 10.4 43.9 37.6 31.9 11.9 18.1 25.7 36.5 35.4 25.2</cell></row><row><cell>HOG-DPM [15]</cell><cell cols="6">6 8 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 33.7</cell></row><row><cell>HSC-DPM [26]</cell><cell cols="6">6 8 32.2 58.3 11.5 16.3 30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1 51.6 39.9 12.4 23.5 34.4 47.4 45.2 34.3</cell></row><row><cell>DetectorNet [3]</cell><cell>n/a n</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Detection average precision (%) on VOC 2010 test. method aero bike bird boat botl bus car cat chair cow table dog horse mbike pers plant sheep sofa train tv mAP DP-DPM max 5 61.0 55.7 36.5 20.7 33.2 52.5 46.1 48.0 22.1 35.0 32.3 45.7 50.2 59.2 55.8 18.7 49.1 28.8 40.6 48.1 42.0 HOG-DPM [15] 49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4 51.2 47.7 10.8 34.2 20.7 43.8 38.3 33.4 UVA [30] 56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5 52.9 32.9 15.3 41.1 31.8 47.0 44.8 35.1 Regionlets [32] 65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2 55.7 43.5 14.3 43.9 32.6 54.0 45.9 39.7 SegDPM [10] 61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3 54.4 47.1 14.8 38.7 35.0 52.8 43.1 40.4 R-CNN FT fc 7 [14] 67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8 65.9 53.6 26.7 56.5 38.1 52.8 50.2 50.2 R-CNN FT fc 7 BB 71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0 69.0 58.1 29.5 59.4 39.3 61.2 52.4 53.7</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The GPUs used in this research were generously donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toshev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.5" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part based models. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pictorial structures for object recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distance transforms of sampled functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory of Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">19</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bottom-up segmentation for top-down detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Elschlager</surname></persName>
		</author>
		<idno>1973. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Computer</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://www.cs.berkeley.edu/?rbg/latent-v5/.4" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with grammar models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">Implementing efficient convnet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Histograms of sparse codes for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Distributed Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proc on Vision, Image, and Signal Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning hierarchical poselets for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
