<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Domain-Adapted Feature Learning for Video Saliency Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellitto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">F Proietto</forename><surname>Salanitri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">S</forename><surname>Palazzo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">F</forename><surname>Rundo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">D</forename><surname>Giordano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">C</forename><surname>Spampinato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellitto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Proietto</forename><surname>Salanitri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palazzo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giordano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Rundo</forename><surname>Stmicrolectronics</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">PeRCeiVe Lab</orgName>
								<orgName type="institution">University of Catania Tel</orgName>
								<address>
									<postCode>+39-095-7387905</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ADG Central R&amp;D</orgName>
								<address>
									<region>Catania</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Domain-Adapted Feature Learning for Video Saliency Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Noname manuscript No. (will be inserted by the editor) Present address: of F. Author * Contribute equally. + Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Video Saliency Prediction ? Conspicuity Networks ? Conspicuity maps ? Domain Adaptation ? Gradient Reversal Layer ? Domain Specific Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a 3D fully convolutional architecture for video saliency prediction that employs hierarchical supervision on intermediate maps (referred to as conspicuity maps) generated using features extracted at different abstraction levels. We provide the base hierarchical learning mechanism with two techniques for domain adaptation and domain-specific learning. For the former, we encourage the model to unsupervisedly learn hierarchical general features using gradient reversal at multiple scales, to enhance generalization capabilities on datasets for which no annotations are provided during training. As for domain specialization, we employ domain-specific operations (namely, priors, smoothing and batch normalization) by specializing the learned features on individual datasets in order to maximize performance. The results of our experiments show that the proposed model yields state-of-the-art accuracy on supervised saliency prediction. When the base hierarchical model is empowered with domain-specific modules, performance improves, outperforming stateof-the-art models on three out of five metrics on the DHF1K benchmark and reaching the second-best results on the other two. When, instead, we test it in an unsupervised domain adaptation setting, by enabling hierarchical gradient reversal layers, we obtain performance comparable to supervised state-of-the-art. Source code, trained models and example outputs are publicly available at https://github.com/perceivelab/ hd2s.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract In this work, we propose a 3D fully convolutional architecture for video saliency prediction that employs hierarchical supervision on intermediate maps (referred to as conspicuity maps) generated using features extracted at different abstraction levels. We provide the base hierarchical learning mechanism with two techniques for domain adaptation and domain-specific learning. For the former, we encourage the model to unsupervisedly learn hierarchical general features using gradient reversal at multiple scales, to enhance generalization capabilities on datasets for which no annotations are provided during training. As for domain specialization, we employ domain-specific operations (namely, priors, smoothing and batch normalization) by specializing the learned features on individual datasets in order to maximize performance. The results of our experiments show that the proposed model yields state-of-the-art accuracy on supervised saliency prediction. When the base hierarchical model is empowered with domain-specific modules, performance improves, outperforming stateof-the-art models on three out of five metrics on the DHF1K benchmark and reaching the second-best results on the other two. When, instead, we test it in an unsupervised domain adaptation setting, by enabling hierarchical gradient reversal layers, we obtain performance comparable to supervised state-of-the-art. Source code, trained models and example outputs are publicly available at https://github.com/perceivelab/ hd2s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video saliency detection is the task of predicting human gaze fixation when perceiving dynamic scenes, and it is typically carried out by estimating spatio-temporal saliency maps from an input video sequence. Saliency detection, in general, can be seen as the upstream processing step of multiple applications that include object detection <ref type="bibr" target="#b14">[15]</ref>, behavior understanding <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref>, video surveillance <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b70">70]</ref> and video captioning <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b6">7]</ref>. Existing video saliency detection methods generally apply single-image saliency estimation on individual frames, and combine the results with recurrent layers to temporally model frame-level features. However, the two separate analysis stages in these models make them unable to fully capture spatio-temporal features simultaneously. Recently, 3D fully-convolutional models have addressed this limitation by progressively aggregating spatio-temporal cues, achieving state-of-the-art performance on standard benchmarks. For example, TASED-Net <ref type="bibr" target="#b43">[44]</ref> adopts a standard encoder-decoder architecture, as largely used in semantic segmentation tasks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b45">46]</ref>, that learns a compact spatio-temporal representation, and feeds it to a decoder subnetwork to perform saliency prediction. While these methods perform well, saliency prediction is constrained by the aggregated representation learned at the model's bottleneck. This leads to learn representations that are more specific to the training data distribution, consequently limiting model generalization. <ref type="figure">Fig. 1</ref>: HD 2 S overview. Our proposed model generates multiple intermediate saliency maps by using features extracted at different abstraction levels, and combines them to predict the output map. We refer to the intermediate saliency maps as conspicuity maps.</p><p>Following the success of 3D convolutional architectures, in this paper we propose a model based on Hierarchical Decoding for Dynamic Saliency prediction -HD 2 S -that, instead of using a compact spatiotemporal representation as in <ref type="bibr" target="#b43">[44]</ref>, generates multiple saliency maps by using features learned at different abstraction levels and then combines them to compute the final output. We refer to the intermediate saliency maps as conspicuity maps, as the employed architecture recalls the multi-scale model proposed in <ref type="bibr" target="#b21">[22]</ref>. Using representations extracted at different abstraction levels (from shallow to deeper) allows the model to learn both generic (and more dataset-independent) and datasetspecific features. The twofold advantage we obtain is to enhance performance on a specific dataset and, at the same time, to improve adaptation capabilities. Our approach takes inspiration from DVA <ref type="bibr" target="#b67">[67]</ref>, but extends it to the video domain by learning spatio-temporal cues for predicting visual saliency. More specifically, HD 2 S, shown in <ref type="figure">Fig. 1</ref>, is a 3D fully-convolutional network that employs an ensemble of multiple prediction models, each producing a conspicuity-like map at a specific abstraction level, for better saliency estimation.</p><p>As an additional contribution, we tackle the problem of generalization for video saliency prediction. Indeed, state-of-the-art methods lack domain adaptation capabilities and require a mandatory fine-tuning step to perform well on datasets that they were not trained on. As the deep learning community is moving to build more generalizable models, we argue that this holds, even more so, for saliency prediction research, given its fundamental nature in an artificial vision pipeline. To address this issue, our saliency prediction network is provided with a multi-scale domain adaption mechanism, based on gradient reversal <ref type="bibr" target="#b13">[14]</ref>, that encourages the model to learn domain-independent features. In particular, each abstraction level of HD 2 S is provided with a gradient reversal layer that prevents the learned representation from becoming dataset-specific.</p><p>We also address the opposite problem, i.e., domainspecific learning, by adding to the model some datasetspecific modules whose parameters are learned in order to maximize performance on a given dataset. We carry out extensive experiments testing of HD 2 S on multiple video saliency benchmarks (DHF1K <ref type="bibr" target="#b63">[63]</ref>, UCF Sports <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b55">55]</ref>, Hollywood2 <ref type="bibr" target="#b41">[42]</ref>) obtaining stateof-the-art performance and outperforming existing models. Performance that are boosted, as expected, when domain-specific learning is enabled. We also test thoroughly the domain adaptation capabilities of HD 2 S to datasets for which no annotations are available during training. Our model shows remarkable results, achieving performance comparable to state-of-the-art models that, instead, are trained (or fine-tuned) on those datasets in a standard supervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Saliency detection has been long investigated in AI and computer vision research. In general, saliency models can be categorized in: saliency prediction <ref type="bibr" target="#b65">[65]</ref> approaches that attempt to predict the fixation points of a human observer during free-viewing (e.g., they aim to predict where people look at in a scene), and salient object detection <ref type="bibr" target="#b37">[38]</ref> methods that, instead, focus on assessing the saliency of pixels w.r.t. objects of interest (e.g., they aim to separate the salient objects from the background). Saliency methods can be further categorized according to whether they process still images (static saliency) or videos (dynamic saliency).</p><p>Static saliency has been studied for decades. Initial models, biologically-inspired <ref type="bibr" target="#b21">[22]</ref> and employing handcrafted features, were followed by recent CNN-based attempts <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23]</ref> that yield superior performance, rapidly becoming state of the art for static saliency prediction. S To overcome the lack of large eye fixation datasets, CNN-based static methods rely mainly on image classification models, as backbone, exploiting their capability to extract features useful for other visual tasks. Different encoder-decoder architectures with various strategies to combine the extracted features have then been proposed. The release of larger dataset for saliency, such as MIT300 <ref type="bibr" target="#b26">[27]</ref>, SALICON <ref type="bibr" target="#b25">[26]</ref>, and CAT2000 <ref type="bibr" target="#b3">[4]</ref>, led to a performance gain. DeepGaze II <ref type="bibr" target="#b31">[32]</ref> investigated the benefit of employing low-and high-level features in saliency prediction. Similarly, ML-NET <ref type="bibr" target="#b8">[9]</ref> proposed to combine low-and high-level features at the bottleneck, while <ref type="bibr" target="#b30">[31]</ref> concatenates the outputs from several layers and processes them with multiple convolutional layers with different dilation rates. Another approach is to use a two-stream encoder architecture as in <ref type="bibr" target="#b20">[21]</ref>, where the image at different spatial scales is fed as input to the model, in order to extract low and high resolution information. <ref type="bibr" target="#b12">[13]</ref>, based on <ref type="bibr" target="#b20">[21]</ref>, used a similar network adding, after feature extraction, a channel weighting subnetwork that encodes contextual information. Differently from the above models, other works exploit adversarial training <ref type="bibr" target="#b15">[16]</ref> for saliency prediction, such as SalGAN <ref type="bibr" target="#b46">[47]</ref> and GazeGAN <ref type="bibr" target="#b5">[6]</ref>. Compared to saliency models for still images, saliency prediction in videos is an even more complex problem, due to the presence of the temporal dimension and to the additional computational effort it requires. Static saliency models have been adapted to dynamic saliency by using them in frame-by-frame mode, but they are outperformed by the dynamic models that jointly process the temporal dimension.</p><p>In recent years, a common strategy has been to extend static saliency models to the video scenario by incorporating motion features <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b57">57]</ref>. For example, <ref type="bibr" target="#b64">[64]</ref> proposes a two-model architecture to exploit spatiotemporal features: the first module performs frame-level saliency prediction; the second module, instead, takes pairs of frames with saliency predicted by the first module, and generates a dynamic saliency map. <ref type="bibr" target="#b54">[54]</ref> basically employs the same architecture as <ref type="bibr" target="#b64">[64]</ref> and self-attention, through non-local operations <ref type="bibr" target="#b66">[66]</ref>. SalEMA <ref type="bibr" target="#b36">[37]</ref>, instead, proposes a 2D encoder-decoder architecture with a recurrent module added to the bottleneck for integrating temporal information provided by the previous frames. Motion cues have been also included in saliency prediction through either recurrent neural networks applied to spatial feature encodings or convolutional recurrent networks. OM-CNN <ref type="bibr" target="#b24">[25]</ref> is a dual-stream network that extracts spatial and temporal features using YOLO <ref type="bibr" target="#b49">[50]</ref> and FlowNet <ref type="bibr" target="#b10">[11]</ref>, whose respective objectness and motion features are then combined via a two-layer ConvL-STM. Similarly, ACLNet <ref type="bibr" target="#b63">[63]</ref> performs static saliency prediction through attention module that performs a global spatial operation on learned features. These features are then given to a ConvLSTM to model temporal information. The recent SalSAC model <ref type="bibr" target="#b68">[68]</ref>, leveraging the success of self-attention for saliency prediction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b63">63]</ref>, proposes an architecture with a shuffled attention mechanism on multi-level features for better modeling of spatial saliency. Correlation features between multi-level features and shuffled attention on the same features are provided to a ConvLSTM for learning temporal cues.</p><p>With the recent availability of a large-scale saliency benchmark, i.e., DHF1K <ref type="bibr" target="#b63">[63]</ref>, 3D fully-convolutional models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44]</ref>, jointly extracting spatial and temporal features, have been proposed. RMDN <ref type="bibr" target="#b2">[3]</ref> processes video clips with a 3D convolutional neural network based on C3D <ref type="bibr" target="#b59">[59]</ref>, and then employs LSTMs to enforce temporal consistency among the segments. TASED-Net <ref type="bibr" target="#b43">[44]</ref> is a 3D fully-convolutional network, based on a standard encoder-decoder architecture, for video saliency detection without any additional feature processing steps. Similarly to the above approaches, our HD 2 S model is a 3D fully-convolutional network extending the multi-abstraction level analysis, proposed in <ref type="bibr" target="#b67">[67]</ref> for static saliency, to the video domain by learning spatiotemporal cues.</p><p>Multi-level feature learning has been already applied in several application domains, most notably in object detection through the use of feature pyramid networks (FPN) <ref type="bibr" target="#b18">[19]</ref>. Most relevant to our approach are the works that carry out salient object detection using multi-level feature hierarchies, such as Amulet <ref type="bibr" target="#b72">[72]</ref> and DSS <ref type="bibr" target="#b19">[20]</ref>. However, beside targeting static saliency prediction in images (and not in videos), those approaches apply an early-fusion mechanism of multi-level features, that are combined (through different concatenation schemas) before being further processed. Our method, instead, performs a late fusion of features: we encourage each decoding path to independently extract information from a certain abstraction layer, making sure that no interbranch "contamination" may happen except at the very last layer, and thus pushing it to learn scale-specific and complementary saliency features. HD 2 S also performs domain adaption to generalize across datasets without the need to be fine-tuned. Indeed, in all prediction tasks, shifts in train and test distributions may lead to a significant degradation of the model's performance. Trying to train a predictor capable of handling these shifts is commonly referred to as domain adaptation. Among the different domain adaptation settings 1 , we focus on unsupervised domain adaptation, which is the task of aligning features extracted from the model across source and target domains, without any labelled samples from the latter. Several techniques have been proposed (though not for saliency prediction), such as regularizing the maximum mean discrepancy <ref type="bibr" target="#b38">[39]</ref>, minimizing correlation <ref type="bibr" target="#b56">[56]</ref>, or adversarial discriminator accuracy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b60">60]</ref>. An effective approach to transfer the feature distribution from source to target domains is proposed in <ref type="bibr" target="#b13">[14]</ref> through the use of gradient reversal layers, treating domain invariance as a binary classification problem. This approach addresses domain adaptation by adversarially forcing a model to solve a given task while learning features that are non-discriminative across datasets. In HD 2 S we apply this strategy on multi-level features (unlike typical single-branch usage), in order to support the generalization of the saliency prediction task to datasets for which no annotations are available during training. While unsupervised domain adaptation has been applied to image classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b60">60]</ref>, face recognition <ref type="bibr" target="#b27">[28]</ref>, object detection <ref type="bibr" target="#b58">[58]</ref>, semantic segmentation <ref type="bibr" target="#b73">[73]</ref> and video action recognition <ref type="bibr" target="#b33">[34]</ref> (among others), our work is, to our knowledge, the first to deal with unsupervised domain adaptation on video saliency prediction. It is worthwhile to note that this is technically and fundamentally different from the form of domain adaptation proposed in UNISAL <ref type="bibr" target="#b11">[12]</ref>, that, instead, learns domain-specific parameters. This means that, at inference time, UNISAL requires to know the source dataset of a given input in order to select domainspecific learned parameters. Our approach, instead, is domain-agnostic as it employs the learned parameters on any tested domain. It is also different from unsupervised salient object detection <ref type="bibr" target="#b71">[71]</ref>, which, instead, attempts to predict saliency by exploiting large unlabelled or weaklylabelled samples. However, we also provide HD 2 S with domain-specific learning capabilities as in <ref type="bibr" target="#b11">[12]</ref>, showing how this mechanism improves performance but cannot be applied in unsupervised domain adaptation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture overview</head><p>The proposed architecture is a fully-convolutional multibranch encoder-decoder network for saliency prediction, illustrated in <ref type="figure">Fig. 2</ref>. An input sequence of consecutive video frames is first processed by a feature extraction path, which computes spatio-temporal features at different scales and abstraction levels. The extracted features serve as input to separate network branches that estimate a set of conspicuity maps at the corresponding points in the model, while at the same time providing skip paths to ease gradient flow during training. At the output of the model, conspicuity maps are combined to predict the saliency map for the last frame in the input sequence. Our model is trained in a supervised way on a source dataset, for which saliency annotations are available.</p><p>Furthermore, the base model is provided with two additional mechanisms (that can be both disabled or enabled exclusively):</p><p>-Domain adaptation modules that aim to make the model learn, in an unsupervised way, generalizable features (see red items in <ref type="figure">Fig. 2</ref>). In particular, each conspicuity subnetwork forks to a domain classification path, that is trained to classify whether an input video sequence (more precisely, the corresponding features at that abstraction level) is taken from the source domain or from a target domain, which cannot be employed for training through direct supervision since annotations are not available. In order to perform this adaptation, we apply the gradient reversal technique: the feature extraction layer, shared by the conspicuity networks and the domain classifiers, is trained in an adversarial way, in order to force the model to learn features that are both discriminative and predictive -saliency-wise -as well as domain-invariant, in order to achieve satisfactory results even on the target domain. -Domain-specific learning mechanism that learns specific parameters to enhance the prediction on a given dataset. More specifically, we add modules (shown as light gray items in <ref type="figure">Fig. 2</ref>), used in a multisource training scenario (i.e., when using in training multiple datasets at the same time), whose parameters are optimized on each individual dataset. These modules aim to modulate features shared across multiple datasets based on the test data domain and include: domain-specific priors, batch-normalization and prediction smoothing.</p><p>At inference time, saliency maps are predicted for each frame by applying the model in a sliding window fashion, as in <ref type="bibr" target="#b43">[44]</ref>; the saliency map S t at time t is predicted from a sequence V t = {I t?T +1 , . . . , I t }, where I t is the video frame at time t. To predict the first T ? 1 frames, we reverse the chronological order of the corresponding input clips: each S t for 1 ? t ? T ? 1 is predicted from the sequence V t = {I t+T ?1 , . . . , I t }. As a final post-processing step, we apply a Gaussian filter (? = 5) for smoothing the output saliency map.</p><p>In the following, we describe each of the components of our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature extractor</head><p>The employed feature extractor performs spatio-temporal encoding of an input videoclip (16 frames of size 128?192), using S3D <ref type="bibr" target="#b69">[69]</ref> as a backbone. It then progressively reduces the dimensions of the feature maps through 3D max pooling to 2?4?6 (time ? height ? width), while increasing the number of channels to 1024. However, in order to exploit the full potential of the learned hierarchical representations, we select feature maps at different <ref type="figure">Fig. 2</ref>: HD 2 S architecture: Our multi-branch decoder predicts four conspicuity maps at different feature abstraction levels, which are then integrated into the final saliency prediction, on which KL-divergence loss L s is minimized. As for unsupervised domain adaption, each decoder branch is equipped with a gradient reversal layer (see red items) that encourages the model to learn features that generalize to a target data domain in an unsupervised way, by maximizing the classification error L d on the prediction of an input sample's domain. Finally, HD 2 S is also provided with domain-specific priors added to encoded features, with removed temporal dimension, and domain-specific smoothing as a last final layer. levels of the extractor, corresponding to different abstraction details, in order to build a skip architecture able to capture multi-headed saliency responses. In our implementation, we select feature maps from the S3D backbone at the output of the second, third and fourth pooling layers and at the input of the last average pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conspicuity networks</head><p>After feature encoding, we learn several conspicuity maps from the partial information produced at different levels of the feature extraction stack through multiple decoder networks (referred as conspicuity networks in <ref type="figure">Fig.2</ref>).</p><p>Each conspicuity network in the model processes one of the spatio-temporal feature blocks coming from the feature extractor and returns a single-channel saliency map, encoding the conspicuity of spatial locations at that level of abstraction. In detail, the temporal dimension of the input feature maps is gradually removed, by applying a cascade of spatially point-wise convolutions (i.e., with kernel 3 ? 1 ? 1 and stride 2 ? 1 ? 1) that halve the temporal dimension at each step. The number of pointwise convolutions varies for each conspicuity network, depending on the size of the input feature maps.</p><p>After that, the (now purely spatial) set of feature maps is processed by a stack of 2D convolutional layers, interleaved with bilinear upsampling blocks, each of which doubles the spatial size of the feature maps until the original resolution of each frame is recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Saliency prediction</head><p>The four conspicuity maps produced by the above subnetworks are finally fused to predict saliency on the last frame of the input video. The global fusion layer consists of concatenating the four maps and performing pixelwise 1?1 convolution followed by logistic activation.</p><p>At training time, the whole model (feature extractor, conspicuity networks and saliency predictor) is trained supervisedly on the source dataset in order to minimize the Kullback-Leibler (KL) divergence <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21]</ref>, between the predicted saliency map and conspicuity maps, and the correct target. More formally, given the predicted output saliency map S t , the four conspicuity maps C t,i with i = 1, 2, 3, 4 and the ground-truth map G t for a given target frame, all normalized over pixels appropriately, our multi-level saliency loss L s is computed as follows:</p><formula xml:id="formula_0">L s (S t , C t , G t ) = 4 j=1 i G t,i log G t,i C t,j,i + i G t,i log G t,i S t,i<label>(1)</label></formula><p>where index i iterates over all pixels, index j iterates over the four conspicuity maps, G t,i , S t,i and C t,i,j are corresponding pixels of, respectively, the ground truth map, the output saliency map and the j-th conspicuity map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Domain adaptation</head><p>In addition to training the model in a supervised way on the source domain, we also encourage the feature extractor to generalize over a target domain, without any supervision. Our unsupervised domain adaptation strategy relies on the Sgradient reversal layer (GRL) approach.</p><p>In particular, we integrate domain adaptation by inserting, in all of the conspicuity subnetworks, a branch with a gradient reversal layer and a domain classifier after the temporal-dimension removal layer (see <ref type="figure">Fig. 2</ref>). More formally and generally, given an input video clip V t with associate binary domain label d ? {0, 1} (source or target, respectively), we compute a set of associated domain classification losses {L d,1 , . . . , L d,4 } from 4 domain classifiers attached to the conspicuity networks. If we indicate byd i the probability of the input being from the target domain estimated by the i-th classifier, the corresponding negative log-likelihood loss is defined as:</p><formula xml:id="formula_1">L d,i d,d i = ?d logd i ? (1 ? d) log 1 ?d i<label>(2)</label></formula><p>The overall domain classification loss is simply computed as the sum of the individual contributions, since the interaction between saliency prediction and domain adaptation is controlled by the ? hyperparameter in the gradient reversal layers. As a result, the comprehensive loss for model training with domain adaptation is the following:</p><formula xml:id="formula_2">L = L s + 4 i=1 L d,i<label>(3)</label></formula><p>During training, we alternately pass a batch of videos from the source domain and a batch of videos from the target domain: on the former, we compute and backpropagate both the saliency prediction loss L s and the domain classification loss L d (with target d = 0); on the latter, we can only compute and backpropagate the domain classification loss L d (with target d = 1), since no saliency annotation is available on the target domain. Minimizing the domain classification loss has the effect to train the classifiers to better discriminate between the source and the target domains, while at the same time adversarially training the feature extractor (and the initial temporal-removal layers in the conspicuity networks) to produce features that confuse the classifier, and hence that are domain-independent.</p><p>Architecturally, each domain classifier consists of a stack of 1?1 spatial convolutions aimed at reducing the number of features, followed by fully-connected layers, the last of which provides binary classification prediction of the input video's domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Domain-specific learning</head><p>In certain multi-source training scenarios (e.g., as done in <ref type="bibr" target="#b11">[12]</ref>), one may assume that annotations are available for all employed datasets, thus enabling supervised training on all of them. When applying our saliency prediction model to this scenario, we provide it with domain-specific operations <ref type="bibr" target="#b11">[12]</ref>, which address the domain shift among different datasets. Unlike the unsupervised domain adaption setting, where we attempt to unsupervisedly learn features that generalize over multiple datasets, we here explicitly tailor learned features to the specific characteristics of each dataset. In practice, we adopt a set of domain-specific techniques which have demonstrated to be effective <ref type="bibr" target="#b11">[12]</ref>: Domain-specific priors. <ref type="bibr" target="#b11">[12]</ref> thoroughly analyzed multiple video saliency benchmarks, identifying the sources of data shift among them and encoding these sources into a set of Gaussian prior maps. We employ the same strategy by initializing domain priors as in <ref type="bibr" target="#b11">[12]</ref>, and then letting the model learn the most suitable filters to weigh the encoded spatio-temporal features depending on the input data domain. Domain priors are used to modulate the encoded features, after removing the temporal dimension (see light gray blocks in <ref type="figure">Fig. 2</ref>). Domain-specific smoothing. The optimal way in which the output map should be smoothed varies between different datasets and depends mostly on how ground truth is created. To address this issue, we learn a different Gaussian kernel (i.e, with a different value of ?) for each input data domain. Unlike <ref type="bibr" target="#b11">[12]</ref>, our layer is parameterized by ? only, with convolution coefficients computed accordingly to make the filter Gaussian, while <ref type="bibr" target="#b11">[12]</ref> initialize domain-specific convolutional filters to be Gaussian, but they may drift to non-Gaussian as the network updates its parameters. This smoothing is applied to the global saliency map (see <ref type="figure">Fig. 2</ref>). Domain-specific batch normalization aims at mitigating the impact of data distribution shift on the statistics estimated by batch normalization for inference, which may become inaccurate when computed over different benchmarks. Thus, we learn batch normalization statistics for each dataset independently and accordingly apply them at inference time, depending on the input domain.  <ref type="bibr" target="#b23">[24]</ref> includes 538 videos of daily action, sports, social activity and art performance; we employ this dataset only as a target dataset for unsupervised domain adaptation. <ref type="figure" target="#fig_0">Fig. 3</ref> provides statistics on the training splits of the datasets employed for training our model: 1) UCF Sports is the smallest one in terms of available videos and average number of frames per video, thus it seems to be unsuitable for models with high capacity as they likely overfit it; 2) Hollywood2 contains the highest number of videos but the majority has a very short number of frames (see the right histogram in <ref type="figure" target="#fig_0">Fig. 3</ref>), thus it may disadvantage methods that model temporal cues; 3) DHF1K is the most balanced in terms of videos and number of frames per videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training procedure</head><p>In our experiments, we pre-train the S3D backbone on the Kinetics-400 <ref type="bibr" target="#b28">[29]</ref> dataset; backbone parameters are not frozen, so they are updated during saliency prediction training. After empirically testing different hyperparameter configurations in order to find the best combination, the networks are trained for 2500 iterations, using Adam as optimizer <ref type="bibr" target="#b29">[30]</ref> with learning rate of 10 ?3 . To reduce overfitting, L 2 regularization is applied, with a weight decay factor of 2 ? 10 ?7 . The ? parameter of the gradient reversal layers during training gradually varies from 0 to 1:</p><formula xml:id="formula_3">? = 2 1 + e ?10?p ? 1<label>(4)</label></formula><p>where p linearly goes from 0 to 1 according to the formula: p = current_iteration total_iterations <ref type="bibr" target="#b4">(5)</ref> Gradually increasing ? also acts as an additional regularizer, since it prevents the model from focusing too much on the saliency prediction objective as training goes on. During training, sequences of T = 16 consecutive frames are randomly sampled from the dataset's videos, and each frame is spatially resized to 128 ? 192.</p><p>We employed a batch size of 200, although for memory limitations we forward batches of 8 samples at each time, which accumulating gradients and updating the model's parameters every 25 such forward steps. When training with domain adaptation, we also forward a batch of samples from the source domain and one of samples from the target domain, and use them to update the domain classifier only.</p><p>To evaluate performance, we use each dataset training/test split when available, with 10% of the training data used as validation split. An exception is represented by DHF1K, since ground-truth annotations for the test set are not provided for blind assessment: in this case, when comparing to state-of-the-art methods (Tab. 1), we report the test accuracy as computed by the dataset curators; while for ablation study (Tab. 3 and 4) and domain adaptation analysis (Tab. 5 and 7), we employ the original validation set as test set.</p><p>Validation results are used to perform model selection for inference on the test set. When evaluating test performance in single-dataset experiments, the training, validation and test sets all come from the same domain. In domain adaptation experiments (with labeled source and unlabeled target datasets), training and validation splits are from the source domain (whose annotations can be used at training time), while the test set is from either an unseen portion of the target domain or from a different dataset altogether. In multi-dataset experiments, we combine the training splits of DHF1K, UCF Sports and Hollywood2 datasets into a single training set; as validation set, we employ only DHF1K's validation split (because of its better balance compared to the other datasets, as mentioned in Sect. 4.1), while inference is carried out on each dataset's test split. In this setting, in order to support domainspecific learning and correctly update domain-specific modules, each training mini-batch contains videos from a dataset at a time, alternating between datasets to deal with different dataset sizes.</p><p>To compare the results obtained by the models, we use five commonly used evaluation metrics for video saliency prediction <ref type="bibr" target="#b4">[5]</ref>: Normalized Scanpath Saliency (NSS), Linear Correlation Coefficient (CC), Area under the Curve by Judd (AUC-J), Shuffled-AUC (s-AUC) and Similarity (SIM). Higher scores on each metric mean better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Video saliency prediction performance</head><p>We first test the performance of our base model (without any form of adaptation) in the supervised scenario on the DHF1K test benchmark, to evaluate its capabilities in the video saliency prediction task. We then integrate domain adaptation by means of GRL layers (as shown in <ref type="figure">Fig. 2)</ref>, using the LEDOV dataset as a target domain, due to its wider subject variability than Hollywood2 and UCF Sports. Finally, we compute the performance of HD 2 S when using domain-specific learning, which is the form of adaptation that is most suitable with supervised learning settings and that can leverage all available annotated datasets (DHF1K, Hollywood2, UCF Sports). Tab. 1 shows the performance of our approach compared to the state of the art. HD 2 S, without domain adaptation (referred to in Tab. 1 simply as HD 2 S), outperforms all state-of-the-art methods on three out of five metrics (NSS, AUC-J, CC) and ranks second-best on SIM and third-best on s-AUC. Note that this variant also outperforms UNISAL <ref type="bibr" target="#b11">[12]</ref>, which already employs domain-specific learning, on four out of five metrics. When we also enable domain-specific learning modules HD 2 S (HD 2 S DSL ), performance (especially NSS, CC and AUC-J) increases sensibly, and it outperforms UNISAL on all metrics, demonstrating better representational and specialization capabilities. When using HD 2 S, with the hierarchical gradient reversal mechanism for domain adaptation(HD 2 S DA ), performance slightly degrades as the model attempts to adapt the learned features to the target datasets (in this case, UCF-Sports, Holly-wood2 and LEDOV). However, remarkably, despite this adaption mechanism, the model yields performance comparable with state-of-the-art ones.   Comparing HD 2 S with TASED-Net <ref type="bibr" target="#b43">[44]</ref>, which also employs S3D <ref type="bibr" target="#b69">[69]</ref> as backbone, it is possible to notice that our method (with and without adaptation) significantly outperforms TASED-Net in four out of five metrics using only half of the frames employed by TASED-Net (16 versus 32). TASED-Net slightly outperforms HD 2 S on s-AUC only, a metric that measures performance at the peripheral areas of the image, where a larger temporal context may allow to better capture the motion of an object. The generally better performance obtained by our method w.r.t TASED-Net demonstrates the importance of hierarchical feature learning, with equal backbone features. While our model yields the highest video saliency performance on DHF1K, and performance comparable to the state of the art on Hol-lywood2, its performance on UCF Sports is lower than UNISAL <ref type="bibr" target="#b11">[12]</ref> and SalSAC <ref type="bibr" target="#b68">[68]</ref>, as reported in <ref type="table" target="#tab_3">Table 2</ref>. This is explained first with the smaller size of UCF Sports w.r.t. DHF1K and Hollywood2. Indeed, during training, although we use all three datasets, UCF Sports accounts to about 1% of the total number of training video frames (DHF1K: 62%, Hollywood2: 37%, UCF <ref type="figure">Fig. 4</ref>: An example of failure, taken from Hollywood2. Despite a good prediction, HD 2 S misses to match the ground truth, as it is collected in a task-driven experiment (action recognition), thus highlighting more actions than salient objects. Sports 1%). This imbalance causes the model to overfit UCF Sports.</p><p>However, the suitability of Hollywood2 and UCF Sports for saliency detection deserves a further discussion. Indeed, both datasets' saliency annotations are collected in task-driven experiments (i.e., action recognition) and, as such, human observers tend to mainly observe specific actions rather than focusing on the salient objects themselves, which defeats the very purpose of saliency detection. An example is given in <ref type="figure">Fig. 4</ref> where our model fails to match the ground truth: indeed, it focuses on the girl's face at the front (correctly, as it is the most salient area), but the ground truth mostly highlights the action of the person behind the girl. Furthermore, both datasets show a huge center bias <ref type="bibr" target="#b11">[12]</ref> and have a rather limited variability of spatio-temporal features, especially Hollywood2, where the majority of video clips is very short in time. Analogously, UCF Sports is significantly smaller in terms of video frames, making it hard to train 3D convolutional models (or deep learning models in general). For all above reasons, we believe that both Hollywood2 and UCF Sports should not be used for saliency prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>To validate the importance and effectiveness of the HD 2 S architectural design choices, we test some model variants (without any domain adaptation or domain-specific learning) on the validation set of the DHF1K:</p><p>1. We first investigate the performance of our network, adding the different conspicuity nets one at a time; 2. We quantitatively and qualitatively evaluate the individual contribution of each conspicuity net, testing them in simple encoder-decoder architecture.</p><p>For the ablation study, we define as Baseline our network in a simple encoder-decoder configuration, i.e., without the intermediate conspicuity maps and multi-level loss.</p><p>More specifically, in the baseline model, the feature extractor remains unchanged, but only the deepest decoder branch (Conspicuity-net 4 ) is used.</p><p>The model variants and their performance are reported in <ref type="table" target="#tab_5">Table 3</ref>. The results show that: a) each conspicuity net makes its own contribution to improving the final performance; c) multi-level loss on conspicuity maps enhances saliency prediction too. Overall, these results clearly verify the effectiveness of all important design features in HD 2 S.</p><p>In our control experiments, we also evaluate the individual contribution of each conspicituity net by testing the performance of the model when the other decoder streams are ablated. For example, when testing the contribution of the first conspicuity map, we use only Feature 1 (see <ref type="figure">Fig. 2</ref>) from the encoder stream and the related decoder stream (Conspicuity-net 1 in <ref type="figure">Fig. 2)</ref> and so on for the other conspicuity nets. Results, reported in <ref type="table">Table 4</ref>, indicate that individually the third conspicuity net performs better than the others.   <ref type="table">Table 4</ref>: Individual contribution of each conspicuity net. Each configuration refers to a simple encoderdecoder architecture with different sets of encoded features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NSS CC SIM AUC-J s-AUC</head><p>To further elucidate this behavior, <ref type="figure" target="#fig_1">Fig. 5</ref> shows the weights learned by the fusion layer of HD 2 S model when integrating the four conspicuity maps for final prediction on the DHF1K dataset. The obtained values confirm that Conspicuity-net 3 contributes the most (see left block in <ref type="figure" target="#fig_1">Fig 5)</ref> on the prediction task for our HD 2 S model. However, it is less important when providing the model with domain-specific capabilities (which allowed it to yield the highest performance on DHF1K; see Tab. 1) to HD 2 S (see right blocks in <ref type="figure" target="#fig_1">Fig. 5</ref>). Furthermore, in the domain adaptation case, it can be noted how the different conspicuity maps contribute almost equally to the prediction, as a consequence of the mechanism to make the features domain-independent. A qualitative interpretation of this behavior and on the contribution of each conspicuity map in the hierarchy is shown in <ref type="figure" target="#fig_2">Fig. 6</ref>. When comparing the behaviour of the different decoder branches on the standard, domain adaption, and domain-specific learning regimes, the following considerations can be drawn: 1) in standard training case (top line in <ref type="figure" target="#fig_2">Fig. 6</ref>), Map 4 does not provide additional information w.r.t. Map 3; 2) in the domain adaptation scenario (middle line in <ref type="figure" target="#fig_2">Fig. 6</ref>), all feature maps appear to contribute equally; 3) in the domain specific learning case (bottom line in <ref type="figure" target="#fig_2">Fig. 6</ref>), Map 4 provides additional (motion) information to Map 3, while on the standard learning approach the two maps encode similar information. This provides an interpretation to the parameters learned by the fusion layers, reported in <ref type="figure" target="#fig_1">Fig. 5</ref>. Analyzing the intermediate maps in the domain specific learning (bottom line in <ref type="figure" target="#fig_2">Fig. 6</ref>), we can observe that the four intermediate maps encode saliency at different levels of detail: Map 1 extracts small background motion, Map 2 focuses mainly on the bull, Map 3 starts highlighting the bullfighter and, finally, Map 4 puts more emphasis on the bullfighter. A standard encoder-decoder architecture would instead use only the last map for saliency, thus missing the bull. This highlights the usefulness of the proposed hierarchical decoding scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Domain adaptation performance</head><p>When testing domain adaption performance, we distinguish two cases: a) the capabilities of the model to address domain shift issues, i.e., the case of reducing the shift between training and test data; and 2) the capabilities of the model to learn generalizable features that can be employed, without any additional tuning.</p><p>Domain-shift. To assess the performance of our hierarchical domain adaptation approach in tackling the problem of domain shift, we run a set of experiments by selecting different combinations of datasets to be employed as source domain (used in a supervised way during training) and target domain, used in an unsupervised way during training; as test set, an unseen portion from the target domain is used. The assumption in these experiments is to perform unsupervised learning on the test domain through our hierarchical gradient reversal approach before testing on it (on the appropriate test split not used for unsupervised learning).</p><p>In particular, we compare the performance of our base model in the three scenarios:</p><p>-Domain generalization, i.e., the model trained supervisedly on the source domain and directly tested on the target domain, with no additional information on the test dataset used during training; -Domain adaptation, i.e., the model trained with unsupervised adaptation on the target domain, enabled through the hierarchy of GRL layers as in our full model in <ref type="figure">Fig. 2</ref>; -Transfer learning, i.e., the model (with gradient reversal disabled) trained on the source dataset and then fine-tuned (in a supervised way) on the target dataset. This scenario represents the upper bound of the evaluation and is, of course, out of the scope of pure domain adaptation, since target domain labels are available at training time.</p><p>Tab. 5 shows the results for different combinations of source and target domains. Two main patterns of results can be identified, depending on whether DHF1K is employed as source domain or not. In the former case (top block of Tab. 5), it can be noticed that the employment of gradient reversal layers improves performance over all target datasets, compared to simply training on the source dataset. When instead DHF1K is employed as target domain (second and third blocks in Tab. 5), the use of gradient reversal layers degrades performance. This may be due to the specific characteristics of Hollywood2 and UCF Sports, which were collected in a task-driven experiments while DHF1K in a free-viewing one. Furthermore, the limited variability of spatio-temporal features from videos in Hollywood2, as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, makes harder for the model to move clustered features and to learn more general representations. Similarly, when UCF Sports is used as source domain, the small size of the dataset makes it easier for the model to focus on the supervised saliency prediction task (on which it can easily achieve a low training loss), rather than minimizing the domain adaptation loss. Overall, as expected, the highest performance are obtained in the transfer learning regime.</p><p>Learning generalizable features. We also test the capabilities of the model to learn general features by using, in the domain adaptation stream, a target dataset different from the one used for test. We specifically compute performance when training on DHF1K, adapting the learned features to LEDOV, and testing on never seen datasets (UCF and Hollywood2). Performance are reported in <ref type="table" target="#tab_8">Table 6</ref>, which reports how the performance gain of HD 2 S, when empowered with hierarchical gradient reversal modules, is higher than in the case of domain-shift experiments (see <ref type="table" target="#tab_7">Table 5</ref>). This demonstrates that our hierarchical domain adaptation mechanism is better at learning salience features that gener-alize well on multiple data domains than at addressing the domain-shift for a given dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-source training</head><p>A recent trend in video saliency prediction <ref type="bibr" target="#b11">[12]</ref> proposes multi-source training as a means for improving performance by leveraging the larger input variability of multiple data sources. This setup also allows for the integration of domain-specific learning capabilities, as mentioned in Sect. 3.6, that attempt to tune general features to specific datasets. The idea is to have a model that learns shared features across multiple datasets and then to employ domain-specific modules to adapt such features to a particular data domain. Although these domain-specific approaches do not strictly comply with the standard unsupervised domain adaptation formulation, as they go in the exact opposite direction to learning generalizable features (since they assume that target domain labels are available at training time), it is interesting to evaluate the impact of domain-specific learning on our architecture. In Sect. 4.3 and Tab. 1, we already showed that the integration of domain-specific capabilities into the HD 2 S model achieves state-of-theart performance on DHF1K, outperforming <ref type="bibr" target="#b11">[12]</ref>, that introduced those techniques. Here, we complete our analysis by assessing the impact of domain-specific layers compared to multi-source domain learning. More specifically, for multi-source domain learning, we use the integration of DHF1K, Hollywood2 and UCF-Sports, as an unified dataset, for training and testing our model.   As for domain specific learning, we enable the domainspecific modules (described in Sect. 3.6) and train their parameters using data from each individual dataset and during inference we provide, as an additional input to the model, the dataset we want to test it. We also compute performance when using single-source domain, i.e., training and test on a single dataset at a time. The results in Tab. 7 confirm that multi-source training by itself does not provide a much larger boost compared to single-source analysis, while domain-specific learning of dataset characteristics significantly improves performance, confirming that saliency prediction models surely benefit from embedding domain-specific layers from multiple datasets at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model size and runtime</head><p>From a computing resource perspective, Tab. 8 compares our model with state-of-the-art techniques, in terms of processing time and model size. Reference values for compared approaches are from <ref type="bibr" target="#b11">[12]</ref>.</p><p>UNISAL is the most resource-efficient approach, in both time and space (thanks to its MobileNetV2 <ref type="bibr" target="#b51">[52]</ref> backbone); our approach achieves average values on those metrics, while performing better than most in terms of prediction accuracy, as shown in the previous sections.   <ref type="bibr" target="#b20">[21]</ref> 117 0.500 DVA <ref type="bibr" target="#b67">[67]</ref> 96 0.100 SalGAN <ref type="bibr" target="#b46">[47]</ref> 130 0.020 ACLNet <ref type="bibr" target="#b63">[63]</ref> 250 0.020 SalEMA <ref type="bibr" target="#b36">[37]</ref> 364 0.010 STRA-Net <ref type="bibr" target="#b32">[33]</ref> 641 0.020 TASED-Net <ref type="bibr" target="#b43">[44]</ref> 82 0.060 UNISAL <ref type="bibr" target="#b11">[12]</ref> 16 0.009 HD 2 S 116 0.027 <ref type="table">Table 8</ref>: Size (in MB) and processing time (in seconds) for the proposed model and state-of-the-art approaches. Best values in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Qualitative analysis</head><p>We here report quantitative analysis of the results obtained by our model. <ref type="figure">Fig. 7</ref> shows examples of saliency predictions made by our HD 2 S model with domainspecific learning on the DHF1K benchmark. The model is able to effectively face object occlusion, multiple objects, fast motion, strong camera motion, stationary objects, saliency shift, camera focus change, low-light condition. Sample videos of how our model works are also given in the GitHub page of the project. <ref type="figure">Fig. 8</ref>, instead, shows example of failures that typically happen in case of small global motion or small objects. These failures can be caused by the spatial resolution at which input images are scaled before being processed by the model (128?192). Indeed, in the first two cases of <ref type="figure">Fig. 8</ref> the models is unable to identify the correct salient region (located in a lateral region of the scene), and instead predicts a generic prior-driven center region. In the last case, the model fails to detect the movement of a golf ball towards the hole (a slow movement of a small object), and erroneously predicts as salient the upper-right region of the scene, where a man with a red shirt significantly stands out from the surroundings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we propose HD 2 S, a new fully-convolutional network for video saliency prediction. The key architectural elements of our proposed approach include a multi-branch decoder which acts at different feature abstraction layers to independently estimate conspicuity maps, which are then combined into the final prediction, and an unsupervised domain adaptation mechanism that enables our model to learn features that, at the same time, allow it to reach state-of-the-art performance on supervised saliency prediction, while generalizing to domains for which no annotations are provided at training time. Additionally, when employing domain-specific learning techniques, as introduced in <ref type="bibr" target="#b11">[12]</ref>, our model's performance on the supervised saliency prediction task further improves. Comparing our approach with state-of-the-art models, we find that our late-fusion mechanism of multilevel saliency features provides a significant boost to performance: our ablation studies show that the gradual integration of multiple abstraction levels positively affects prediction accuracy. This is also confirmed by analyzing the learned weights. Interestingly, the impact of each conspicuity map (and, therefore, of each abstraction level of learned features) seems to vary depending on the employed domain adaptation mechanism: highlevel features become predominant when domain-specific learning is applied (possibly due to the larger data distribution variability introduced by multi-source training, which causes shallower features to generalize less), while all conspicuity maps become similarly important when unsupervised domain adaptation is applied, which can be explained through the action of gradient reversal layers, which actively encourage features to become domain-independent and thus to be equally effective at multiple scales.</p><p>While the model performs well in several complex cases -e.g., in presence of multiple objects, occlusions  <ref type="figure">Fig. 7</ref>: Qualitative evaluation of the proposed model on the DHF1K validation set. Comparison of the saliency predicted by our model with the ground truth on some frames: (left block) saliency with multiple objects, (upperright block) saliency on an occluded object, (lower-right block) saliency on moving objects whose appearance changes rapidly among consecutive frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>GT OURS <ref type="figure">Fig. 8</ref>: Examples of failures. The model struggles with small objects and small motion: in the first two cases, the model missed the salient region and highlights a generic prior; in the third example, the model does not manage to identify the golf ball, focusing instead on a man in a red shirt, standing out from the surroundings. and appareance changes -there are certain conditions in which we find room for improvement. Most of them involve the presence of small objects and small motion, where the model fails to correctly locate areas of interest, and the prediction is dominated by the prior. These situations could benefit from working at higher resolution, given sufficient computing resources, or in a patch-based fashion, to the detriment of inference times. However, major failures seem to be related to the specific characteristics of datasets: Hollywood2 and UCF Sports, for instance, are annotated with task-driven gaze fixations, rather then free-view the scene. This, of course, negatively affects methods that instead attempt to predict bottom-up saliency. Improved dataset availability curation for video saliency prediction may be an enabling factor for the advancement in the field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Statistics of the training sets of DHF1K, Hollywood and UCF Sports.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Weights learned by the fusion layer when integrating the four conspicuity maps on DHF1K dataset: (left block) Full HD 2 S model, (middle block) HD 2 S model with domain adaptation, (right block) HD 2 S model with domain specific learning. For the HD 2 S model with domain adaptation, we use LEDOV as a target dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative interpretation of the contribution of hierarchical decoding used under different settings. (Top line) HD 2 S, (Middle line) HD 2 S with domain adaptation and (Bottom Line) HD 2 S with domain-specific learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of HD 2 S, with domain adaptation (HD 2 S DA ) and with domain-specific learning (HD 2 S DSL ), with other state-of-the-art methods on the DHF1K test set. Our variant with domain-specific learning outperforms all state-of-the-art methods on three out of five metrics (NSS, CC, AUC-J), while ranking second-best on SIM and s-AUC.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Hollywood2</cell></row><row><cell>Method</cell><cell>NSS</cell><cell>CC</cell><cell cols="2">SIM AUC-J s-AUC</cell></row><row><cell>SALICON</cell><cell cols="3">2.013 0.452 0.321</cell><cell>0.856</cell><cell>0.711</cell></row><row><cell>DVA</cell><cell cols="3">2.459 0.482 0.372</cell><cell>0.886</cell><cell>0.727</cell></row><row><cell>ACLNet</cell><cell cols="3">3.086 0.623 0.542</cell><cell>0.913</cell><cell>0.757</cell></row><row><cell>SalEMA</cell><cell cols="3">3.186 0.613 0.487</cell><cell>0.919</cell><cell>0.708</cell></row><row><cell>STRA-Net</cell><cell cols="3">3.478 0.662 0.536</cell><cell>0.923</cell><cell>0.774</cell></row><row><cell cols="4">TASED-Net 3.302 0.646 0.507</cell><cell>0.918</cell><cell>0.768</cell></row><row><cell>SalSAC</cell><cell cols="3">3.356 0.670 0.529</cell><cell>0.931</cell><cell>0.712</cell></row><row><cell>UNISAL</cell><cell cols="3">3.901 0.673 0.542</cell><cell>0.934</cell><cell>0.795</cell></row><row><cell>HD 2 S</cell><cell cols="3">3.426 0.668 0.558</cell><cell>0.927</cell><cell>0.797</cell></row><row><cell>HD 2 S DA</cell><cell cols="3">3.139 0.653 0.520</cell><cell>0.927</cell><cell>0.774</cell></row><row><cell>HD 2 S DSL</cell><cell cols="3">3.352 0.670 0.551</cell><cell>0.936</cell><cell>0.807</cell></row><row><cell></cell><cell></cell><cell cols="2">UCF Sports</cell></row><row><cell>Method</cell><cell>NSS</cell><cell>CC</cell><cell cols="2">SIM AUC-J s-AUC</cell></row><row><cell>SALICON</cell><cell cols="3">1.838 0.375 0.304</cell><cell>0.848</cell><cell>0.738</cell></row><row><cell>DVA</cell><cell cols="3">2.311 0.439 0.339</cell><cell>0.872</cell><cell>0.725</cell></row><row><cell>ACLNet</cell><cell cols="3">2.567 0.510 0.406</cell><cell>0.897</cell><cell>0.744</cell></row><row><cell>SalEMA</cell><cell cols="3">2.638 0.544 0.431</cell><cell>0.906</cell><cell>0.740</cell></row><row><cell>STRA-Net</cell><cell cols="3">3.018 0.593 0.479</cell><cell>0.910</cell><cell>0.751</cell></row><row><cell cols="4">TASED-Net 2.920 0.582 0.469</cell><cell>0.899</cell><cell>0.752</cell></row><row><cell>SalSAC</cell><cell cols="4">3.523 0.671 0.534 0.926</cell><cell>0.806</cell></row><row><cell>UNISAL</cell><cell cols="3">3.381 0.644 0.523</cell><cell>0.918</cell><cell>0.775</cell></row><row><cell>HD 2 S</cell><cell cols="3">3.001 0.594 0.493</cell><cell>0.913</cell><cell>0.773</cell></row><row><cell>HD 2 S DA</cell><cell cols="3">2.756 0.579 0.478</cell><cell>0.905</cell><cell>0.759</cell></row><row><cell>HD 2 S DSL</cell><cell cols="3">3.114 0.604 0.507</cell><cell>0.904</cell><cell>0.768</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison of HD 2 S and its variants (with do- main adaptation: HD 2 S DA ; with domain-specific learn- ing: HD 2 S DSL ) with other state-of-the-art methods on Hollywood2 and UCF Sports datasets. In bold the best results, in italic the second-best results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of various HD 2 S (without DA and DSL) configurations. The Consp-net4 configuration refers to the network in a simple encoderdecoder configuration, i.e., with Conspicuity-net 4 only. The full model includes all four conspicuity networks with multi-level loss on conspicuity and saliency maps, defined in Equation 5.</figDesc><table><row><cell></cell><cell>NSS</cell><cell>CC</cell><cell>SIM AUC-J s-AUC</cell></row><row><cell cols="4">Only Consp-net1 2.191 0.392 0.301</cell><cell>0.871</cell><cell>0.689</cell></row><row><cell cols="4">Only Consp-net2 2.605 0.461 0.359</cell><cell>0.893</cell><cell>0.690</cell></row><row><cell cols="4">Only Consp-net3 2.663 0.480 0.359</cell><cell>0.902</cell><cell>0.697</cell></row><row><cell cols="4">Only Consp-net4 2.602 0.468 0.355</cell><cell>0.902</cell><cell>0.697</cell></row><row><cell>Full model</cell><cell cols="3">2.806 0.489 0.403 0.904</cell><cell>0.705</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Analysis of domain-shift capabilities. Performance evaluation in the domain generalization (supervised training on source; testing on target) and domain adaptation (supervised training on source; unsupervised training and test on target) scenarios. Upper-bound performance is measured by the transfer learning scenario (supervised training on source and fine-tuning on target).</figDesc><table><row><cell cols="5">source: DHF1K -target: LEDOV</cell></row><row><cell>Test</cell><cell>Setting</cell><cell>NSS</cell><cell>CC</cell><cell>SIM AUC-J</cell></row><row><cell>UCF</cell><cell cols="4">Generaliz. Adaptation 2.584 0.555 0.452 0.900 2.494 0.536 0.442 0.889</cell></row><row><cell>Hollywood2</cell><cell cols="4">Generaliz. Adaptation 3.066 0.623 0.505 0.926 3.011 0.622 0.502 0.922</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Analysis of generalization capabilities. Domain adaptation performance, respectively, with source: DHF1K, target: LEDOV; test: UCF Sports and Hollywood2. Best results in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance evaluation on the multi-source and domain specific learning scenarios.</figDesc><table><row><cell>Model</cell><cell cols="2">Size (MB) Runtime (s)</cell></row><row><cell>Deep Net [48]</cell><cell>103</cell><cell>0.080</cell></row><row><cell>SALICON</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">An extensive review of domain adaptation approaches is out of the scope of this paper and can be found in<ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b62">62]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The DHF1K benchmark is available at https://mmcheng. net/videosal/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Erkut Erdem, and Aykut Erdem. Spatio-temporal saliency networks for dynamic saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagdas</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysun</forename><surname>Kocak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1688" to="1698" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recurrent mixture density network for spatiotemporal visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08199</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03581</idno>
		<title level="m">Cat2000: A large scale fixation dataset for boosting saliency research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What do different evaluation metrics tell us about saliency models? IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoya</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilke</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="740" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How is gaze influenced by image transformations? dataset and model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2287" to="2300" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Saliency-based spatiotemporal attention for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting human eye fixations via an lstm-based saliency attentive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5142" to="5154" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep multi-level network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3488" to="3493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting human eye fixations via an lstm-based saliency attentive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5142" to="5154" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unified image and video saliency modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Droste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Alison</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="419" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotional attention: A study of image sentiment and visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Shaojing Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7521" to="7531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Generative adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predictive saliency maps for surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faouzi</forename><forename type="middle">Alaya</forename><surname>Fahad Fazal Elahi Guraya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubing</forename><surname>Tremeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Ninth International Symposium on Distributed Computing and Applications to Business</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graphbased visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eml-net: An expandable multi-layer network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">103887</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepvs: A deep learning based video saliency prediction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zulin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="602" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Predicting video saliency with object-to-motion cnn and two-layer convolutional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zulin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06316</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Salicon: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengsheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanyong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilke</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bi-shifting auto-encoder for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contextual encoder-decoder network for visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kroner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Senden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Driessens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding low-and high-level contributions to fixation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Kummerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video saliency prediction using spatiotemporal residual attentive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1113" to="1126" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of view-invariant action representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1254" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast visual tracking using motion saliency in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1073</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crowd saliency detection via global similarity structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ven Jyn</forename><surname>Mei Kuan Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Seng</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3957" to="3962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Linardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Mohedano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Jose</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01869</idno>
		<title level="m">Simple vs complex temporal recurrences for video saliency prediction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crowd behavior understanding through siof feature analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanping</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Adu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 23rd International Conference on Automation and Computing (ICAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2929" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1408" to="1424" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1408" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tased-net: Temporallyaggregating spatial encoder-decoder network for video saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2394" to="2403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Static saliency vs. dynamic saliency: a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdi</forename><surname>Tam V Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="987" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cristian Canton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01081</idno>
		<title level="m">Salgan: Visual saliency prediction with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel E O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Tracking algorithm using background-foreground motion models and multiple cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Shaohua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>surveillance video applications</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">233</biblScope>
			<date type="published" when="2005" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Salient object detection in video using deep non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahad</forename><surname>Harati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimya</forename><surname>Taba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">102769</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Action recognition in realistic sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir R Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision in sports</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="181" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sg-fcn: A motion and memory-based deep learning model for video saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2900" to="2911" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spotting and aggregating salient regions for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1519" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Revisiting video saliency: A largescale benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4894" to="4903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Revisiting video saliency prediction in the deep learning era. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="220" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen Wenguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Salsac: A video saliency prediction model with shuffled attentions and correlation-based convlstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12410" to="12417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A spatiotemporal saliency model for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yubing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Faouzi Alaya Cheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fazal Elahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Guraya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Konik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tr?meau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="241" to="263" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep unsupervised saliency detection: A multiple noisy labeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9029" to="9038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
