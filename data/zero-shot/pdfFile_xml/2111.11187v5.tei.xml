<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointMixer: MLP-Mixer for Point Cloud Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Choe</surname></persName>
							<email>jaesung.choe@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunghyun</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
							<email>frameau@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
							<email>jaesik.park@postech.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<email>iskweon77@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PointMixer: MLP-Mixer for Point Cloud Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/LifeBeyondExpectations/ECCV22-PointMixer</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MLP-Mixer has newly appeared as a new challenger against the realm of CNNs and Transformer. Despite its simplicity compared to Transformer, the concept of channel-mixing MLPs and token-mixing MLPs achieves noticeable performance in image recognition tasks. Unlike images, point clouds are inherently sparse, unordered and irregular, which limits the direct use of MLP-Mixer for point cloud understanding. To overcome these limitations, we propose PointMixer, a universal point set operator that facilitates information sharing among unstructured 3D point cloud. By simply replacing token-mixing MLPs with Softmax function, PointMixer can "mix" features within/between point sets. By doing so, PointMixer can be broadly used for intra-set, inter-set, and hierarchical-set mixing. We demonstrate that various channel-wise feature aggregation in numerous point sets is better than self-attention layers or dense token-wise interaction in a view of parameter efficiency and accuracy. Extensive experiments show the competitive or superior performance of PointMixer in semantic segmentation, classification, and reconstruction against Transformer-based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D scanning devices, such as LiDAR or RGB-D sensors, are widely used to capture a scene as 3D point clouds. Unlike images, point clouds are inherently sparse, unordered, and irregular. These properties make standard neural network architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref> hardly applicable. To tackle these challenges, there have been numerous ad hoc solutions, such as sparse convolution networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, graph neural networks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b5">6]</ref>, and point convolution networks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b82">83]</ref>. Despite their structural differences, these techniques have all been designed to extract meaningful feature representation from point clouds <ref type="bibr" target="#b17">[18]</ref>. Among existing solutions, Transformer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b53">54]</ref> appears to be particularly beneficial to extract features from point clouds. Indeed, the self-attention layer that encompasses the dense token-wise relations is specifically relevant in the context of processing irregular and unordered 3D points. ? Both authors have equally contributed to this work. arXiv:2111.11187v5 [cs.CV] 20 Jul 2022</p><p>Beyond the well established realm of CNNs and Transformer, MLP-Mixer <ref type="bibr" target="#b67">[68]</ref> proposes a new architecture that exclusively uses MLPs. Based on the pioneering study <ref type="bibr" target="#b67">[68]</ref>, concurrent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b88">89]</ref> address the locality issue of MLPs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b88">89]</ref> and discuss the necessity of self-attention <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b91">92]</ref>. More recently, Yu et al . <ref type="bibr" target="#b89">[90]</ref> claim that the general architecture formulation is more important than the specific token-wise interaction strategies, such as selfattention and token-mixing MLPs. Despite an increasing interest, the MLP-like architectures for point clouds have not yet been fully explored.  <ref type="figure">Fig. 1</ref>. We present a MLP-like architecture for various point cloud processing, which considers numerous point sets with larger receptive fields to "mix" information.</p><p>In this paper, we introduce the PointMixer that newly extends the philosophy of MLP-like architectures to point cloud analysis. Specifically, we demonstrate the dense token-wise interaction are not essential factors in this context. Instead of using token-mixing MLPs, we extend the usage of channel-mixing MLPs into numerous point sets. As illustrated in <ref type="figure">Fig. 1</ref> and <ref type="table">Table 1</ref>, PointMixer layer shares and mixes point features <ref type="bibr" target="#b0">(1)</ref> within grouped points (intra-set), <ref type="bibr" target="#b1">(2)</ref> between point sets (inter-set), or (3) points in different hierarchical sets. In particular, we newly introduce the concepts of the inter-set mixing and the hierarchical-set mixing, which is clearly different from previous studies <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b95">96]</ref> that only focus on the intra-set mixing. To this end, PointMixer layer is a universal point set operator that can propagate point responses into various point sets. We claim that various channel MLPs on numerous point sets can outperform self-attention layers or token-mixing MLPs for point clouds. Moreover, the PointMixer network is a general architecture that has symmetric encoder-decoder blocks fully equipped with PointMixer layers.</p><p>We conduct extensive experiments on various 3D tasks. These large-scale experiments demonstrate that our method achieves compelling performance among Transformer-based studies <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b95">96]</ref>. Our contributions are summarized as below:</p><p>? PointMixer as a universal point set operator that facilitates mixing of point features through various point sets: intra-set, inter-set, and hierarchical-set. ? Highly accurate and parameter-efficient block design that purely consists of channel-mixing MLPs without token-mixing MLPs. ? Symmetric encoder-decoder network to propagate hierarchical point responses through PointMixer layer, instead of trilinear interpolation. ? Extensive experiments in various 3D point cloud tasks that highlight the efficacy of PointMixer network against recent Transformer-based studies. <ref type="table">Table 1</ref>. Technical comparisons. Locality represents the local feature aggregation among sampled points. We split the function of the set operator as "intra"-set mix, "inter"-set mix, and "hierarchical"-set mix. Also, we present the symmetric property of encoder-decoder architecture of related work <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b95">96]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Locality</head><p>Set operator Symmetric Token-mix Intra Inter Hier pyramid arch PointNet <ref type="bibr" target="#b54">[55]</ref> Pooling PointNet++ <ref type="bibr" target="#b55">[56]</ref> Pooling PointTrans <ref type="bibr" target="#b95">[96]</ref> Self-attn PointMLP <ref type="bibr" target="#b46">[47]</ref> Affine PointMixer (ours) Softmax</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we revisit previous approaches for point cloud understanding, and then briefly introduce Transformers and MLP-like architectures.</p><p>Deep learning on point clouds. Point clouds are naturally sparse, unordered, and irregular, which makes it difficult to design a deep neural network for point cloud understanding. To handle such complex data structures, two distinct philosophies have been investigated: voxel-based and point-based methods.</p><p>Voxel-based methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b97">98]</ref> first quantize an irregular point cloud into the regular voxel grids, which makes it efficient to search neighbor voxels. However, the voxelization process inevitably loses the geometric details of the original point cloud. This issue often leads to infer inaccurate predictions though several recent methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b92">93]</ref> try to alleviate the quantization artifacts. Therefore, point-based methods have been actively studied.</p><p>PointNet <ref type="bibr" target="#b54">[55]</ref> is a pioneering paper that processes an unordered and irregular points in deep neural architectures. Based on this seminal work, PointNet++ <ref type="bibr" target="#b55">[56]</ref> presents the ways of involving feature hierarchy as well as points' locality. In details, this paper adopts k-Nearest Neighbor (kNN) for local neighborhood sampling and the Farthest Point Sampling algorithm (FPS) for feature hierarchy (e.g., transition downsampling). This pyramid encoder-decoder network largely influences on the MLP-based methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b84">85]</ref>, point convolution studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b82">83]</ref>, and graph-based networks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b83">84]</ref>. However, as stated in <ref type="table">Table 1</ref>, PointNet++ is limited to capture local point responses within the scope of intra-set. Moreover, we found that this vanilla architecture is asymmetric between transition down layers and transition up layers. While downsampling layers adopt pooling with kNN and FPS, upsampling layers re-compute kNN and trilinear interpolation, which brings asymmetric feature propagation in the encoder-decoder architecture. More recently, Transformerbased study for point cloud processing <ref type="bibr" target="#b95">[96]</ref> still suffers from the same issue in the pyramid architecture design.</p><p>Our work unifies a local feature aggregation layer, a downsampling layer, and an upsampling layer into an universal set operator, named PointMixer layer. This novel layer brings the symmetric and learnable down/upsampling architecture for various 3D perception tasks such as object shape classification <ref type="bibr" target="#b79">[80]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2]</ref> and point cloud reconstruction tasks <ref type="bibr" target="#b6">[7]</ref>.</p><p>Transformers and MLP-like architectures. Transformer-based architecture has recently become a game changer in both natural language processing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b72">73]</ref> and computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b90">91]</ref>.</p><p>Vision Transformer (ViT) <ref type="bibr" target="#b11">[12]</ref> has opened the applicability of Transformers on visual recognition tasks. Because of the quadratic runtime of the self-attention layers in Transformers, ViT adopts tokenized inputs that divide the image into small region of patches <ref type="bibr" target="#b70">[71]</ref>. The idea of patch embeddings is widely used in the following studies that focus on various issues in ViT <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b65">66]</ref>: locality <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref> and hierarchy <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b3">4]</ref>. With those self-attention layers <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b93">94]</ref>, Transformerbased point cloud studies <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51]</ref> demonstrate accurate predictions in both 3D shape classification <ref type="bibr" target="#b79">[80]</ref> and semantic segmentation <ref type="bibr" target="#b1">[2]</ref>. However, a generalpurpose layer for both local feature learning and down/upsampling has drawn little attention in handling 3D points.</p><p>Recently, there exist trials to go beyond the hegemony of CNNs and Transformer by introducing MLP-like architectures. The pioneering paper, MLP-Mixer <ref type="bibr" target="#b67">[68]</ref>, presents a MLP-like network constituted of token-mixing MLPs and channel-mixing MLPs for image classification task. Especially in computer vision, this MLP-like architectures appear to be a new paradigm with their simple formulation and superior performance given large-scale training data. Subsequent papers raise issues and develop potentials in MLP-like architectures: <ref type="bibr" target="#b0">(1)</ref> can MLPs handle position-sensitive information or locality <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref>? and (2) does self-attention is truly needed <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b91">92]</ref>? Though these issues are still controversial, more recent paper, Metaformer <ref type="bibr" target="#b89">[90]</ref>, addresses the importance of general architecture formulation instead of the specific dense tokenwise interaction strategies such as self-attention layers <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b11">12]</ref> or token-mixing MLPs <ref type="bibr" target="#b67">[68]</ref>. Simply, by replacing complicated token-mixing operators with the average pooling layer, MetaFormer achieves remarkable performance against the recent MLP-based and Transformer-based studies.</p><p>Despite their success, modern MLP-like approaches have not yet been fully exploited to point clouds. In contrast to recent Transformer-based point cloud studies <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b53">54]</ref>, our PointMixer network is a general-purpose architecture that symmetrically upsamples/downsamples points' responses and truly exploits the strength of MLPs to operate mixing within/beyond sets of points. By doing so, we successfully conduct various tasks, 3D semantic segmentation, point cloud reconstruction, and object classification tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b79">80</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe the details of our PointMixer design. For the sake of clarity, we compare the general formulation of MLP-Mixer with representative point-based approaches, such as PointNet++ and Point Transformer (Sec. 3.1). Then, we examine whether MLP-Mixer is of relevance to a point set operator (Sec. 3.2). Finally, we introduce our PointMixer layer (Sec. 3.3) that is adopted in our entire network (Sec. 3.4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>Let's assume a point cloud P = {p i } N i=1 and its corresponding point</p><formula xml:id="formula_0">features X = {x i } N i=1 where p i ? R 3 is the position of the i-th point and x i ? R C is its corresponding feature. The objective is to learn a function f : X ? Y to produce the output point features Y = {y i } N i=1</formula><p>. Instead of processing the entire point cloud directly, most approaches treat data locally based on points' proximity. For this purpose, k-Nearest Neighbor (kNN) <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b96">97]</ref> is widely used to get an index map of neighbor points, which is denoted as:</p><formula xml:id="formula_1">M i = kNN(P, p i ),<label>(1)</label></formula><p>where M i is an index map of the K closest points for a query point p i ? R 3 . In other words, given a query index i, the index map M i is defined as a set of K nearest neighbor indices (K = |M i |). Accordingly, kNN can be understood as a directional graph that links each query point p i to its K closest neighbor points P i = {p j ? P|j ? M i } and the corresponding features X i = {x j ? X |j ? M i }. PointNet++ <ref type="bibr" target="#b55">[56]</ref> addresses the problem of PointNet <ref type="bibr" target="#b54">[55]</ref> that has difficulty in capturing local responses. To cope with this problem, it utilizes kNN and Farthest Point Sampling algorithm and builds asymmetric encoder-decoder network. Instead of dealing with the entire point cloud directly, PointNet++ aggregates set-level responses locally as follows:</p><formula xml:id="formula_2">y i = maxpool j?Mi MLP [x j ; p i ? p j ] ,<label>(2)</label></formula><p>where y i is the output feature vector for the i-th point and [; ] denotes vector concatenation. By adopting this grouping and sampling strategy, MLPs can capture local responses from unordered 3D points. Point Transformer <ref type="bibr" target="#b95">[96]</ref> adopts vector subtraction attention as a similarity measurement for token-wise communication. Following PointNet++, Point Transformer also uses kNN to compute local responses. Given a query point feature x i and its neighbor feature set X i , Point Transformer operates self-attention layers to densely relate token interaction as:</p><formula xml:id="formula_3">y i = j?Mi softmax ? W 1 x i ? W 2 x j + ?(p i ? p j ) W 3 x j + ?(p i ? p j ) ,<label>(3)</label></formula><p>where W indicates a linear transformation matrix, ?(?) denotes an MLPs to calculate vector similarities, ?(p i ? p j ) is a positional encoding vector to embed local structures of 3D points, and p i ?p j is the relative distance between a query point p i and its neighbor point p j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MLP-Mixer as Point Set Operator</head><p>MLP-Mixer <ref type="bibr" target="#b67">[68]</ref> has achieved remarkable success by only using MLPs for image classification. However, when dealing with sparse and unordered points, the direct application of the MLP-Mixer network is restricted. Let us revisit MLP-Mixer to ease the understanding of our PointMixer. The MLP-Mixer layer 3 consists of token-mixing MLPs and channel-mixing MLPs. MLP-Mixer takes K tokens having C-dimensional features, denoted as X ? R K?C , where tokens are features from image patches. It begins with tokenmixing MLPs that transposes the spatial axis and channel axis to mix spatial information. Then, it continues with channel-mixing MLPs so that input tokens are mixed in spatial and channel dimensions.</p><formula xml:id="formula_4">X = X + (W 2 ?(W 1 (LayerNorm(X)) )) ,<label>(4)</label></formula><formula xml:id="formula_5">Y = X + W 4 ?(W 3 LayerNorm(X )),<label>(5)</label></formula><p>where W is the weight matrix of a linear function and ? is GELU <ref type="bibr" target="#b21">[22]</ref>. By Eq. (4), token-mixing MLPs are sensitive to the order of the input tokens, which is permutation-variant property. Due to its property, positional encoding is not required in the vanilla MLP-Mixer as stated in the paper <ref type="bibr" target="#b67">[68]</ref>. However, this property is not desirable for processing irregular and unordered point clouds, which is different characteristics of uniform and ordered pixels in the image. To cope with this issue, previous point-based layers <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96]</ref> are independent to orders of input point, i.e., permutation-invariance 4 . Moreover, as a point set operator, vanilla MLP-Mixer layer only computes intra-set relations as PointNet++ and Point Transformer do. From this analysis, we observe room for improvement in point cloud understanding. We propose PointMixer layer that is permutationinvariant and can also be used for a learnable upsampling in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PointMixer: Universal Point Set Operator</head><p>We introduce an approach to embed geometric relations between points' features into the MLP-Mixer's framework. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the PointMixer layer takes a point set P i = {p j } and its associated point features set X i = {x j } as inputs in order to compute the output feature vector y i . For a point p i , we first compute a score vector s = [s 1 , ..., s K ] to aggregate X i as follows:</p><formula xml:id="formula_6">s j = g 2 g 1 (x j ); ?(p i ? p j ) where ?j ? M i ,<label>(6)</label></formula><p>where g(?) is the channel-mixing MLPs, ?(?) is the positional encoding MLPs, and x j is a j-th element of the feature vector set X i . Note that we follow the relative positional encoding scheme <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96]</ref> to deal with unstructured 3D points. As a result, we obtain the score vector s ? R K . Finally, the features of the K adjacent points are gathered to produce a new feature vector y i as:</p><formula xml:id="formula_7">y i = j?Mi softmax(s j ) g 3 (x j ),<label>(7)</label></formula><p>where softmax(?) is the Softmax function that normalizes the spatial dimension, and indicates an element-wise multiplication. Note that this symmetric operation (SOP) in the proposed PointMixer is different from both the average pooling in MLP-Mixer and the max pooling in PointNet++, as described in <ref type="table">Table 1</ref>.</p><p>As a set operator, PointMixer layer has different characteristics compared to both MLP-Mixer layer <ref type="bibr" target="#b67">[68]</ref>. First, PointMixer layer sees relative positional information ?(p i ? p j ) to encode the local structure of a point set. Second, the vanilla MLP-Mixer layer does not have the Softmax function. Last, PointMixer layer does not have token-mixing MLPs for scalability to arbitrary number of neighbor points and for permutation-invariance to deal with unordered points. Let us explain the reasons behind these differences.</p><p>No token-mixing MLPs. There are two reasons that we do not put tokenmixing MLPs into PointMixer layer. First, token-mixing MLPs are permutationvariant, which makes it incompatible with unordered point clouds. As stated in Point Transformer <ref type="bibr" target="#b95">[96]</ref>, permutation-invariant property is a necessary condition, also for PointMixer layer. Second, while a given pixel in an image systematically admits 8 adjacent pixels, each 3D point does not have pre-defined number of neighbors, which is determined by the clustering algorithms, such as kNN <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b95">96]</ref>, radius neighborhood <ref type="bibr" target="#b66">[67]</ref>, or hash table <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>. Since token-mixing MLPs can only take fixed number of input points 5 , token-mixing MLPs are not suitable for handling various cardinality of point sets.</p><p>Inspired by Synthesizer <ref type="bibr" target="#b64">[65]</ref>, we alleviate this problem by replacing the tokenmixing MLPs with the Softmax function. We conjecture that the Softmax function weakly binds token-wise information in a non-parametric manner (i.e., normalization). By doing so, PointMixer layer can calculate arbitrary cardinality of point sets and have a permutation-invariant property. As described in <ref type="figure" target="#fig_2">Fig. 3</ref> and in <ref type="table">Table 1</ref>, PointMixer layer can be used as a universal point operator for mixing various types of point sets: intra-set, intra-set, and hierarchical-set. Intra-set mixing. Given a point set P i and its corresponding feature set X i , intra-set mixing aims to compute point-wise interaction within each set. Usually, kNN is widely used to cluster neighbor points into groups. For example in <ref type="figure" target="#fig_2">Fig. 3</ref>-(a), we apply kNN on each query point where K=3. As a result, a red point ( ) has three neighbor points ( ). Based on the index map M i from kNN, the PointMixer layer updates a query feature x i using its neighbor feature set X i , as in Eq. (6) and Eq. <ref type="bibr" target="#b6">(7)</ref>. While intra-set mixing is proven to be useful in various methods <ref type="bibr" target="#b17">[18]</ref>, the receptive field is bounded within a set, as depicted in <ref type="figure" target="#fig_2">Fig. 3-(a)</ref>. To overcome this restriction, we propose the inter-set mixing operation.  Inter-set mixing. This is a new concept of spreading point features between different sets. Using the index mapping M i , we can trace back to find another set P j that includes a query point p i as their neighbors. This process can be viewed as the inverse version of kNN, and we define the inverse mapping of M i as M ?1 i = {j|i ? M j }. For example in <ref type="figure" target="#fig_2">Fig. 3</ref>-(b), given index mapping M , we compute inverse index mapping M ?1 . Then, we can find the two adjacent sets whose query points are black points ( ). It implies the red point is included in two adjacent sets, as drawn in <ref type="figure" target="#fig_2">Fig. 3</ref>-(b) <ref type="bibr" target="#b5">6</ref> . In shorts, inverse mapping M ?1 i finds the set index j that includes a point p i . By doing so, inter-set mixing can aggregate point responses between neighbor sets P j into the query point p i . Hierarchical-set mixing. The PointMixer layer is universally applicable for the transition down/up layers as shown in <ref type="figure" target="#fig_2">Fig. 3</ref></p><formula xml:id="formula_8">-(c). For instance, let's prepare a point set P s = {p j } N j=1 that is sampled from original point set P o = {p i } N i=1 (P s ? P o )</formula><p>. Using a point p j ? P s , we calculate its neighbors from P o and obtain index mapping M o?s j . By putting M o?s j in Eq. (6) and Eq. <ref type="formula" target="#formula_7">(7)</ref>, we readily pass the feature from P o to P s (i.e., point downsampling), which is computed as:</p><formula xml:id="formula_9">M o?s j = kNN(P o , p j ) where ?p j ? P s .<label>(8)</label></formula><p>For point upsampling, we notice that conventional U-Net in both PointNet++ and Point Transformer is not symmetric in terms of downsampling and upsampling. This is because the spatial grouping is performed asymmetrically as visualized in <ref type="figure" target="#fig_3">Fig. 4</ref>. In details, conventional approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b94">95]</ref> build another kNN map M s?o i for the upsampling as below:</p><formula xml:id="formula_10">M s?o i = kNN(P s , p i ) where ?p i ? P o .<label>(9)</label></formula><p>Transition up Transition down However, this is not symmetric because nearest neighbor is not a symmetric function: even if p i 's nearest neighbor is p j , p j 's nearest neighbor may not p i . Instead of creating a new index map M s?o i , our PointMixer layer can use M o?s j ?1 for upsampling ( <ref type="figure" target="#fig_2">Fig. 3-(b)</ref>) by re-using the original index mapping from the downsampling M o?s j . See <ref type="figure" target="#fig_3">Fig. 4</ref> for the symmetric upsampling. The benefit of this approach is that it does not introduce additional k-Nearest Neighbor search. Furthermore, we can propagate point responses in different hierarchy based on their scores computed by our PointMixer, instead of using trilinear interpolation <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96]</ref>. These technical differences results in higher performance than that of asymmetric design on dense prediction tasks (see Sec. 4.4).</p><p>As illustrated in <ref type="figure" target="#fig_4">Fig. 5-(b)</ref> of the transition down block, we use Farthest Point Sampling algorithm to produce P s from P o . Then, we utilize kNN to sample P s from P o . The resulting point locations are used to calculate the relative distance p i ? p j . In the transition up block, we keep using the index map M o?s j calculated in the transition down block. To this end, we apply the PointMixer layer in transition up/down while maintaining the symmetric relation between the sampled point cloud P s and the original points P o . We empirically prove that this symmetric upsampling layer helps the network to predict dense point-level representations accurately in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture</head><p>In this section, we describe the details of our MLP-like encoder-decoder architecture as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Our network is composed of several MLP blocks, such as the transition down blocks, transition up blocks, and Mixer blocks. For a fair comparison, our network mainly follows the proposed hyper-parameters in Point Transformer <ref type="bibr" target="#b95">[96]</ref> for network composition. Overall, our network takes a deep pyramid-style network that progressively downsamples points to obtain global features. For dense prediction tasks, such as semantic segmentation or point reconstruction, we include upsampling blocks for per-point estimation. Finally, our header block is designed for task-specific solutions. For classification, we take fully-connected layers, dropout layers, and global pooling layer. For semantic segmentation and point reconstruction, the header block consists of MLPs without pooling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLPs</head><p>Mixer (a) PointMixer network for the dense prediction tasks (top) and the classification task (down).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"airplane"</head><p>(b) Blocks design using the PointMixer layer (?) as intra-set, inter-set, and hierarchical-set mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NN</head><p>Intra-set mix Inter-set mix SOP ( , ) ( , )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixer block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample &amp; NN</head><p>Hier-set mix</p><formula xml:id="formula_11">( ! , ! )</formula><p>Transition down block</p><formula xml:id="formula_12">( ! , ! )</formula><p>Transition up block</p><formula xml:id="formula_13">( " , " )</formula><p>Hier-set mix </p><formula xml:id="formula_14">( " , " ) Linear ( " , " ) ( ! , ! ) Sample ? "?! from trans. down</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the efficacy and versatility of the proposed Point-Mixer for various point cloud understanding tasks: semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, point reconstruction <ref type="bibr" target="#b6">[7]</ref>, and object classification <ref type="bibr" target="#b79">[80]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Segmentation</head><p>To validate the effectiveness of PointMixer for semantic segmentation, we propose an evaluation on the large-scale point cloud dataset S3DIS <ref type="bibr" target="#b1">[2]</ref> consisting of 271 room reconstructions. Each 3D point of this dataset is assigned to one label among 13 semantic categories. For the sake of fairness, we meticulously follow the widely used evaluation protocol proposed by Point Transformer <ref type="bibr" target="#b95">[96]</ref>. For training, we set the batch size as 4 and use the SGD optimizer with momentum and weight decay set to 0.9 and 0.0001 respectively. For evaluation, we use the class-wise mean Intersection of Union (mIoU), class-wise mean accuracy (mAcc), and overall point-wise accuracy (OA).</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure">Fig. 6</ref>, PointMixer achieves the state-of-the-art performance in S3DIS [2] Area 5, though PointMixer network consumes less parameters (6.5M) than that of Point Transformer (7.8M). Even in class-wise IoU, PointMixer network outperforms Point Transformer <ref type="bibr" target="#b95">[96]</ref>, except for a few classes. While various studies <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b16">17]</ref> underline the necessity of dense pointwise interaction (i.e., self-attention layer), PointMixer successfully outperforms these approaches purely using Channel MLPs. These results consistently support that dense token communication is not an essential factor as stated in Synthesizer <ref type="bibr" target="#b64">[65]</ref> and Metaformer <ref type="bibr" target="#b89">[90]</ref>. We claim that it is much more crucial to mix information through various point sets. Moreover, the experimental result shows that our symmetric upsampling layer is more effective for semantic segmentation than heuristic sampling-based asymmetric upsampling layer which all of previous approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b95">96]</ref> have used. We further discuss the effectiveness of our hierarchical-set mixing layer with ablation studies in Sec. 4.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Point Cloud Reconstruction</head><p>To highlight the versatility of our approach, we propose a large-scale assessment for the newly introduced task of point cloud reconstruction <ref type="bibr" target="#b6">[7]</ref> where an inaccurate point cloud is jointly denoised, densified, and completed. This experiment is also particularly interesting to evaluate the generalization capabilities of networks since it allows us to test methods in unmet environments. Specifically, we train on the synthetic objects of ShapeNetPart <ref type="bibr" target="#b86">[87]</ref> and evaluate the reconstruction accuracy on on unmet indoor scenes from ScanNet <ref type="bibr" target="#b8">[9]</ref> (real reconstruction) and ICL-NUIM <ref type="bibr" target="#b19">[20]</ref> (synthetic data). Note that in <ref type="bibr" target="#b6">[7]</ref>, the point reconstruction is performed in two stages: 1) point upsampling via a sparse hourglass network, 2) denoising and refinement via Transformer network. For this evaluation, we propose to replace the second stage of this pipeline with various architectures (i.e., PointMixer network and previous studies <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b95">96]</ref>) to compare their performances. Under the same data augmentation and data pre-processing as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, we train PointMixer and previous studies <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b95">96]</ref>. For evaluation, we utilize the Chamfer distance (CD) to measure the distance between the predictions and the ground truth point clouds. Additionally, we use the accuracy (Acc.), completeness (Cp.), and F1 score to measure the performance in occupancy aspects <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b31">32]</ref>. Though our network is solely trained in a synthetic/object dataset, our network can generalize towards unmet scenes including real-world 3D scans <ref type="bibr" target="#b8">[9]</ref> and synthetic/room-scale point clouds <ref type="bibr" target="#b19">[20]</ref> as in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure">Fig. 7</ref>. Moreover, our method compares favorably to previous Transformer-based <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b6">7]</ref> and MLP-based <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> studies. In particular, the performance gap between ours and previous studies become larger in the ScanNet <ref type="bibr" target="#b8">[9]</ref> and ICL-NUIM dataset <ref type="bibr" target="#b19">[20]</ref>, which indicates better generalization performance.  The ModelNet40 <ref type="bibr" target="#b79">[80]</ref> dataset has 12K CAD models with 40 object categories. We follow the official train/test splits to train and evaluate ours and the previous studies. For fair comparison, we follow the data augmentation and pre-processing as proposed in Point-Net++ <ref type="bibr" target="#b55">[56]</ref>, which is also adopted in the recent studies <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b82">83]</ref>. For evaluation, we adopt the mean accuracy within each category (mAcc), and the overall accuracy over all classes (OA) with the same evaluation protocol with previous approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b82">83]</ref>. The results presented in <ref type="table" target="#tab_4">Table 4</ref> show that our approach outperforms the recently proposed techniques. Especially, PointMixer achieves the highest mAcc with outperforming other methods by a large margin as 0.8 mAcc, using 1024 points without normals. Moreover, our network shows this competitive performance with less parameters (3.9M) against previous studies (Point Transformer 7 5.3M, KPConv 15.2M). Based on these comparisons with Point Transformer and related work, we conclude that PointMixer network effectively and efficiently aggregates various point responses through intra-set mixing, inter-set mixing, and downsampling layers, even for shape classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Shape Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We conduct an extensive ablation study about the proposed PointMixer in semantic segmentation on the S3DIS dataset <ref type="bibr" target="#b1">[2]</ref> and in point cloud reconstruction on the ShapeNet-Part dataset <ref type="bibr" target="#b86">[87]</ref>. First, we proceed the case study of our Point-Mixer <ref type="table" target="#tab_5">(Table 5</ref>, top-right one). Second, we analyze the influence of positional encoding on PointMixer ( . This ablation study aims to validate the inverse mapping as well as a functionality of PointMixer layer. As in <ref type="table" target="#tab_5">Table 5</ref>, especially, when we combine the usage of hierarchical-set mixing and inter-set mixing, the synergy brings large performance improvement in both two tasks, 2.2 mIoU in semantic segmentation and 0.02 Chamfer distance in point reconstruction. It implies that the various ways of sharing points' responses are beneficial for point cloud understanding. Unnecessary token-mixing MLPs. We validate our claims about the role of the Softmax function, i.e., weakly binding tokens. For this purpose, we replace the Softmax functions with token-mixing MLPs, as proposed in the vanilla MLP-Mixers. <ref type="table" target="#tab_5">Table 5</ref> demonstrates that even without explicit use of tokenmixing MLPs, our PointMixer successfully achieves similar accuracy in semantic segmentation and point reconstruction. Moreover, it takes 3 days for training the PointMixer with token-mixing MLPs while our original PointMixer requires a day (twice faster) with less parameter consumption (18% less). These results are consistently support to Metaformer <ref type="bibr" target="#b89">[90]</ref> and Synthesizer <ref type="bibr" target="#b64">[65]</ref> in that dense token-wise interaction is not an essential choice. Positional encoding. MLP-Mixer does not use positional encoding since token-mixing MLPs are sensitive to the order of tokens. However, relative position information is an important factor to handle unstructured 3D points. Also, our PointMixer layer is free from token-mixing MLPs to obtain permutationinvariant property to function as a universal point set operator. Without any modification on the layer except positional encoding layers ?(p i ? p j ), we experiment the effectiveness of positional encoding in all of these usages. As in <ref type="table" target="#tab_5">Table 5</ref>, there is large performance drop when we intentionally omit positional encodings in our PointMixer layer. Different from MLP-Mixer, we claim the relative positional encoding is a necessary condition to deal with 3D points. Symmetric architecture. When we think of the convolutional neural networks for image recognition, it is natural to design symmetric encoder network and decoder network. For instance in the previous paper <ref type="bibr" target="#b52">[53]</ref>, this paper verify the importance of symmetric transposed convolution layer design for semantic segmentation task. In contrast, point-based studies <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b94">95]</ref> dominantly rely on the asymmetric PointNet++ architecture, which is similar to FCN <ref type="bibr" target="#b45">[46]</ref>.</p><p>The top-right sub-table in <ref type="table" target="#tab_5">Table 5</ref> supports our claim. When we do not use hierarchical-set mixing, the architecture become asymmetric, and transition down and up layers are identical to that in PointNet++ <ref type="bibr" target="#b55">[56]</ref> and Point Transformer <ref type="bibr" target="#b95">[96]</ref>. It turns out that the asymmetric architecture (the second row) degrades the performance of the network (the last row) in terms of both mIoU and Chamfer distance by 2.3 absolute percentage and 0.01, respectively. Moreover, when we apply inter-set mixing, the performance gap further increases in both semantic segmentation and point reconstruction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a MLP-like architecture for point cloud understanding, which focuses on sharing point responses in numerous and diverse point sets through a universal point set operator, PointMixer. Regardless of the cardinality of a point set, PointMixer layer can "mix" point responses in intra-set, inter-set, and hierarchical-set. Moreover, we present a point-based general architecture that involves symmetric encoder-decoder blocks for propagating information through hierarchical point sets. Extensive experiments validate the efficacy of our PointMixer network with superior or compelling performance compared to Transformer-based studies. Through out this paper, we claim that dense tokenwise calculation, such as self-attention layers or token-mixing MLPs, is not an essential choice for point cloud processing. Instead, we emphasize the importance of information sharing toward various point sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This is a supplementary material for the paper, PointMixer: MLP-Mixer for Point Cloud Understanding. We will further describe the details: efficiency analysis (Sec. A), point receptive fields comparison (Sec. B), limited cardinality issues in token-mixing MLPs (Sec. C), and task-specific training details (Sec. D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Efficiency analysis</head><p>In this section, we analyze the latency and memory consumption of each point set layer: PointNet++ layer (Eq. (2)), Point Transformer layer (Eq. (3)) and Point-Mixer layer (Eq. (6) and Eq. <ref type="formula" target="#formula_7">(7)</ref>). We conduct this ablation study based on the PointMixer network for the 3D shape classification task. For a fair comparison, we strictly maintain to use the same downsampling layers. Furthermore, we do not use inter-set mixing layer to keep the other components of the network the same. We measure mAcc, OA, the average inference time per object, and the peak GPU memory usage of each method on ModelNet40 dataset <ref type="bibr" target="#b79">[80]</ref>. Note that we re-implement PointNet++ 8 and Point Transformer with the same number of residual blocks as our PointMixer, and train those models on the ModelNet40 dataset <ref type="bibr" target="#b79">[80]</ref> with the same training configuration for a fair comparison. As shown <ref type="table" target="#tab_7">Table 6</ref>, the network with Point Transformer layer <ref type="bibr" target="#b95">[96]</ref> consumes the largest amount of GPU memory to infer a 3D object since it calculates a memory-consuming vector similarity. On the other hand, our PointMixer layer computes a scalar score, denoted by s j , to aggregate neighbor features, denoted by x j . It consequently consumes 8MB less GPU memory than Point Transformer layer <ref type="bibr" target="#b95">[96]</ref> although both Point Transformer and Point Mixer layers are slower than PointNet++ layer since both of them use the expensive softmax operation. Furthermore, the PointMixer with only intra-set mixing outperforms PointNet++ <ref type="bibr" target="#b55">[56]</ref> layer by 0.3 mAcc and 0.1 OA although PointNet++ <ref type="bibr" target="#b55">[56]</ref> also requires much less memory than Point Transformer <ref type="bibr" target="#b95">[96]</ref>. This result implies that our score-based aggregation can embed local responses more effectively than simple pooling-based aggregation which PointNet++ <ref type="bibr" target="#b55">[56]</ref> uses. As a result, our PointMixer layer can encode local relations within a point set both more effectively and efficiently than previous approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96]</ref>, along with the other  strengths that inter-set mixing and hierarchical-mixing layers have, which are already shown in <ref type="table" target="#tab_2">Table 2</ref>, <ref type="table" target="#tab_3">Table 3</ref>, and <ref type="table" target="#tab_5">Table 5</ref> of the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Point receptive field analysis</head><p>Throughout this paper, we emphasize the importance of information sharing among unstructured point clouds. As a universal point set operator, PointMixer layer can function as intra-, inter-, and hierarchical-set mixing while previous studies <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b82">83]</ref> only focus on the intra-set mixing operations as shown in Table 1 of the manuscript. For your visual understanding, let us illustrate the receptive fields of our PointMixer layers in different usages as in <ref type="figure" target="#fig_5">Fig. 8</ref>. Given a query point ( ), we visualize the receptive fields of intra-set/inter-set mixing (top row) and upsampling layers (bottom row). In particular, we compare trilinear-based upsampling layers (PointNet++ and Point Transformer) and hierarchical-set mixing from our PointMixer layer. We colorize points as red if the red query point influences these. The PointMixer layer has overall larger receptive fields than previous studies <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96]</ref>, which further facilitates point response propagation. Top row in <ref type="figure" target="#fig_5">Fig. 8</ref>. As stated in Sec. 3.3 of the manuscript, intra-set mixing is limited to k closest neighbor points. However, combined use of intra-set and inter-set mixing can propagate point responses into variable length of more neighbors from the neighboring point sets M j , which is consistently supported by <ref type="figure" target="#fig_5">Fig. 8</ref>. Bottom row in <ref type="figure" target="#fig_5">Fig. 8</ref>. Our PointMixer network constitutes a symmetric encoder-decoder network while previous studies do not, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref> of the manuscript. In particular, since creating a new kNN graph with k = 16 is expensive, the trilinear interpolation in upsampling layers of PointNet++ <ref type="bibr" target="#b55">[56]</ref> and Point Transformer <ref type="bibr" target="#b95">[96]</ref> usually interpolates three nearest neighbor points, which is the much smaller number of neighbors than 16 in the downsampling layer, and limits their receptive fields as well. On the other hand, our PointMixer reuses the kNN graph of the downsampling layer to maximize the receptive fields without additional computational costs. As a result, PointMixer layer can encode point responses in larger contexts than previous approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b95">96]</ref> as shown in <ref type="figure" target="#fig_5">Fig. 8</ref>. Moreover, these results can be reasons for our superior performance in dense prediction tasks compared to the previous state-of-the-art methods <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limited cardinality issues in token-mixing MLPs</head><p>There are two dominant reasons that we remove token-mixing MLPs from our PointMixer layer: limited cardinality and permutation-variant property. In this section, we further describe the technical reasons of the limited cardinality in token-mixing MLPs.</p><p>While a given pixel in an image systematically admits eight adjacent pixels, each point can have an arbitrary number of neighbors in a point cloud. In this context, shared MLPs are particularly desirable since they can handle variable input lengths <ref type="bibr" target="#b68">[69]</ref>. However, in the vanilla MLP-Mixer layer, token-mixing MLPs limit the process of an arbitrary number of points.</p><p>Let us briefly explain the reason. Channel-mixing MLPs require the predefined dimensionality in channels for the affine transformation of the input data. In contrast, token-mixing MLPs (Eq. (4) of the manuscript) switch the channel axis and spatial axis, which results in the pre-defined the number of input tokens (e.g., points). Accordingly, we can only take the pre-defined number of points with fixed channel length as an input of token-mixing MLPs. Thus, token-mixing MLPs cannot operate inter-set mixing whose cardinality varies depending on the point cloud distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training details</head><p>Semantic segmentation We set batch size as 2. Each batch consists of 40K points. We initially set the learning rate as 0.1 and decrease the initial learning rate 10 times smaller at 40, 50 epochs. In total, we train our network for 60 epochs. We use two NVIDIA 1080-Ti GPUs for training. The total training time takes 44 hours. Point cloud reconstruction We set batch size as 4. The rest of the training conditions are identical to that of semantic segmentation. To train our network, we modify the header layer of the network that we used for semantic segmentation task. Specifically, we change the channel dimensionality of the output as 3 that represents [x, y, z]. Object classification We set batch size as 32. We train our network for 300 epochs and schedule the learning rate using cosine-annealing decay. We use the same SGD optimizer that we used in the semantic segmentation task. In our header network, we utilize MLPs with dropout layers and set the ratio as 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation study (rebuttal)</head><p>In this section, we provide the more ablation studies requested by the reviewers. PointMixer vs. previous studies for 3D points. We agree that there are similarities between PointMixer and existing methods (e.g., PointNet++ <ref type="bibr" target="#b55">[56]</ref>, PointConv <ref type="bibr" target="#b78">[79]</ref>, and Point Transformer <ref type="bibr" target="#b95">[96]</ref>) when it comes to the intra-set mixing only. However, in this paper, we aim (1) to improve the network expressiveness via complementary set operations (intra/inter/hier-set mixing), (2) to develop a universal set operator, and (3) to design a symmetric network using PointMixer.</p><p>We integrate inter/hier-set mixing blocks into other existing backbones, and compare those variants with our PointMixer as shown in <ref type="table">Table 7</ref>. Note that PointNet++ and Point Transformer use max and vector-attention 9 for intra-set mixing, respectively. The results show that inter/hier-set mixing itself consistently improves the performance of PointNet++ and Point Transformer 10 regardless of the block designs. Interestingly, our mixing scheme (softmax) seems to be more effective than the simple operator (max) as well as the complex layer (vector-attention). PointMixer as a 3D version of MLP-Mixer. In <ref type="bibr" target="#b67">[68]</ref>, "MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e., "mixing" the per-location features), and one with MLPs applied across patches (i.e., "mixing" spatial information).". Similarly, Synthesizer <ref type="bibr" target="#b64">[65]</ref> also claims that "we show that Random Synthesizers are a form of MLP-Mixers. Random Synthesizers apply a weight matrix on the length dimension as a form of projection across the dimension.". Based on theses concepts, PointMixer can be seen as a form of both MLP-Mixer and Synthesizer through softmax that acts as a projection across the token dimensions, i.e., token-mixing.</p><p>Moreover, MLP-Mixer variants <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b88">89]</ref> also focus on the improved tokenmixer. For example, CycleMLP <ref type="bibr" target="#b4">[5]</ref> samples pixels in a cyclic style for linear complexity in token-mixing parts. AS-MLP <ref type="bibr" target="#b38">[39]</ref> also removes token-mixing MLPs, and proposes Axial Shift operations for a better local token communication. From these token-mixing analyses, the direction of PointMixer is aligned <ref type="table">Table 7</ref>. Semantic segmentation results of existing methods with the proposed inter-/hier-set mixing schemes on the S3DIS dataset. Note that 'max' represents PointNet++ block using maxpool, 'attn' means the vector-attention based Point Transformer block, and 'softmax' implies our PointMixer block. All methods are trained for 30 epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Intra with that of MLP-Mixer variants. Therefore, we respectively argue that Point-Mixer is a 3D version of MLP-Mixer 11 . Softmax function is not new. Nonetheless, our paper revisits the existing module to emphasize the extended use of kNN graph structure. While previous studies focus on the directional kNN graph (intra-set mixing), PointMixer newly notices the 'bi'-directional characteristics of kNN (inter/hier-set mixing). Furthermore, to fully utilize this newly-revisited property, the softmax function can be one choice instead of using complex modules. In conclusion, our design choice (replacement token-mixing MLPs with softmax function) is supported by our analysis of permutation-invariant point set operators (Sec. 3.2) across many recent publications <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b95">96]</ref> and our extensive experiments ( <ref type="table" target="#tab_5">Table 5 and Table 7</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Block design comparison and PointMixer layer details. Note that SOP means symmetric operation, such as pooling and summation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>PointMixer is a universal point set operator: (a) intra-set mixing, (b) inter-set mixing, and (c) hierarchical-set mixing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Build a kNN index map using query (?) for data (?)AsymmetricSet another kNN index map using query (?) for data (?)(PointNet++, PointTrans)    Reuse the same kNN index map used in transition down Symmetric (Ours) Comparison of transition layer: symmetric (ours), asymmetric<ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b95">96]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Overall architecture of PointMixer network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Point receptive fields. Given a query point ( ), we colorize the neighbor points that are affected by the query: intra/inter-set (top) and hier-set (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Semantic segmentation results on S3DIS Area 5 test dataset<ref type="bibr" target="#b1">[2]</ref>. Param. mAcc mIoU ceiling floor wall column window doortable chair sofa book. board clutter PAConv [83] -73.0 66.6 94.6 98.6 82.4 26.4 58.0 60.0 80.4 89.7 69.8 74.3 73.5 57.7 KPConv deform [67] -72.8 67.1 92.8 97.3 82.4 23.9 58.0 69.0 81.5 91.0 75.4 75.3 66.7 58.9 MinkowskiNet [8] 37.9M 71.7 65.4 91.8 98.7 86.2 34.1 48.9 62.4 81.6 89.8 47.2 74.9 74.4 58.6 PointTrans [96] 7.8M 76.5 70.4 94.0 98.5 86.3 38.0 63.4 74.3 89.1 82.4 74.3 80.2 76.0 59.3 FastPointTrans [54] 37.9M 77.9 70.3 94.2 98.0 86.0 53.8 61.2 77.3 81.3 89.4 60.1 72.8 80.4 58.9 PointMixer (ours) 6.5M 77.4 71.4 94.2 98.2 86.0 43.8 62.1 78.5 90.6 82.2 73.9 79.8 78.5 59.4</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>PointTransformer</cell><cell>PointMixer</cell><cell>Ground Truth</cell></row></table><note>Fig. 6. Qualitative results in semantic segmentation on S3DIS Area 5 test dataset [2].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Point cloud reconstruction results. PointMixer (ours) 1.11 77.1 41.5 53.7 2.74 42.1 33.5 37.8 2.43 56.5 38.2 44.7</figDesc><table><row><cell>Method</cell><cell cols="9">ShapeNet-Part [87] CD(?) Acc.(?) Cp.(?) F1(?) CD(?) Acc.(?) Cp.(?) F1(?) CD(?) Acc.(?) Cp.(?) F1(?) ScanNet [9] ICL-NUIM [20]</cell></row><row><cell>PointNet [55]</cell><cell>1.33</cell><cell>63.2</cell><cell>38.8 48.2</cell><cell>3.05</cell><cell>37.5</cell><cell>27.8 32.6</cell><cell>2.98</cell><cell>46.9</cell><cell>33.2 38.1</cell></row><row><cell>PointNet++ [56]</cell><cell>1.25</cell><cell>65.1</cell><cell>39.0 50.1</cell><cell>2.97</cell><cell>38.3</cell><cell>29.5 33.4</cell><cell>2.88</cell><cell>48.8</cell><cell>35.8 39.9</cell></row><row><cell>PointRecon [7]</cell><cell>1.19</cell><cell>81.0</cell><cell>40.4 53.4</cell><cell>2.86</cell><cell>40.4</cell><cell>30.2 34.1</cell><cell>2.78</cell><cell>54.1</cell><cell>38.1 43.6</cell></row><row><cell>PointTrans [96]</cell><cell>1.12</cell><cell>75.9</cell><cell>40.9 52.7</cell><cell>2.79</cell><cell>41.1</cell><cell>32.1 35.6</cell><cell>2.57</cell><cell>51.1</cell><cell>36.4 41.6</cell></row><row><cell>Input</cell><cell></cell><cell cols="2">PointTransformer</cell><cell></cell><cell></cell><cell>PointMixer</cell><cell></cell><cell cols="2">Ground Truth</cell></row></table><note>Fig. 7. Qualitative results in point reconstruction on ScanNet dataset [9].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Shape classification results on ModelNet40 dataset<ref type="bibr" target="#b79">[80]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Param. mAcc OA</cell></row><row><cell>PointNet [55]</cell><cell>-</cell><cell>86.2 89.2</cell></row><row><cell>PointNet++ [56]</cell><cell cols="2">1.4M -90.7</cell></row><row><cell>PointConv [79]</cell><cell cols="2">18.6M -92.5</cell></row><row><cell>DGCNN [78]</cell><cell>-</cell><cell>90.2 92.9</cell></row><row><cell cols="3">KPConv rigid [67] 15.2M -92.9</cell></row><row><cell>PointTrans [96]</cell><cell cols="2">5.3M 90.6 93.7</cell></row><row><cell>PointMixer (ours)</cell><cell cols="2">3.9M 91.4 93.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>, left one). Last, we compare PointMixer that is free from token-mixing MLPs (Table 5, bottom-right one). Universal point set operator. PointMixer can be applicable to act as intraset mixing, inter-set mixing, and hierarchical-set mixing. Technically speacking, intra-set mixing and hierarchical-set mixing for point downsampling utilizes M i , but inter-set mixing and hierarchical-set mixing for point upsampling use M ?1 i</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study about positional encoding (left), types of mixing (top-right) and token-mixing MLPs (bottom-right). In the bottom-right table, we denote channelmixing MLPs and token-mixing MLPs as C-MLP and T-MLP, respectively. Note that number of parameters (Param.) and training time (Time) is measured under semantic segmentation task on S3DIS dataset<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell>Preserve ( )</cell><cell>Semantic seg</cell><cell cols="2">Point recon</cell><cell cols="3">Preserve ( )</cell><cell>Semantic seg</cell><cell>Point recon</cell></row><row><cell>Pos. enc. in Intra Inter Hier</cell><cell cols="3">mIoU mAcc CD(?) F1(?)</cell><cell>Intra</cell><cell>Inter</cell><cell>Hier</cell><cell>mIoU mAcc CD(?) F1(?) 69.2 75.8 1.13 53.5</cell></row><row><cell></cell><cell>64.7 71.8</cell><cell>1.13</cell><cell>53.0</cell><cell></cell><cell></cell><cell>69.1</cell><cell>75.7</cell><cell>1.12 53.5</cell></row><row><cell></cell><cell>65.9 72.5</cell><cell>1.13</cell><cell>53.0</cell><cell></cell><cell></cell><cell>69.3</cell><cell>76.6</cell><cell>1.11 53.5</cell></row><row><cell></cell><cell>64.0 70.5</cell><cell>1.13</cell><cell>53.0</cell><cell></cell><cell></cell><cell>71.4 77.4 1.11 53.7</cell></row><row><cell></cell><cell>66.1 72.9</cell><cell>1.12</cell><cell>53.6</cell><cell cols="3">C-MLP T-MLP Softmax Param. Time mIoU CD(?)</cell></row><row><cell></cell><cell>69.6 76.3</cell><cell>1.12</cell><cell>53.6</cell><cell></cell><cell></cell><cell>6.5M 40h</cell><cell>58.3 1.23</cell></row><row><cell></cell><cell>70.2 76.8</cell><cell>1.13</cell><cell>52.5</cell><cell></cell><cell></cell><cell>6.5M 44h</cell><cell>71.4 1.11</cell></row><row><cell></cell><cell>69.9 76.5</cell><cell>1.13</cell><cell>53.5</cell><cell></cell><cell></cell><cell>7.4M 88h</cell><cell>71.1 1.12</cell></row><row><cell></cell><cell>71.4 77.4</cell><cell>1.11</cell><cell>53.7</cell><cell></cell><cell></cell><cell>7.4M 95h</cell><cell>71.1 1.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>A comparison of efficiency.</figDesc><table><row><cell>Layer type</cell><cell cols="3">Param. (M) Memory (MB) Latency (ms) mAcc OA</cell></row><row><cell>PointNet++ [56]</cell><cell>3.3</cell><cell>1463</cell><cell>13.04 90.9 93.2</cell></row><row><cell>PointTrans [96]</cell><cell>5.3</cell><cell>1473</cell><cell>19.77 90.2 93.1</cell></row><row><cell>PointMixer (ours)</cell><cell>3.5</cell><cell>1465</cell><cell>20.72 91.2 93.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Inter/Hier Param. (M) mIoU (%)</figDesc><table><row><cell>PointNet++</cell><cell>max</cell><cell></cell><cell>2.0</cell><cell>57.3</cell></row><row><cell></cell><cell>max</cell><cell>max</cell><cell>2.3 (? 0.3)</cell><cell>62.7 (? 5.4)</cell></row><row><cell></cell><cell>max</cell><cell>attn</cell><cell>8.3 (? 6.3)</cell><cell>57.8 (? 0.5)</cell></row><row><cell></cell><cell>max</cell><cell>softmax</cell><cell>2.7 (? 0.7)</cell><cell>66.9 (? 9.6)</cell></row><row><cell>PointTransformer 10</cell><cell>attn</cell><cell></cell><cell>7.8</cell><cell>70.0</cell></row><row><cell></cell><cell>attn</cell><cell>max</cell><cell>8.1 (? 0.3)</cell><cell>70.2 (? 0.2)</cell></row><row><cell></cell><cell>attn</cell><cell>attn</cell><cell>14.1 (? 6.3)</cell><cell>70.1 (? 0.1)</cell></row><row><cell></cell><cell>attn</cell><cell>softmax</cell><cell>8.5 (? 0.7)</cell><cell>70.3 (? 0.3)</cell></row><row><cell cols="3">PointMixer (ours) softmax softmax</cell><cell>6.5</cell><cell>71.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We set the relation of terminologies as layer ? block ? network.<ref type="bibr" target="#b3">4</ref> To deal with unordered points, layers are permutation-invariant (f layer : Xi ? yi) and blocks are permutation-equivariant (f block : X ? Y).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Please refer to the supplementary material for further details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">There is a chance to collect variable number of points after an inverse mapping M ?1 i .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Since there is no official release of codes, we use the best implementation of Point Transformer available in the public domain, which contains the official code provided by the authors of Point Transformer and reproduces the reported accuracy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">PointMixer: MLP-Mixer for Point Cloud Understanding</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Since the original implementation of PointNet++<ref type="bibr" target="#b55">[56]</ref> does not use the residual connection, this re-implementation brings performance gain to the model<ref type="bibr" target="#b55">[56]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Since vector-attention with an inverse mapping requires many scatter operations, it also heavily consumes GPU memory.<ref type="bibr" target="#b9">10</ref> Since there are no available pre-trained weights of Point Transformer on both S3DISand ModelNet40, we trained the Point Transformer ourselves with the official codes provided by the Point Transformer authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">In this context, PointNet++ also can be seen as one of the simplest versions of MLP-Mixer for point cloud understanding. However, as shown inTable 7, it is outperformed by PointMixer in the set mixing schemes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cyclemlp: A mlp-like architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Volumetric propagation network: Stereolidar fusion for long-range depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imtiaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4672" to="4679" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep point cloud reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Repmlp: Reparameterizing convolutions into fully-connected layers for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2286" to="2296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13341</idno>
		<title level="m">Hire-mlp: Vision mlp via hierarchical rearrangement</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<title level="m">Transformer in transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A benchmark for rgb-d visual odometry, 3d reconstruction and slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation. (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12368</idno>
		<title level="m">Vision permutator: A permutable mlp-like architecture for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale multi-view stereopsis evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aanaes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10433" to="10441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A-cnn: Annularly convolutional neural networks on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7421" to="7430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01316</idno>
		<title level="m">Putting 3d spatially sparse networks on a diet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04454</idno>
		<title level="m">Convmlp: Hierarchical convolutional mlps for vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Point cloud upsampling via disentangled refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">AS-MLP: An axial shifted MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fpconv: Learning local flattening for point convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pay attention to MLPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W.</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking network design and local geometry in point cloud: A simple residual MLP framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Voxel transformer for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3164" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cloud transformers: A universal approach to point cloud processing tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="10715" to="10724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VISAPP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="331" to="340" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16949" to="16958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Assanet: An anisotropic separable set abstraction for efficient point cloud representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="685" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An allmlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09792</idno>
		<title level="m">Patches are all you need? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions of Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Geometry sharing network for 3d point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3173" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5661" to="5670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Cn: Channel normalization for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="600" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07477</idno>
		<title level="m">S 2 -mlp: Spatial-shift mlp architecture for vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">S 2 -mlpv2: Improved spatial-shift mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12527</idno>
		<title level="m">Morphmlp: A self-attention free, mlp-like backbone for image and video</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="644" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<title level="m">Open3d: A modern library for 3d data processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
