<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Amortized Variational Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
						</author>
						<title level="a" type="main">Semi-Amortized Variational Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Variational inference (VI) <ref type="bibr" target="#b29">(Jordan et al., 1999;</ref><ref type="bibr" target="#b61">Wainwright &amp; Jordan, 2008</ref>) is a framework for approximating an intractable distribution by optimizing over a family of tractable surrogates. Traditional VI algorithms iterate over the observed data and update the variational parameters with closed-form coordinate ascent updates that exploit conditional conjugacy <ref type="bibr" target="#b15">(Ghahramani &amp; Beal, 2001)</ref>. This style of optimization is challenging to extend to large datasets and non-conjugate models. However, recent advances in stochastic <ref type="bibr" target="#b24">(Hoffman et al., 2013)</ref>, black-box <ref type="bibr" target="#b46">(Ranganath et al., 2014;</ref>, and amortized <ref type="bibr" target="#b42">(Mnih &amp; Gregor, 2014;</ref><ref type="bibr" target="#b31">Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b49">Rezende et al., 2014)</ref> variational inference have made it possible to scale to large datasets and rich, non-conjugate models (see <ref type="bibr" target="#b3">Blei et al. (2017)</ref>, <ref type="bibr" target="#b64">Zhang et al. (2017)</ref> for a review of modern methods).</p><p>In stochastic variational inference (SVI), the variational parameters for each data point are randomly initialized and then optimized to maximize the evidence lower bound (ELBO) with, for example, gradient ascent. These updates are based on a subset of the data, making it possible to scale the approach. In amortized variational inference (AVI), the local variational parameters are instead predicted by an inference (or recognition) network, which is shared (i.e. amortized) across the dataset. Variational autoencoders (VAEs) are deep generative models that utilize AVI for inference and jointly train the generative model alongside the inference network.</p><p>SVI gives good local (i.e. instance-specific) distributions within the variational family but requires performing optimization for each data point. AVI has fast inference, but having the variational parameters be a parametric function of the input may be too strict of a restriction. As a secondary effect this may militate against learning a good generative model since its parameters may be updated based on suboptimal variational parameters. <ref type="bibr" target="#b11">Cremer et al. (2018)</ref> observe that the amortization gap (the gap between the log-likelihood and the ELBO due to amortization) can be significant for VAEs, especially on complex datasets.</p><p>Recent work has targeted this amortization gap by combining amortized inference with iterative refinement during training <ref type="bibr" target="#b23">(Hjelm et al., 2016;</ref><ref type="bibr" target="#b34">Krishnan et al., 2018)</ref>. These methods use an encoder to initialize the local variational parameters, and then subsequently run an iterative procedure to refine them. To train with this hybrid approach, they utilize a separate training time objective. For example <ref type="bibr" target="#b23">Hjelm et al. (2016)</ref> train the inference network to minimize the KL-divergence between the initial and the final variational distributions, while <ref type="bibr" target="#b34">Krishnan et al. (2018)</ref> train the inference network with the usual ELBO objective based on the initial variational distribution.</p><p>In this work, we address the train/test objective mismatch and consider methods for training semi-amortized variational autoencoders (SA-VAE) in a fully end-to-end manner. We propose an approach that leverages differentiable optimization <ref type="bibr" target="#b13">(Domke, 2012;</ref><ref type="bibr" target="#b37">Maclaurin et al., 2015;</ref><ref type="bibr" target="#b2">Belanger et al., 2017)</ref> and differentiates through SVI while training the inference network/generative model. We find that this method is able to both improve estimation of variational parameters and produce better generative models. arXiv:1802.02550v7 [stat.ML] 23 Jul 2018</p><p>We apply our approach to train deep generative models of text and images, and observe that they outperform autoregressive/VAE/SVI baselines, in addition to direct baselines that combine VAE with SVI but do not perform end-to-end training. We also find that under our framework, we are able to utilize a powerful generative model without experiencing the "posterior-collapse" phenomenon often observed in VAEs, wherein the variational posterior collapses to the prior and the generative model ignores the latent variable <ref type="bibr" target="#b4">(Bowman et al., 2016;</ref><ref type="bibr" target="#b8">Chen et al., 2017;</ref><ref type="bibr" target="#b65">Zhao et al., 2017)</ref>. This problem has particularly made it very difficult to utilize VAEs for text, an important open issue in the field. With SA-VAE, we are able to outperform an LSTM language model by utilizing an LSTM generative model that maintains non-trivial latent representations. Code is available at https://github.com/harvardnlp/sa-vae. <ref type="bibr">ui)</ref> to be the i-th block of the gradient of f evaluated at? = [? 1 , . . . ,? m ], and further use df dv to denote the total derivative of f with respect to v, which exists if u is a differentiable function of v. Note that in general ? ui f (?) = df dui since other components of u could be a function of u i . <ref type="bibr">1</ref> We also let H ui,uj f (?) ? R dim(ui)?dim(uj ) be the matrix formed by taking the i-th group of rows and the j-th group of columns of the Hessian of f evaluated at?. These definitions generalize straightforwardly when f : R n ? R p is a vector-valued function (e.g. df du ? R n?p ). 2 2.1. Variational Inference Consider the following generative process for x,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><formula xml:id="formula_0">Notation Let f : R n ? R be a scalar valued func- tion with partitioned inputs u = [u 1 , . . . , u m ] such that m i=1 dim(u i ) = n. With a slight abuse of notation we define f (u 1 , . . . , u m ) = f ([u 1 , . . . , u m ]). We de- note ? ui f (?) ? R dim(</formula><formula xml:id="formula_1">z ? p(z) x ? p(x | z; ?)</formula><p>where p(z) is the prior and p(x | z; ?) is given by a generative model with parameters ?. As maximizing the loglikelihood log p(x; ?) = log z p(x | z; ?)p(z)dz is usually intractable, variational inference instead defines a variational family of distributions q(z; ?) parameterized by ? and maximizes the evidence lower bound (ELBO)</p><formula xml:id="formula_2">log p(x; ?) ? E q(z;?) [log p(x | z)] ? KL[q(z; ?) p(z)] = ELBO(?, ?, x)</formula><p>The variational posterior, q(z; ?), is said to collapse to the prior if KL[q(z; ?) p(z)] ? 0. In the general case we are 1 This will indeed be the case in our approach: when we calculate ELBO(?K , ?, x), ?K is a function of the data point x, the generative model ?, and the inference network ? (Section 3).</p><p>2 Total derivatives/Jacobians are usually denoted with row vectors but we denote them with column vectors for clearer notation.</p><p>given a dataset x (1) , . . . , x (N ) and need to find variational parameters ? (1) , . . . , ? (N ) and generative model parameters ? that jointly maximize N i=1 ELBO(? (i) , ?, x (i) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stochastic Variational Inference</head><p>We can apply SVI <ref type="bibr" target="#b24">(Hoffman et al., 2013)</ref> with gradient ascent to approximately maximize the above objective: 3</p><formula xml:id="formula_3">1. Sample x ? p D (x) 2. Randomly initialize ? 0 3. For k = 0, . . . , K ? 1, set ? k+1 = ? k + ?? ? ELBO(? k , ?, x) 4. Update ? based on ? ? ELBO(? K , ?, x)</formula><p>Here K is the number of SVI iterations and ? is the learning rate. (Note that ? is updated based on the gradient ? ? ELBO(? K , ?, x) and not the total derivative</p><formula xml:id="formula_4">d ELBO(? K ,?,x) d?</formula><p>. The latter would take into account the fact that ? k is a function of ? for k &gt; 0.) SVI optimizes directly for instance-specific variational distributions, but may require running iterative inference for a large number of steps. Further, because of this block coordinate ascent approach the variational parameters ? are optimized separately from ?, potentially making it difficult for ? to adapt to local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Amortized Variational Inference</head><p>AVI uses a global parametric model to predict the local variational parameters for each data point. A particularly popular application of AVI is in training the variational autoencoder (VAE) <ref type="bibr" target="#b31">(Kingma &amp; Welling, 2014)</ref>, which runs an inference network (i.e. encoder) enc(?) parameterized by ? over the input to obtain the variational parameters:</p><formula xml:id="formula_5">1. Sample x ? p D (x) 2. Set ? = enc(x; ?) 3. Update ? based on ? ? ELBO(?, ?, x) (which in this case is equal to the total derivative) 4. Update ? based on d ELBO(?, ?, x) d? = d? d? ? ? ELBO(?, ?, x)</formula><p>The inference network is learned jointly alongside the generative model with the same loss function, allowing the pair to coadapt. Additionally inference for AVI involves running the inference network over the input, which is usually much faster than running iterative optimization on the ELBO. Despite these benefits, requiring the variational parameters to be a parametric function of the input may be too strict of a restriction and can lead to an amortization gap. This gap can propagate forward to hinder the learning of the generative model if ? is updated based on suboptimal ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semi-Amortized Variational Autoencoders</head><p>Semi-amortized variational autoencoders (SA-VAE) utilize an inference network over the input to give the initial variational parameters, and subsequently run SVI to refine them. One might appeal to the universal approximation theorem <ref type="bibr" target="#b25">(Hornik et al., 1989)</ref> and question the necessity of additional SVI steps given a rich-enough inference network. However, in practice we find that the variational parameters found from VAE are usually not optimal even with a powerful inference network, and the amortization gap can be significant especially on complex datasets <ref type="bibr" target="#b11">(Cremer et al., 2018;</ref><ref type="bibr" target="#b34">Krishnan et al., 2018)</ref>.</p><p>SA-VAE models are trained using a combination of AVI and SVI steps:</p><formula xml:id="formula_6">1. Sample x ? p D (x) 2. Set ? 0 = enc(x; ?) 3. For k = 0, . . . , K ? 1, set ? k+1 = ? k + ?? ? ELBO(? k , ?, x) 4. Update ? based on d ELBO(? K ,?,x) d? 5. Update ? based on d ELBO(? K ,?,x) d?</formula><p>Note that for training we need to compute the total derivative of the final ELBO with respect to ?, ? (i.e. steps 4 and 5 above). Unlike with AVI, in order to update the encoder and generative model parameters, this total derivative requires backpropagating through the SVI updates. Specifically this requires backpropagating through gradient ascent <ref type="bibr" target="#b13">(Domke, 2012;</ref><ref type="bibr" target="#b37">Maclaurin et al., 2015)</ref>.</p><p>Following past work, this backpropagation step can be done efficiently with fast Hessian-vector products <ref type="bibr">(Le-Cun et al., 1993;</ref><ref type="bibr" target="#b44">Pearlmutter, 1994)</ref>. In particular, consider the case where we perform one step of refinement, ? 1 = ? 0 + ?? ? ELBO(? 0 , ?, x), and for brevity let L = ELBO(? 1 , ?, x). To backpropagate through this, we receive the derivative dL d?1 and use the chain rule,</p><formula xml:id="formula_7">dL d? 0 = d? 1 d? 0 dL d? 1 = (I + ?H ?,? ELBO(? 0 , ?, x)) dL d? 1 = dL d? 1 + ?H ?,? ELBO(? 0 , ?, x) dL d? 1</formula><p>We can then backpropagate dL d?0 through the inference network to calculate the total derivative, i.e. dL d? = d?0 d? dL d?0 . Similar rules can be used to derive dL d? . <ref type="bibr">4</ref> The full forward/backward step, which uses gradient descent with momentum on the negative ELBO, is shown in Algorithm 1. <ref type="bibr">4</ref> We refer the reader to <ref type="bibr" target="#b13">Domke (2012)</ref> for the full derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Semi-Amortized Variational Autoencoders</head><p>Input: inference network ?, generative model ?, inference steps K, learning rate ?, momentum ?,</p><formula xml:id="formula_8">loss function f (?, ?, x) = ? ELBO(?, ?, x) Sample x ? p D (x) ? 0 ? enc(x; ?) v 0 ? 0 for k = 0 to K ? 1 do v k+1 ? ?v k ? ? ? f (? k , ?, x) ? k+1 ? ? k + ?v k+1 end for L ? f (? K , ?, x) ? K ? ? ? f (? K , ?, x) ? ? ? ? f (? K , ?, x) v K ? 0 for k = K ? 1 to 0 do v k+1 ? v k+1 + ?? k+1 ? k ? ? k+1 ? H ?,? f (? k , ?, x)v k+1 ? ? ? ? H ?,? f (? k , ?, x)v k+1 v k ? ?v k+1 end for dL d? ? ? dL d? ? d?0 d? ? 0 Update ?, ? based on dL d? , dL d?</formula><p>In our implementation we calculate Hessian-vector products with finite differences <ref type="bibr" target="#b36">(LeCun et al., 1993;</ref><ref type="bibr" target="#b13">Domke, 2012)</ref>, which was found to be more memory-efficient than automatic differentiation (and therefore crucial for scaling our approach to rich inference networks/generative models). Specifically, we estimate H ui,uj f (?)v with</p><formula xml:id="formula_9">H ui,uj f (?)v ? 1 ? ui f (? 0 , . . . ,? j + v, . . . ,? m ) ? ? ui f (? 0 , . . . ,? j . . . ,? m )</formula><p>where is some small number (we use = 10 ?5 ). <ref type="bibr">5</ref> We further clip the results (i.e. rescale the results if the norm exceeds a threshold) before and after each Hessian-vector product as well as during SVI, which helped mitigate exploding gradients and further gave better training signal to the inference network. 6 See Appendix A for details. <ref type="bibr">5</ref> Since in our case the ELBO is a non-deterministic function due to sampling (and dropout, if applicable), care must be taken when calculating Hessian-vector product with finite differences to ensure that the source of randomness is the same when calculating the two gradient expressions. <ref type="bibr">6</ref> Without gradient clipping, in addition to numerical issues we empirically observed the model to degenerate to a case whereby it learned to rely too much on iterative inference, and thus the initial parameters from the inference network were poor. Another way to provide better signal to the inference network is to train against a weighted sum K k=0 w k ELBO(? k , ?, x) for w k ? 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We apply our approach to train generative models on a synthetic dataset in addition to text/images. For all experiments we utilize stochastic gradient descent with momentum on the negative ELBO. Our prior is the spherical Gaussian N (0, I) and the variational posterior is diagonal Gaussian, where the variational parameters are given by the mean vector and the diagonal log variance vector, i.e. ? = [?, log ? 2 ].</p><p>In preliminary experiments we also experimented with natural gradients, other optimization algorithms, and learning the learning rates, but found that these did not significantly improve results. Full details regarding hyperparameters/model architectures for all experiments are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Data</head><p>We first apply our approach to a synthetic dataset where we have access to the true underlying generative model of discrete sequences. We generate synthetic sequential data according to the following oracle generative process with 2-dimensional latent variables and x t :</p><formula xml:id="formula_10">z 1 , z 2 ? N (0, 1) h t = LSTM(h t?1 , x t ) x t+1 ? softmax(MLP([h t , z 1 , z 2 ]))</formula><p>We initialize the LSTM/MLP randomly as ?, where the LSTM has a single layer with hidden state/input dimension equal to 100. We generate for 5 time steps (so each example is given by ELBO landscape in <ref type="figure" target="#fig_0">Figure 1</ref> as a function of the variational posterior means (? 1 , ? 2 ) learned from the different methods.</p><formula xml:id="formula_11">x = [x 1 , . . . , x 5 ]) with</formula><p>For SVI/SA-VAE we run iterative optimization for 20 steps. Finally we also show the optimal variational parameters found from grid search.</p><p>As can be seen from <ref type="figure" target="#fig_0">Figure 1</ref>, the variational parameters from running SA-VAE are closest to the optimum while those obtained from SVI and VAE are slightly further away.</p><p>In <ref type="table" target="#tab_0">Table 1</ref> we show the variational upper bounds (i.e. negative ELBO) on the negative log-likelihood (NLL) from training the various models with both the oracle/learned generative model, and find that SA-VAE outperforms VAE/SVI in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Text</head><p>The next set of experiments is focused on text modeling on the Yahoo questions corpus from <ref type="bibr" target="#b63">Yang et al. (2017)</ref>. Text modeling with deep generative models has been a challenging problem, and few approaches have been shown to produce rich generative models that do not collapse to standard language models. Ideally a deep generative model trained with variational inference would make use of the latent space (i.e. maintain a nonzero KL term) while accurately modeling the underlying distribution.</p><p>Our architecture and hyperparameters are identical to the LSTM-VAE baselines considered in <ref type="bibr" target="#b63">Yang et al. (2017)</ref>, except that we train with SGD instead of Adam, which was found to perform better for training LSTMs. Specifically, both the inference network and the generative model are onelayer LSTMs with 1024 hidden units and 512-dimensional word embeddings. The last hidden state of the encoder is used to predict the vector of variational posterior means/log variances. The sample from the variational posterior is used to predict the initial hidden state of the generative LSTM and additionally fed as input at each time step. The latent variable is 32-dimensional. Following previous works <ref type="bibr" target="#b4">(Bowman et al., 2016;</ref><ref type="bibr" target="#b57">S?nderby et al., 2016;</ref><ref type="bibr" target="#b63">Yang et al., 2017)</ref>, for all the variational models we utilize a KL-cost annealing strategy whereby the multiplier on the KL term is increased linearly from 0.1 to 1.0 each batch over 10 epochs. Appendix B.2 has the full architecture/hyperparameters. In addition to autoregressive/VAE/SVI baselines, we consider two other approaches that also combine amortized inference with iterative refinement. The first approach is from <ref type="bibr" target="#b34">Krishnan et al. (2018)</ref>, where the generative model takes a gradient step based on the final variational parameters and the inference network takes a gradient step based on the initial variational parameters, i.e. we update ? based on ? ? ELBO(? K , ?, x) and update ? based on d? 0 d? ? ? ELBO(? 0 , ?, x). The forward step (steps 1-3 in Section 3) is identical to SA-VAE. We refer to this baseline as VAE + SVI.</p><p>In the second approach, based on <ref type="bibr" target="#b52">Salakhutdinov &amp; Larochelle (2010)</ref> and <ref type="bibr" target="#b23">Hjelm et al. (2016)</ref>, we train the inference network to minimize the KL-divergence between the initial and the final variational distributions, keeping the latter fixed.</p><formula xml:id="formula_12">Specifically, let- ting g(?, ?) = KL[q(z; ?) q(z; ?)], we update ? based on ? ? ELBO(? K , ?, x) and update ? based on d? 0 d? ? ? g(? 0 , ? K ).</formula><p>Note that the inference network is not updated based on dg d? , which would take into account the fact that both ? 0 and ? K are functions of ?. We found g(? 0 , ? K ) to perform better than the reverse direction g(? K , ? 0 ). We refer to this setup as VAE + SVI + KL.</p><p>Results from the various models are shown in <ref type="table" target="#tab_1">Table 2</ref>. Our baseline models (LM/VAE/SVI in <ref type="table" target="#tab_1">Table 2</ref>) are already quite strong and outperform the models considered in <ref type="bibr" target="#b63">Yang et al. (2017)</ref>. However models trained with VAE/SVI make neg- ligible use of the latent variable and practically collapse to a language model, negating the benefits of using latent variables. <ref type="bibr">8</ref> In contrast, models that combine amortized inference with iterative refinement make use of the latent space and the KL term is significantly above zero. 9 VAE + SVI and VAE + SVI + KL do not outperform a language model, and while SA-VAE only modestly outperforms it, to our knowledge this is one of the first instances in which we are able to train an LSTM generative model that does not ignore the latent code and outperforms a language model. One might wonder if the improvements are coming from simply having a more flexible inference scheme at test time, rather than from learning a better generative model. To test this, for the various models we discard the inference network at test time and perform SVI for a variable number of steps from random initialization. The results are shown in <ref type="figure" target="#fig_1">Figure 2</ref> (left). It is clear that the learned generative model (and the associated ELBO landscape) is quite different-it is not possible to train with VAE and perform SVI at test time to obtain the same performance as SA-VAE (although the performance of VAE does improve slightly from 62.7 to 62.3 when we run SVI for 40 steps from random initialization). <ref type="figure" target="#fig_1">Figure 2</ref> (right) has the results for a similar experiment where we refine the variational parameters initialized from the inference network for a variable number of steps at test time. We find that the inference network provides better initial parameters than random initialization and thus requires fewer iterations of SVI to reach the optimum. We do not observe improvements for running more refinement steps than was used in training at test time. Interestingly, SA-VAE without any refinement steps at test time has a substantially nonzero KL term (KL = 6.65, PPL = 62.0). This indicates that the posterior-collapse phenomenon when 8 Models trained with word dropout (+ WORD-DROP in <ref type="table" target="#tab_1">Table 2</ref>) do make use of the latent space but significantly underperform a language model. 9 A high KL term does not necessarily imply that the latent variable is being utilized in a meaningful way (it could simply be due to bad optimization). In Section 5.1 we investigate the learned latent space in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL NLL</head><p>IWAE <ref type="bibr" target="#b6">(Burda et al., 2015a)</ref> 103.38 LADDER VAE <ref type="bibr" target="#b57">(S?nderby et al., 2016)</ref> 102.11 RBM <ref type="bibr" target="#b7">(Burda et al., 2015b)</ref> 100.46 DISCRETE VAE <ref type="bibr" target="#b50">(Rolfe, 2017)</ref> 97.43 DRAW <ref type="bibr" target="#b18">(Gregor et al., 2015)</ref> ? 96.50 CONV DRAW <ref type="bibr" target="#b19">(Gregor et al., 2016)</ref> ? 91.00 VLAE <ref type="bibr" target="#b8">(Chen et al., 2017)</ref> 89.83 VAMPPRIOR <ref type="bibr" target="#b59">(Tomczak &amp; Welling, 2018)</ref> 89 training LSTM-based VAEs for text is partially due to optimization issues. Finally, while <ref type="bibr" target="#b63">Yang et al. (2017)</ref> found that initializing the encoder with a pretrained language model improved performance (+ INIT in <ref type="table" target="#tab_1">Table 2</ref>), we did not observe this on our baseline VAE model when we trained with SGD and hence did not pursue this further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Images</head><p>We next apply our approach to model images on the OM-NIGLOT dataset <ref type="bibr" target="#b35">(Lake et al., 2015)</ref>. 10 While posterior collapse is less of an issue for VAEs trained on images, we still expect that improving the amortization gap would result in generative models that better model the underlying data and make more use of the latent space. We use a three-layer ResNet <ref type="bibr" target="#b22">(He et al., 2016)</ref> as our inference network. The generative model first transforms the 32-dimensional latent vector to the image spatial resolution, which is concatenated with the original image and fed to a 12-layer Gated Pixel-CNN (van den Oord et al., 2016) with varying filter sizes, followed by a final sigmoid layer. We employ the same KL-cost annealing schedule as in the text experiments. See Appendix B.3 for the exact architecture/hyperparameters.</p><p>Results from the various models are shown in <ref type="table" target="#tab_2">Table 3</ref>. Our findings are largely consistent with results from text: the semi-amortized approaches outperform VAE/SVI baselines, and further they learn generative models that make more <ref type="bibr">10</ref> We focus on the more complex OMNIGLOT dataset instead of the simpler MNIST dataset as prior work has shown that the amortization gap on MNIST is minimal <ref type="bibr" target="#b11">(Cremer et al., 2018)</ref>. use of the latent representations (i.e. KL portion of the loss is higher). Even with 80 steps of SVI we are unable to perform as well as SA-VAE trained with 10 refinement steps, indicating the importance of good initial parameters provided by the inference network. In Appendix C we further investigate the performance of VAE and SA-VAE as we vary the training set size and the capacity of the inference network/generative model. We find that SA-VAE outperforms VAE and has higher latent variable usage in all scenarios. We note that we do not outperform the state-of-the-art models that employ hierarchical latent variables and/or more sophisticated priors <ref type="bibr" target="#b8">(Chen et al., 2017;</ref><ref type="bibr" target="#b59">Tomczak &amp; Welling, 2018)</ref>. However these additions are largely orthogonal to our approach and we hypothesize they will also benefit from combining amortized inference with iterative refinement. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Learned Latent Space</head><p>For the text model we investigate what the latent variables are learning through saliency analysis with our best model (SA-VAE trained with 20 steps). Specifically, we calculate the output saliency of each token x t with respect to z as</p><formula xml:id="formula_13">E q(z;?) d log p(x t | x &lt;t , z; ?) dz 2</formula><p>where ? 2 is the l 2 norm and the expectation is approximated with 5 samples from the variational posterior. Saliency is therefore a measure of how much the latent variable is being used to predict a particular token.</p><p>We visualize the saliency of a few examples from the test set in <ref type="figure" target="#fig_2">Figure 3</ref> (top). Each example consists of a question followed by an answer from the Yahoo corpus. From a qualitative analysis several things are apparent: the latent variable seems to encode question type (i.e. if, what, how, why, etc.) and therefore saliency is high for the first word; content words (nouns, adjectives, lexical verbs) have much higher saliency than function words (determiners, prepositions, conjunctions, etc.); saliency of the &lt;/s&gt; token is quite high, indicating that the length information is also encoded in the latent space. In the third example we observe that the left parenthesis has higher saliency than the right parenthesis (0.32 vs. 0.24 on average across the test set), as the latter can be predicted by conditioning on the former rather than on the latent representation z.</p><p>The previous definition of saliency measures the influence of z on the output x t . We can also roughly measure the influence of the input x t on the latent representation z, which we refer to as input saliency:</p><formula xml:id="formula_14">E q(z;?) d z 2 dw t</formula><p>where can i buy an affordable stationary bike ? try this place , they have every type imaginable with prices to match . who is the best soccer player in the world ? i think he is the best player in the world . ronaldinho is the best player in the world . he is a great player . &lt;/s&gt; will ghana be able to play the next game in 2010 fifa world cup ? yes , they will win it all . &lt;/s&gt; Here w t is the encoder word embedding for x t . <ref type="bibr">12</ref> We visualize the input saliency for a test example <ref type="figure" target="#fig_2">(Figure 3, middle)</ref> and a made-up example <ref type="figure" target="#fig_2">(Figure 3, bottom)</ref>. Under each input example we also visualize a two samples from the variational posterior, and find that the generated examples are often meaningfully related to the input example. <ref type="bibr">13</ref> We quantitatively analyze output saliency across part-ofspeech, token position, word frequency, and log-likelihood in <ref type="figure" target="#fig_3">Figure 4</ref>: nouns (NN), adjectives (JJ), verbs (VB), numbers (CD), and the &lt;/s&gt; token have higher saliency than conjunctions (CC), determiners (DT), prepositions (IN), and the TO token-the latter are relatively easier to predict by conditioning on previous tokens; similarly, on average, tokens occurring earlier have much higher saliency than those <ref type="bibr">12</ref> As the norm of z is a rather crude measure, a better measure would be obtained by analyzing the spectra of the Jacobian dz dw t . However this is computationally too expensive to calculate for each token in the corpus. <ref type="bibr">13</ref> We first sample z ? q(z; ?K ) then x ? p(x | z; ?). When sampling xt ? p(xt | x&lt;t, z) we sample with temperature T = 0.25, i.e. p(xt | x&lt;t, z) = softmax( 1 T st) where st is the vector with scores for all words. We found the generated examples to be related to the original (in some way) in roughly half the cases.</p><p>occurring later <ref type="figure" target="#fig_3">(Figure 4</ref> shows absolute position but the plot is similar with relative position); the latent variable is used much more when predicting rare tokens; there is some negative correlation between saliency and log-likelihood (-0.51), though this relationship does not always hold-e.g. &lt;/s&gt; has high saliency but is relatively easy to predict with an average log-likelihood of -1.61 (vs. average log-likelihood of -4.10 for all tokens). Appendix D has the corresponding analysis for input saliency, which are qualitatively similar.</p><p>These results seem to suggest that the latent variables are encoding interesting and potentially interpretable aspects of language. While left as future work, it is possible that manipulations in the latent space of a model learned this way could lead to controlled generation/manipulation of output text <ref type="bibr" target="#b43">Mueller et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Limitations</head><p>A drawback of our approach (and other non-amortized inference methods) is that each training step requires backpropagating through the generative model multiple times, which can be costly especially if the generative model is expensive to compute (e.g. LSTM/PixelCNN). This may potentially be mitigated through more sophisticated meta learning ap- proaches <ref type="bibr" target="#b1">(Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b38">Marino et al., 2018)</ref>, or with more efficient use of the past gradient information during SVI via averaging <ref type="bibr" target="#b54">(Schmidt et al., 2013)</ref> or importance sampling <ref type="bibr" target="#b51">(Sakaya &amp; Klami, 2017)</ref>. One could also consider employing synthetic gradients <ref type="bibr" target="#b27">(Jaderberg et al., 2017)</ref> to limit the number of backpropagation steps during training. <ref type="bibr" target="#b34">Krishnan et al. (2018)</ref> observe that it is more important to train with iterative refinement during earlier stages (we also observed this in preliminary experiments), and therefore annealing the number of refinement steps as training progresses could also speed up training.</p><p>Our approach is mainly applicable to variational families that avail themselves to differentiable optimization (e.g. gradient ascent) with respect to the ELBO, which include much recent work on employing more flexible variational families with VAEs. In contrast, VAE + SVI and VAE + SVI + KL are applicable to more general optimization algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Our work is most closely related the line of work which uses a separate model to initialize variational parameters and subsequently updates them through an iterative procedure <ref type="bibr" target="#b52">(Salakhutdinov &amp; Larochelle, 2010;</ref><ref type="bibr" target="#b9">Cho et al., 2013;</ref><ref type="bibr" target="#b53">Salimans et al., 2015;</ref><ref type="bibr" target="#b23">Hjelm et al., 2016;</ref><ref type="bibr" target="#b34">Krishnan et al., 2018;</ref><ref type="bibr" target="#b45">Pu et al., 2017)</ref>. <ref type="bibr" target="#b38">Marino et al. (2018)</ref> utilize meta-learning to train an inference network which learns to perform iterative inference by training a deep model to output the variational parameters for each time step.</p><p>While differentiating through inference/optimization was initially explored by various researchers primarily outside the area of deep learning <ref type="bibr" target="#b58">(Stoyanov et al., 2011;</ref><ref type="bibr" target="#b13">Domke, 2012;</ref><ref type="bibr" target="#b5">Brakel et al., 2013)</ref>, they have more recently been explored in the context of hyperparameter optimization <ref type="bibr" target="#b37">(Maclaurin et al., 2015)</ref> and as a differentiable layer of a deep model <ref type="bibr" target="#b2">(Belanger et al., 2017;</ref><ref type="bibr">Kim et al., 2017;</ref><ref type="bibr" target="#b39">Metz et al., 2017;</ref><ref type="bibr" target="#b0">Amos &amp; Kolter, 2017)</ref>.</p><p>Initial work on VAE-based approaches to image modeling focused on simple generative models that assumed independence among pixels conditioned on the latent variable <ref type="bibr" target="#b31">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b49">Rezende et al., 2014)</ref>. More recent works have obtained substantial improvements in loglikelihood and sample quality through utilizing powerful autoregressive models (PixelCNN) as the generative model <ref type="bibr" target="#b8">(Chen et al., 2017;</ref><ref type="bibr" target="#b20">Gulrajani et al., 2017)</ref>.</p><p>In contrast, modeling text with VAEs has remained challenging. <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> found that using an LSTM generative model resulted in a degenerate case whereby the variational posterior collapsed to the prior and the generative model ignored the latent code (even with richer variational families). Many works on VAEs for text have thus made simplifying conditional independence assumptions <ref type="bibr" target="#b40">(Miao et al., 2016;</ref>, used less powerful generative models such as convolutional networks <ref type="bibr" target="#b55">Semeniuta et al., 2017)</ref>, or combined a recurrent generative model with a topic model <ref type="bibr" target="#b12">(Dieng et al., 2017;</ref><ref type="bibr" target="#b62">Wang et al., 2018)</ref>. Note that unlike to sequential VAEs that employ different latent variables at each time step <ref type="bibr" target="#b10">(Chung et al., 2015;</ref><ref type="bibr" target="#b14">Fraccaro et al., 2016;</ref><ref type="bibr" target="#b33">Krishnan et al., 2017;</ref><ref type="bibr" target="#b56">Serban et al., 2017;</ref><ref type="bibr" target="#b16">Goyal et al., 2017a)</ref>, in this work we focus on modeling the entire sequence with a global latent variable.</p><p>Finally, since our work only addresses the amortization gap (the gap between the log-likelihood and the ELBO due to amortization) and not the approximation gap (due to the choice of a particular variational family) <ref type="bibr" target="#b11">(Cremer et al., 2018)</ref>, it can be combined with existing work on employing richer posterior/prior distributions within the VAE framework <ref type="bibr" target="#b48">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b32">Kingma et al., 2016;</ref><ref type="bibr" target="#b28">Johnson et al., 2016;</ref><ref type="bibr" target="#b60">Tran et al., 2016;</ref><ref type="bibr" target="#b17">Goyal et al., 2017b;</ref><ref type="bibr" target="#b21">Guu et al., 2017;</ref><ref type="bibr" target="#b59">Tomczak &amp; Welling, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This work outlines semi-amortized variational autoencoders, which combine amortized inference with local iterative refinement to train deep generative models of text and images. With the approach we find that we are able to train deep latent variable models of text with an expressive autogressive generative model that does not ignore the latent code.</p><p>From the perspective of learning latent representations, one might question the prudence of using an autoregressive model that fully conditions on its entire history (as opposed to assuming some conditional independence) given that p(x) can always be factorized as T t=1 p(x t | x &lt;t ), and therefore the model is non-identifiable (i.e. it does not have to utilize the latent variable). However in finite data regimes we might still expect a model that makes use of its latent variable to generalize better due to potentially better inductive bias (from the latent variable). Training generative models that both model the underlying data well and learn good latent representations is an important avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials for Semi-Amortized Variational Autoencoders</head><p>A. Training Semi-Amortized Variational Autoencoders with Gradient Clipping</p><p>For stable training we found it crucial to modify Algorithm 1 to clip the gradients at various stages. This is shown in Algorithm 2, where we have a clipping parameter ?. The clip(?) function is given by</p><formula xml:id="formula_15">clip(u, ?) = ? u 2 u , if u 2 &gt; ? u , otherwise</formula><p>We use ? = 5 in all experiments. The finite difference estimation itself also uses gradient clipping. See https://github. com/harvardnlp/sa-vae/blob/master/optim n2n.py for the exact implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head><p>For all the variational models we use a spherical Gaussian prior. The variational family is the diagonal Gaussian parameterized by the vector of means and log variances. For models trained with SVI the initial variational parameters are randomly initialized from a Gaussian with standard deviation equal to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Synthetic Data</head><p>We generate synthetic data points according to the following generative process:</p><formula xml:id="formula_16">z 1 , z 2 ? N (0, 1) h t = LSTM(h t?1 , x t ) x t+1 ? softmax(MLP([h t , z 1 , z 2 ]))</formula><p>Here LSTM is a one-layer LSTM with 100 hidden units where the input embedding is also 100-dimensional. The initial hidden/cell states are set to zero, and we generate for 5 time steps for each example (so x = [x 1 , . . . , x 5 ]).</p><p>The MLP consists of a single affine transformation to project out to the vocabulary space, which has 1000 tokens. LSTM/MLP parameters are randomly initialized with U(?1, 1), except for the part of the MLP that directly connects to the latent variables, which is initialized with U(?5, 5). This is done to make sure that the latent variables have more influence in predicting x. We generate 5000 training/validation/test examples.</p><p>When we learn the generative model the LSTM is initialized over U(?0.1, 0.1). The inference network is also a one-layer LSTM with 100-dimensional hidden units/input </p><formula xml:id="formula_17">loss function f (?, ?, x) = ? ELBO(?, ?, x), gradient clipping parameter ? Sample x ? p D (x) ? 0 ? enc(x; ?) v 0 ? 0 for k = 0 to K ? 1 do v k+1 ? ?v k ? clip(? ? f (? k , ?, x), ?) ? k+1 ? ? k + ?v k+1 end for L ? f (? K , ?, x) ? K ? ? ? f (? K , ?, x) ? ? ? ? f (? K , ?, x) v K ? 0 for k = K ? 1 to 0 do v k+1 ? v k+1 + ?? k+1 ? k ? ? k+1 ? H ?,? f (? k , ?, x)v k+1 ? k ? clip(? k , ?) ? ? ? ? clip(H ?,? f (? k , ?, x)v k+1 , ?) v k ? ?v k+1 end for dL d? ? ? dL d? ? d?0 d? ? 0 Update ?, ? based on dL d? , dL d? embeddings,</formula><p>where the variational parameters are predicted via an affine transformation on the final hidden state of the encoder. All models are trained with stochastic gradient descent with batch size 50, learning rate 1.0, and gradient clipping at 5. The learning rate starts decaying by a factor of 2 each epoch after the first epoch at which validation performance does not improve. This learning rate decay is not triggered for the first 5 epochs. We train for 20 epochs, which was enough for convergence of all models. For SVI/SA-VAE we perform 20 steps of iterative inference with stochastic gradient descent and learning rate 1.0 with gradient clipping at 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Text</head><p>We use the same model architecture as was used in <ref type="bibr" target="#b63">Yang et al. (2017)</ref> We train for 30 epochs or until the learning rate has decayed 5 times, which was enough for convergence for all models. Model parameters are initialized over U(?0.1, 0.1) and gradients are clipped at 5. We employ a KL-cost annealing schedule whereby the multiplier on the KL-cost term is increased linearly from 0.1 to 1.0 each batch over 10 epochs. For models trained with iterative inference we perform SVI via stochastic gradient descent with momentum 0.5 and learning rate 1.0. Gradients are clipped after each step of SVI (also at 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Images</head><p>The preprocessed OMNIGLOT dataset does not have a standard validation split so we randomly pick 2000 images from training as validation. As with previous works the pixel value is scaled to be between 0 and 1 and interpreted as probabilities, and the images are dynamically binarized dur-ing training.</p><p>Our inference network consists of 3 residual blocks where each block is made up of a standard residual layer (i.e. two convolutional layers with 3 ? 3 filters, ReLU nonlinearities, batch normalization, and residual connections) followed by a downsampling convolutional layer with filter size and stride equal to 2. These layers have 64 feature maps. The output of residual network is flattened and then used to obtain the variational means/log variances via an affine transformation.</p><p>The sample from the variational distribution (which is 32dimensional) is first projected out to the image spatial resolution with 4 feature maps (i.e. 4?28?28) via a linear transformation, then concatenated with the original image, and finally fed as input to a 12-layer Gated PixelCNN (van den <ref type="bibr">Oord et al., 2016)</ref>. The PixelCNN has three 9 ? 9 layers, followed by three 7 ? 7 layers, then three 5 ? 5 layers, and finally three 3 ? 3 layers. All the layers have 32 feature maps, and there is a final 1 ? 1 convolutional layer followed by a sigmoid nonlinearity to produce a distribution over binary output. The layers are appropriately masked to ensure that the distribution over each pixel is conditioned only on the pixels left/top of it. We train with Adam with learning rate 0.001, ? 1 = 0.9, ? 2 = 0.999 for 100 epochs with batch size of 50. Gradients are clipped at 5.</p><p>For models trained with iterative inference we perform SVI via stochastic gradient descent with momentum 0.5 and learning rate 1.0, with gradient clipping (also at 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Size/Model Capacity</head><p>In <ref type="table" target="#tab_5">Table 4</ref> we investigate the performance of VAE/SA-VAE as we vary the capacity of the inference network, size of the training set, and the capacity of the generative model. The MLP inference network has two ReLU layers with 128 hidden units. For varying the PixelCNN generative model, we sequentially remove layers from our baseline 12-layer model starting from the bottom (so the 9-layer PixelCNN has three 7 ? 7 layers, three 5 ? 5 layers, three 3 ? 3 layers, all with 32 feature maps).</p><p>Intuitively, we expect iterative inference to help more when the inference network and the generative model are less powerful, and we indeed see this in <ref type="table" target="#tab_5">Table 4</ref>. Further, one might expect SA-VAE to be more helpful in small-data regimes as it is harder for the inference network amortize inference and generalize well to unseen data. However we find that SA-VAE outperforms VAE by a similar margin across all training set sizes.</p><p>Finally, we observe that across all scenarios the KL portion of the loss is much higher for models trained with SA-VAE, indicating that these models are learning generative models that make more use of the latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Input Saliency Analysis</head><p>In <ref type="figure" target="#fig_4">Figure 5</ref> we show the input saliency by part-of-speech tag (left), position (center), and frequency (right). Input saliency of a token x t is defined as:</p><formula xml:id="formula_18">E q(z;?) d z 2 dw t</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>ELBO landscape with the oracle generative model as a function of the variational posterior means ?1, ?2 for a randomly chosen test point. Variational parameters obtained from VAE, SVI are shown as ?VAE, ?SVI and the initial/final parameters from SA-VAE are shown as ?0 and ?K (along with the intermediate points). SVI/SA-VAE are run for 20 iterations. The optimal point, found from grid search, is shown as ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(Left) Perplexity upper bound of various models when trained with 20 steps (except for VAE) and tested with varying number of SVI steps from random initialization. (Right) Same as the left except that SVI is initialized with variational parameters obtained from the inference network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(Top) Saliency visualization of some examples from the test set. Here the saliency values are rescaled to be between 0-100 within each example for easier visualization. Red indicates higher saliency values. (Middle) Input saliency of the first test example from the top (in blue), in addition to two sample outputs generated from the variational posterior (with their saliency values in red). (Bottom) Same as the middle except we use a made-up example. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Output saliency by part-of-speech tag, position, log frequency, and log-likelihood. See Section 5.1 for the definitions of output saliency. The dotted gray line in each plot shows the average saliency across all words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Input saliency by part-of-speech tag (left), position (center), and log frequency (right). The dotted gray line in each plot shows the average saliency across all words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>a vocabulary size of 1000 for each x t . Training set consists of 5000 points. See Appendix B.1 for the exact setup. Variational upper bounds for the various models on the synthetic dataset, where SVI/SA-VAE is trained/tested with 20 steps. TRUE NLL (EST) is an estimate of the true negative loglikelihood (i.e. entropy of the data-generating distribution) estimated with 1000 samples from the prior. ORACLE GEN uses the oracle generative model and LEARNED GEN learns the generative network.</figDesc><table><row><cell>MODEL</cell><cell cols="2">ORACLE GEN LEARNED GEN</cell></row><row><cell>VAE</cell><cell>? 21.77</cell><cell>? 27.06</cell></row><row><cell>SVI</cell><cell>? 22.33</cell><cell>? 25.82</cell></row><row><cell>SA-VAE</cell><cell>? 20.13</cell><cell>? 25.21</cell></row><row><cell>TRUE NLL (EST)</cell><cell>19.63</cell><cell>?</cell></row></table><note>We fix this oracle generative model p(x | z; ?) and learn an inference network (also a one-layer LSTM) with VAE and SA-VAE.7 For a randomly selected test point, we plot the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on text modeling on the Yahoo dataset. Top results are from<ref type="bibr" target="#b63">Yang et al. (2017)</ref>, while the bottom results are from this work (+ INIT means the encoder is initialized with a pretrained language model, while models with + WORD-DROP are trained with word-dropout). NLL/KL numbers are averaged across examples, and PPL refers to perplexity. K refers to the number of inference steps used for training/testing.</figDesc><table><row><cell>MODEL</cell><cell>NLL</cell><cell>KL</cell><cell>PPL</cell></row><row><cell>LSTM-LM</cell><cell>334.9</cell><cell>?</cell><cell>66.2</cell></row><row><cell>LSTM-VAE</cell><cell>? 342.1</cell><cell cols="2">0.0 ? 72.5</cell></row><row><cell>LSTM-VAE + INIT</cell><cell>? 339.2</cell><cell cols="2">0.0 ? 69.9</cell></row><row><cell>CNN-LM</cell><cell>335.4</cell><cell>?</cell><cell>66.6</cell></row><row><cell>CNN-VAE</cell><cell>? 333.9</cell><cell cols="2">6.7 ? 65.4</cell></row><row><cell>CNN-VAE + INIT</cell><cell cols="3">? 332.1 10.0 ? 63.9</cell></row><row><cell>LM</cell><cell>329.1</cell><cell>?</cell><cell>61.6</cell></row><row><cell>VAE</cell><cell cols="3">? 330.2 0.01 ? 62.5</cell></row><row><cell>VAE + INIT</cell><cell cols="3">? 330.5 0.37 ? 62.7</cell></row><row><cell>VAE + WORD-DROP 25%</cell><cell cols="3">? 334.2 1.44 ? 65.6</cell></row><row><cell>VAE + WORD-DROP 50%</cell><cell cols="3">? 345.0 5.29 ? 75.2</cell></row><row><cell>SVI (K = 10)</cell><cell cols="3">? 331.4 0.16 ? 63.4</cell></row><row><cell>SVI (K = 20)</cell><cell cols="3">? 330.8 0.41 ? 62.9</cell></row><row><cell>SVI (K = 40)</cell><cell cols="3">? 329.8 1.01 ? 62.2</cell></row><row><cell>VAE + SVI (K = 10)</cell><cell cols="3">? 331.2 7.85 ? 63.3</cell></row><row><cell>VAE + SVI (K = 20)</cell><cell cols="3">? 330.5 7.80 ? 62.7</cell></row><row><cell cols="4">VAE + SVI + KL (K = 10) ? 330.3 7.95 ? 62.5</cell></row><row><cell cols="4">VAE + SVI + KL (K = 20) ? 330.1 7.81 ? 62.3</cell></row><row><cell>SA-VAE (K = 10)</cell><cell cols="3">? 327.6 5.13 ? 60.5</cell></row><row><cell>SA-VAE (K = 20)</cell><cell cols="3">? 327.5 7.19 ? 60.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>.76</cell></row></table><note>. Results on image modeling on the OMNIGLOT dataset. Top results are from prior works, while the bottom results are from this work. GATED PIXELCNN is our autoregressive baseline, and K refers to the number of inference steps during training/testing. For the variational models the KL portion of the ELBO is shown in parentheses.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>http : UNK &lt;/s&gt; if our economy collapses , will canada let all of us cross their border ? no , a country would have to be stupid to let that many people cross their borders and drain their resources . &lt;/s&gt; does the flat earth society still exist ? i 'm curious to know whether the original society still exists . i 'm not especially interested in discussion about whether the earth is flat or round . although there is no currently active website for the society , someone ( apparently a relative of samuel UNK ) maintains the flat earth society forums . this website , which offers a discussion forum and an on-line archive of flat earth society UNK from the 1970s and 1980s , represents a serious attempt to UNK the original flat earth society . &lt;/s&gt; &lt;s&gt; where can i buy an affordable stationary bike ? try this place , they have every type imaginable with prices to match .</figDesc><table><row><cell>http : UNK &lt;/s&gt;</cell></row><row><cell>where can i find a good UNK book for my daughter ? i am looking for a website that sells christmas gifts for the UNK .</cell></row><row><cell>thanks ! UNK UNK &lt;/s&gt;</cell></row><row><cell>where can i find a good place to rent a UNK ? i have a few UNK in the area , but i 'm not sure how to find them</cell></row><row><cell>. http : UNK &lt;/s&gt;</cell></row><row><cell>&lt;s&gt; which country is the best at soccer ? brazil or germany . &lt;/s&gt;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 2 Semi-Amortized Variational Autoencoders with Gradient Clipping Input: inference network ?, generative model ?, inference steps K, learning rate ?, momentum ?,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Upper bounds on negative log-likelihood (i.e. negative ELBO) of VAE/SA-VAE trained on OMNIGLOT, where we vary the capacity of the inference network (3-layer ResNet vs 2-layer MLP). KL portion of the loss is shown in parentheses. (Top) Here we vary the training set size from 25% to 100%, and use a 12-layer PixelCNN as the generative model. (Bottom) Here we fix the training set size to be 100%, and vary the capacity of the generative model. hidden states where the input word embedding is 512dimensional. We use the final hidden state of the encoder to predict (via an affine transformation) the vector of variational means and log variances. The latent space is 32dimensional. The sample from the variational posterior is used to initialize the initial hidden state of the generative LSTM (but not the cell state) via an affine transformation, and additionally fed as input (i.e. concatenated with the word embedding) at each time step. There are dropout layers with probability 0.5 between the input-to-hidden layer and the hidden-to-output layer on the generative LSTM only.</figDesc><table><row><cell>. The inference network and the generative</cell></row><row><cell>model are both one-layer LSTMs with 1024-dimensional</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">While we describe the various algorithms for a specific data point, in practice we use mini-batches.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">With a fixed oracle, these models are technically not VAEs as VAE usually implies that the the generative model is learned (alongside the encoder).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">11 Indeed,<ref type="bibr" target="#b11">Cremer et al. (2018)</ref> observe that the amortization gap can be substantial for VAE trained with richer variational families.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here w t is the encoder word embedding for x t . Part-ofspeech tagging is done using NLTK.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Rahul Krishnan, Rachit Singh, and Justin Chiu for insightful comments/discussion. We additionally thank Zichao Yang for providing the text dataset. YK and AM are supported by Samsung Research. SW is supported by an Amazon AWS ML Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable Optimization as a Layer in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to Learn by Gradient Descent by Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Misha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend Learning for Structured Prediction Energy Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training Energy-Based Models for Time-Series Imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Stroobandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2771" to="2797" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Importance Weighted Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prafulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Variational Lossy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoencoder</surname></persName>
		</author>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Two-Stage Pretraining Algorithm for Deep Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tapani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICANN</title>
		<meeting>ICANN</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Recurrent Latent Variable Model for Sequential Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kratarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inference Suboptimality in Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TopicRNN: A Recurrent Neural Network With Long-Range Semantic Dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adji</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generic Methods for Optimization-based Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequential Neural Models with Stochastic Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soren</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Propagation algorithms for variational bayesian learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Z-Forcing: Training Stochastic Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc-Alexandre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosemary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonparametric Variational Auto-encoders for Hierarchical Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DRAW: A Recurrent Neural Network for Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards Conceptual Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frederic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PixelVAE: A Latent Variable Model for Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kundan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faruk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francesco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08878</idno>
		<title level="m">Generating Sentences by Editing Prototypes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Iterative Refinement of the Approximate Posterior for Directed Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilayer Feedforward Networks are Universal Approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kur</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toward Controlled Generation of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zichao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupled Neural Interfaces using Synthetic Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandeep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Introduction to Variational Methods for Graphical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR, 2017. Semi-Amortized Variational Autoencoders</title>
		<meeting>ICLR, 2017. Semi-Amortized Variational Autoencoders</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving Variational Inference with Autoregressive Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured Inference Networks for Nonlinear State Space Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the Challenges of Learning with Inference Networks on Sparse, High-dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human-level Concept Learning through Probabilistic Program Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic Learning Rate Maximization by On-line Estimation of the Hessians Eigenvectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gradient-based Hyperparameter Optimization through Reversible Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative Amortized Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unrolled Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural Variational Inference for Text Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discovering Discrete Latent Topics with Neural Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural Variational Inference and Learning in Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andryi</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence to Better Sequence: Continuous Revision of Combinatorial Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast exact multiplication by the hessian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">VAE Learning via Stein Variational Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chunyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Black Box Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical Variational Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Variational Inference with Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discrete Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Importance Sampled Stochastic Optimization for Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Sakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Klami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient Learning of Deep Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Markov Chain Monte Carlo and Variational Inference: Bridging the Gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.2388</idno>
		<title level="m">Minimizing Finite Sums with the Stochastic Average Gradient</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A Hybrid Convolutional Variational Autoencoder for Text Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanislau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ladder Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tapani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Empirical Risk Minimization of Graphical Model Parameters Given Approximate Inference, Decoding, and Model Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ropson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">VAE with a VampPrior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Conditional Image Generation with PixelCNN Decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M ;</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR, 2016. van den Oord</title>
		<meeting>ICLR, 2016. van den Oord</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of NIPS</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Introduction to Variational Methods for Graphical Models. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Topic Compositional Neural Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wenqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improved Variational Autoencoders for Text Modeling using Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05597</idno>
		<title level="m">Advances in Variational Inference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Towards Deeper Understanding of Variational Autoencoding Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shengjia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
