<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SwinTrack: A Simple and Strong Baseline for Transformer Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">South China Univ. of Tech</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Fan</surname></persName>
							<email>heng.fan@unt.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
							<email>zhipeng.zhang.cv@outlook.com</email>
							<affiliation key="aff3">
								<orgName type="department">DiDi Chuxing</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">South China Univ. of Tech</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hling@cs.stonybrook.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SwinTrack: A Simple and Strong Baseline for Transformer Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently Transformer has been largely explored in tracking and shown state-of-theart (SOTA) performance. However, existing efforts mainly focus on fusing and enhancing features generated by convolutional neural networks (CNNs). The potential of Transformer in representation learning remains under-explored. In this paper, we aim to further unleash the power of Transformer by proposing a simple yet efficient fully-attentional tracker, dubbed SwinTrack, within classic Siamese framework. In particular, both representation learning and feature fusion in SwinTrack leverage the Transformer architecture, enabling better feature interactions for tracking than pure CNN or hybrid CNN-Transformer frameworks. Besides, to further enhance robustness, we present a novel motion token that embeds historical target trajectory to improve tracking by providing temporal context. Our motion token is lightweight with negligible computation but brings clear gains. In our thorough experiments, SwinTrack exceeds existing approaches on multiple benchmarks. Particularly, on the challenging LaSOT, SwinTrack sets a new record with 0.713 SUC score. It also achieves SOTA results on other benchmarks. We expect SwinTrack to serve as a solid baseline for Transformer tracking and facilitate future research. Our codes and results are released at https://github.com/LitingLin/SwinTrack.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual tracking has seen considerable progress since deep learning. In particular, the recent Transformer <ref type="bibr" target="#b37">[38]</ref> has significantly pushed the state-of-the-art in tracking owing to its ability in modeling long-range dependencies. However, existing methods usually leverage Transformer for fusing and enhancing features generated from convolutional neural networks (CNNs), e.g., ResNet <ref type="bibr" target="#b17">[18]</ref>. The potential of exploiting Transformer for feature representation learning is largely under-explored.</p><p>Recently, Vision Transformer (ViT) <ref type="bibr" target="#b8">[9]</ref> has exhibited great potential in robust feature representation learning. Particularly, its extension Swin Transformer <ref type="bibr" target="#b27">[28]</ref> has achieved state-of-the-art (SOTA) results on multiple tasks. Taking inspiration from this, we argue, besides the feature fusion, the representation learning in tracking can also benefit from Transformer via attention. Thus motivated, we propose to develop a fully attentional tracking framework based on Siamese architecture. Specifically, both the feature representation learning and the feature fusion of template and search region are realized by Transformer. More concretely, we borrow the architecture of the powerful  <ref type="bibr" target="#b10">[11]</ref>. Our tracker (SwinTrack-B-384) sets a new record with 0.713 SUC score and still runs efficiently at around 45 fps. A lighter version (SwinTrack-T-224) achieves 0.672 SUC score and runs at around 96 fps, which is on par with existing SOTAs in accuracy but much faster. Best viewed in color for all figures.</p><p>Swin Transformer <ref type="bibr" target="#b27">[28]</ref> and adapt it to Siamese tracking. Note that, other Transformer architectures can be used. For feature fusion, we introduce a simple homogeneous concatenation-based fusion architecture, without a query-based decoder.</p><p>Moreover, taking into consideration that tracking is a temporal task, we propose a novel motion token to improve robustness. Inspired by that the target usually moves smoothly in a short period, motion token is represented by the historical target trajectory within a local temporal window. We incorporate the (single) motion token in the decoder of feature fusion to leverage motion information during tracking. Despite being conceptually simple, our motion token can effectively boost tracking performance, with negligible computation.</p><p>We name our framework SwinTrack. As a pure Transformer framework, SwinTrack enables better interactions inside the feature learning of template and search region and their fusion compared to pure CNN-based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> and hybrid CNN-Transformer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46]</ref> frameworks, leading to more robust performance (see <ref type="figure" target="#fig_0">Fig. 1</ref>). <ref type="figure">Fig. 2</ref> demonstrates the architecture of SwinTrack. We conduct extensive experiments on five large-scale benchmarks to verify the effectiveness of SwinTrack, including LaSOT <ref type="bibr" target="#b10">[11]</ref>, LaSOT ext <ref type="bibr" target="#b9">[10]</ref>, TrackingNet <ref type="bibr" target="#b32">[33]</ref>, GOT-10k <ref type="bibr" target="#b18">[19]</ref> and TNL2k <ref type="bibr" target="#b41">[42]</ref>. On all benchmarks, SwinTrack achieves promising results and meanwhile runs fast at 45 fps. In particular, on the challenging LaSOT, SwinTrack sets a new record of 71.3 SUC score, surpassing the strongest prior tracker <ref type="bibr" target="#b45">[46]</ref> (to date) by 3.1 absolute percentage points and crossing the 0.7 SUC threshold for the first time (see <ref type="figure" target="#fig_0">Fig. 1 again)</ref>. It also achieves 49.1 SUC, 84.0 SUC, 72.4 AO and 55.9 SUC scores on LaSOT ext , TrackingNet, GOT-10k and TNL2k respectively, which are better than or on par with state-of-the-arts (SoTAs). In addition, we provide a lighter version of SwinTrack that obtains comparable results to SoTAs but runs much faster at around 98 fps.</p><p>In summary, our contributions are as follows: (i) We propose SwinTrack, a simple and strong baseline for fully attentional tracking; (ii) We present a simple yet effective motion token, enabling the integration of rich motion context during tracking, further boosting the robustness of SwinTrack, with negligible computation; (iii) Our proposed SwinTrack achieves state-of-the-art performance on multiple benchmarks. We believe SwinTrack further shows the potential of Transformer and expect it to serve as a baseline for future research.  <ref type="figure">Figure 2</ref>: Architecture of SwinTrack, which contains three parts including Transformer-based feature representation extraction, Transformer-based feature fusion and prediction head. Our SwinTrack is a simple and neat tracking framework without complex designs such as multi-scale features or temporal template updating, yet demonstrating state-of-the-art performance. Best viewed in color. a fully convolutional Siamese network for tracking and shows a good balance between accuracy and speed. To improve Siamese tracking in handling scale variation, the work of <ref type="bibr" target="#b24">[25]</ref> incorporates the region proposal network (RPN) <ref type="bibr" target="#b33">[34]</ref> into the Siamese network and proposes the anchor-based tracker, showing higher accuracy with faster speed. Later, numerous extensions have been presented to improve Siamese tracking, including deeper backbone <ref type="bibr" target="#b23">[24]</ref>, multi-stage architecture <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, anchor-free Siamese trackers <ref type="bibr" target="#b51">[52]</ref>, deformable attention <ref type="bibr" target="#b47">[48]</ref>, to name a few.</p><p>Transformer in Vision. Transformer <ref type="bibr" target="#b37">[38]</ref> originates from natural language processing (NLP) for machine translation and has been introduced to vision recently and shows great potential. The work of <ref type="bibr" target="#b2">[3]</ref> first uses Transformer for object detection and achieved promising results. To explore the capability of Transformer in representation learning, the work of <ref type="bibr" target="#b8">[9]</ref> applies Transformer to construct backbone network, and the resulting Vision Transformer (ViT) attains excellent performance compared to convolutional networks while requiring fewer training resources, which encourages many extensions upon ViT <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b27">28]</ref>. Among them, the Swin Transformer <ref type="bibr" target="#b27">[28]</ref> has received extensive attention. It proposes a simple shifted window strategy to replace the fixed-patch method in ViT, which significantly improves efficiency and meanwhile demonstrates state-of-the-art results on multiple image tasks. Our work is inspired by Swin Transformer, but differently, we focus on the video task of visual tracking.</p><p>Transformer in Tracking. Inspired by the success in other fields, researchers have leveraged Transformer for tracking. The method of <ref type="bibr" target="#b4">[5]</ref> applies Transformer to enhance and fuse features in the Siamese tracking for improvement. The approach of <ref type="bibr" target="#b39">[40]</ref> uses Transformer to exploit temporal features to improve tracking robustness. The work of <ref type="bibr" target="#b45">[46]</ref> introduces a new transformer architecture dedicated to visual tracking, explores the Spatio-temporal Transformer by integrating the model updating operations into a Transformer module.</p><p>Our SwinTrack is related to but significantly different from the above Transformer-based trackers. Specifically, the aforementioned methods mainly apply Transformer to fuse convolutional features and belong to the hybrid CNN-Transformer architecture. Unlike them, SwinTrack is a pure Transformerbased tracking architecture where both representation learning and feature fusion are realized with Transformer, enabling the exploration of better features for robust tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tracking via Vision-Motion Transformer</head><p>We present SwinTrack, a vision-motion integrated Transformer for object tracking, in <ref type="figure">Fig. 2</ref>. The proposed framework contains three main components, i.e., the Swin-Transformer backbone for feature extraction, the encoder-decoder network for mixing vision-motion cues, and the head network for localizing targets. In the following sections, we first shortly describe the Swin backbone network, then elaborate on the proposed vision-motion encoder-decoder. Afterward, we give a discussion about our method and shortly describe the network head and training loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Swin-Transformer for Feature Extraction</head><p>The deep convolutional neural network has significantly improved the performance of trackers. Along with the advancement of trackers, the backbone network has evolved twice: AlexNet <ref type="bibr" target="#b21">[22]</ref> and ResNet <ref type="bibr" target="#b17">[18]</ref>. Swin-Transformer <ref type="bibr" target="#b27">[28]</ref>, in comparison to ResNet, can give a more compact feature representation and richer semantic information to assist succeeding networks in better localizing the target objects (demonstrate in the ablation study demonstrated in the ablation study), which is thus chosen for basic feature extraction in our model.</p><p>Our tracker, following Siamese tracking framework <ref type="bibr" target="#b0">[1]</ref>, requires a pair of image patches as inputs, i.e., template image z ? R Hz?Wz?3 and search region image x ? R Hx?Wx?3 . As in the typical Swin-Transformer procedure, template and search region images are divided to non-overlapped patches and sent to the network, which generates template tokens (dubbed T-tokens) ?(z) ? R Since there is no dimension projection in our model, C is the hidden dimension of the whole model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vision-Motion Representation Learning</head><p>The essential step for matching-based visual tracking is injecting the template information into the search region. In our framework, we adopt an encoder to fuse the features from the template and the search region, meanwhile, a decoder is arranged to achieve vision-motion representation learning, as illustrated in <ref type="figure">Fig. 2</ref>.</p><p>Encoder for fusing template and search tokens. The encoder contains a sequence of Transformer blocks where each consists of a multi-head self-attention (MSA) module and a feed-forward network (FFN). FFN contains a two-layers multi-layer perceptron (MLP), GELU activation layer is inserted after the first linear layer. Layer normalization (LN) is always conducted before every module (MSA and FFN). Residual connection is applied to MSA and FFN modules.</p><p>Before feeding the features into the encoder, the template and search region tokens are concatenated along spatial dimensions to generate a mixing representation f m . For each block, the MSA module computes self-attention over mixing union representation, which equals to separately conducting self-attention on T-tokens/S-tokens and meanwhile performing cross-attention between T-tokens and S-tokens, but more efficient. FFN refines the features generated by MSA. When the tokens get out of the encoder, a de-concatenation operation is arranged to decouple the template and search region tokens. The process of encoder can be expressed as:</p><formula xml:id="formula_0">f 1 m = Concat(?(z), ?(x))</formula><p>. . .</p><formula xml:id="formula_1">f l m = f l m + MSA(LN(f l m )) f l+1 m = f l m + FFN(LN(f l m )) . . . f L z , f L x = DeConcat(f L ),<label>(1)</label></formula><p>where l denotes the l-th layer and L denotes the number of blocks.</p><p>Decoder for fusing vision and motion information. Before describing the architecture of decoder, we first detail how to generate a motion token (dubbed M-token used to describe the process. Giving the point in the input image as (x i , y i ), the corresponding point in the search region as (x o , y o ), we can formulate the cropping process employed in pre-processing of the Siamese Tracker as</p><formula xml:id="formula_2">x o = (x i ? i x )s x + o x and y o = (y i ? i y )s y + o y , where (i x , i y )</formula><p>is the center of the cropping window in the input image, (s x , s y ) is the scaling factor, (o x , o y ) is the center of cropped and scaled window in the search region. We apply the same transformation on the sampled object trajectory to make the object trajectory invariant to the cropping, denotin? T = {? s(1) ,? s(2) , ...,? s(n) } as the result.</p><p>Then, to embed the transformed object trajectory into the network, we adopt four embedding matrices to embed the elements in box coordinates separately. We denotes the embedding matrix as W ? R (g+1)?d , g controls the embedding granularity of the object trajectory, d is the size of each embedding vector. The last entry of the embedding matrix is used as the padding vector, indicating an invalid state, like object absence or out of the search region. Thus, we normalize the sampled target object box coordinates in the range <ref type="bibr">[1, g]</ref>, and quantize to integers to get the index of embedding vector:T</p><formula xml:id="formula_3">= {? s(1) ,? s(2) , ...,? s(n) }, where? s(i) = [n(? x1 s(i) , w), n(? y1 s(i) , h), n(? x2 s(i) , w), n(? y2 s(i) , h)], n(o, l) = o l ? g if valid, g + 1 else,<label>(3)</label></formula><p>where (w, h) is the size of search region feature map.</p><p>Finally, the motion token E motion ? R 1?d is given by a concatenation of all box coordinate embedding of the sampled object trajectory. FLOPs is negligible because the construction of motion token is just a composition of embedding lookups and token concatenation. </p><formula xml:id="formula_4">Concat(E motion , f L z , f L x )</formula><p>. The decoder is akin to a layer in the encoder, except that the correlation between the template tokens and the search tokens is dropped since we do not need to update the features from the template image in the last layer. The process of the decoder is formulated as:</p><formula xml:id="formula_5">f D m = Concat(E motion , f L z , f L x ) f vm = f L x + MCA(LN(f L x ), LN(f D m )) f vm = f vm + FFN(LN(f vm )).</formula><p>(4) f vm will feed to the head network to generate a classification response map and a bounding box regression map.</p><p>Positional encoding. Transformer requires a positional encoding to identify the position of the current processing token <ref type="bibr" target="#b37">[38]</ref> because the self-attention module is permutation-invariance. We adopt the untied positional encoding <ref type="bibr" target="#b19">[20]</ref> as our positional encoding method. The untied positional encoding enhances the expressiveness of the model through untie the positional embeddings from token embeddings with an isolated positional embedding matrix. It also considers the case of special tokens, like the motion token in this paper. We generalize the untied positional encoding to multidimensions multi-sources data to comply with concatenated-based fusion in our tracker. See the appendix for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Why concatenated attention? To simplify the description, we call the method described above concatenation-based fusion. To fuse and process features from multiple sources, it is intuitive to perform self-attention on the feature from each source separately and then compute cross-attention across features from different sources. We call this method cross-attention-based fusion. Transformer makes fewer assumptions about the spatial structure of data, which provides great modeling flexibility.</p><p>In comparison to cross-attention-based fusion, concatenation-based fusion can save computation cost through operation sharing and reduce model parameters through weight sharing. From the perspective of metric learning, weight sharing is an essential design to ensure the metric between two branches of data is symmetric. Through concatenation-based fusion, we implement this property not only in the feature extraction stage but also in the feature fusion stage. In general, concatenation-based fusion improves both efficiency and performance.</p><p>Why not window-based self/cross-attention? Since we select stage 3 of the Swin-Transformer as the output, the number of tokens involved is significantly reduced, window-based attention cannot save too many FLOPs. Furthermore, considering the extra latency introduced by the window partition and window reverse operations, window-based attention may even be the slower one.</p><p>Why not a query-based decoder? Derivated from vanilla Transformer decoder, many transformerbased models in vision tasks leverage a learnable query to extract the desired objective features from the encoder, like object queries in <ref type="bibr" target="#b2">[3]</ref>, target query in <ref type="bibr" target="#b45">[46]</ref>. However, in our experiment, a query-based decoder suffers from slow convergence and inferior performance. Most Siamese trackers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b15">16]</ref> formulate tracking as a foreground-background classification problem, which can better exploit the background information. The vanilla Transformer decoder is a generative model, the generative approaches are considered not suitable for the classification tasks. In another aspect, learning a general target query for any kind of object might cause a bottleneck. In terms of vanilla Transformer encoder-decoder architecture, SwinTrack is an "encoder" only model. Furthermore, quite a little domain knowledge can be easily applied on a classic Siamese tracker to improve the performance, like introducing the smooth movement assumption by using Hanning penalty window on the response map.</p><p>Are other forms of motion token feasible? Other forms to construct motion token are possible, such as constructing motion token by summing up the past box coordinate embeddings or representing past object trajectories by one token per box. In our early experiments, we find that the proposed motion token is more effective with the best performance. Summing up the past box coordinate embeddings may result in over-parameterization on the coordinate embeddings. While adding temporal motion representation along with visual features to the single-layer decoder in a multi-token form is ineffective, precise temporal modeling may be required in this form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Head and Loss</head><p>Head. The head network is split into two branches: classification and bounding box regression. Each of them is a three-layer perceptron. And both of them receives the feature map from the decoder as input to predict the classification response map r cls ? R (Hx?Wx)?1 and bounding box regression map r reg ? R (Hx?Wx)?4 , respectively.</p><p>Classification loss. In classification branch, we employ the IoU-aware classification score as the training target and the varifocal loss <ref type="bibr" target="#b49">[50]</ref> as the training loss function. IoU-aware design has been very popular recently, but most works consider IoU prediction as an auxiliary branch to assist classification or bounding box regression <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">44]</ref>. To remove the gap between different prediction branches, <ref type="bibr" target="#b49">[50]</ref> and <ref type="bibr" target="#b25">[26]</ref> replace the hard classification target from the ground-truth value, (i.e., 1 for positive samples, 0 for negative samples), to the IoU between the predicted bounding box and the ground-truth one, which is named the IoU-aware classification score (IACS). IACS can help the model select a more accurate bounding box prediction candidate from the pool by trying to predict the quality of the bounding box prediction in another branch at the same position. Along with the IACS, the varifocal loss was proposed in <ref type="bibr" target="#b49">[50]</ref> to help the IACS approach outperform other IoU-aware designs.</p><p>The classification loss can be formulated as:</p><formula xml:id="formula_6">L cls = L VFL (p, IoU(b,b)),<label>(5)</label></formula><p>where p denotes the predicted IACS, b denotes the predicted bounding box, andb denotes the ground-truth bounding box.</p><p>Regression loss. For bounding box regression, we employ the generalized IoU loss <ref type="bibr" target="#b34">[35]</ref>. The regression loss function can be formulated as:</p><formula xml:id="formula_7">L reg = j 1 {IoU(bj ,b)&gt;0} [pL GIoU (b j ,b)].<label>(6)</label></formula><p>The GIoU loss is weighted by p to emphasize the high classification score samples. The training signals from the negative samples are ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>Model. We design two variants of SwinTrack with different configurations as follows:</p><p>? where C and N are the channel number of the hidden layers in the first stage of Swin Transformer and the number of encoder blocks in feature fusion, respectively. In all variants, we use the output after the third stage of Swin Transformer for feature extraction. Thus, the backbone stride s is 16.</p><p>For motion token, the number of sampled object trajectory n is set to 16, the fixed sampling interval ? is set to 15. If the frame rate of the video sequence is available, the sampling interval is adjusted according to the frame rate. Suppose the frame rate is f, the new sampling interval is getting by ? 30 f, 30 fps is the standard frame rate we assumed. g, which controls the embedding granularity, is set to the same size as the search region feature map, like 14 for SwinTrack-T-224, and 24 for SwinTrack-B-384. For the model for GOT-10k sequences, n is set to 8, ? is set to 8, and no frame rate adjustment is applied.</p><p>Training. We train SwinTrack using the training splits of LaSOT <ref type="bibr" target="#b10">[11]</ref>, TrackingNet <ref type="bibr" target="#b32">[33]</ref>, GOT-10k <ref type="bibr" target="#b18">[19]</ref> (1,000 videos are removed following <ref type="bibr" target="#b45">[46]</ref> for fair comparison) and COCO 2017 <ref type="bibr" target="#b26">[27]</ref>. In addition, we report the results of SwinTrack-T-224 and SwinTrack-B-384 with GOT-10k training split only to follow the protocol described in <ref type="bibr" target="#b18">[19]</ref>.</p><p>The model is optimized with AdamW <ref type="bibr" target="#b28">[29]</ref>, with a learning rate of 5e-4, and a weight decay of 1e-4. The learning rate of the backbone is set to 5e-5. We train the network on 8 NVIDIA V100 GPUs for 300 epochs with 131,072 samples per epoch. The learning rate is dropped by a factor of 10 after 210 epochs. A 3-epoch linear warmup is applied to stabilize the training process. DropPath <ref type="bibr" target="#b22">[23]</ref> is applied on the backbone and the encoder with a rate of 0.1. For the models trained for the GOT-10k evaluation protocol, to prevent over-fitting, the training epoch is set to 150, and the learning rate is dropped after 120 epochs.</p><p>For the motion token, the object trajectory for the Siamese training pair is generated with the method described above. The frames that object annotated as absent or out of the video sequence are marked as invalid, the corresponding box coordinates set to ??. Since the coarse granularity of the coordinate embedding in our setting is already can be seen as an augmentation of historical object trajectory, no additional data augmentation is applied.</p><p>Inference. We follow the common procedures for Siamese network-based tracking <ref type="bibr" target="#b0">[1]</ref>. The template image is cropped from the first frame of the video sequence. The target object is in the center of the image with a background area factor of 2. The search region is cropped from the current tracking frame, and the image center is the target center position predicted in the previous frame. The background area factor for the search region is 4.</p><p>Our SwinTrack takes the template image and search region as inputs and output classification map r cls and regression map r reg . To utilize positional prior in tracking, we apply hanning window penalty on r cls , and the final classification map r cls is obtained via r cls = (1 ? ?) ? r cls + ? ? h, where ? is the weight parameter and h is the Hanning window with the same size as r cls . The target position is determined by the largest value in r cls and the scale is estimated based on the corresponding regression results in r reg .</p><p>For the motion token, the predicted confidence score and bounding box are collected on the fly. A confidence threshold ? conf is applied, if the confidence score given by the classification branch of the head is lower than the threshold, the target object in the current frame is marked as lost by setting the collected bounding box to ??. ? conf is set to 0.4 for LaSOT, the rests are set to 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons to State-of-the-arts</head><p>We conduct experiments and compare SwinTrack with SoTA trackers on five benchmarks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Experiment</head><p>Comparison with ResNet backbone. We compare the Swin-Transformer backbone with popular ResNet-50 <ref type="bibr" target="#b17">[18]</ref>. As shown in Tab. 3 ( vs. ). The Swin Transformer backbone significantly boosts the performance by 2.5% SUC score in LaSOT, 5.1% SUC score in LaSOT ext . The result shows that the strong appearance modeling capability provided by the Swin Transformer plays a crucial role.</p><p>Feature fusion. As displayed in Tab. 3 ( vs. ), compared with the concatenation-based fusion, the cross attention-based fusion runs at a slower speed, occupies much more memory, and also has an inferior performance on all datasets. Slower speed can be due to the latency brought by the extra operations. The parameter-sharing strategy not only just reduces the number of parameters but also benefits metric learning.</p><p>Comparison with the query-based decoder. Queries is commonly adopted in the decoder of Transformer network in vision tasks, e.g. object query <ref type="bibr" target="#b2">[3]</ref> and target query <ref type="bibr" target="#b45">[46]</ref>. Nevertheless, our empirical results in Tab. 3 ( vs. ) show that a target query-based decoder degrades the tracking performance on all benchmarks, even with 2? training pairs. As discussed, one possible reason is the generative model is not suitable for classification. Besides, learning a general target query for any kind of object may also be difficult.</p><p>Position encoding. We compare the united positional encoding used in SwinTrack and the original absolute position encoding in Transformer <ref type="bibr" target="#b37">[38]</ref>. Notice, We make a little modification to the original absolute position encoding: Except for the 2D embedding, the index of token source (e.g. 1 for the tokens from the template patch, 2 for the tokens from the search region patch) is also embedded. As 2 Multiply-accumulate operation <ref type="bibr" target="#b2">3</ref> The GOT-10k results in this column are trained with full training datasets. Loss function. From Tab. 3 ( vs. ), we observe that the model trained with varifocal loss significantly outperforms the one with binary cross entropy (BCE) loss without loss of efficiency. This result indicates that the varifocal loss can assist the classification branch of the head to generate an IoU-aware response map, and thus help the tracker to improve the tracking performance.</p><p>Post processing. One may wonder with highly discriminative Transformer architecture and IoUaware classification loss does the hanning penalty window is still functional, which introduces a strong smooth movement assumption. In the experiments, we remove the hanning penalty window in post-processing, as shown in Tab. 3 ( vs. ), the performance is dropped by 1.0 SUC for LaSOT, 1.3 AO for GOT-10k in absolute percentage, and less than 1% in the SUC metric of other datasets. This suggests that the strong smooth movement assumption is still applicable for our tracker. But compared with the former Transformer-based tracker <ref type="bibr" target="#b4">[5]</ref>, the performance gap between with and without penalty window post-processing is narrowing.</p><p>Effectiveness of motion token. We study the effectiveness of the motion token by conducting comparison experiments. As shown in Tab. 4 ( vs. and vs. ), the models with motion token outperforms the models without motion token on all datasets, especially on LaSOT ext and GOT-10k.</p><p>The results indicate that the motion token can assist the tracker to handle hard similar distractors in LaSOT ext and stabilize the short-term tracking like the sequences in GOT-10k test set. We also study whether the effectiveness of the motion token is simply from the extra embedding vector. We set up an experiment as in Tab. 4 (), which replaces the motion token with a learnable embedding token.</p><p>The result shows that the extra embedding vector has negative impacts indicating the effectiveness of the embedding of object trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present SwinTrack, a simple and strong baseline for Transformer tracking. In SwinTrack, both representation learning and feature fusion are implemented with the attention mechanism. Extensive experiments demonstrate the effectiveness of such architecture. Besides, we propose the motion token to enhance the robustness of the tracker by providing the historical object trajectory, showing the flexibility of the Transformer model in architectural design. With the power of sequence-to-sequence model architecture, a context-rich tracker is possible, and more contextual cues can be incorporated. Finally, We hope this work can inspire and facilitate future research.</p><p>to the multi-head version, the positional embedding p i is shared across different heads, while U Q and U K are different for each head.</p><p>Relative positional bias. According to <ref type="bibr" target="#b35">[36]</ref>, relative positional encoding is a necessary supplement to absolute positional encoding. In <ref type="bibr" target="#b19">[20]</ref>, a relative positional encoding is applied by adding a relative positional bias to equation <ref type="formula" target="#formula_1">(12)</ref>:</p><formula xml:id="formula_8">? ij = 1 ? 2d (x l i W Q,l )(x l j W K,l ) T + 1 ? 2d (p i U Q )(p j U K ) T + b j?i ,<label>(13)</label></formula><p>where for each j ? i, b j?i is a learnable scalar. The relative positional bias is also shared across layers. When extending to the multi-head version, b j?i is different for each head.</p><p>Generalize to multiple dimensions. Before working with our tracker's encoder and decoder network, we need to extend the untied positional encoding to a multi-dimensional version. One straightforward method is allocating a positional embedding matrix for every dimension and summing up all embedding vectors from different dimensions at the corresponding index to represent the final embedding vector. Together with relative positional bias, for an n-dimensional case, we have:</p><formula xml:id="formula_9">? ij . . . n ,mn . . . n = 1 ? 2d (x ij . . . n W Q )(x mn . . . n W K ) T + 1 ? 2d [(p 1 i + p 2 j + . . . ) n U Q ][(p 1 m + p 2 n + . . . ) n U K ] T + b m ? i, n ? j, . . . n .<label>(14)</label></formula><p>Generalize to concatenation-based fusion. In order to work with concatenation-based fusion, the untied absolute positional encoding is also concatenated to match the real position, the indexing tuple of relative positional bias now appends with a pair of indices to reflect the origination of query and key involved currently.</p><p>Take l-th layer in the encoder as the example:</p><formula xml:id="formula_10">? ij,mn,g,h = 1 ? 2d (x l ij,g W Q,l )(x l mn,h W K,l ) T + 1 ? 2d [(p 1 i,g + p 2 j,g )U Q g ][(p 1 m,h + p 2 n,h )U K h ] T + b m?i,n?j,g,h ,<label>(15)</label></formula><p>where g and h are the index of the origination of query and key respectively, for instance, 1 for the tokens from the template image, 2 for the tokens from the search image. The form in the decoder is similar, except that g is fixed. In our implementation, the parameters of untied positional encoding are shared inside the encoder and the decoder, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The Effect of Pre-training Datasets</head><p>The two variants of our tracker, SwinTrack-T-224 and SwinTrack-B-384 are using different pretraining datasets, which are derived from the settings from Swin Transformer <ref type="bibr" target="#b27">[28]</ref>. Specifically, SwinTrack-T-224 adopts ImageNet-1k and SwinTrack-B-384 adopts ImageNet-22k.</p><p>To analyze the effect of different pre-training datasets, we conduct an experiment on the performance of our tracker with different pre-training datasets. Other than the pre-training datasets, The experiment follows the same settings in the ablation study in the paper, the motion token is not used and the results on GOT-10k are trained on the full datasets as described in the paper. From Tab. 5, we can observe that, for smaller model SwinTrack-T-224 (23M # parameters), pre-training on ImageNet-22k brings small improvements on LaSOT (+0.6%) and TrackingNet (+0.4%) but degrades the performance on <ref type="table">Table 5</ref>: The effect of Imagenet-22k pre-training. The results are following the settings in the ablation study in the paper (motion token is not used and the result on GOT-10k is trained on the full dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trackers</head><p>Pre-training LaSOT <ref type="bibr" target="#b10">[11]</ref> LaSOT ext <ref type="bibr" target="#b9">[10]</ref> TrackingNet <ref type="bibr" target="#b32">[33]</ref> GOT-10k <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Comparison with Newly Released Transformer-based Trackers</head><p>We compare our tracker with some newly released Transformer-based trackers, including STARK <ref type="bibr" target="#b45">[46]</ref>, SBT <ref type="bibr" target="#b42">[43]</ref>, ToMP <ref type="bibr" target="#b29">[30]</ref>, MixFormer <ref type="bibr" target="#b5">[6]</ref>, AiATrack <ref type="bibr" target="#b14">[15]</ref>, Unicorn <ref type="bibr" target="#b44">[45]</ref>, OSTrack <ref type="bibr" target="#b46">[47]</ref> in Tab. 6 in four challenging benchmarks. The result shows our tracker is still competitive. <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 4</ref> show the success plot and the precision plot respectively. The comparison includes our SwinTrack-T-224, our SwinTrack-B-384, TransT <ref type="bibr" target="#b4">[5]</ref>, STARK <ref type="bibr" target="#b45">[46]</ref>, MixFormer <ref type="bibr" target="#b5">[6]</ref>, AiATrack <ref type="bibr" target="#b14">[15]</ref> and ToMP <ref type="bibr" target="#b29">[30]</ref>. Our tracker obtained the best performance on this benchmark. By looking into the curves of the figures, there is a significant advantage in the bounding box accuracy compared with other trackers due to our fully attentional architecture.</p><p>The success AUC score under different attributes of LaSOT <ref type="bibr" target="#b10">[11]</ref> Test set in shown in <ref type="figure">Fig. 5</ref>. <ref type="figure">Fig. 5</ref> indicates that our tracker has no obvious shortcomings except the viewpoint change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Results on UAV123 and VOT Benchmark</head><p>In this section, we report the performance of the tracker on three additional benchmarks, including UAV123 <ref type="bibr" target="#b31">[32]</ref>, VOT2020 and VOT-STB2022 <ref type="bibr" target="#b20">[21]</ref>.</p><p>UAV123 <ref type="bibr" target="#b31">[32]</ref> is an aerial video dataset and benchmark for low-altitude UAV target tracking, containing 123 video sequences. Our tracker is on par with the state-of-the-art, AiATrack <ref type="bibr" target="#b14">[15]</ref>, on this benchmark. The results are shown in Tab. 7.</p><p>Finally, we evaluate our tracker on the two versions of the VOT Challenge: VOT2020 and VOT-STB2022. The VOT2020 dataset contains 60 videos with segmentation masks annotated. Since our tracker is a bounding box only method, we compare the results with the trackers that produce the bounding boxes as well. The result in Tab.8 shows that SwinTrack-T-224 has a better performance than the larger SwinTrack-B-384 on this benchmark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Quantitative Analysis of the Effectiveness of Motion Token</head><p>To give a further analysis of the effectiveness of motion token, we provide the success plot ( <ref type="figure">Fig. 6</ref>) and precision plot <ref type="figure">(Fig. 7)</ref> on LaSOT test set, and the success AUC score under different attributes of LaSOT test set in <ref type="figure">Fig. 8</ref>. The success plot and the precision plot show that the motion token improves the performance of the trackers by boosting robustness. While the <ref type="figure">Fig. 8</ref> further points out that the motion token can assist the tracker to recover from a failure state when the vision features are not reliable like an object is getting out of view or fully occluded by other objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Response Visualization for Qualitative Analysis</head><p>We provide the heatmap visualization of the response map generated by the IoU-aware classification branch of the head in our SwinTrack-B-384 in <ref type="figure" target="#fig_4">Fig. 9</ref>. The visualized sequences are from LaSOT ext <ref type="bibr" target="#b9">[10]</ref>, with challenges include fast motion, full occlusion, hard distractor, etc. The results demonstrate the great discriminative power of our tracker. Many trackers will show a multi-peak on the response map when the target object is occluded or multiple similar objects exist. With the vision-motion integrated Transformer architecture, our tracker eases such phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Failure Case</head><p>We show some typical failure cases of our tracker (SwinTrack-B-384 on LaSOT ext <ref type="bibr" target="#b9">[10]</ref> and VOT-STB2022 <ref type="bibr" target="#b20">[21]</ref>) in <ref type="figure" target="#fig_0">Fig. 10</ref>. The first case suffers from a mixture of low resolution, fast motion, and background clutter. The second case suffers from a fast occlusion by a distractor. The third case suffers from the non-semantic target.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison on LaSOT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>search region tokens (dubbed S-tokens) ?(x) ? R Hx s Wx s ?C . s is the stride of the backbone network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The decoder consists of a multi-head cross-attention(MCA) module and a feed-forward network(FFN). The decoder takes the outputs from the encoder and the motion token as input, generating the final vision-motion representation f vm ? RHx s ? Wx s ?C of by computing cross-attention over f L x and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :Figure 7 :Figure 8 :</head><label>5678</label><figDesc>Comparison to the state-of-the-art trackers using success (SUC) AUC score under different attributes of LaSOT<ref type="bibr" target="#b10">[11]</ref> Test set. Success (SUC) AUC score on LaSOT<ref type="bibr" target="#b10">[11]</ref> Test set assessing the effectiveness of the motion token. Precision (PRE) AUC score on LaSOT<ref type="bibr" target="#b10">[11]</ref> Test set assessing the effectiveness of the motion token. -224-NoMToken [70.6] SwinTrack-T-224 [70.8] SwinTrack-B-384-NoMToken [75.3] SwinTrack-B-384 [76.5] Success (SUC) AUC score under different attributes of LaSOT [11] Test set assessing the effectiveness of the motion token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>SwinTrack-T-224-NoMTokenSwinTrack-B-384-NoMTokenSwinTrack-T-224SwinTrack-B-384 Heatmap visualization of the tracking response map of our SwinTrack-B-384 on LaSOT ext<ref type="bibr" target="#b9">[10]</ref>. The odd rows visualize the search region patches with ground-truth bounding box (in red rectangles). The even rows visualize the search region patches blended with the heatmap visualization of the response map. The sequences and challenges involved: atv-10 (POC, ROT, VC, SV, LR, ARC), wingsuit-10 (CM, BC, VC, SV, FOC, LR, ARC), rhino-9 (DEF, SV, ARC) and misc-3 (POC, MB, ROT, BC, SV, FOC, FM, LR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Heatmap visualization of the failure cases. The organizational form is the same asFig. 9. The sequences and challenges involved: badminton-3 in LaSOT ext (MB, SV, FOC, FM, OV, LR, ARC), skatingshoe-2 in LaSOT ext (POC, MB, ROT, BC, SV, FOC, FM, LR, ARC) and conduction1 (non-semantic target) in VOT-STB2022.<ref type="bibr" target="#b3">4</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Motion token is the embedding of the historical trajectory of the target object. The past object trajectory is represented as a set of target object box coordinates, T = {o 1 , o 2 , ..., o t }, where t represents the frame index, o is the bounding box of target object. o is defined by the top-left and bottom-right corners of the target object, denotes as o t = (o x1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Experiments and comparisons on five benchmarks: LaSOT, LaSOT ext , TrackingNet, GOT-10k and TNL2k.</figDesc><table><row><cell>Tracker</cell><cell cols="6">LaSOT [11] LaSOT ext [10] TrackingNet [33] SUC P SUC P SUC P</cell><cell cols="4">GOT-10k [19] AO SR 0.5 SR 0.75 SUC TNL2k [42] P</cell></row><row><cell cols="4">C-RPN [12] 45.5 44.3 27.5</cell><cell>32.0</cell><cell>66.9</cell><cell>61.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">SiamPRN++ [24] 49.6 49.1 34.0</cell><cell>39.6</cell><cell>73.3</cell><cell>69.4</cell><cell cols="2">51.7 61.6</cell><cell>32.5</cell><cell>41.3 41.2</cell></row><row><cell cols="3">Ocean [52] 56.0 56.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">61.1 72.1</cell><cell>47.3</cell><cell>38.4 37.7</cell></row><row><cell cols="4">DiMP [2] 56.9 56.7 39.2</cell><cell>45.1</cell><cell>74.0</cell><cell>68.7</cell><cell cols="2">61.1 71.7</cell><cell>49.2</cell><cell>44.7 43.4</cell></row><row><cell cols="4">LTMU [7] 57.2 57.2 41.4</cell><cell>47.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.5 47.3</cell></row><row><cell cols="2">SiamR-CNN [39] 64.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.2</cell><cell>80.0</cell><cell cols="2">64.9 72.8</cell><cell>59.7</cell><cell>52.3 52.8</cell></row><row><cell cols="3">STMTrack [14] 60.6 63.3</cell><cell>-</cell><cell>-</cell><cell>80.3</cell><cell>76.7</cell><cell cols="2">64.2 73.7</cell><cell>57.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">AutoMatch [51] 58.3 59.9 37.6</cell><cell>43.0</cell><cell>76.0</cell><cell>72.6</cell><cell cols="2">65.2 76.6</cell><cell>54.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">TrDiMP [40] 63.9 61.4</cell><cell>-</cell><cell>-</cell><cell>78.4</cell><cell>73.1</cell><cell cols="2">67.1 77.7</cell><cell>58.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">TransT [5] 64.9 69.0</cell><cell>-</cell><cell>-</cell><cell>81.4</cell><cell>80.3</cell><cell cols="2">67.1 76.8</cell><cell>60.9</cell><cell>51.0</cell><cell>-</cell></row><row><cell cols="2">STARK [46] 67.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.0</cell><cell>-</cell><cell cols="2">68.8 78.1</cell><cell>64.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">KeepTrack [31] 67.1 70.2 48.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">SwinTrack-T-224 67.2 70.8 47.6</cell><cell>53.9</cell><cell>81.1</cell><cell>78.4</cell><cell cols="2">71.3 81.9</cell><cell>64.5</cell><cell>53.0 53.2</cell></row><row><cell cols="4">SwinTrack-B-384 71.3 76.5 49.1</cell><cell>55.6</cell><cell>84.0</cell><cell>82.8</cell><cell cols="2">72.4 80.5</cell><cell>67.8</cell><cell>55.9 57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison on running speed and # parameters with other Transformer-based trackers.</figDesc><table><row><cell cols="4">Tracker Speed (fps) MACs 2 (G) Params (M)</cell></row><row><cell>TrDiMP [40]</cell><cell>26</cell><cell>-</cell><cell>-</cell></row><row><cell>TransT [5]</cell><cell>50</cell><cell>-</cell><cell>23</cell></row><row><cell>STARK-ST50 [46]</cell><cell>42</cell><cell>10.9</cell><cell>24</cell></row><row><cell>STARK-ST101 [46]</cell><cell>32</cell><cell>18.5</cell><cell>42</cell></row><row><cell>SwinTrack-T-224</cell><cell>98</cell><cell>6.4</cell><cell>23</cell></row><row><cell>SwinTrack-B-384</cell><cell>45</cell><cell>69.7</cell><cell>91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments of SwinTrack on four benchmarks. The experiments are conducted on SwinTrack-T-224 without the motion token. : baseline method, i.e., SwinTrack-T-224 without motion token; : replacing Transformer backbone in the baseline method with ResNet-50; : replacing our feature fusion with cross attention-based fusion in the baseline method; : replacing the decoder in baseline with a target query-based; : replacing united positional encoding with absolute sine position encoding in the baseline method; : replacing the IoU-aware classification loss with the plain binary cross entropy loss; : removing the Hanning penalty window in the baseline</figDesc><table><row><cell>method inference.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LaSOT</cell><cell>LaSOT ext</cell><cell>TrackingNet</cell><cell>GOT-10k 3</cell><cell>Speed</cell><cell>Params</cell></row><row><cell>SUC (%)</cell><cell>SUC (%)</cell><cell>SUC (%)</cell><cell>mAO (%)</cell><cell>fps</cell><cell>M</cell></row><row><cell>66.7</cell><cell>46.9</cell><cell>80.8</cell><cell>70.9</cell><cell>98</cell><cell>22.7</cell></row><row><cell>64.2</cell><cell>41.8</cell><cell>79.5</cell><cell>68.2</cell><cell>121</cell><cell>20.0</cell></row><row><cell>66.6</cell><cell>45.4</cell><cell>80.2</cell><cell>69.3</cell><cell>72</cell><cell>34.6</cell></row><row><cell>66.6</cell><cell>43.2</cell><cell>79.6</cell><cell>69.0</cell><cell>91</cell><cell>25.3</cell></row><row><cell>65.7</cell><cell>45.0</cell><cell>80.0</cell><cell>70.0</cell><cell>103</cell><cell>21.6</cell></row><row><cell>66.2</cell><cell>46.7</cell><cell>79.4</cell><cell>68.2</cell><cell>98</cell><cell>22.7</cell></row><row><cell>65.7</cell><cell>46.0</cell><cell>80.0</cell><cell>69.6</cell><cell>98</cell><cell>22.7</cell></row><row><cell cols="6">STARK-ST50 with 32 fps and 42 fps, SwinTrack-T-224 is 3? and 2? faster. Despite using a larger</cell></row><row><cell cols="6">model, our SwinTrack-B-384 is still faster than STARK-ST101 and STARK-ST50.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments on our proposed motion token on the tracking performance on four benchmarks. The experiments are conducted on SwinTrack-T-224. : SwinTrack-T-224; : SwinTrack-B-384; : SwinTrack-T-224 without motion token; : SwinTrack-B-384 without motion token; : replacing the motion token in SwinTrack-T-224 with a learnable embedding token.</figDesc><table><row><cell>LaSOT</cell><cell>LaSOT ext</cell><cell>TrackingNet</cell><cell>GOT-10k</cell><cell>Speed</cell></row><row><cell>SUC (%)</cell><cell>SUC (%)</cell><cell>SUC (%)</cell><cell>mAO (%)</cell><cell>fps</cell></row><row><cell>67.2</cell><cell>47.6</cell><cell>81.1</cell><cell>71.3</cell><cell>96</cell></row><row><cell>71.3</cell><cell>49.1</cell><cell>84.0</cell><cell>72.4</cell><cell>45</cell></row><row><cell>66.7</cell><cell>47.0</cell><cell>80.8</cell><cell>70.0</cell><cell>98</cell></row><row><cell>70.2</cell><cell>48.5</cell><cell>84.0</cell><cell>70.7</cell><cell>45</cell></row><row><cell>66.3</cell><cell>45.2</cell><cell>81.2</cell><cell>70.0</cell><cell>96</cell></row><row><cell cols="5">shown in Tab. 3 ( vs. ), our method with united positional encoding obtains improvements with</cell></row><row><cell cols="5">0.8-1.9 absolute percentage points on the benchmarks with negligible loss in speed (98 vs. 103).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons with newly released Transformer-based Trackers on four benchmarks: LaSOT, LaSOT ext , TrackingNet and GOT-10k.</figDesc><table><row><cell>Tracker Pre-training</cell><cell cols="6">LaSOT [11] LaSOT ext [10] TrackingNet [33] SUC P SUC P SUC P</cell><cell cols="3">GOT-10k [19] AO SR 0.5 SR 0.75</cell></row><row><cell>STARK [46] ImageNet-1k</cell><cell>67.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.0</cell><cell>-</cell><cell cols="2">68.8 78.1</cell><cell>64.1</cell></row><row><cell>SBT [43] ImageNet-1k</cell><cell cols="2">66.7 71.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">70.4 80.8</cell><cell>64.7</cell></row><row><cell>ToMP [30] ImageNet-1k</cell><cell cols="3">68.5 73.5 45.9</cell><cell>-</cell><cell>81.5</cell><cell>78.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">MixFormer [6] ImageNet-22k 70.1 76.3</cell><cell>-</cell><cell>-</cell><cell>83.9</cell><cell>83.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AiATrack [15] ImageNet-1k</cell><cell cols="3">69.0 73.8 47.7</cell><cell>55.4</cell><cell>82.7</cell><cell>80.4</cell><cell cols="2">69.6 80.0</cell><cell>63.2</cell></row><row><cell>Unicorn [45] ImageNet-1k</cell><cell cols="2">68.5 74.1</cell><cell>-</cell><cell>-</cell><cell>83.0</cell><cell>82.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OSTrack [47] MAE [17]</cell><cell cols="3">71.1 77.6 50.5</cell><cell>57.6</cell><cell>83.9</cell><cell>83.2</cell><cell cols="2">73.7 83.2</cell><cell>70.8</cell></row><row><cell>SwinTrack-T-224 ImageNet-1k</cell><cell cols="3">67.2 70.8 47.6</cell><cell>53.9</cell><cell>81.1</cell><cell>78.4</cell><cell cols="2">71.3 81.9</cell><cell>64.5</cell></row><row><cell cols="4">SwinTrack-B-384 ImageNet-22k 71.3 76.5 49.1</cell><cell>55.6</cell><cell>84.0</cell><cell>82.8</cell><cell cols="2">72.4 80.5</cell><cell>67.8</cell></row><row><cell cols="10">GOT-10k (-1.4%). For larger model SwinTrack-B-384 (91M # parameters), pre-training on ImageNet-</cell></row><row><cell cols="10">22k shows significant performance gains on LaSOT (+2.2%) and GOT-10k (+3.0%) but slightly</cell></row><row><cell cols="10">degrades the result on TrackingNet (-0.6%). On LaSOT ext , ImageNet-22k shows a performance</cell></row><row><cell cols="10">degradation on smaller model SwinTrack-T-224 (-0.9%) and brings small improvements on larger</cell></row><row><cell cols="2">model SwinTrack-B-384 (+0.2%).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison to the state-of-the-arts on UAV123<ref type="bibr" target="#b31">[32]</ref> benchmark.</figDesc><table><row><cell></cell><cell>Ocean</cell><cell>DiMP</cell><cell>TransT</cell><cell>ToMP</cell><cell>MixFormer</cell><cell>AiATrack</cell><cell>SwinTrack</cell><cell>SwinTrack</cell></row><row><cell></cell><cell>[52]</cell><cell>[2]</cell><cell>[5]</cell><cell>50[30]</cell><cell>22k[6]</cell><cell>[15]</cell><cell>T-224</cell><cell>B-384</cell></row><row><cell>AUC (%)</cell><cell>62.1</cell><cell>65.3</cell><cell>69.1</cell><cell>69.0</cell><cell>70.4</cell><cell>70.6</cell><cell>68.8</cell><cell>70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comparison to the state-of-the-art bounding box only methods on VOT2020ST<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table><row><cell></cell><cell>ATOM</cell><cell>DiMP</cell><cell>STARK</cell><cell>STARK</cell><cell>ToMP</cell><cell>ToMP</cell><cell>SwinTrack</cell><cell>SwinTrack</cell></row><row><cell></cell><cell>[8]</cell><cell>[2]</cell><cell>50[46]</cell><cell>101[46]</cell><cell>50[30]</cell><cell>101[30]</cell><cell>T-224</cell><cell>B-384</cell></row><row><cell>EAO</cell><cell>0.271</cell><cell>0.274</cell><cell>0.308</cell><cell>0.303</cell><cell>0.297</cell><cell>0.309</cell><cell>0.302</cell><cell>0.283</cell></row><row><cell>Accuracy</cell><cell>0.462</cell><cell>0.457</cell><cell>0.478</cell><cell>0.481</cell><cell>0.453</cell><cell>0.453</cell><cell>0.471</cell><cell>0.472</cell></row><row><cell cols="2">Robustness 0.734</cell><cell>0.734</cell><cell>0.799</cell><cell>0.775</cell><cell>0.789</cell><cell>0.814</cell><cell>0.775</cell><cell>0.741</cell></row><row><cell cols="9">In addition, We report the results on VOT-STB2022 in Tab.9. SwinTrack-T-224 has a better perfor-</cell></row><row><cell cols="9">mance on VOT-STB2022 as well. No comparison is made since VOT-STB2022 is a newly released</cell></row><row><cell>benchmark.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Results on VOT-STB2022<ref type="bibr" target="#b20">[21]</ref>. Comparison to the state-of-the-art trackers on LaSOT<ref type="bibr" target="#b10">[11]</ref> Test set using success (SUC) AUC score. Comparison to the state-of-the-art trackers on LaSOT<ref type="bibr" target="#b10">[11]</ref> Test set using precision (PRE) AUC score.</figDesc><table><row><cell></cell><cell>SwinTrack</cell><cell>SwinTrack</cell></row><row><cell></cell><cell>T-224</cell><cell>B-384</cell></row><row><cell>EAO</cell><cell>0.505</cell><cell>0.477</cell></row><row><cell>Accuracy</cell><cell>0.777</cell><cell>0.790</cell></row><row><cell>Robustness</cell><cell>0.790</cell><cell>0.759</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , o y1 t , o x2 t , o y2 t ).For flexible modeling, a sampling process is required to ensure the following properties: 1) fixed length, 2) focusing on the latest trajectories and 3) reducing redundancy. In our method, we sample object trajectory as:T = {o s(1) , o s(2) , ..., o s(n) }, where s(i) = max(t ? i ? ?, 1),(2)n is the number of sampled object trajectories, ? is the fixed sampling interval. For Siamese tracker, the search region is cropped from the input image. In detail, a cropping with resizing operation can be</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">IV: Illumination Variation, POC: Partial Occlusion, DEF: Deformation, MB: Motion Blur, CM: Camera Motion, ROT: Rotation, BC: Background Clutter, VC: Viewpoint Change, SV: Scale Variation, FOC: Full Occlusion, FM: Fast Motion, OV: Out-of-View, LR: Low Resolution, ARC: Aspect Ration Change</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Positional Encoding</head><p>Transformer requires a positional encoding to identify the position of the current processing token <ref type="bibr" target="#b37">[38]</ref>. Through a series of comparison experiments, we choose untied positional encoding, which is proposed in TUPE <ref type="bibr" target="#b19">[20]</ref>, as the positional encoding solution of our tracker. In addition, we generalize the untied positional encoding to arbitrary dimensions to fit with other components in our tracker.</p><p>The original transformer <ref type="bibr" target="#b37">[38]</ref> proposes a absolute positional encoding method to represent the position: a fixed or learnable vector p i is assigned to each position i. Starting from the basic attention module, we have:</p><p>where Q,K,V are the query vector, key vector and value vector, which are the parameters of the attention function, d k is the dimension of key. Introducing the linear projection matrix and multi-head attention to the attention module <ref type="formula">(7)</ref>, we get the multi-head variant defined in <ref type="bibr" target="#b37">[38]</ref>:</p><p>where</p><p>i ? R hdv?d model and h is the number of heads. For simplicity, as in <ref type="bibr" target="#b19">[20]</ref>, we assume that d k = d v = d model , and use the single-head version of self-attention module. Denoting the input sequence as x = x 1 , x 2 , . . . , x n , where n is the length of sequence, x i is the i-th token in the input data. Denoting the output sequence as z = (z 1 , z 2 , . . . , z n ). Self-attention module can be rewritten as</p><p>where</p><p>Obviously, the self-attention module is permutation-invariance. Thus it can not "understand" the order of input tokens.</p><p>Untied absolute positional encoding. By adding a learnable positional encoding <ref type="bibr" target="#b37">[38]</ref> to the singlehead self-attention module, we can obtain the following equation:</p><p>The equation <ref type="formula">(11)</ref> is expanded into four terms: token-to-token, token-to-position, position-to-token, position-to-position. <ref type="bibr" target="#b19">[20]</ref> discuss the problems that exist in the equation and proposes the untied absolute positional encoding, which unties the correlation between tokens and positions by removing the token-position correlation terms in equation <ref type="bibr" target="#b10">(11)</ref>, and using an isolated pair of projection matrices U Q and U K to perform linear transformation upon positional embedding vector. The following is the new formula for obtaining ? ij using the untied absolute positional encoding in the l-th layer:</p><p>where p i and p j is the positional embedding at position i and j respectively, U Q ? R d?d and U K ? R d?d are learnable projection matrices for the positional embedding vector. When extending</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixformer: End-to-end tracking with iterative mixed attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">High-performance long-term tracking with meta-updater</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality large-scale single object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="439" to="461" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cract: Cascaded regression-align-classification for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stmtrack: Template-free visual tracking with space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Aiatrack: Attention in attention for transformer visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning to fuse asymmetric feature maps in siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1562" to="1577" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>?ehovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Transforming model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning target candidate association to keep track of what not to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generalized intersection over union</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<title level="m">Self-attention with relative position representations. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Siam r-cnn: Visual tracking by re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Transformer meets tracker: Exploiting temporal context for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Towards more flexible and accurate object tracking with natural language: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Correlation-aware deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards grand unification of object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal transformer for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Joint feature learning and relation modeling for tracking: A one-stream framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Varifocalnet: An iou-aware dense object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learn to match: Automatic matching network design for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
