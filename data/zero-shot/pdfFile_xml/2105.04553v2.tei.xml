<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning with Swin Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning with Swin Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are witnessing a modeling shift from CNN to Transformers in computer vision. In this work, we present a self-supervised learning approach called MoBY, with Vision Transformers as its backbone architecture. The approach basically has no new inventions, which is combined from MoCo v2 and BYOL and tuned to achieve reasonably high accuracy on ImageNet-1K linear evaluation: 72.8% and 75.0% top-1 accuracy using DeiT-S and Swin-T, respectively, by 300-epoch training. The performance is slightly better than recent works of MoCo v3 and DINO which adopt DeiT as the backbone, but with much lighter tricks. More importantly, the general-purpose Swin Transformer backbone enables us to also evaluate the learnt representations on downstream tasks such as object detection and semantic segmentation, in contrast to a few recent approaches built on ViT/DeiT which only report linear evaluation results on ImageNet-1K due to ViT/DeiT not tamed for these dense prediction tasks. We hope our results can facilitate more comprehensive evaluation of self-supervised learning methods designed for Transformer architectures. Our code and models are available at https://github.com/SwinTransformer/Transformer-SSL, which will be continually enriched. * Equal contribution. ? Interns at MSRA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The vision field is undergoing two revolutionary trends since about two years ago. The first trend is self-supervised visual representation learning pioneered by MoCo <ref type="bibr" target="#b8">[9]</ref>, which for the first time demonstrated superior transferring performance on seven downstream tasks over the previous standard supervised methods by ImageNet-1K classification. The second is the Transformer-based backbone architecture <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14]</ref>, which has strong potential to replace the previous standard convolutional neural networks such as ResNet <ref type="bibr" target="#b10">[11]</ref>. The pioneer work is ViT <ref type="bibr" target="#b6">[7]</ref>, which demonstrated strong performance on image classification by directly applying the standard Transformer encoder <ref type="bibr" target="#b16">[17]</ref> in NLP on non-overlapping image patches. The follow-up work, DeiT <ref type="bibr" target="#b15">[16]</ref>, tuned several training strategies to make ViT work well on ImageNet-1K image classification. While ViT/DeiT are designed for the image classification task and has not been well tamed for downstream tasks requiring dense prediction, Swin Transformer <ref type="bibr" target="#b13">[14]</ref> is proposed to serve as a general-purpose vision backbone by introducing useful inductive biases of locality, hierarchy and translation invariance.</p><p>While the two revolutionary waves appeared independently, the community is curious about what kind of adaptation is needed and what it will behave when they meet each other. Nevertheless, until very recently, a few works started to explore this space: MoCo v3 <ref type="bibr" target="#b5">[6]</ref> presents a training recipe to let ViT perform reasonably well on ImageNet-1K linear evaluation; DINO <ref type="bibr" target="#b2">[3]</ref> presents a new self-supervised learning method which shows good synergy with the Transformer architecture.</p><p>Although these works produce encouraging results on ImageNet-1K linear evaluation, there are no assessment of the transferring performance on downstream tasks such as object detection and semantic segmentation, probably due to that ViT/DeiT are not well tamed for these downstream tasks. To enable more comprehensive evaluations of the self-supervised learnt representations on also these downstream tasks, we propose to adopt Swin Transformer as the backbone architecture instead of the previous used ViT architecture, thanks to that Swin Transformer is designed as general-purpose and performs strong on downstream tasks.</p><p>In addition to this backbone architecture change, we also present a self-supervised learning approach by combining MoCo v2 <ref type="bibr" target="#b4">[5]</ref> and BYOL <ref type="bibr" target="#b7">[8]</ref>, named MoBY (by picking the first two letters of each). We tune a training recipe to make the approach performing reasonably high on ImageNet-1K linear evaluation: 72.8% top-1 accuracy using DeiT-S with 300-epoch training which is slightly better than that in MoCo v3 and DINO but with lighter tricks. Using Swin-T architecture instead of DeiT-S, it achieves 75.0% top-1 accuracy with 300-epoch training, which is 2.2% higher than that using DeiT-S. Initial study shows that some tricks in MoCo v3 and DINO are also useful for MoBY, e.g. replacing the LayerNorm layers before the MLP blocks by BatchNorm like that in MoCo v3 bring additional +1.1% gains using 100 epoch training, indicating the strong potential of MoBY.</p><p>When transferred to downstream tasks of COCO object detection and ADE20K semantic segmentation, the representations learnt by this self-supervised learning approach achieves on par performance compared to the supervised method. Noting self-supervised learning with ResNet architectures has shown significantly stronger transferring performance on downstream tasks than supervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref>, the results indicate large space to improve for self-supervised learning with Transformers.</p><p>The proposed approach basically has no new inventions. What we provide is an approach which combines the previous good practice but with lighter tricks, associated with tuned hyper-parameters to achieve reasonably high accuracy on ImageNet-1K linear evaluation. We also provide baselines to aid the evaluation of transferring performance on downstream tasks for the future study of self-supervised learning on Transformer architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Baseline SSL Method with Swin Transformers</head><p>MoBY: a self-supervised learning approach MoBY is a combination of two popular selfsupervised learning approaches: MoCo v2 <ref type="bibr" target="#b4">[5]</ref> and BYOL <ref type="bibr" target="#b7">[8]</ref>. It inherits the momentum design, the key queue, and the contrastive loss used in MoCo v2, and inherits the asymmetric encoders, asymmetric data augmentations and the momentum scheduler in BYOL. We name it MoBY by picking the first two letters of each method.</p><p>The MoBY approach is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. There are two encoders: an online encoder and a target encoder. Both two encoders consist of a backbone and a projector head (2-layer MLP), and the online encoder introduces an additional prediction head (2-layer MLP), which makes the two encoders asymmetric. The online encoder is updated by gradients, and the target encoder is a moving average of the online encoder by momentum updating in each training iteration. A gradually increasing momentum updating strategy is applied for on the target encoder: the value of momentum term is gradually increased to 1 during the course of training. The default starting value is 0.99.</p><p>A contrastive loss is applied to learn the representations. Specifically, for an online view q, its contrastive loss is computed as</p><formula xml:id="formula_0">L q = ?log exp(q ? k + /? ) K i=0 exp(q ? k i /? ) ,<label>(1)</label></formula><p>where k + is the target feature for the other view of the same image; k i is a target feature in the key queue; ? is a temperature term; K is the size of the key queue (4096 by default).</p><p>In training, like most Transformer-based methods, we also adopt the AdamW <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref> optimizer, in contrast to previous self-supervised learning approaches built on ResNet backbone where usually SGD <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref> or LARS <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19]</ref> is used. We also introduce a regularization method of asymmetric drop path which proves crucial for the final performance.</p><p>In the experiments, we adopt a fixed learning rate of 0.001 and a fixed weight decay of 0.05, which performs stably well. We tune hyper-parameters of the key queue size K, the starting momentum value of the target branch, the temperature ? , and the drop path rates.</p><p>A pseudo code of MoBY in a PyTorch-like style is shown in Algorithm 1. Swin Transformer as the backbone Swin Transformer is a general-purpose backbone for computer vision and achieved state-of-the-art performance on various vision tasks such as COCO object detection (58.7 box AP and 51.1 mask AP on test-dev set) and ADE20K semantic segmentation (53.5 mIoU on validation set). It is basically a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.</p><p>In this work, we adopt the tiny version of Swin Transformer (Swin-T) as our default backbone, such that the transferring performance on downstream tasks of object detection and semantic segmentation can be also evaluated. The Swin-T has similar complexity with ResNet-50 and DeiT-S. The details of specific architecture design and hyper-parameters can be found in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear Evaluation on ImageNet-1K</head><p>Linear evaluation on ImageNet-1K dataset is a common evaluation protocol to assess the quality of learnt representations <ref type="bibr" target="#b8">[9]</ref>. In this protocol, a linear classifier is applied on the backbone, with the backbone weights frozen and only the linear classifier trained. After training this linear classifier, the top-1 accuracy using center crop is reported on the validation set.</p><p>During training, we follow <ref type="bibr" target="#b8">[9]</ref> to use random resize cropping with scale from [0.08, 1] and horizontal flipping as the data augmentation. 100-epoch training with a 5-epoch linear warm-up stage is conducted. The weight decay is set as 0. The learning rate is set as the optimal one of {0.5, 0.75, 1.0, 1.25} through grid search for each pre-trained model. We note that MoCo v3 and DINO adopt heavy tricks to achieve the same accuracy as ours:</p><p>? Tricks in MoCo v3 <ref type="bibr" target="#b5">[6]</ref>. MoCo v3 adopts a fixed patch embedding, batch normalization layers to replace the layer normalization ones before the MLP blocks, and a 3-layer MLP head. It also uses large batch size (i.e. 4096) which is unaffordable for many research labs. ? Tricks in DINO <ref type="bibr" target="#b2">[3]</ref>. DINO adopts asymmetric temperatures between student and teacher, a linearly warmed-up teacher temperature, varying weight decay during pre-training, the last layer fixed at the first epoch, tuning whether to put weight normalization in the head, a concatenation of the last few blocks or CLS tokens as the input to the linear classifier, and etc.</p><p>In contrast, we mainly adopt standard settings from MoCo v2 <ref type="bibr" target="#b4">[5]</ref> and BYOL <ref type="bibr" target="#b7">[8]</ref>, and use a small batch size of 512 such that the experimental settings will be affordable for most labs. We have also started to try applying some tricks of MoCo v3 <ref type="bibr" target="#b5">[6]</ref>/DINO <ref type="bibr" target="#b2">[3]</ref> to MoBY, though they are not included in the standard settings. Our initial exploration reveals that the fixed patch embedding has no use to MoBY, and replacing the layer normalization layers before the MLP blocks by batch normalization can bring +1.1% top-1 accuracy using 100-epoch training, as shown in <ref type="table" target="#tab_2">Table 2</ref>. This indicates that some of these tricks may be useful for the MoBY approach, and the MoBY approach has potential to achieve much higher accuracy on ImageNet-1K linear evaluation. This will be left as our future study.</p><p>Swin-T v.s. DeiT-S We also compare the use of different Transformer architectures in selfsupervised learning. As shown in <ref type="table" target="#tab_0">Table 1</ref>, Swin-T achieves 75.0% top-1 accuracy, surpassing DeiT-S by +2.2%. Also note the performance gap is larger than that of using supervised learning (+1.5%).  We leave more comprehensive study of Transformer architecture improvements in the context of SSL as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transferring Performance on Downstream Tasks</head><p>We evaluate the transferring performance of the learnt representation on downstream tasks of COCO object detection/instance segmentation and ADE20K semantic segmentation.</p><p>COCO object detection and instance segmentation Two detectors are adopted in the evaluation: Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b0">[1]</ref>, following the implementation of [14] 1 . <ref type="table" target="#tab_4">Table 3</ref> shows the comparison of the learnt representation by MoBY and the pretrained supervised method in <ref type="bibr" target="#b13">[14]</ref>, in both 1x and 3x settings. For each experiment, we follow all the settings used for supervised pre-trained models <ref type="bibr" target="#b13">[14]</ref>, except that we tune the drop path rate in {0, 0.1, 0.2} and report the best results (for also supervised models).</p><p>It can be seen that the representations learnt by the self-supervised method (MoBY) and the supervised method are similarly well on transferring performance. While we note that previous SSL works using ResNet as the backbone architecture usually report stronger performance over the supervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref>, no gains over supervised methods are observed using Transformer architectures. We hypothesis it is partly because the supervised pre-training on Transformers has involved strong data augmentations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>, while supervised training of ResNet usually employs much weaker data augmentation. These results also imply space to improve for self-supervised learning using Transformer architectures.</p><p>ADE20K Semantic Segmentation The UPerNet approach <ref type="bibr" target="#b17">[18]</ref> and the ADE20K dataset are adopted in the evaluation, following <ref type="bibr" target="#b13">[14]</ref> 2 . The fine-tuning and testing settings also follow <ref type="bibr" target="#b13">[14]</ref> except that the learning rate of each experiment is tuned using {3?10 ?5 , 6?10 ?5 , 1?10 ?4 }. <ref type="table">Table 4</ref> shows the comparisons of supervised and self-supervised pre-trained models on this evaluation. It indicates that MoBY performs slightly worse than the supervised method, implying a space to improve for self-supervised learning using Transformer architectures.   <ref type="table">Table 4</ref>: Comparison of the supervised method by ImageNet-1K classification and the self-supervised MoBY approach on transferring performance to ADE20K semantic segmentation. ? denotes the results with multi-scale testing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We perform ablation study using the ImageNet-1K linear evaluation protocol. Swin-T is used as the backbone architecture. In each ablation, we vary one hyper-parameter and other hyper-parameters are set as the default ones.  Asymmetric drop path rates is beneficial Drop path has proved a useful regularization for supervised representation learning using the image classification task and Transformer architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>. We also ablate the effect of this regularization in <ref type="table" target="#tab_5">Table 5</ref>. Increasing the drop path regularization from 0.05 to 0.1 to the online encoder is beneficial for representation learning, especially in longer training, probably due to the relief of over-fitting. Additionally adding drop path regularization to the target encoder results in 1.9% top-1 accuracy drop (70.9% to 69.0%), indicating a harm. We thus adopt an asymmetric drop path rates in pre-training.</p><p>Other hyper-parameters <ref type="table" target="#tab_7">Table 6a</ref> ablates the effect of key queue size K from 1024 to 16384. The approach stably performs across various K (from 1024 to 16384), and we adopt 4096 as default. <ref type="table" target="#tab_7">Table 6b</ref> ablates the effect of temperature ? and 0.2 performs best which is set as the default value. <ref type="table" target="#tab_7">Table 6c</ref> ablates the effect of the starting momentum value of the target encoder. 0.99 performs best and is set as the default value.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a self-supervised learning approach called MoBY, with Vision Transformers as its backbone architecture. With a proper training recipe and much lighter tricks than MoCo v3/DINO, MoBY can achieve reasonably high performance on ImageNet-1K linear evaluation: 72.8% and 75.0% top-1 accuracy using DeiT-S and Swin-T, respectively, by 300-epoch training. More importantly, in contrast to ViT/DeiT, the general-purpose Swin Transformer backbone enables us to also evaluate the learnt representations on downstream tasks such as object detection and semantic segmentation. MoBY can perform comparably or slightly worse than the supervised methods, indicating a space to improve for self-supervised learning with Transformer architectures. We hope our results can facilitate more comprehensive evaluation of self-supervised learning methods designed for Transformer architectures. Our code and models are available and will be continually enriched at https://github.com/SwinTransformer/Transformer-SSL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The pipeline of MoBY.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Pseudo code of MoBY in a PyTorch-like style. # encoder: transformer-based encoder # proj: projector # pred: predictor # odpr: online drop path rate # tdpr: target drop path rate # m: momentum coefficient # t: temperature coefficient # queue1, queue2: two queues for storing negative samples f_online = lambda x: pred(proj(encoder(x, drop_path_rate=odpr))) f_target = lambda x: proj(encoder(x, drop_path_rate=tdpr)) for v1, v2 in loader: # load two views q1, q2 = f_online(v1), f_online(v2) # queries: NxC k1, k2 = f_target(v1), f_target(v2) # keys: NxC # symmetric loss loss = contrastive_loss(q1, k2, queue2) + contrastive_loss(q2, k1, queue1) loss.backward() update(f_online) # optimizer update: f_online f_target = m * f_target + (1. -m) * f_online # momentum update: f_target update(m) # update momentum coefficient def contrastive_loss(q, k, queue): # positive logits: Nx1 l_pos = torch.einsum('nc,nc-&gt;n', [q, k.detach()]).unsqueeze(-1) # negative logits: NxK l_neg = torch.einsum('nc,ck-&gt;nk', [q, queue.clone().detach()]) # logits: Nx(1+K) logits = torch.cat([l_pos, l_neg], dim=1) # labels: positive key indicators labels = torch.zeros(N) loss = F.cross_entropy(logits / t, labels)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Epochs</head><label></label><figDesc>Online dpr Target dpr Top-1 acc (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>listed the major results of pre-trained models using different self-supervised learning methods and backbone architectures.Comparison with other SSL methods using Transformer architectures Regarding previous methods such as MoCo v3<ref type="bibr" target="#b5">[6]</ref> and DINO<ref type="bibr" target="#b2">[3]</ref> adopt ViT/DeiT as their backbone architecture, we first report results of MoBY using DeiT-S<ref type="bibr" target="#b15">[16]</ref> for fair comparison with them. Under 300-epoch training, MoBY achieves 72.8% top-1 accuracy, which is slightly better than MoCo v3 and DINO (without the multi-crop trick), as shown inTable 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different SSL methods and different Transformer architectures on ImageNet-1K linear evaluation. ? denotes DINO with a multi-crop scheme in training.</figDesc><table><row><cell></cell><cell>Arch.</cell><cell cols="5">Epochs Params (M) FLOPs (G) img/s Top-1 acc (%)</cell></row><row><cell>Sup.</cell><cell>DeiT-S</cell><cell>300</cell><cell>22</cell><cell>4.6</cell><cell>940.4</cell><cell>79.8</cell></row><row><cell>Sup.</cell><cell>Swin-T</cell><cell>300</cell><cell>29</cell><cell>4.5</cell><cell>755.2</cell><cell>81.3</cell></row><row><cell cols="2">MoCo v3 DeiT-S</cell><cell>300</cell><cell>22</cell><cell>4.6</cell><cell>940.4</cell><cell>72.5</cell></row><row><cell>DINO</cell><cell>DeiT-S</cell><cell>300</cell><cell>22</cell><cell>4.6</cell><cell>940.4</cell><cell>72.5</cell></row><row><cell>DINO  ?</cell><cell>DeiT-S</cell><cell>300</cell><cell>22</cell><cell>4.6</cell><cell>940.4</cell><cell>75.9</cell></row><row><cell>MoBY</cell><cell>DeiT-S</cell><cell>300</cell><cell>22</cell><cell>4.6</cell><cell>940.4</cell><cell>72.8</cell></row><row><cell>MoBY</cell><cell>Swin-T</cell><cell>100</cell><cell>29</cell><cell>4.5</cell><cell>755.2</cell><cell>70.9</cell></row><row><cell>MoBY</cell><cell>Swin-T</cell><cell>300</cell><cell>29</cell><cell>4.5</cell><cell>755.2</cell><cell>75.0</cell></row><row><cell cols="7">Fixed Patch Embedding Replace LN before MLP with BN Top-1 acc (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Initial study of applying tricks in MoCo v3 to the MoBY approach using 100-epoch training and Swin-T backbone architecture. Note although replacing the layer norm layer before each MLP block with a batch norm layer performs better (72.0 vs. 70.9), it changes the original Swin architecture and is currently not used as our standard settings in experiments.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the supervised method by ImageNet-1K classification and the self-supervised MoBY approach on transferring performance to COCO object detection and instance segmentation.</figDesc><table><row><cell>Method</cell><cell cols="2">Model Schd. mIoU</cell></row><row><cell></cell><cell>Sup.</cell><cell>160K 44.51</cell></row><row><cell>Swin-T</cell><cell cols="2">MoBY 160K 44.06</cell></row><row><cell>(UPerNet)</cell><cell>Sup.  ?</cell><cell>160K 45.81</cell></row><row><cell></cell><cell cols="2">MoBY  ? 160K 45.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the drop path rates of online and target encoders.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on other hyper-parameters using 100-epoch training.</figDesc><table /><note>* denotes the default values.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/SwinTransformer/Swin-Transformer-Object-Detection 2 https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parametric instance classification for unsupervised visual feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10957</idno>
		<title level="m">Efficient visual pretraining with contrastive detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
