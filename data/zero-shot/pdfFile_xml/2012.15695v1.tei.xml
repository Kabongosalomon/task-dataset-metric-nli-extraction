<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFICIENTNET-ABSOLUTE ZERO FOR CONTINUOUS SPEECH KEYWORD SPOTTING A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-01">January 1, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><forename type="middle">Mohammad</forename><surname>Rostami</surname></persName>
							<email>a.m.rostami@aut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Karimi</surname></persName>
							<email>aliiikarimi@ut.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Tehran University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Ali</forename><surname>Akhaee</surname></persName>
							<email>akhaee@ut.ac.ir</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Tehran University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EFFICIENTNET-ABSOLUTE ZERO FOR CONTINUOUS SPEECH KEYWORD SPOTTING A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-01">January 1, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Keyword Spotting ? Football Keywords Dataset ? Continuous Speech Synthesis Method</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Keyword spotting is a process of finding some specific words or phrases in recorded speeches by computers. Deep neural network algorithms, as a powerful engine, can handle this problem if they are trained over an appropriate dataset. To this end, the football keyword dataset (FKD), as a new keyword spotting dataset in Persian, is collected with crowdsourcing. This dataset contains nearly 31000 samples in 18 classes. The continuous speech synthesis method proposed to made FKD usable in the practical application which works with continuous speeches. Besides, we proposed a lightweight architecture called EfficientNet-A0 (absolute zero) by applying the compound scaling method on EfficientNet-B0 for keyword spotting task. Finally, the proposed architecture is evaluated with various models. It is realized that EfficientNet-A0 and Resnet models outperform other models on this dataset.</p><p>Keyword spotting in a continuous speech can be considered as a limited or particular case of large vocabulary continuous speech recognition (LVCSR) systems. Lightweight models that reduce the training and inference runtime, along with the growing usage of smartphones, has made this field applicable in business projects. Hey Siri, Hey Bixbi, and Hey Google, which is known as wake up or trigger systems are a particular mode of keyword spotting. In addition, home voice assistant systems are one of the keyword spotting applications.</p><p>Over the past several years, deep neural network-based methods have been used in various researches due to their remarkable performance and achievements. In order to use these methods in the keyword spotting field, it is necessary to have a sufficient and appropriate dataset along with an architecture that can be appropriately performed on smartphones or embedded systems. The dataset used for this purpose should have sufficient samples of each class or word from different aspects like word expression, accent, unique speakers, recording environment, recording systems, and ambient noise to improve the models' final performance in practical applications. On the other hand, to obtain excellent performance in continuous speeches, the training data must also contain this type of samples. The football keywords dataset (FKD) attempts to get all of these requirements in Persian language. This dataset is open-source 1 for any research project. Determining the events of a football game using the reporter's speech is another reason for collecting this</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>dataset. Our primary focus is on introducing the prepared dataset and examining the EfficientNet model <ref type="bibr" target="#b0">[1]</ref> along with other methods on this dataset. The rest of the paper is organized as follows. Section 2 is dealing with the related works and datasets in keyword spotting. Section 3 describes the details of preparing FKD and its features. In the next section, how the EfficientNet is used in this task will be introduced. In Section 5, the experiment configuration as well as the results have been described. Finally, Section 6 concludes the paper. EfficientNet architecture is a powerful method that has achieved the best performance in recent years in many image processing tasks, especially on the Imagenet dataset. Moreover, the performance and efficiency of this architecture in speech recognition <ref type="bibr" target="#b1">[2]</ref>, which is a more complicated task than keyword spotting, is significant and has attracted much attentions.</p><p>In this field, many works have employed shallow or deep models based on their purposes, resources, and trends of that time. Various researches have used convolutional neural networks (CNN) <ref type="bibr" target="#b2">[3]</ref>[4] <ref type="bibr" target="#b4">[5]</ref>, but the main point is that this model has high computations and some works tried to solve this challenge with small-footprint architectures[6] <ref type="bibr" target="#b6">[7]</ref>. For instance, Residual architecture methods that have the potential to go deeper have been employed in most signal processing areas such as image recognition, speaker identification/verification, speech recognition, and keyword spotting <ref type="bibr" target="#b5">[6]</ref>. Feed-forward deep neural network (DNN) methods that paid attention to being small-footprint also used in this field <ref type="bibr" target="#b7">[8]</ref>[9] <ref type="bibr" target="#b9">[10]</ref>. However, these methods have lower accuracy and performance compared to convolution methods. In addition to these architectures, recurrent neural networks (RNN) are recently widespread in this field <ref type="bibr" target="#b10">[11]</ref>. Although RNNs introduce some advantages, we do not consider them because of their high computational complexities and run times. Given these points, some works have also tried to combine convolutional and recurrent methods to achieve all benefits <ref type="bibr" target="#b11">[12]</ref>[13] <ref type="bibr" target="#b13">[14]</ref>. These methods performed well, but computational complexity remains their main challenge.</p><p>Besides deep neural networks, some researches are dedicated to applying several methods like hidden Markov models (HMM) <ref type="bibr" target="#b14">[15]</ref>, hybrid HMM-NN models <ref type="bibr" target="#b15">[16]</ref>, segmental Gaussian mixture models <ref type="bibr" target="#b16">[17]</ref>, and minimum edit distance <ref type="bibr" target="#b17">[18]</ref>. However, these methods are less investigated these days due to deep neural networks' excellent performance which is achieved thanks to sufficient data and graphical processor units (GPUs).</p><p>Nowadays, a dataset published by Google has led to much progress in this field <ref type="bibr" target="#b18">[19]</ref>. Google speech command (GSC) is available as an open-source for researchers and commercial works. In GSC, they considered most of the required phonemes, words of commands, and numbers. The dataset published by Mozilla <ref type="bibr" target="#b19">[20]</ref> has been one of the main factors in developing speech recognition systems because it is multilingualism and open-source. Deep mine corpus <ref type="bibr" target="#b20">[21]</ref> is another Persian dataset released recently for text-dependent, text-independent, and text-prompted speaker identification/verification and speech recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Football Keywords Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main approach</head><p>Detecting the events and actions of a football game and making an appropriate dataset for Persian keywords spotting in both single word and continuous mode (i.e., in the sentence) are our main reasons for preparing this dataset. In order to achieve this goal and obtain a generalized model, we consider the diversity of FKD from different aspects such as age, gender, mood, and tone of an utter, recording device, and recording environment in the collecting data phase.</p><p>The keywords used in this dataset are very close to the same words in English used in football. However, there exist some minor differences in pronunciation and accent. Words like goal, offside, corner, penalty, and hand in most languages have the same meaning and usage. Therefore, models employing English datasets can use this dataset for generalization and improvement of their systems.</p><p>The list of words is shown in <ref type="table" target="#tab_0">Table 1</ref>. We consider the majority of football events, which can be recognized through the football reporter's voice. On the other hand, during data collection, seven types of expression such as normal, emphasized, upset, surprised, emotional, fast, and stretched have been collected from the speakers so that the prepared dataset is comprehensiveness to some extent. This point, along with the variety of speakers, recording systems, and ambient noise, makes this dataset general and applicable in practical applications.</p><p>Since it is decided to collect the dataset from crowdsourcing, an attempt has been made not to ask the age, gender, and names of the participants. However, according to the collected speeches, the majority of utterances are men and young people. Fortunately, since the number of speakers is high, there exist enough examples from other age groups and women voices in the dataset. The dataset was collected by over 1,700 unique people, 16?14 utterances from each person with a median 14, and for each class, we have 407?31 unique speakers. This uniqueness and variety make this dataset suitable to be used in the speaker verification task too.</p><p>Finally, the dataset is divided into three parts: train, test, and continuous speech samples (CSS). In the test set, we select 300 samples in each class, considering that it contains at least 100 samples from speakers who are only available in the test part. Also, to evaluate the final models in continuous speeches, we prepared the CSS part. The CSS part consists of 1002 continuous 2-second speech samples, wherein each sample, the speaker utters one of the desired keywords in a continuous speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collecting and cleaning systems</head><p>In order to gather desired words, we developed two telegram bots to collect OGG Vorbis audio files as crowdsourcing.</p><p>The bots ask users to record and send the required six words based on the amount of data collected. Then users choose one of these words. Subsequently, the user was asked to utter the chosen word seven times, and each time with a different expression mood to obtain a diverse mood for each word. At any time, the user could go to the next word and is not forced to fill in all seven moods; however, results show that most of the users went to the next word after completing seven moods and then complete the next words in the same way. Another bot was also developed to determine the similarity of the audio files sent by users with famous Persian football reporters. To find out the most similar reporter to their voice, users must submit all 42 samples to this bot. This bot makes some fun and motivates people to send their voices because of its gamification idea.</p><p>There exists no time limit during data collection, and only users are asked to utter the keyword. The collected data was given to a cleaning system after they were converted to the wav format. We train the cleaning system with 20000 samples cleaned by a human operator. To provide the required clean data, first, samples are trimmed systematically to 1.25-second based on more energy segment. Then, a human operator verifies and labels them. The cleaning system is based on the Res-8 architecture proposed in <ref type="bibr" target="#b5">[6]</ref>. Then in the cleaning phase, for each audio file, by moving the 1.25-second sliding window with a stride of 0.1 second, 1.25-second audio parts are cropped. Next, among these samples, the sample with the highest probability of keyword occurrence is stored as the output of that audio file if it has a higher probability than a threshold value which is set to 0.97. If the audio file is less than 1.25 seconds, zero paddings are made equally to the beginning and end of the file to make the audio file at least 1.25-second. Then, three human operators clean the dataset, and we consider the intersection of each operator output samples as the final dataset. <ref type="table" target="#tab_0">Table 1</ref> indicates the number of samples of each keyword.</p><p>In order to improve the generalization and performance of any system using this dataset, we prepared 11 different noises, including white, pink, restaurant, exhibition, car, crowd cheering, running, subway, talking, doing the dishes, and babble. These noises have adequate diversities to simulate real situations and can be added to the samples during the training and test phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Synthesizing continuous speech</head><p>FKD consists of samples that contain only a single word like other datasets in this field. However, this type of datasets are not suitable for continuous speeches. Consequently, we need to convert samples into continuous ones. With a continuous speech synthesis method (CSSM), noncontinuous speech datasets can be converted to a proper dataset for continuous speeches.</p><p>To prepare synthesis continuous sample from i th sample (s i ) with the sample rate of 16 KHz, first, we randomly select one of the continuous speech audio files (s g ) from some football report files or any continuous speech file like movies, radio programs, etc. s g ? {background f iles} Then, we select a two-second slice from the selected file as the background speech b g .</p><formula xml:id="formula_0">b g = s g [n, n + 32000]</formula><p>where n is a random integer in the range of [0, |s g | ? 32000] and |s g | means the length of b g . Therefore, two background and keyword windows according to equation 1 and 2 are created, respectively:</p><formula xml:id="formula_1">w kw (j) = I 0 1.5 1 ? 4j 2 (M ?1) 2 I 0 (1.5)<label>(1)</label></formula><p>w bg (j) = 1.05 ?</p><formula xml:id="formula_2">I 0 2.5 1 ? 4j 2 (M ?1) 2 I 0 (2.5)<label>(2)</label></formula><p>where I 0 is the modified zeroth-order Bessel function. To makes the final output more natural, w bg has 0.125 second zero-padding on both sides. We randomly choose k in the range [bound, |b g | ? (bound + 20000)] and multiply the background window by b g to prepare the desired background speech.</p><formula xml:id="formula_3">b g [i] = b g [i] if i / ? [k, k + 20000] b g [i] ? w bg [i ? k] o.w.<label>(3)</label></formula><p>Then, by multiplying the keyword window by sample and placing the result in the desired background speech, the final synthesized continuous speech is obtained (s s ).</p><formula xml:id="formula_4">s s [j] = b g [i] if i / ? [k + 2000, k + 18000] b g [i] + s [i ? (k + 2000)] o.w.<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">s [p] = s[p] ? w kw [p]</formula><p>The steps of this process, along with the shape of the windows, are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As can be seen, the synthesized sample looks like a real continuous speech based on the graph. We also investigate the impact of using this method in improving the performance of our system in the continuous speech.  <ref type="table" target="#tab_0">3x3  16  1  2  MBConv1  3x3  8  1  3  MBConv6  5x5  16  2  4  MBConv6  3x3  24  1  5  MBConv6  3x3  32  2  6  MBConv6  5x5  56  2  7  MBConv6  3x3  96  2  8</ref> Conv, Pooling, FC 1x1 384 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluating the models</head><p>Looking for a high-performance keyword spotting system with low latency and runtime, we used EfficientNet architecture <ref type="bibr" target="#b0">[1]</ref>. The original article of this architecture mentioned that there exist eight models named B-0 to B-7 according to the compound scaling approach, in which the number of parameters has been added to the depth, width, and resolution of the input image, respectively. The result of these networks are significantly better than other existing structures such as ResNet, Inception, and MobileNet in cases with even fewer parameters. This network is powerful and effective in the field of speech processing too. As a consequence, we use this approach and scaling down the B0 model to achieve the proposed goal.</p><p>It is desired to have a model with about 200000 to 250000 parameters to satisfy our problem's objectives. To do this, we could only change the depth and width of the network because the input size was appropriate due to its fixed and unchangeable time and features. Therefore, the goal is to reduce the model size as bellow, contrary to the method introduced in the EfficientNet's article:</p><formula xml:id="formula_6">depth : d = ? ? width : w = ? ? resolution : r = ? ? s.t. ?.? 2 .? 2 ? 0.05 ? 0.003<label>(5)</label></formula><p>where 0.05 is come from how much we want to reduce model's parameters which is 200000 4000000 = 0.05, and 0.003 is the maximum bound that the final model can pass throw the limitation. This values is set based on 5% of the number of Res15's parameters. we set ? = 1, and select values in the range of [0.25, 0.6] for ? and ? parameters. Finally, it is required to train 75 models according to the existing conditions and grid search with the step size of 0.01. To train and evaluate these models, only 100 random samples from each category are selected. The architecture of EfficientNet-A0 is depicted in <ref type="figure" target="#fig_1">Figure 2</ref> and details about the stages, kernel size, and the number of channels are shown in <ref type="table" target="#tab_1">Table 2</ref>. We reduce the EfficientNet-B0 parameters from 4032595 to 238250 and obtain EfficientNet-A0. This reduction is achieved by decreasing the number of layers and channels.</p><p>In order to model input, Honk <ref type="bibr" target="#b6">[7]</ref> procedure is executed. To this end, a band-pass filter of 20Hz/4kHz is applied to the input audio for noise reduction; then, 40 Mel-frequency cepstrum coefficients (MFCC) are extracted from each 30-milliseconds frame. Finally, to create two-dimensional input, this frame window is moved with steps of 10-milliseconds and the output frames are stacked. In order to have a generalized model, two approaches in the training phase are exploited. The first one is using a fast and novel SpecAugment method <ref type="bibr" target="#b21">[22]</ref> for data augmentation during the training phase. The frequency and time masks are applied on each batch sample with equal probabilities. In the second approach, during the training process, the proposed continuous speech synthesis method is employed to make each 1.25-second sample into a 2-second continuous speech. With this procedure, the trained model could see several continuous speeches with different backgrounds for each sample.</p><p>To make the proposed method usable in practical applications, we have considered two silence and unknown labels besides classes in the FKD. For silent samples, just a random 2-second file from one of the noises for each required sample is separated. Besides, two types of samples for the unknown class is considered during the training. Samples that are synthesized using the GSC test part, along with the samples which are just 2-second of the background files contaminated with additional noise. Models are trained by minimizing a weighted cross-entropy loss function to mitigate the imbalance in the training dataset. Finally, to evaluate the proposed model, several available models with proper results in GSC have been tested. These models are Res8, Res15, and three convolutional methods trad-fpool13, tpool12, and one-srtide1 proposed in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section provides an overview of the general configuration, results, and a description of the experiments performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">General configuration</head><p>Honk open-source project is used as our primary method and is modified it based on our requirements. The implementation is available on the git-hub 2 . To do the experiments, as mentioned in the previous section, we employed the eight models whose performances are significant on the GSC, along with new EfficientNet architecture, which to the best of our knowledge, are used for the first time in this field. To evaluate the models in a fair conditions, constant values are set for common parameters. In other words, we set epoch number 40; batch-size 128; Adam optimizer with ?=(0.9,0.999); schedule learning rate based on cosine annealing with ? max =0.1 and ? min =0.01 ; and the probability value of 0.5 for augmentation functions including frequency and time masking with F=5 and T=8; and finally, we set noise multiplier N=0.12 based on the GSC. To avoid over-fitting, 20% of training data is considered to evaluate the models at each epoch. We choose the model with the best result on evaluation data in the training phase as a final model. Then for the final evaluation, test data is used. <ref type="table" target="#tab_2">Table 3</ref> shows the performance of different models on the FKD. As shown in this table, the EfficientNet-A0 model with spec augmentation (SA) and transfer learning (TL) has the best performance in continuous speech samples (CSS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Besides, in the test part, all Resnet models and EfficientNet-A0 have nearly the same performances. We also try Resnet-Narrow models, but their results are inferior, showing us increase the number of channels in each layer is more useful than increasing the model depth.</p><p>In order to evaluate the complexity, two metrics, the number of parameters and floating-point operations (FLOPS), which indicate the model's complexity and execution time during the inference phase have been used[23] <ref type="bibr" target="#b23">[24]</ref>. As it can be observed in <ref type="table" target="#tab_2">Table 3</ref>, the number of parameters of EfficientNet-A0 is much less than other traditional convolutional methods and is approximately equal to the number of parameters of ResNets. However, GFLOPS (G is for Giga) required in the inference phase of EfficientNet-A0 is significantly fewer, which indicates that this model outperforms all models with FLOPS equal to 7439100.</p><p>Also, we investigate EfficientNet-A0 with two approaches SA and TL. In transfer learning models, the model is first trained on the GSC dataset and then its pre-trained weights are used as the initial weight. Results show that this mechanism works well with the GSC and FKD and achive the best result in the CSS part. By taking the advatage of spec augmentation method, we improve the model generalization in the test phase especially in the CSS part. Finally, we evaluated the CSSM impact by training EfficientNet-A0 without using this approach in the training phase. The last row of <ref type="table" target="#tab_2">Table 3</ref> shows that this model has 63.45?0.412% accuracy on the CSS part, indicating that using the CSSM models performs better in continuous speeches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented a new keyword spotting dataset for football event detection in Persian. This dataset was collected with crowdsourcing and is available as an open-source project. It contains 18 classes, which are football events keywords. The FKD contains about 31000 samples and is divided into three parts including the train, test, and continuous speech samples. Moreover, the continuous speech synthesis method is employed to convert samples containing only a single keyword to continuous speech samples in order to achive models working better with continuous speeches.</p><p>Finally, we used the compound scaling approach on EfficientNet-B0 architecture to create our proposed architecture EfficientNet-A0. This architecture with specAugmentation and transfer learning achieved the best result for the CSS part by 76.84?0.240% accuracy and also 95.83?0.401% accuracy in the test part. The result shows that ResNets and EfficientNet-A0 outperform traditional convolutional methods. Also, the EfficientNet-A0 has only 0.01 GFPLOS, which is a long way from other models, especially Res26, which is the closest in terms of accuracy. This point, along with the number of parameters, indicates the efficiency of this model in real-time applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Steps of the continuous speech synthesis method. Windows that applied on the keyword (a) and background (b), sample signal waveform (c) and signal multiplied by the keyword window (d), sample background signal waveform (e) and position of the background window in left and background after multiply the window (f), and finally synthesis continuous speech sample (g).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>EfficientNet-A0 architecture. D, S, SW, SG, GA, R, and BN are abbreviations of Dropout, Softmax, Swiss, Sigmoid, Global Average, Reshape, and Batch Normalization, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Football keywords dataset classes with the international phonetic alphabet (IPA)</figDesc><table><row><cell>Class</cell><cell>IPA</cell><cell cols="3">Number of Utterances Train Test CSS</cell></row><row><cell>Corner</cell><cell>/korner/</cell><cell>1322</cell><cell>300</cell><cell>22</cell></row><row><cell>Foul</cell><cell>/xat?/</cell><cell>1369</cell><cell>300</cell><cell>3</cell></row><row><cell>Free kick</cell><cell>/k?Steh/</cell><cell>1506</cell><cell>300</cell><cell>80</cell></row><row><cell>Goal</cell><cell>/gol/</cell><cell>1370</cell><cell>300</cell><cell>53</cell></row><row><cell>Goalposts</cell><cell>/tirak/</cell><cell>1376</cell><cell>300</cell><cell>91</cell></row><row><cell>Hand</cell><cell>/hand/</cell><cell>1588</cell><cell>300</cell><cell>53</cell></row><row><cell>Laying off</cell><cell>/exr?j/</cell><cell>1354</cell><cell>300</cell><cell>72</cell></row><row><cell>Mulct</cell><cell>/jarimeh/</cell><cell>1533</cell><cell>300</cell><cell>12</cell></row><row><cell>Notice</cell><cell>/ext?r/</cell><cell>1311</cell><cell>300</cell><cell>39</cell></row><row><cell>Offside</cell><cell>/?fsajd/</cell><cell>1268</cell><cell>300</cell><cell>56</cell></row><row><cell>Out</cell><cell>/ot/</cell><cell>1323</cell><cell>300</cell><cell>56</cell></row><row><cell>Penalty</cell><cell>/pen?lti/</cell><cell>1375</cell><cell>300</cell><cell>91</cell></row><row><cell>Red card</cell><cell cols="2">/k?rte qermez/ 1219</cell><cell>300</cell><cell>30</cell></row><row><cell>Strike</cell><cell>/zarbeh/</cell><cell>1358</cell><cell>300</cell><cell>93</cell></row><row><cell>Substitute</cell><cell>/ta?viz/</cell><cell>1298</cell><cell>300</cell><cell>81</cell></row><row><cell>Tackle</cell><cell>/takl/</cell><cell>1245</cell><cell>300</cell><cell>34</cell></row><row><cell>Throw-in</cell><cell>/part?b/</cell><cell>1331</cell><cell>300</cell><cell>73</cell></row><row><cell cols="2">Yellow card /k?rte zard/</cell><cell>1289</cell><cell>300</cell><cell>23</cell></row><row><cell>Silence</cell><cell>-</cell><cell>-</cell><cell>300</cell><cell>20</cell></row><row><cell>Unknown</cell><cell>-</cell><cell>-</cell><cell>300</cell><cell>20</cell></row><row><cell>Total</cell><cell></cell><cell cols="3">24435 6000 1002</cell></row><row><cell>2 Related works</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Stage</cell><cell>Operator</cell><cell>Kernel size #Channels #Layers</cell></row><row><cell>1</cell><cell>Conv</cell><cell></cell></row></table><note>The details of EfficientNet-A0 stages. Each row describes the number of stage alone with information about operator, kernel size, number of channels and number of layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test and CSS accuracy (Acc.) of each model with 95% confidence intervals (across five trials) on the FKD. The number of model parameters is normalized based on the size of EfficientNet-A0 model, which is 238250 parameters.</figDesc><table><row><cell>Model</cell><cell cols="4"># Parameters # GFLOPS Test Acc. (%) CSS Acc. (%)</cell></row><row><cell>Res8</cell><cell>0.44x</cell><cell>0.08</cell><cell>94.38?0.451</cell><cell>75.54?0.358</cell></row><row><cell>Res15</cell><cell>0.95x</cell><cell>1.93</cell><cell>94.85?0.349</cell><cell>74.85?0.312</cell></row><row><cell>Res26</cell><cell>1.75x</cell><cell>0.89</cell><cell>95.88?0.253</cell><cell>69.46?0.263</cell></row><row><cell>trad-fpool3</cell><cell>0.51x</cell><cell>0.15</cell><cell>74.46?0.403</cell><cell>44.01?0.308</cell></row><row><cell>tpool2</cell><cell>9.15x</cell><cell>0.24</cell><cell>89.91?0.468</cell><cell>61.17?0.355</cell></row><row><cell>one-srtide1</cell><cell>4.4x</cell><cell>0.58</cell><cell>78.60?0.832</cell><cell>51.99?0.538</cell></row><row><cell>EfficientNet-A0</cell><cell>1x</cell><cell>0.01</cell><cell>94.56?0.302</cell><cell>73.25?0.212</cell></row><row><cell>EfficientNet-A0 + SA</cell><cell>1x</cell><cell>0.01</cell><cell>94.83?0.377</cell><cell>76.22?0.231</cell></row><row><cell>EfficientNet-A0 + SA + TL</cell><cell>1x</cell><cell>0.01</cell><cell>95.83?0.401</cell><cell>76.84?0.240</cell></row><row><cell>EfficientNet-A0 (without CSSM)</cell><cell>1x</cell><cell>0.01</cell><cell>-</cell><cell>63.45?0.412</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/AmirmohammadRostami/KeywordsSpotting-EfficientNet-A0</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Speech recognition using efficientnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qidong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional neural network for search term detection-a draft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barfett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dowdell</surname></persName>
		</author>
		<idno>abs/1708.02238</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cnn-based bottleneck feature for noise robust query-by-example spoken term detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1278" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The 2016 bbn georgian telephone speech keyword spotting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alum?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5484" to="5488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4704" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Model compression applied to small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaran</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="1878" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Max-pooling loss training of long short-term memory networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaran</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikko</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Temporal feedback convolutional recurrent neural networks for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taejun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective combination of densenet and bilstm for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10767" to="10775" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hidden markov model based keyword recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Utterance verification of short keywords using hybrid neural-network/hmm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhi</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaijiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongge</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No.01EX479)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="671" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Keyword spotting of arbitrary words using minimal speech resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings</title>
		<meeting><address><addrLine>I-I</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Keyword search using modified minimum edit distance measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Verma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-05" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">929</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno>abs/1804.03209</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Common voice: A massively-multilingual speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosana</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multi purpose and large scale speech corpus in persian and english for speaker and speech recognition: The deepmine database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeinali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>?ernock?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="397" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Specaugment: A simple augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu ; Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<meeting><address><addrLine>Barret Zoph, Ekin Dogus Cubuk, and</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Temporal convolution for real-time keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomjun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An experimental analysis of the power consumption of convolutional neural networks for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5479" to="5483" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
