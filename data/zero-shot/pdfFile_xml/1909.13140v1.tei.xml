<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Weighting and Boosting for Few-Shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
							<email>nguyenkh@oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<postCode>97330</postCode>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
							<email>sinisa@oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<postCode>97330</postCode>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Weighting and Boosting for Few-Shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is about few-shot segmentation of foreground objects in images. We train a CNN on small subsets of training images, each mimicking the few-shot setting. In each subset, one image serves as the query and the other(s) as support image(s) with ground-truth segmentation. The CNN first extracts feature maps from the query and support images. Then, a class feature vector is computed as an average of the support's feature maps over the known foreground. Finally, the target object is segmented in the query image by using a cosine similarity between the class feature vector and the query's feature map. We make two contributions by: (1) Improving discriminativeness of features so their activations are high on the foreground and low elsewhere; and (2) Boosting inference with an ensemble of experts guided with the gradient of loss incurred when segmenting the support images in testing. Our evaluations on the PASCAL-5 i and COCO-20 i datasets demonstrate that we significantly outperform existing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper is about few-shot segmentation of foreground objects in images. As <ref type="figure" target="#fig_2">Fig. 1</ref> shows, given only a few training examples -called support images -and their groundtruth segmentation of the target object class, our goal is to segment the target class in the query image. This problem is challenging, because the support and query images may significantly differ in the number of instances and 3D poses of the target class, as illustrated in <ref type="figure" target="#fig_2">Fig. 1</ref>. This important problem arises in many applications dealing with scarce training examples of target classes.</p><p>Recently, prior work has addressed this problem by training an object segmenter on a large training set, under the few-shot constraint <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. The training set is split into many small subsets. In every subset, one image serves as the query and the other(s) as the support image(s) with known ground truth(s). As shown in <ref type="figure" target="#fig_2">Fig. 1</ref>, their framework uses a CNN -e.g., VGG <ref type="bibr" target="#b26">[27]</ref> or ResNet <ref type="bibr" target="#b11">[12]</ref>   <ref type="figure" target="#fig_2">Figure 1</ref>. Our goal is to segment a foreground object class in a query image, given a single (few) support image(s) showing the same class with the known ground-truth segmentation mask(s). The target class may appear in the query and support images in different numbers of instances and 3D poses. Recent work uses cosine similarity to first estimate a similarity map between CNN features of the query and support images. The similarity map is then used for segmentation of the target class in the query image. extracting feature maps from the support and query images. The support's feature maps are first pooled over the known ground-truth foreground. Then, the support's masked-pooled features are used to estimate a cosine similarity map with the query's features. The resulting similarity map and the query's features are finally passed to a few convolutional layers in order to segment the target object class in the query. The incurred loss between the prediction and the query's ground-truth is used for the CNN's training.</p><p>The above framework has two critical limitations which we address in this paper. First, we experimentally found that the CNN has a tendency to learn non-discriminative features with high activations for different classes. To address this issue, as <ref type="figure">Fig. 2</ref> shows, our first contribution extends prior work by efficiently estimating feature relevance so as to encourage that their activations are high inside the groundtruth locations of the target class, and low elsewhere in the  <ref type="figure">Figure 2</ref>. Our few-shot learning from a single support image: Prior work averages a class feature vector over the known foreground of the support image. A dot product of this class feature vector and feature maps of the query image gives a similarity map. The similarity map and features of the query image are used for segmenting the target class in the query. We extend prior work by additionally estimating feature relevance (contribution 1). Our contribution 2 is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>image. This is formulated as an optimization problem, for which we derive a closed-form solution.</p><p>Second, learning from few support images is prone to overfitting and poor generalization to the query in the face of the aforementioned large variations of the target class. To address this issue, as <ref type="figure" target="#fig_1">Fig. 3</ref> shows, our second contribution is a new boosted inference, motivated by the traditional ensemble learning methods which are robust to overfitting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. We specify an ensemble of experts, where each expert adapts the features initially extracted from the support image. This feature adaptation is guided by the gradient of loss incurred when segmenting the support image relative to its provided ground truth. The ensemble of experts produce the corresponding ensemble of object segmentations of the query image, whose weighted average is taken as our final prediction. Importantly, while we use the first contribution in both training and testing, similar to the traditional ensemble learning methods, our second contribution is applied only in testing for boosting the performance of our CNN-based segmenter.</p><p>For K-shot setting, both contributions are naturally extended for segmenting the query image by jointly analyzing the provided support images and their ground-truths rather than treating support images independently as in prior work.</p><p>For evaluation, we compare with prior work on the benchmark PASCAL-5 i dataset <ref type="bibr" target="#b25">[26]</ref>. Our results demonstrate that we significantly outperform the state of the art. In addition, we perform evaluation on the larger and more challenging COCO-20 i dataset <ref type="bibr" target="#b15">[16]</ref>. To the best of our knowledge, we are the first to report results of few-shot object segmentation on COCO-20 i .</p><p>In the following, Sec. 2 reviews previous work, Sec. 3 specifies our two contributions, Sec. 4 describes our imple-mentation details and complexity, and Sec. 5 presents our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews related work on few-shot image classification and semantic segmentation.</p><p>Few-shot classification predicts image class labels with access to few training examples. Prior work can be broadly divided into three groups: transfer learning of models trained on classes similar to the target classes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, meta-learning approaches that learn how to effectively learn new classes from small datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, and generative approaches aimed at data-augmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Semantic segmentation labels all pixels in the image. Recently, significant advances have been made by using fully convolutional network (FCN) <ref type="bibr" target="#b16">[17]</ref> and its variantsincluding SegNet <ref type="bibr" target="#b0">[1]</ref>, UNet <ref type="bibr" target="#b22">[23]</ref>, RefineNet <ref type="bibr" target="#b14">[15]</ref>, PSPNet <ref type="bibr" target="#b32">[33]</ref>, DeepLab v1 <ref type="bibr" target="#b1">[2]</ref>, v2 <ref type="bibr" target="#b2">[3]</ref>, v3 <ref type="bibr" target="#b3">[4]</ref>, v3+ <ref type="bibr" target="#b4">[5]</ref> -all of which are usually evaluated on the PASCAL VOC 2012 <ref type="bibr" target="#b6">[7]</ref> and MSCOCO <ref type="bibr" target="#b15">[16]</ref> datasets. However, these approaches typically require very large training sets, which limits their application to a wide range of domains.</p><p>Few-shot semantic segmentation labels pixels of the query image that belong to a target object class, conditioned by the ground-truth segmentation masks of a few support images. Prior work typically draws from the above mentioned approaches to few-shot image classification and semantic segmentation. For example, the one-shot learning method OSLSM <ref type="bibr" target="#b25">[26]</ref> and its extensions -namely, Co-FCN <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, PL+SEG <ref type="bibr" target="#b5">[6]</ref>, and SG-One <ref type="bibr" target="#b31">[32]</ref> -consist of the conditioning and segmentation branches implemented as VGG <ref type="bibr" target="#b26">[27]</ref> and FCN-32s <ref type="bibr" target="#b16">[17]</ref>, respectively. The condi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine Similarity and Conv</head><p>Cross Entropy Loss</p><formula xml:id="formula_0">Predicted ! " # $ Support mask " # Score % $ = '()( ! " # $ , " # )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine Similarity and Conv</head><p>Feature maps -# Feature maps -.</p><p>Final Prediction:  tioning branch analyzes the target class in the support image, and conditions the segmentation branch for object segmentation in the query image. Co-FCN improves OSLSM by segmenting the query image based on a concatenation of pooled features from the support image and feature maps from the query image. PL+SEG first estimates a distance between the query's feature maps and prototypes predicted from the support image, and then labels pixels in the query image with the same class as their nearest neighbor prototypes. SG-One also estimates similarity between a pooled feature from the support image and feature maps of the query for predicting the query's segmentation. Our approach extends SG-One with the two contributions specified in the next section.</p><formula xml:id="formula_1">! " . = / $ ! " . $ % $</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>An object class is represented by K support images with ground-truth segmentation masks. Given a query image showing the same object class in the foreground, our goal is predict the foreground segmentation mask. For K = 1, this problem is called one-shot semantic segmentation. Below, and in the following Sec. 3.1 and Sec. 3.2, we consider the one-shot setting, for simplicity. Then, we discuss the K-shot setting, K &gt; 1, in Sec. 3.3</p><p>Given a large set of training images showing various object classes and the associated ground-truth segmentation masks, our approach follows the common episodic training strategy. In each training episode, we randomly sam-ple a pair of support and query images, x s and x q , with binary segmentation masks, m s and m q , of the target object class in the foreground. Elements of the mask are set to 1, m s,i = 1, for pixels i occupied by the target class; otherwise, m s,i = 0. The same holds for m q . We use m s to condition the target class in the query image.</p><p>The standard cross-entropy loss, L(m q , m q ), between the binary ground-truth m q and predicted query maskm q = f ? (x s , m s , x q ), is used for the end-to-end training of parameters ? of our deep architecture, ? * = arg min ? L(m q , m q ). <ref type="figure">Fig. 2</ref> shows the episodic training of a part of our deep architecture (without our second contribution which is not trained) on a pair of support x s and query x q images. We first use a CNN to extract feature maps F s ? R d?w?h from x s , and feature maps F q ? R d?w?h from x q , where d is the feature dimensionality, and w and h denote the width and height of the feature map. Then, we average F s over the known foreground locations in m s , resulting in the average class feature vector f s of the support image. For this masked feature averaging, m s is down-sampled t? m s ? {0, 1} w?h with the size w ? h of the feature maps, and f s is estimated as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training of Our Deep Architecture</head><formula xml:id="formula_2">f s = 1 |m s | wh i=1 F s,ims,i .<label>(1)</label></formula><p>where |m s | = im s,i is the number of foreground locations inm s . Next, we compute the cosine similarity between f s and every feature vector F q,i from the query feature maps F q . This gives a similarity map, ? q ? [0, 1] w?h , between the support and query image:</p><formula xml:id="formula_3">? q,i = cos(f s , F q,i ) = f T s F q,i f s 2 ? F q,i 2 , i = 1, . . . , w h.</formula><p>(2) We expect that ? q provides informative cues for object segmentation, as high values of ? q,i indicate likely locations of the target class in the query image.</p><p>Before we explain how to finally predictm q from ? q and F q , in the following, we specify our first technical contribution aimed at extending the above framework.</p><p>Contribution 1: Feature Weighting. For learning more discriminative features of the target class from a single (or very few) support image(s), we introduce a regularization that encourages high feature activations on the foreground and simultaneoulsy low feature activations on the background of the support image. This is formalized as an optimization problem for maximizing a sum of relevant differences between feature activations. Let ? s ?R d?1 denote a vector of feature differences normalized over the foreground and background areas in the segmentation map of the support image as</p><formula xml:id="formula_4">? s = wh i=1 F s,i m s,i |m s | ? 1 ?m s,i wh ? |m s | ,<label>(3)</label></formula><p>The relevance r ? R d?1 of features in ? s is estimated by maximizing a sum of the feature differences:</p><formula xml:id="formula_5">maximize r ? s r, s.t. r 2 = 1 .<label>(4)</label></formula><p>The problem in (4) has a closed-form solution:</p><formula xml:id="formula_6">r * = ? s ? s 2 .<label>(5)</label></formula><p>We use the estimated feature relevance r * when computing the similarity map between the support and query images. Specifically, we modify the cosine similarity between f s and F q , given by <ref type="formula">(2)</ref>, as</p><formula xml:id="formula_7">? * q,i = cos(f s , F q,i , r * ) = (f s r * ) T (F q,i r * ) f s r * 2 ? F q,i r * 2 ,<label>(6)</label></formula><p>where is the element-wise product between two vectors.</p><p>Note that we account for feature relevance in both training and testing. As r * has a closed-form solution, it can be computed very efficiently. Also, the modification of the similarity map in (6) is quite simple and cheap to implement in modern deep learning frameworks.</p><p>As shown in <ref type="figure">Fig. 2</ref>, in the final step of our processing, we concatenate ? * q and F q together, and pass them to a network with only two convolutional layers for predictingm q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contribution 2: Feature Boosting</head><p>In testing, the CNN is supposed to address a new object class which has not been seen in training. To improve generalization to the new class, in testing, we use a boosted inference -our second contribution -inspired by the gradient boosting <ref type="bibr" target="#b9">[10]</ref>. Alg.1 summarizes our boosted inference in testing. Note that in testing parameters of the CNN and convolutional layers remain fixed to the trained values.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, given a support image with groundtruth m s and a query image, in testing, we predict not only the query maskm q , but also the support maskm s using the same deep architecture as specified in Sec. 3.1.m s is estimated in two steps. First, we compute a similarity map, ? * s , as a dot product between f s r * and F s,i r * , as in <ref type="bibr" target="#b5">(6)</ref>. Second, we pass ? * s and F s to the two-layer convolutional network for predictingm s . Third, we estimate the standard cross-entropy loss L(m s , m s ), and iteratively update the average class features as</p><formula xml:id="formula_8">f n+1 s = f n s ? ? ?L(m n s , m s )/?f n s ,<label>(7)</label></formula><p>where ? is the learning rate. f n s , n = 1, . . . , N , are experts that we use for predicting the corresponding query masksm n q , by first estimating the similarity map ? * q,i = cos(f n s , F q,i , r * ), i = 1, ...wh, as in <ref type="formula" target="#formula_7">(6)</ref>, and then passing ? * q and F q to the two-layer network for computingm n q . Finally, we fuse the ensemble {m n q : n = 1, . . . , N } into the final segmentation,m q , a? where ? n denotes our estimate of the expert's confidence in correctly segmenting the target class, computed as the intersection-over-union score betweenm n s and m s :</p><formula xml:id="formula_9">m q = N n=1m n q ? n ,<label>(8)</label></formula><formula xml:id="formula_10">? n = IoU(m n s , m s ).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">K-shot Setting</head><p>When the number of support images K &gt; 1, prior work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32]</ref> predictsm k q for each support image independently, and then estimatesm q as an average over these predictions,m q = 1 K K k=1m k q . In contrast, our two contributions can be conveniently extended to the K-shot setting so as to further improve our robustness, beyond the standard averaging over K independent segmentations of the query.</p><p>Our contribution 1 is extended by estimating relevance r ? R d?1 of a more general difference vector of feature activations defined as</p><formula xml:id="formula_11">? s = K k=1 wh i F k s,i m k s,i |m k s | ? 1 ?m k s,i wh ? |m k s |<label>(10)</label></formula><p>Similar to <ref type="formula" target="#formula_5">(4)</ref> and <ref type="formula" target="#formula_6">(5)</ref>, the optimal feature relevance has a closed-form sulution r * = ?s ?s 2 . Note that we estimate r * jointly over all K support images, rather than as an average of independently estimated feature relevances for each support image. We expect the former (i.e., our approach) to be more robust than the latter.</p><p>Our contribution 2 is extended by having a more robust update of f n+1 s than in <ref type="formula" target="#formula_8">(7)</ref>:</p><formula xml:id="formula_12">f n+1 s = f n s ? ? ? K k=1 L(m k s , m k s )/?f n s ,<label>(11)</label></formula><p>where L(m k s , m k s ) is the cross entropy loss incurred for predicting the segmentation maskm k s using the unique vector f n s given by <ref type="bibr" target="#b10">(11)</ref> for every support image k, as explained in Sec. 3.2. Importantly, we do not generate K independent ensembles of experts {f n s : n = 1, . . . , N } k=1,K for each of the K support images. Rather, we estimate a single ensemble of experts more robustly over all K support images, starting with the initial expert f 1</p><formula xml:id="formula_13">s = 1 K K k=1 f k s .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details and Complexity</head><p>Implementation. The CNN we use is a modified version of either VGG-16 <ref type="bibr" target="#b26">[27]</ref> or ResNet-101 <ref type="bibr" target="#b11">[12]</ref>. Our CNN has the last two convolutional layers modified so as to have the stride equal to 1 instead of 2 in the original networks. This is combined with a dilated convolution to enlarge the receptive field with rates 2 and 4, respectively. So the final feature maps of our network have stride = 8, which is 1/8 of the input image size. For the two-layer convolutional network (Conv) aimed at producing the final segmentation, we use a 3 ? 3 convolution with ReLU and 128 channels, and a 1 ? 1 convolution with 2 output channels -background and foreground. It is worth noting that we do not use a CRF as a common post-processing step <ref type="bibr" target="#b13">[14]</ref>.</p><p>For implementation, we use Pytorch <ref type="bibr" target="#b18">[19]</ref>. Following the baselines <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, we pretrain the CNN on ImageNet <ref type="bibr" target="#b23">[24]</ref>. Training images are resized to 512 ? 512, while keeping the original aspect ratio. All test images keep their original size. Training is done with the SGD, learning rate 7e ?3 , batch size 8, and in 10,000 iterations. For the contribution 2, the number of experts N is analyzed in Sec. <ref type="bibr" target="#b4">5</ref>. For updating f n s in <ref type="formula" target="#formula_8">(7)</ref>, we use Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with ? = 1e ?2 .</p><p>Complexity. In training, prior work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>: <ref type="formula" target="#formula_2">(1)</ref>  In testing, complexity of prior work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> is the same as O(Train). Our contribution 2 increases complexity in testing by additionally estimating the ensemble of N segmentations of the query image. Therefore, in testing, our complexity is O(Test) = O(CNN)+O(N d w h). Thus, in testing, we increase only the smaller term of the total complexity. For small N , we have that the first term O(CNN) still dominates the total complexity. As we show in Sec. 5, for N = 10, we significantly outperform the state of the art, which justifies our slight increase in testing complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets. For evaluation, we use two datasets: (a) PASCAL-5 i which combines images from the PASCAL VOC 2012 <ref type="bibr" target="#b6">[7]</ref> and Extended SDS <ref type="bibr" target="#b10">[11]</ref> datasets; and (b) COCO-20 i which is based on the MSCOCO dataset <ref type="bibr" target="#b15">[16]</ref>. For PASCAL-5 i , we use the same 4-fold cross-validation setup as prior work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6]</ref>. Specifically, from the 20 object classes in PASCAL VOC 2012, for each fold i = 0, ..., 3, we sample five as test classes, and use the remaining 15 classes for training. Tab. 1 specifies our test classes for each fold of PASCAL-5 i . As in <ref type="bibr" target="#b25">[26]</ref>, in each fold i, we use 1000 support-query pairs of test images sampled from the selected five test classes.</p><p>We create COCO-20 i for evaluation on a more challenging dataset than PASCAL-5 i , since MSCOCO has 80 object classes and its ground-truth segmentation masks have Dataset Test classes PASCAL-5 0 aeroplane, bicycle, bird, boat, bottle PASCAL-5 <ref type="bibr" target="#b0">1</ref> bus, car, cat, chair, cow PASCAL-5 <ref type="bibr" target="#b1">2</ref> diningtable, dog, horse, motorbike, person PASCAL-5 <ref type="bibr" target="#b2">3</ref> potted plant, sheep, sofa, train, tv/monitor     <ref type="table" target="#tab_2">Phase  B  B+C1 B+C2 B+C1+C2  VGG  Train  171  172  171  172  Test  148  150  211  211  ResNet Train  386  388  386  388  Test  268  280  360  360   Table 3</ref>. Training and test times per example in milliseconds, with 1 Nvidia 1080 Ti GPU on PASCAL-5 i , for different ablations indicated in the top row. Using C2 increases only the test time. Specifically, we use K = 10 in our experiments</p><p>Metrics. As in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6]</ref>, we use the mean intersectionover-union (mIoU) for quantitative evaluation. IoU of class l is defined as IoU l = T P l T P l +F P l +F N l , where T P, F P and F N are the number of pixels that are true positives, false positives and false negatives of the predicted segmentation masks, respectively. The mIoU is an average of the IoUs of different classes, mIoU = 1 n l l IoU l , where n l is the number of test classes. We report the mIoU averaged over the four folds of cross-validation.</p><p>Baselines, Ablations, and Variants of Our Approach. As a baseline B, we consider the approach depicted in the gray box "PRIOR WORK" in <ref type="figure">Fig. 2</ref> and specified in Sec. 3.1 before our contribution 1. We also consider several ablations of our approach: B+C1 -extends the baseline B with contribution 1 only; B+C2 -extends the baseline B with contribution 2 only; and B+C1+C2 -represents our full approach. These ablations are aimed at testing the effect of each of our contributions on performance. In addition, we consider two alternative neural networks -VGG 16 and ResNet 101 -as the CNN for extracting image features. VGG 16 has also been used in prior work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6]</ref>. We also compare with an approach called Upper-bound that represents a variant of our full approach B+C1+C2 trained such that both training and testing datasets consist of the same classes. As Upper-bound does not encounter new classes in testing, it represents an upper bound of our full approach. Finally, in the K-shot setting, we consider another baseline called Average. It represents our full approach B+C1+C2 that first independently predicts segmentations of the query image for each of the K &gt; 1 support images, and then averages all of the predictions. Our approach for the K-shot setting is called Our-K-shot, and differs from Average in that we rather jointly analyze all of the K support images than treat them independently, as explained in Sec. 3.3.</p><p>Training/testing time. The training/testing time is reported in Tab. 3. We can see that the contribution 1 just adds very small computational overhead over the baseline but significantly outperforms the baseline. Additionally, although contribution 2 has substantially larger testing time (about 40% with VGG backbone and 35% with ResNet backbone compare to the baseline), but it yields more significant performance gain than contribution 1 does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Methods PASCAL-5 0 PASCAL-5 1 PASCAL-5 2 PASCAL-5 <ref type="bibr" target="#b2">3</ref>   <ref type="table">Table 5</ref>. Mean IoU of five-shot segmentation on PASCAL-5 i . The best results are in bold.</p><p>One-shot Segmentation.</p><p>Tab. 4 compares our B+C1+C2 with the state of the art, ablations, and aforementioned variants in the one-shot setting on PASCAL-5 i . B+C1+C2 gives the best performance for both VGG 16 and ResNet 101, where the latter configuration significantly outperforms the state of the art with the increase in the mIoU averaged over the four folds of cross-validation by 13.49%. Relative to B, our first contribution evaluated with B+C1 gives relatively modest performance improvements. From the results for B+C2, our second contribution produces larger gains in performance relative to B and B+C1, suggesting that contribution 2 in and of itself is more critical than contribution 1. Interestingly, combining both contribution 1 and contribution 2 significantly improves the results relative to using either contribution only. We also observe that performance of our B+C1+C2 for some folds of crossvalidation (e.g., PASCAL-5 2 and PASCAL-5 3 ) comes very close to that of Upper-bound, suggesting that our approach is very effective in generalizing to new classes in testing. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the mIoU of B+C1+C2 as a function of the number of experts N in the one-shot setting on PASCAL-5 i . As can be seen, for N ? 10 our approach is not sensitive to a particular choice of N . We use N = 10 as a good trade-off between complexity and accuracy. <ref type="figure">Figure 5</ref>. Examples from PASCAL-5 0 . Top row: the support images with ground-truth segmentations in yellow. Middle row: the query images and segmentations in red predicted by B+C1+C2 in the one-shot setting. Bottom row: the query images and segmentations in red predicted by Our-K-shot in the five-shot setting (the remaining four support images are not shown). Our-K-shot in general improves performance over B+C1+C2, as Our-K-shot effectively uses the higher level of supervision in the five-shot setting. Best viewed in color.</p><p>Five-shot Segmentation. Tab. 5 compares Our-K-shot with the state of the art and Average in the five-shot setting  on PASCAL-5 i . Our-K-shot gives the best performance for both VGG 16 and ResNet 101, where the latter configuration significantly outperforms the state of the art with the increase in the mIoU averaged over the four folds of crossvalidation by 15.97%. In comparison with Average, the joint analysis of K support images by Our-K-shot appears to be more effective, as Our-K-shot gives superior performance in every fold of cross-validation. Results on COCO-20 i . Tab. 6 and Tab. 7 shows our ablations' results in the one-shot and five-shot settings on COCO-20 i . The former results are obtained with B+C1+C2 and the latter, with Our-K-shot. The lower values of mIoU relative to those in Tab. 4 and Tab. 5 indicate that COCO-20 i is more challenging than PASCAL-5 i . Surprisingly, in fold COCO-20 0 , B+C1+C2 with VGG 16 outperforms its counterpart with ResNet 101 in the one-shot setting. The same holds for Our-K-shot in the five-shot setting. On average, using ResNet 101 gives higher results. As expected, the increased supervision in the five-shot setting in general gives higher accuracy than the one-shot setting.</p><p>Qualitative Results. <ref type="figure">Fig. 5</ref> shows challenging examples from PASCAL-5 0 , and our segmentation results obtained with B+C1+C2 with ResNet 101 for the one-shot setting, and Our-K-shot with ResNet 101 for the five-shot setting. In the leftmost column, the bike in the support image has different pose from the bike in the query image. While this example is challenging for B+C1+C2, our performance improves when using Our-K-shot. In the second column from left, the query image shows a partially occluded target -a part of the bottle. With five support images, Our-K-shot improves performance by capturing the bottle's shadow. The third column from left shows that the bike's features in the support image are insufficiently discriminative as the person also gets segmented along with the bike. With more ex-amples, the bike is successfully segmented by Our-K-shot. In the rightmost column, the plane in the support image is partially occluded, and thus in the query image B+C1+C2 can only predict the head of the airplane while Our-K-shot's predicted segment covers most of the airplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have addressed one-shot and few-shot object segmentation, where the goal is to segment a query image, given a support image and the support's ground-truth segmentation. We have made two contributions. First, we have formulated an optimization problem that encourages high feature responses on the foreground and low feature activations on the background for more accurate object segmentation. Second, we have specified the gradient boosting of our model for fine-tuning to new classes in testing. Both contributions have been extended to the few-shot setting for segmenting the query by jointly analyzing the provided support images and their ground truths, rather than treating the support images independently. For evaluation, we have compared with prior work, strong baselines, ablations and variants of our approach on the PASCAL-5 i and COCO-20 i datasets. We significantly outperform the state of the art on both datasets and in both one-shot and five-shot settings. Using only the second contribution gives better results than using only the first contribution. Our integration of both contributions gives a significant gain in performance over each.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Our new boosted inference: We generate an ensemble of the support's features guided by the gradient of loss, incurred when segmenting the target object in the support image. A dot product between the query's feature maps and the ensemble of the support's features gives the corresponding ensemble of similarity maps, whose weighted average is taken as our final segmentation of the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Guided Ensemble Inference in Testing Input: F s , f s , F q , m s , ? Output:m q // Guided ensemble of experts 1. Initialize: f 1 s = f s , E = {}, R = {}; 2. for n = 1, . . . , N do a. ? * s,i = cos(f n s , F n s,i , r * ), i = 1, ...wh, as in (6) ; b.m n s = Conv(? * s , F n s ); c. ? n = IoU(m n s , m s ); d. Compute cross-entropy loss: L(m n s , m s ); e. f n+1 s = f n s ? ? ?L(m n s , m s )/?f n s ; f. E = E ? {f n s }, R = R ? {? n }, end // Inference using E and R 4. for n = 1, . . . , N do a. ? * q,i = cos(f n s , F q,i , r * ), i = 1, ...wh, as in (6) ; b.m n q = Conv(? * q , F q ); end 5.m q = N n=1m n q ? n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Uses a CNN with complexity O(CNN) for extracting features from the support and query images, (2) Computes the similarity map with complexity O(d w h), and (3) [32] additionally uses a convolutional network for segmenting the query with complexity O(Conv). Note that O(Conv) = O(d w h) as both the similarity map and convolutions in the Conv network are computed over feature maps with the size d ? w ? h. Also, note that O(d w h) is significantly smaller than O(CNN). Our contribution 1 additionally computes the feature relevance using the closed-form solution with a linear complexity in the size of the feature maps O(d w h). Therefore, our total training complexity is equal to that of prior work [32, 20, 21]: O(Train) = O(CNN) + O(d w h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The mIoU of B+C1+C2 as a function of the number of experts N in the one-shot setting on PASCAL-5 i . Part 0 -Part 3 denote the four folds of cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Evaluation on PASCAL-5 i uses the 4-fold crossvalidation. The table specifies 5 test classes used in each fold i = 0, ..., 3. The remaining 15 classes are used for training.lower quality than those in PASCAL VOC 2012. To the best of our knowledge, no related work has reported oneshot object segmentation on MSCOCO. For evaluation on COCO-20 i , we use 4-fold cross-validation. From the 80 object classes in MSCOCO, for each fold i = 0, ..., 3, we sample 20 as test classes, and use the remaining 60 classes for training. Tab. 2 specifies our test classes for each fold of COCO-20 i . In each fold, we sample 1000 support-query pairs of test images from the selected 20 test classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Evaluation on COCO-20 i uses the 4-fold cross-validation. The table specifies 20 test classes used in each fold i = 0, ..., 3. The remaining 60 classes are used for training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Mean IoU of one-shot segmentation on PASCAL-5 i . The best results are in bold.</figDesc><table><row><cell>Mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The mIoU of ablations in the one-shot setting on COCO-20 i .</figDesc><table><row><cell>Backbone</cell><cell>Methods</cell><cell cols="5">COCO-20 0 COCO-20 1 COCO-20 2 COCO-20 3 Mean</cell></row><row><cell>VGG 16</cell><cell>B</cell><cell>12.13</cell><cell>11.65</cell><cell>14.86</cell><cell>22.09</cell><cell>13.26</cell></row><row><cell></cell><cell>B+C1</cell><cell>15.65</cell><cell>13.91</cell><cell>15.36</cell><cell>23.50</cell><cell>17.11</cell></row><row><cell></cell><cell>B+C2</cell><cell>13.28</cell><cell>14.91</cell><cell>19.50</cell><cell>23.22</cell><cell>17.73</cell></row><row><cell></cell><cell>B+C1+C2</cell><cell>18.35</cell><cell>16.72</cell><cell>19.59</cell><cell>25.43</cell><cell>20.02</cell></row><row><cell cols="2">ResNet 101 B</cell><cell>13.11</cell><cell>14.80</cell><cell>14.54</cell><cell>26.48</cell><cell>17.23</cell></row><row><cell></cell><cell>B+C1</cell><cell>15.48</cell><cell>14.76</cell><cell>16.85</cell><cell>28.35</cell><cell>18.86</cell></row><row><cell></cell><cell>B+C2</cell><cell>14.09</cell><cell>17.82</cell><cell>18.46</cell><cell>27.85</cell><cell>19.56</cell></row><row><cell></cell><cell>B+C1+C2</cell><cell>16.98</cell><cell>17.98</cell><cell>20.96</cell><cell>28.85</cell><cell>21.19</cell></row><row><cell>Backbone</cell><cell>Methods</cell><cell cols="5">COCO-20 0 COCO-20 1 COCO-20 2 COCO-20 3 Mean</cell></row><row><cell>VGG 16</cell><cell>Average</cell><cell>20.76</cell><cell>16.87</cell><cell>20.55</cell><cell>27.61</cell><cell>21.45</cell></row><row><cell cols="2">ResNet 101 Average</cell><cell>18.73</cell><cell>18.46</cell><cell>21.27</cell><cell>29.20</cell><cell>21.92</cell></row><row><cell>VGG 16</cell><cell>Our-K-shot</cell><cell>20.94</cell><cell>19.24</cell><cell>21.94</cell><cell>28.39</cell><cell>22.63</cell></row><row><cell cols="2">ResNet 101 Our-K-shot</cell><cell>19.13</cell><cell>21.46</cell><cell>23.93</cell><cell>30.08</cell><cell>23.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The mIoU of B+C1+C2 in the five-shot setting on COCO-20 i .</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported in part by DARPA XAI Award N66001-17-2-4029 and AFRL STTR AF18B-T002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot segmentation propagation with guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07373</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. U-Net</forename></persName>
		</author>
		<title level="m">Convolutional networks for biomedical image segmentation. International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2850" to="2860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th British Machine Vision Conference (BMVC)</title>
		<meeting>the 28th British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Irfan Essa, and Byron Boots</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
