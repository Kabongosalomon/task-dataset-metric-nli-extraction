<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Streaming Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengtian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uiuc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Argo</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Towards Streaming Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. The responsiveness of the agent is largely governed by latency of its processing pipeline. While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the Pareto optimal latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular frame, the surrounding world has changed. To these ends, we present an approach that coherently integrates latency and accuracy into a single metric for real-time online perception, which we refer to as "streaming accuracy". The key insight behind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ignored while computation is occurring. More broadly, building upon this metric, we introduce a meta-benchmark that systematically converts any single-frame task into a streaming perception task. We focus on the illustrative tasks of object detection and instance segmentation in urban video streams, and contribute a novel dataset with high-quality and temporally-dense annotations. Our proposed solutions and their empirical analysis demonstrate a number of surprising conclusions: (1) there exists an optimal "sweet spot" that maximizes streaming accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous tracking and future forecasting naturally emerge as internal representations that enable streaming perception, and (3) dynamic scheduling can be used to overcome temporal aliasing, yielding the paradoxical result that latency is sometimes minimized by sitting idle and "doing nothing".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. A crucial quantity governing the responsiveness of the agent is its reaction time. Practical applications, such as self-driving vehicles or augmented reality and virtual reality (AR/VR), may require reaction time that rivals that of humans, which is typically 200 milliseconds (ms) for visual stimuli <ref type="bibr">[22]</ref>. In such settings, low-latency algorithms are imperative to ensure safe operation or enable a truly immersive experience.</p><p>Historically, the computer vision community has not particularly focused on algorithmic latency. This is one reason why a disparate set of techniques (and arXiv:2005.10420v2 [cs.CV] 25 Aug 2020</p><formula xml:id="formula_0">A B = 1 = 2</formula><p>Computation <ref type="figure">Fig. 1</ref>. Latency is inevitable in a real-world perception system. The system takes a snapshot of the world at t1 (the car is at location A), and when the algorithm finishes processing this observation, the surrounding world has already changed at t2 (the car is now at location B, and thus there is a mismatch between prediction A and ground truth B). If we define streaming perception as a task of continuously reporting back the current state of the world, then how should one evaluate vision algorithms under such a setting? We invite the readers to watch a video on the project website that compares a standard frame-aligned visualization with our latency-aware visualization <ref type="bibr">[Link]</ref>.</p><p>conference venues) have been developed for robotic vision. Interestingly, latency has been well studied recently (e.g., fast but not necessarily state-of-the-art accurate detectors such as <ref type="bibr">[34,</ref><ref type="bibr">27,</ref><ref type="bibr">25]</ref>). But it has still been primarily explored in an offline setting. Vision-for-online-perception imposes quite different latency demands as shown in <ref type="figure">Fig. 1</ref>, because by the time an algorithm finishes processing a particular frame -say, after 200ms -the surrounding world has changed! This forces perception to be ultimately predictive of the future. In fact, such predictive forecasting is a fundamental property of human vision (e.g., as required whenever a baseball player strikes a fast ball <ref type="bibr">[31]</ref>). So we argue that streaming perception should be of interest to general computer vision researchers.</p><p>Contribution (meta-benchmark) To help explore embodied vision in a truly online streaming context, we introduce a general meta-benchmark that systematically converts any single-frame task into a streaming perception task. Our key insight is that streaming perception requires understanding the state of the world at all time instants -when a new frame arrives, streaming algorithms must report the state of the world even if they have not done processing the previous frame. Within this meta-benchmark, we introduce an approach to measure the real-time performance of perception systems. The approach is as simple as querying the state of the world at all time instants, and the quality of the response is measured by the original task metric. Such an approach naturally merges latency and accuracy into a single metric. Therefore, the trade-off between accuracy versus latency can now be measured quantitatively. Interestingly, our meta-benchmark naturally evaluates the perception stack as a whole.</p><p>For example, a stack may include detection, tracking, and forecasting modules.</p><p>Our meta-benchmark can be used to directly compare such modular stacks to end-to-end black-box algorithms <ref type="bibr">[28]</ref>. In addition, our approach addresses the issue that overall latency of concurrent systems is hard to evaluate (e.g., latency cannot be simply characterized by the runtime of a single module).</p><p>Contribution (analysis) Motivated by perception for autonomous vehicles, we instantiate our meta-benchmark on the illustrative tasks of object detection and instance segmentation in urban video streams. Accompanied with our streaming evaluation is a novel dataset with high-quality, high-frame-rate, and temporallydense annotations of urban videos. Our evaluation on these tasks demonstrates a number of surprising conclusions. <ref type="bibr" target="#b0">(1)</ref> Streaming perception is significantly more challenging than offline perception. Standard metrics like object-detection average precision (AP) dramatically drop (from 38.0 to 6.2), indicating the need for the community to focus on such problems. (2) Decision-theoretic scheduling, asynchronous tracking, and future forecasting naturally emerge as internal representations that enable accurate streaming perception, recovering much of the performance drop (boosting performance to <ref type="bibr">17.8)</ref>. With simulation, we can verify that infinite compute resources modestly improves performance to 20.3, implying that our conclusions are fundamental to streaming processing, no matter the hardware. (3) It is well known that perception algorithms can be tuned to trade off accuracy versus latency. Our analysis shows that there exists an optimal "sweet spot" that uniquely maximizes streaming accuracy. This provides a different perspective on such well-explored trade-offs. (4) Finally, we demonstrate the effectiveness of decision-theoretic reasoning that dynamically schedules which frame to process at what time. Our analysis reveals the paradox that latency is minimized by sometimes sitting idle and "doing nothing"! Intuitively, it is sometimes better to wait for a fresh frame rather than to begin processing one that will soon become "stale".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Latency evaluation Latency is a well-studied subject in computer vision. One school of research focuses on reducing the FLOPS of backbone networks <ref type="bibr">[20,</ref><ref type="bibr">40]</ref>, while another school focuses on reducing the runtime of testing time algorithms <ref type="bibr">[34,</ref><ref type="bibr">27,</ref><ref type="bibr">25]</ref>. We follow suit and create a latency-accuracy plot under our experiment setting <ref type="figure">(Fig. 2</ref>). While such a plot is suggestive of the trade-off for offline data processing (e.g., archived video footage), it fails to capture the fact that when the algorithm finishes processing, the surrounding world has already changed. Therefore, we believe that existing plots do not reveal the streaming performance of these algorithms. Aside from computational latency, prior work has also investigated algorithmic latency <ref type="bibr">[30]</ref>, evaluated by running algorithms on a video in the offline fashion and measuring how many frames are required to detect an object after it appears. In comparison, our evaluation is done in the more realistic online real-time setting, and applies to any single-frame task, instead of just object detection. Prior art routinely explores the trade-off between detection accuracy versus runtime. We generate the above plot by varying the input resolution of each detection network. We argue that such plots are exclusive to offline processing and fail to capture latencyaccuracy trade-offs in streaming perception. AP stands for average precision, and is a standard metric for object detection <ref type="bibr">[26]</ref>.</p><p>Real-time evaluation There has not been much prior effort to evaluate vision algorithms in the real-time fashion in the research community. Notable exceptions include work on real-time tracking and real-time simultaneous localization and mapping (SLAM). First, the VOT2017 tracking benchmark specifically included a real-time challenge <ref type="bibr">[23]</ref>. Its benchmark toolkit sends out frames at 20 FPS to participants' trackers and asks them to report back results before the next frame arrives. If the tracker fails to respond in time, the last reported result is used. This is equivalent to applying zero-order hold to trackers' outputs. In our benchmarks, we adopt a similar zero-order hold strategy, but extend it to a broader context of arbitrary single-frame tasks and allow for a more delicate interplay between detection, tracking, and forecasting. Second, the literature on real-time SLAM also considers benchmark evaluation under a "hard-enforced" real-time requirement <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>. Our analysis suggests that hardenforcement is too stringent of a formulation; algorithms should be allowed to run longer than the frame rate, but should still be scored on their ability to report the state of the world (e.g., localized map) at frame rate.</p><p>Progressive and anytime algorithms There exists a body of work on progressive and anytime algorithms that can generate outputs with lower latency. Such work can be traced back to classic research on intelligent planning under resource constraints <ref type="bibr" target="#b3">[4]</ref> and flexible computation <ref type="bibr" target="#b18">[19]</ref>, studied in the context of AI with bounded rationality <ref type="bibr">[35]</ref>. Progressive processing [42] is a paradigm that splits up an algorithm into sequential modules that can be dynamically scheduled. Often, scheduling is formulated as a decision-theoretic problem under resource constraints, which can be solved in some cases with Markov decision processes (MDPs) <ref type="bibr">[41,</ref><ref type="bibr">42]</ref>. Anytime algorithms are capable of returning a solution at any point in time <ref type="bibr">[41]</ref>. Our work revisits these classic computation paradigms in the context of streaming perception, specifically demonstrating that classic visual tasks (like tracking and forecasting) naturally emerge in such bounded resource settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Evaluation</head><p>In the previous section, we have shown that existing latency evaluation fails to capture the streaming performance. To address this issue, here we propose a new  <ref type="figure">Fig. 3</ref>. Our proposed streaming perception evaluation. A streaming algorithm f is provided with (timestamped) observations up until the current time t and refreshes an output buffer with its latest prediction of the current state of the world. At the same time, the benchmark constantly queries the output buffer for estimates of world states. Crucially, f must consider the amount of streaming observations that should be ignored while computation is occurring. method of evaluation. Intuitively, a streaming benchmark no longer evaluates a function, but a piece of executable code over a continuous time frame. The code has access to a sensor input buffer that stores the most recent image frame. The code is responsible for maintaining an output buffer that represents the up-todate estimate of the state of the world (e.g., a list of bounding boxes of objects in the scene). The benchmark examines this output buffer, comparing it with a ground truth stream of the actual world state ( <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal definition</head><p>We model a data stream as a set of sensor observations, ground-truth world states, and timestamps, denoted respectively as</p><formula xml:id="formula_1">{(x i , y i , t i )} T i=1 .</formula><p>Let f be a streaming algorithm to be evaluated. At any continuous time t, the algorithm f is provided with observations (and timestamps) that have appeared so far:</p><formula xml:id="formula_2">{(x i , t i )|t i ? t} [accessible input at time t] (1)</formula><p>We allow the algorithm f to generate an output prediction at any time. Let s j be the timestamp that indicates when a particular prediction? j is produced. The subscript j indexes over the N outputs generated by f over the entire stream:</p><formula xml:id="formula_3">{(? j , s j )} N j=1</formula><p>[all outputs by f ] <ref type="bibr" target="#b1">(2)</ref> Note that this output stream is not synchronized with the input stream, and N has no direct relationship with T . Generally speaking, we expect algorithms to run slower than the frame rate (N &lt; T ).</p><p>We benchmark the algorithm f by comparing its most recent output at time t i to the ground-truth y i . We first compute the index of the most recent output:</p><formula xml:id="formula_4">?(t) = arg max j s j &lt; t [real-time constraint] (3)</formula><p>This is equivalent to the benchmark applying a zero-order hold for the algorithm's outputs to produce continuous estimation of the world states. Given an arbitrary single-frame loss L, the benchmark formally evaluates:</p><formula xml:id="formula_5">L streaming = L({(y i ,? ?(ti) )} T i=1 ) [evaluation] (4)</formula><p>By construction, the streaming loss above can be applied to any single-frame task that computes a loss over a set of ground truth and prediction pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Emergent tracking and forecasting</head><p>At first glance, "instant" evaluation may seem unreasonable: the benchmark at time t queries the state at time t. Although x t is made available to the algorithm, any finite-time algorithm cannot make use of it to generate its prediction. For example, if the algorithm takes time ?t to perform its computation, then to make a prediction at time t, it can only use data before time t ? ?t. We argue that this is the realistic setting for streaming perception, both in biological and robotic systems. Humans and autonomous vehicles must react to the instantaneous state of the world when interacting with dynamic scenes. Such requirements strongly suggest that perception should be inherently predictive of the future. Our benchmark similarly "forces" algorithms to reason and forecast into the future, to compensate for the mismatch between the last processed observation and the present. One may also wish to take into account the inference time of downstream actuation modules (that say, need to optimize a motion plan that will be executed given the perceived state of the world). It is straightforward to extend our benchmark to require algorithms to generate a forecast of the world state when the downstream module finishes its processing. For example, at time t the benchmark queries the state of the world at time t + ?, where ? &gt; 0 represents the inference time of the downstream actuation module.</p><p>In order to forecast, the algorithms need to reason temporally through tracking (in the case of object detection). For example, constant velocity forecasting requires the tracks of each object over time in order to compute the velocity. Generally, there are two categories of trackers -post-hoc association <ref type="bibr" target="#b2">[3]</ref> and template-based visual tracking <ref type="bibr">[29]</ref>. In this paper, we refer them in short as "association" and "tracking", respectively. Association of previously computed detections can be made extremely lightweight with simple linking of bounding boxes (e.g., based on the overlap). However, association does not make use of the image itself as done in (visual) tracking. We posit that trackers may produce better streaming accuracy for scenes with highly unpredictable motion. As part of emergent solutions to our streaming perception problem, we include both association and tracking in our experiments in the next section.</p><p>Finally, it is natural to seek out an end-to-end system that directly optimizes streaming perception accuracy. We include one such method in Appendix C.2 to show that tracking and forecasting-based representations may also emerge from gradient-based learning. Because our metric is runtime dependent, we need to specify the computational constraints to enable a fair comparison between algorithms. We first investigate a single GPU model ( <ref type="figure" target="#fig_4">Fig. 4a</ref>), which is used for existing latency analysis in prior art. In the single GPU model, only a single GPU job (e.g., detection or visual tracking) can run at a time. Such a restriction avoids multi-job interference and memory capacity issues. Note that a reasonable number of CPU jobs are allowed to run concurrently with the GPU job. For example, we allow bounding box association and forecasting modules to run on the CPU in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computational constraints</head><p>Nowadays, it is common to have multiple GPUs in a single system. We investigate an infinite GPU model ( <ref type="figure">Fig. 4b)</ref>, with no restriction on the number of GPU jobs that can run concurrently. We implement this infinite computation model with simulation, described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Challenges for practical implementation</head><p>While our benchmark is conceptually simple, there are several practical hurdles. First, we require high-frame-rate ground truth annotations. However, due to high annotation cost, most existing video datasets are annotated at rather sparse frame rates. For example, YouTube-VIS is annotated at 6 FPS, while the video data rate is 30 FPS <ref type="bibr">[39]</ref>. Second, our evaluation is hardware dependent -the same algorithm on different hardware may yield different streaming performance. Such hardware-in-the-loop testing is commonplace in control systems <ref type="bibr" target="#b0">[1]</ref> and arguably vital for embodied perception (which should by definition, depend on the agent's body!). Third, stochasticity in actual runtimes yields stochasticity in the streaming performance. Note that the last two issues are also prevalent in existing offline runtime analyses. Here we present high-level ideas for the solutions and leave additional details to Appendix A.2 &amp; A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo ground truth</head><p>We explore the use of pseudo ground truth labels as a surrogate to manual high-frame-rate annotations. The pseudo labels are obtained by running state-of-the-art, arbitrarily expensive offline algorithms on each frame of a benchmark video. While the absolute performance numbers (when benchmarked on ground truth and pseudo ground truth labels) differ, we find that the rankings of algorithms are remarkably stable. The Pearson correlation coefficient of the scores of the two ground truth sets is 0.9925, suggesting that the real score is literally a linear function of the pseudo score. Moreover, we find that offline pseudo ground truth could also be used to self-supervise the training of streaming algorithms.</p><p>Simulation While streaming performance is hardware dependent, we now demonstrate that the benchmark can be evaluated on simulated hardware. In simulation, the benchmark assigns a runtime to each module of the algorithm, instead of measuring the wall-clock time. Then based on the assigned runtime, the simulator generates the corresponding output timestamps. The assigned runtime to each module provides a layer of abstraction on the hardware.</p><p>The benefit of simulation is to allow us to assess the algorithm performance on non-existent hardware, e.g., a future GPU that is 20% faster or infinite GPUs in a single system. Simulation also allows our benchmark to inform practitioners about the design of future hardware platforms, e.g., one can verify with simulation that 4 GPUs may be "optimal" (producing the same streaming accuracy as infinite GPUs).</p><p>Runtime-induced variance Due to algorithmic choice and system scheduling, different runs of the same algorithm may end up with different runtimes. This variation across runs also affects the overall streaming performance. Fortunately, we empirically find that such variance causes a standard deviation of up to 0.5% under our experiment setting. Therefore, we omit variance report in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Solutions and Analysis</head><p>In this section, we instantiate our meta-benchmark on the illustrative task of object detection. While we show results on streaming detection, several key ideas also generalize to other tasks. An instantiation on instance segmentation can be found in Appendix A.6. We first explain the setup and present the solutions and analysis. For the solutions, we first consider single-frame detectors, and then add forecasting and tracking one by one into the discussion. We focus on the most effective combination of detectors, trackers, and forecasters which we have evaluated, but include additional methods in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We extend the publicly available video dataset Argoverse 1.1 <ref type="bibr" target="#b6">[7]</ref> with our own annotations for streaming evaluation, which we name Argoverse-HD (High-framerate Detection). It contains diverse urban outdoor scenes from two US cities. We select Argoverse for its embodied setting (autonomous driving) and its highframe-rate sensor data (30 FPS). We focus on the task of 2D object detection for our streaming evaluation. Under this setting, the state of the world y t is a list of bounding boxes of the objects of interest. While Argoverse has multiple sensors, we only use the center RGB camera for simplicity. We collect our own  <ref type="bibr" target="#b6">[7]</ref>, overlaid with our dense 2D annotation (at 30 FPS). Bottom presents results of Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> (ResNet 50) evaluated on the two datasets. APL, APM and APS denote AP for large, medium and small objects respectively. AP50, AP75 denote AP with IoU (Intersection over Union) thresholds at 0.5 and 0.75 respectively. We first observe that the APs are roughly comparable, showing that our annotation is reasonable in evaluating object detection performance. Second, we see a significant drop in APS from COCO to ours, suggesting that the detection of small objects is more challenging in our setting. For self-driving vehicle applications, those small objects are important to identify when the ego-vehicle is traveling at a high speed or making unprotected turns.</p><p>annotations since the dataset does not provide dense 2D annotations 1 . For the annotations, we follow MS COCO [26] class definitions and format. For example, we include the "iscrowd" attribute for ambiguous cases where each instance cannot be identified, and therefore the algorithms will not be wrongfully penalized. We use only a subset of 8 classes (from 80 MS COCO classes) that are directly relevant to autonomous driving: person, bicycle, car, motorcycle, bus, truck, traffic light, and stop sign. This definition allows us to evaluate off-the-shelf models trained on MS COCO. No training is involved in the following experiments unless otherwise specified. All numbers are computed on the validation set, which contains 24 videos ranging from 15-30 seconds each (the total number of frames is 15k). <ref type="figure">Figure 5</ref> shows a comparison of our annotation with that of MS COCO. Additional comparison with other related datasets can be found in Appendix A.4. All output timing is measured on a single Geforce GTX 1080 Ti GPU (a Tesla V100 counterpart is provided in Appendix A.7). <ref type="table" target="#tab_4">Table 1</ref> includes the main results of using just detectors for streaming perception. We first examine the case of running a state-of-the-art detector -Hybrid Task Cascade (HTC) <ref type="bibr" target="#b7">[8]</ref>, both in the offline and the streaming settings. The AP drops <ref type="table" target="#tab_4">Table 1</ref>. Performance of existing detectors for streaming perception. The number after @ is the input scale (the full resolution is 1920 ? 1200). * means using GPU for image pre-processing as opposed to using CPU in the off-the-shelf setting. The last column is the mean runtime of the detector for a single frame in milliseconds (mask branch disabled if applicable). The first baseline is to run an accurate detector (row 1), and we observe a significant drop of AP in the online real-time setting (row 2). Another commonly adopted baseline for embodied perception is to run a fast detector (row 3-4), whose runtime is smaller than the frame interval (33ms for 30 FPS streams). Neither of these baselines achieves good performance. Searching over a wide suite of detectors and input scales, we find that the optimal solution is Mask R-CNN (ResNet 50) operating at 0.5 input scale (row 5-6). In addition, our scheduling algorithm (Alg. 1) boosts the performance by 1.0/2.3 for AP/APL (row 7). In the hypothetical infinite GPU setting, a more expensive detector yields better trade-off (input scale switching from 0.5 to 0.75, almost doubling the runtime), and it further boosts the performance to 14.4 (row 8), which is the optimal solution achieved by just running the detector. Simulation suggests that 4 GPUs suffice to maximize streaming accuracy for this solution ID Method Detector AP APL APM APS AP50 AP75 Runtime significantly in the streaming setting. Such a result is not entirely surprising due to its high runtime (700ms). A commonly adopted strategy for real-time applications is to run a detector that is within the frame rate. We point out that this strategy may be problematic, since such a hard-constrained time budget results in poor accuracy for challenging tasks <ref type="table" target="#tab_4">(Table 1 row 3-4</ref>). In addition, we find that many existing network implementations are optimized for throughput rather than latency, reflecting the bias of the community for offline versus online processing! For example, image pre-processing (e.g., resizing and normalizing) is often done on CPU, where it can be pipelined with data pre-fetching. By moving it to GPU, we save 21ms in latency (for an input of size 960 ? 600).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detection-Only</head><p>Our benchmarks allow streaming algorithms to choose which frames to process/ignore. <ref type="figure">Figure 6</ref> compares a straight-forward schedule with our dynamic schedule (Alg. 1), which attempts to address temporal aliasing of the former. While spatial aliasing and quantization has been studied in computer vision <ref type="bibr" target="#b17">[18]</ref>, temporal quantization in the streaming setting has not been well explored. Noteably, it is difficult to pre-compute the optimal schedule because of the stochasticity of actual runtimes. Our proposed scheduling policy (Alg. 1) tries to minimize the expected temporal mismatch of the output stream and the data stream, thus increasing the overall streaming performance. Empirically, we find that it raises the AP for the detector (  <ref type="figure">Fig. 6</ref>. Algorithm scheduling for streaming perception with a single GPU. (a) A fast detector finishes processing the current frame before the next frame arrives. An accurate (but slow) detector cannot process every frame due to high latency. In this example, frame 1 is skipped. Note that the goal of streaming perception is not to process every frame but to produce accurate state estimates in a timely manner. (b) One straightforward schedule is to simply process the latest available frame upon the completion of the previous processing (idle-free). However, if latest available frame will soon become stale, it might be better to idle and wait for a fresh frame (our dynamic schedule, Alg. 1). In this illustration, Alg. 1 determines that frame 2 will soon become stale and decides to wait (visualized in red) for frame 3 by comparing the tails ?2 and ?3.</p><p>Algorithm 1 Shrinking-tail policy 1: Given finishing time s and algorithm runtime r in the unit of frames (assuming r &gt; 1), this policy returns whether the algorithm should wait for the next frame 2: Define tail function</p><formula xml:id="formula_6">? (t) = t ? t 3: return [? (s + r) &lt; ? (s)] (Iverson bracket)</formula><p>the algorithm and additional empirical results for a wide suite of detectors in Appendix B.1. Note that Alg. 1 is by construction task agnostic (not specific to object detection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Forecasting</head><p>Now we expand our solution space to include forecasting methods. We experimented with both constant velocity models and first-order Kalman filters. We find good performance with the latter, given a small modification to handle asynchronous sensor measurements <ref type="figure">(Fig. 7)</ref>. The classic Kalman filter [21] operates on uniform time steps, coupling prediction and correction updates at each step. In our case, we perform correction updates only when a sensor measurement is available, but predict at every step. Second, due to frame-skipping, the Kalman filter should be time-varying (the transition and the process noise depend on the length of the time interval, details can be found in Appendix B.2). Association for bounding boxes across frames is required to update the Kalman filter, and we apply IoU-based greedy matching. For association and forecasting, the computation involves only bounding box coordinates and therefore is very lightweight (&lt; 2ms on CPU). We find that such overhead has little influence on the overall AP. The results are summarized in <ref type="table" target="#tab_7">Table 2</ref>.</p><p>Streamer (meta-detector) Note that our dynamic scheduler (Alg. 1) and asynchronous Kalman forecaster can be applied to any off-the-shelf detector,  <ref type="figure">Fig. 7</ref>. Scheduling for association and forecasting. Association takes place immediately after a new detection result becomes available, and it links the bounding boxes in two consecutive detection results. Forecasting takes place right before the next time step and it uses an asynchronous Kalman filter to produce an output as the estimation of the current world state. By default, the prediction step also updates internal states in the Kalman filter and is always called before the update step. In our case, we perform multiple update-free predictions (green blocks) until we receive a frame result. <ref type="table" target="#tab_7">Table 2</ref>. Streaming perception with joint detection, association, and forecasting. Association is done by IoU-based greedy matching, while forecasting is done by an asynchronous Kalman filter. First, we observe that forecasting greatly boosts the performance (from <ref type="table" target="#tab_4">Table 1</ref> row 7's 13.0 to row 1's 16.7). Also, with forecasting compensating for algorithm latency, it is now desirable to run a more expensive detector (row 2). Searching again over a large suite of detectors after adding forecasting, we find that the optimal detector is still Mask R-CNN (ResNet 50), but at input scale 0.75 instead of 0.5 (runtime 93ms and 57ms) ID Method AP APL APM APS AP50 AP75 regardless of its underlying latency (or accuracy). This means that we can assemble these modules into a meta-detector -which we call Streamer -that converts any detector into a streaming detection system that reports real-time detections at an arbitrary framerate. Appendix B.4 evaluates the improvement in streaming AP across 80 different settings (8 detectors ? 5 image scales ? 2 compute models), which vary from 4% to 80% with an average improvement of 33%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visual tracking</head><p>Visual tracking is an alternative for low-latency inference, due to its faster speed than a detector. For our experiments, we adopt the state-of-the-art multi-object tracker <ref type="bibr" target="#b1">[2]</ref> (which is second place in the MOT'19 challenge <ref type="bibr" target="#b10">[11]</ref> and is open sourced), and modify it to only track previously identified objects to make it faster than the base detector (see Appendix B.3). This tracker is built upon a two-stage detector and for our experiment, we try out the configurations of Mask R-CNN with different backbones and with different input scales. Also, we need a scheduling scheme for this detection plus tracking setting. For simplicity, we only explored running detection at fixed strides of 2, 5, 15, and 30. For example, <ref type="table" target="#tab_7">Table 3</ref>. Streaming perception with joint detection, visual tracking, and forecasting. We see that initially visual trackers do not outperform simple association <ref type="table" target="#tab_7">(Table 2)</ref> with the corresponding setting in the single GPU case. But that is reversed if the tracker can be optimized to run faster (2x) while maintaining the same accuracy (row 6). Such an assumption is not unreasonable given the fact that the tracker's job is as simple as updating locations of previously detected objects ID Method AP APL APM APS AP50 AP75 stride 30 means that we run the detector once and then run the tracker 29 times, with the tracker getting reset after each new detection. <ref type="table" target="#tab_7">Table 3</ref> row 1 contains the best configuration over backbone, input scale, and detection stride.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Streaming perception remains a challenge Our analysis suggests that streaming perception involves careful integration of detection, tracking, forecasting, and dynamic scheduling. While we present several strong solutions for streaming perception, the gap between the streaming performance and the offline performance remains significant (20.3 versus 38.0 in AP). This suggests that there is considerable room for improvement by building a better detector, tracker, forecaster, or even an end-to-end model that blurs boundary of these modules.</p><p>Formulations of real-time computation Common folk wisdom for real-time applications like online detection requires that detectors run within the sensor frame rate. Indeed, classic formulations of anytime processing require algorithms to satisfy a "contract" that they will finish under a compute budget <ref type="bibr">[41]</ref>. Our analysis suggests that this view of computation might be too myopic as evidenced by contemporary robotic systems <ref type="bibr">[33]</ref>. Instead, we argue that the sensor rate and compute budget should be seen as design choices that can be tuned to optimize a downstream task. Our streaming benchmark allows for such a global perspective.</p><p>Generalization to other tasks By construction, our meta-benchmark and dynamic scheduler (Alg. 1) are not restricted to object detection. We illustrate such generalization with an additional task of instance segmentation ( <ref type="figure">Fig. 9</ref>). However, there are several practical concerns that need to be addressed. Densely annotating video frames for instance segmentation is almost prohibitively expensive. Therefore, we adopt offline pseudo ground truth (Section 3.4) to evaluate streaming performance. Another concern is that the forecasting module is taskspecific. In the case of instance segmentation, we implement it as forecasting the bounding boxes and then warping the masks accordingly. Please refer to Appendix A.6 for the complete streaming instance segmentation benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We introduce a meta-benchmark for systematically converting any single-frame task into a streaming perception task that naturally trades off computation between multiple modules (e.g., detection versus tracking). We instantiate this meta-benchmark on tasks of object detection and instance segmentation. In general, we find online perception to be dramatically more challenging than its offline counterpart, though significant performance can be recovered by incorporating forecasting. We use our analysis to develop a simple meta-detector that converts any detector (with any internal latency) into a streaming perception system that can operate at any frame rate dictated by a downstream task (such as a motion planner). We hope that our analysis will lead to future endeavor in this under-explored but crucial aspect of real-time embodied perception. For example, streaming benchmarks can be used to motivate attentional processing; by spending more compute only on spatially <ref type="bibr" target="#b15">[16]</ref> or temporally [32] challenging regions, one may achieve even better efficiency-accuracy tradeoffs. We summary the contents of the appendix as follows. Appendix A describes additional details of our meta-benchmark, including discussion on the definition, pseudo ground-truth, simulation, dataset and instantiations for novel hardware and task. Appendix B provides additional details of our proposed solutions, including scheduling, tracking and forecasting. Finally, Appendix C includes additional baselines for a more thorough evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Benchmark Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Discussion on the Benchmark Definition</head><p>In Section 3.1, we defined our benchmark as evaluation over a discrete set of frames. One might point out that a continuous definition is more consistent with the notion of estimating the state of the world at all time instants for streaming perception. First, we note that it is possible to define a continuous-time counterpart, where the ground truth can be obtained via polynomial interpolation and the algorithm prediction can be represented as a function of time (e.g., simply derived from extrapolating the discrete output). Also in Eq 4, the aggregation function (implicit in L) could be integration. However, our choice of a discrete definition is mainly for two reasons: (1) we believe a high-frame-rate data stream is able to approximate the continuous evaluation; (2) most existing single-frame metrics (L, e.g., average-precision) is defined with a discrete set of input and we prefer that our streaming metric is compatible with these existing metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pseudo Ground Truth</head><p>We use manually obtained ground-truth for bounding-box-based object detection. As we point out in the main text, one could make use of pseudo ground truth by simply running an (expensive but accurate) off-line detector to generate detections that could be used to evaluate on-line streaming detectors.</p><p>Here, we analyze the effectiveness of pseudo ground truth detection as a proxy for ground-truth. We adopt the state-of-the-art detector -Hybrid Task Cascade (HTC) <ref type="bibr" target="#b7">[8]</ref> for computing the offline pseudo ground truth. As shown in <ref type="table" target="#tab_4">Table 1</ref>, this offline detector dramatically outperforms all real-time streaming methods by a large margin. As shown in the main text, pseudo-streaming AP correlates extraordinarily well with ground-truth-streaming AP, with a normalized correlation coefficient of 0.9925. This suggests that pseudo ground truth can be used to rank streaming perception algorithms.</p><p>We emphasize that since we have constructed Argoverse-HD by deliberately annotating high frame rate bounding boxes, we use real ground truth for evaluating detection performance. However, obtaining such high-frame-rate annotations for instance segmentation is expensive. Hence we make use of pseudo groundtruth instance masks (provided by HTC) to benchmark streaming instance segmentation (Section A.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Simulation</head><p>In true hardware-in-the-loop benchmarking, the output timestamp s j is simply the wall-clock time at which an algorithm produces an output. While we hold this as the gold-standard, one can dramatically simplify benchmarking by making use of simulation, where s j is computed using runtimes of different modules. For example, s j for a single-frame detector on a single GPU can be simulated by adding its runtime to the time when it starts processing a frame. Complicated perception stacks require considering runtimes of all modules (we model those that contribute &gt; 1 ms) in order to accurately simulate timestamps.</p><p>Modeling runtime distribution Existing latency analysis [34,27,25] usually reports only the mean runtime of an algorithm. However, empirical runtimes are in fact stochastic <ref type="figure" target="#fig_4">(Fig. A)</ref>, due to the underlying operating system scheduling and even due to the algorithm itself (e.g., proposal-based detectors often take longer when processing a scene with many objects). Because scene-complexity is often correlated across time, runtimes will also be correlated (a long runtime for a given frame may also hold for the next frame).</p><p>We performed a statistical analysis of runtimes, and found that a marginal empirical distribution to work well. We first run the algorithm over the entire dataset to get the empirical distribution of runtimes. At test time, we randomly sample a runtime when needed from the empirical distribution, without considering the correlation across time. Empirically, we found that the results (streaming AP) from a simulated run is within the variance of a real run. 5XQWLPHPV 'HQVLW\ Simulation for non-existent hardware/algorithm Through simulation, our evaluation protocol does not directly depend on hardware, but on a collection of runtime distributions for different modules (known as a runtime profile).</p><p>One thus has the freedom to alter the distributions. For example, we can simulate a faster algorithm simply by scaling down the runtime profile. <ref type="table" target="#tab_7">Table 3</ref>, uses simulation to evaluate the streaming performance of a non-existent tracker that runs twice as fast as the actual implementation on-hand. The reduced runtime could have arisen from better hardware; one can run the benchmark on a Geforce GTX 1080 Ti GPU and simulate the performance on a Tesla V100 GPU. We find that Tesla V100 makes our detectors run 16% faster, implying we can scale runtime profiles accordingly. For example, Mask R-CNN R50 @ s0.5 produces a simulated-streaming AP of 12.652 while the real-streaming AP (on a V100) is 12.645, suggesting that effectivness of simulated benchmarking.</p><p>Infinite GPUs In simulation, we are not restricted by the number of physical GPUs present in a system. Therefore, we are able to perform analysis in the infinite GPU setting. In this setting, each detector or visual tracker runs on a different device without any interference with each other. Equivalently, we run a new GPU job on an existing device as long as it is idle. As a result, the simulation also provides information on how many GPUs are required for a particular infinite GPU experiment in practice (i.e., the maximum number of concurrent jobs). We summarize the number of GPUs required for the experiments in the main text in <ref type="table" target="#tab_7">Table A</ref>. This implies that our streaming benchmark can be used to inform hardware design of future robotic platforms. Runtime-induced variance As mentioned in the previous section, runtime is stochastic and has a variance up to 11.1% (standard deviation normalized by mean). Fortunately, such a variance does not transfer to the variance of our streaming metric. Empirically, we found that the variance of streaming AP of different runs (by varying the random seed) is around 0.5% for the same algorithm. In comparison, independent training runs of Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> on MS COCO [26] with the same random seed yield a variance of 0.3% on the AP (cudnn back-propagation is stochastic by default) <ref type="bibr">[24]</ref>. Since the stochastic noise of streaming evaluation is at the same scale as CNN training, we ignore runtime-induced variance for our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Dataset Annotation and Comparison</head><p>Based on the publicly available video dataset Argoverse 1.1 <ref type="bibr" target="#b6">[7]</ref>, we build our dataset with high-frame-rate annotations for streaming evaluation -Argoverse-HD (High-frame-rate Detection). One key feature is that the annotation follows MS COCO [26] standards, thus allowing direct evaluation of COCO pre-trained models on this self-driving vehicle dataset. The annotation is done at 30 FPS without any interpolation used. Unlike some self-driving vehicle datasets where only cars on the road are annotated [37], we also annotate background objects since they can potentially enter the drivable area. Of course, objects that are too small are omitted and our minimum size is 5 ? 15 or 15 ? 5 (based on the aspect ratio of the object). We outsourced the annotation job to Scale AI. In <ref type="table" target="#tab_7">Table B</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Experiment Settings</head><p>Platforms The CPU used in our experiments is Xeon Gold 5120, and the GPU is Geforce GTX 1080 Ti. The software environment is PyTorch 1.1 with CUDA 10.0.</p><p>Timing The setup which we time single-frame algorithms mimics the scenario in real-world applications. The offline pipeline involves several steps: loading data from the disk, image pre-processing, neural network forward pass, and result post-processing. Our timing excludes the first step of loading data from the disk. This step is mainly for dataset-based evaluation. In actual embodied applications, data come from sensors instead of disks. This is implemented by loading the entire video to the main memory before the evaluation starts. In summary, our timing (e.g., the last column of <ref type="table" target="#tab_4">Table 1</ref>) starts at the time when the algorithm receives the image in the main memory, and ends at the time when the results are available in the main memory (instead of in the GPU memory). In the main text, we propose a meta-benchmark and mention that it can be instantiated with different tasks. In this section, we include full benchmark evaluation for streaming instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Alternate Task: Instance Segmentation</head><p>Instance segmentation is a more fine-grained task than object detection. This creates challenges for streaming evaluation as annotation becomes more expensive and forecasting is not straight-forward. We address these two issues by leveraging pseudo ground truth and warping masks according to the forecasted bounding boxes.</p><p>Another issue which we observed is that off-the-shelf pipelines are usually designed for benchmark evaluation or visualization. First, similar to object detection, we adopt GPU image pre-processing by default. Second, we found that more than 90% of the time within the mask head of Mask R-CNN is spent on transforming masks from the RoI space to the image space and compressing them in a format to be recognized by the COCO evaluation toolkit. Clearly, compression can be disabled for streaming perception. We point out that mask transformation can also be disabled. In practice, masks are used to tell if a specific point or region contains the object. Instead of transforming the mask (which involves object-specific image resizing operations), we can transform the query points or regions, which is simply a linear transformation over points or control points. Therefore, our timing does not include RoI-to-image transformation or mask compression. Furthermore, this also implies that we do not pay an additional cost for masks in forecasting, since only the box coordinates are updated but the masks remain in the RoI space.</p><p>For the instance segmentation benchmark, we use the same dataset and the same method HTC <ref type="bibr" target="#b7">[8]</ref> for the pseudo ground truth as for detection, and we include 4 methods: Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> with ResNet 50 and ResNet 101 backbones. Since these are hybrid methods that produce both instance boxes and masks, we can measure the overhead of including masks as the difference between runtime with and without the mask head in <ref type="table" target="#tab_7">Table C</ref>. We find that the average overhead is around 13%. We include the streaming evaluation in Tables D and E (with forecasting). <ref type="table" target="#tab_7">Table D</ref>. Streaming evaluation for instance segmentation. We find that many of our observations for object detection still hold for instance segmentation: (1) AP drops significantly when moving from offline to real time, (2) the optimal "sweet spot" is not the fastest algorithm but the algorithm with runtime more than the unit frame interval, and (3) both our dynamic scheduling and infinite GPUs further boost the performance. Note that the absolute numbers might appear higher than the tables in the main text since we use pseudo ground truth here</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Method</head><p>Detector AP APL APM APS AP50 AP75 Runtime In the main text, we propose a meta-benchmark and mention that it can be instantiated with different hardware platforms. In this section, we include full benchmark evaluation for streaming detection with Tesla V100 (a faster GPU than GTX 1080 Ti used in the main text). While our benchmark is hardware dependent, the method of evaluation generalizes across hardware platforms, and our conclusions largely hold when the hardware environment changes. We follow the same setup as in the experiments in the main text, except that we use Tesla V100 from Amazon Web Services (EC2 instance of type p3.2xlarge). We provide the results for detection, forecasting, and tracking in Tables F, G, and H, respectively. We see that the improvement due to better hardware is largely orthogonal to the algorithmic improvement proposed in the main text. <ref type="table" target="#tab_7">Table F</ref>. Performance of detectors for streaming perception on Tesla V100 (a faster GPU than the Geforce GTX 1080 Ti used in the main text). By comparing with <ref type="table" target="#tab_4">Table 1</ref> in the main text, we see that runtime is shortened and the AP is increased due to the boost of hardware performance. Different from <ref type="table" target="#tab_4">Table 1</ref>, we only consider GPU image pre-processing here for simplicity. Interestingly, with additional computation power, Tesla V100 enables more expensive models like input scale 0.75 (row 4) and Cascade Mask R-CNN (row 5) to be the optimal configurations (detector and scale) under their corresponding settings. Note that the improvement from our dynamic scheduler is orthogonal to the boost from hardware performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Method</head><p>Detector AP APL APM APS AP50 AP75 Runtime  <ref type="table" target="#tab_7">Table G</ref>. Streaming perception with joint detection, association, and forecasting on Tesla V100 (corresponding to <ref type="table" target="#tab_7">Table 2</ref> in the main text). We observe similar boost as in the detection only setting <ref type="table" target="#tab_7">(Table F)</ref>. The "re-optimize detection" step finds that Mask R-CNN R50 @ s1.0 outperforms Cascade Mask R-CNN R50 @ s0.5 with forecasting (row2), and it also happens to be the optimal detector with infinite GPUs (row 3)</p><p>ID Method AP APL APM APS AP50 AP75 In the main text, we propose the dynamic scheduling algorithm (Alg. 1) to reduce temporal aliasing. Such an algorithm is counter-intuitive in that it minimizes latency by sometimes sitting idle. In this subsection, we provide additional theoretical analysis and empirical results for algorithm scheduling. We first introduce the framework to study algorithm scheduling for streaming perception. Next, we show theoretically that our dynamic scheduling outperforms naive idlefree scheduling for any constant runtime larger than the frame interval and any long-enough sequence length. Lastly, we verify empirically the superiority of our dynamic scheduling.</p><p>To study algorithm scheduling, we assume no concurrency (i.e., a single job at a time) and that jobs are not interruptible. For notational simplicity, we assume a fixed input frame rate where frame x i is the frame available at time i ? {0, . . . , T ? 1} (i.e., zero-based indexing), and therefore i can be used to denote both frame index and time. We assume that time (time axis, runtime, and latency) is represented in the units of the number of frames. We also assume g to be a single-frame algorithm, and the streaming algorithm f is thus composed of g and a scheduling policy. No tracking or forecasting is used in the discussion below. Let k j be the input frame index that was processed to generate output o j = (? j , s j ): if? j = g(x i ), then k j = i. We denote the runtime of g as r.</p><p>Definition (Temporal Mismatch) When the benchmark queries for the state of the world at frame i, the temporal mismatch is ? i := i ? k j , where j =   <ref type="figure">Fig. B</ref>. Temporal mismatch for single-frame algorithms. Take t = 3 (query index i = 3) as an example (highlighted in orange): when the benchmark queries for y3, the latest prediction is g(x0), whose input index is 0, thus leading to a temporal mismatch of 3 (frames).</p><formula xml:id="formula_7">( 1 ) ( 0 ) ( 0 ) ( 1 ) ? ? Input Index For Each Query 0 0 3 1 1 ? ?</formula><p>arg max j s j &lt; i. If there is no output available, ? i := 0. We denote the average temporal mismatch over the entire sequence as?.</p><p>Intuitively, the temporal mismatch measures the latency of a streaming algorithm f in the unit of the number of frames <ref type="figure">(Fig. B</ref>). This latency is typically higher than the runtime of the single-frame algorithm g itself due to the blocking effect of consecutive execution blocks. For example, in <ref type="figure">Figure B</ref>, although runtime r &lt; 2, the average mismatch? = 15/7 &gt; 2 for T = 7. Note that we define ? i := 0 if there is no output available. To avoid the degenerate case where an algorithm processes nothing and yields a zero cumulative temporal mismatch, we assume that all schedules start processing the first frame immediately at t = 0.</p><p>MDP Naive idle-free scheduling processes the next available frame immediately after the previous execution is finished. However, a scheduler can choose when and which frames to process. Selection among such choices over the data sequence can be modeled as a decision policy under a Markov decision process (MDP). An MDP formulation allows one to compute the expected future cumulative mismatch for a given policy under stochastic runtimes r. In theory, one may also be able to compute the optimal schedule (that minimizes expected cumulative mismatches) through policy search algorithms. However, <ref type="figure" target="#fig_4">Figure A</ref> shows that practical runtime profiles have low variance and are unimodal. If one assumes that runtimes are deterministic and fixed at a constant value, we will now show that our shrinking-tail policy outperforms idle-free over a range of runtimes r and sequence lengths T . We believe that constant runtime is a reasonable assumption for our setting, and empirically verify so after our theoretical analysis.</p><p>Pattern analysis Crucially, constant runtimes ensure that all transitions are deterministic, allowing for a global view of the sequence. Our key observation is that the global sequence will contain repeating mismatch patterns. Analysis of one such pattern sheds light on the cumulative mismatch of the entire sequence. For example, r = 1.5 under idle-free repeats itself every 2 processing blocks. However, different patterns emerge for different values of r and for different policies. We assume that r &gt; 1 to avoid the trivial schedule where an algorithm consis- Input <ref type="table" target="#tab_4">Index ?  ?  0  0  2  2  4  4  6  6  8  8  10   0  0  2  3  3  4  3  4  3  4  3  2  3</ref> Shrinking-Tail tently finishes before the next frame arrives. We write? if and? st for the average temporal mismatch? for the idle-free and shrinking-tail policies, respectively. Our analysis is based on the concept of tail: ? (t) := t ? t . We denote ? (r) as ? r for short. Note that the integral part of runtime does not contribute to the temporal quantization effect, and we thus focus on the discussion of 1 &lt; r ? 2 for simplicity. We split our analysis into 3 different cases: r = 2, 1.5 ? r &lt; 2, and 1 &lt; r &lt; 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 1</head><p>The first is a special case where ? r = 0. It can be easily verified that idle-free is equivalent to shrinking-tail, and thus? st =? if .</p><p>Case 2 Now we inspect the case with 1.5 ? r &lt; 2. Since ? (2r) &lt; 0.5 ? ? (r), the shrinking-tail policy will output true (waiting) after processing the first frame. The waiting aligns the execution again with the integral time step, and thus for the subsequent processing blocks, it also outputs true (waiting). In summary, shrinking-tail always outputs true in this case, and its pattern in mismatch is agnostic to the specific runtime r <ref type="figure">(Fig. C)</ref>. Let? r denote? with runtime r, then we can draw the conclusion that? r1 st =? r2 st for r 1 = r 2 , ? (r 1 ) ? 0.5, and ? (r 2 ) ? 0.5.</p><p>We then focus on a particular case of r = 1.5. As shown in <ref type="figure">Figure D</ref>, idle-free repeats itself in a period of 3 frames, and shrinking-tail repeats itself in a period of 2 frames. Together, they form a joint pattern that repeats itself in a period of 6 frames (their least common multiple). The diagram shows that within each common period, the difference of cumulative mismatch between idle-free and  <ref type="table">0  0  2  3  3  2  3  3  2  3  3  2  3</ref> Difference in Cumulative Mismatch <ref type="table" target="#tab_4">0  0  2  2  4  4  6  6  8  8  10   0  0  2  3  2  3  2  3  2  3  2  3  2</ref> Period Period <ref type="figure">Fig. D</ref>. For r = 1.5, shrinking-tail achieves less cumulative mismatch than idle-free. Note that each policy has its own repeating period and shrinking-tail always achieves 1 less cumulative mismatch within each common period.</p><formula xml:id="formula_8">0 0 0 0 1 0 1 1 1 1 2 1 2 Mismatch Input Index ? ?</formula><p>shrinking-tail is increased by 1. And it is the same for all common periods. Therefore, if T = 6n + 1 for some positive integer n (intuitively, the entire sequence is a multiple of several common periods),? 1.5 st &lt;? 1.5 if . Additionally, <ref type="figure">Figure D</ref> enumerates all possible cases, where the sequence ends before a common period is over or in the middle of a common period. All these cases have? 1.5 st ? ? 1.5 if .</p><p>Next, it is straightforward to see thatthe cumulative mismatch will not decrease if one increases the runtime r of g:? r1 ?? r2 if r 1 ? r 2 . Therefore, for 1.5 ? r &lt; 2, we have?</p><formula xml:id="formula_9">r st =? 1.5 st ?? 1.5 if ?? r if<label>(5)</label></formula><p>Case 3 The last case with 1 &lt; r &lt; 1.5 (i.e., ? r &lt; 0.5) is more complicated than previous cases because the underlying repeating pattern never exactly repeats itself. To address this issue, we must introduce several new concepts to characterize such near-repeating patterns. We first observe a special type of execution block: Definition (Shrinking-Tail Block) Denoting the start and the end time of an execution block as t 1 and t 2 , a shrinking-tail block is an execution block such that ? (t 1 ) &gt; ? (t 2 ). As shown in <ref type="figure">Figure E</ref>, a shrinking-tail block increases temporal mismatch. Definition (Shrinking-Tail Cycle) A sequence of execution blocks can be divided into segments by a shrinking-tail block or an idle gap. A shrinkingtail cycle is a set of queries covered by the segment between these dividers. Specifically, the cycle starts from the 0-th query, the last query of a shrinkingtail block, or the query at the end of an idle gap. The cycle ends either when the sequence ends or the next cycle starts. The length of a cycle is the number of queries it covers. <ref type="bibr" target="#b12">13</ref> 14 <ref type="table" target="#tab_4">15  16  17  18  19  20  21  22  23  24</ref> 25 26</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Idle-Free</head><p>Shrinking- <ref type="table" target="#tab_4">Tail   Mismatch   Mismatch   Input Index   Input Index   10  10  13  14  15  15  16  18  19  19  20  22  23  23   10  12  13  14  14  16  17  18  18  20  21  22  22  24   3  3  2  2  2  3  3  2  2  3  3  2  2  3   3  2  2  2  3  2  2  2  3  2  2  2  3  2   13  15  14  16  18  20  19  22  23  24   14  13  20  22  21  24  25   1  2  3  4   Cycle 1  Cycle 2  Cycle 3   5  6  7</ref> Cycle 4 Cycle 5 16 18 17 <ref type="figure">Fig. F</ref>. Shrinking-tail cycle. Intuitively, blocks within each shrinking-tail cycle has tails increasing (?1 &lt; ?2 &lt; ?3 and ?5 &lt; ?6 &lt; ?7). It ends when the tail decreases or there is an idle gap, and thus the tail "shrinks".</p><p>As shown in <ref type="figure">Figure F</ref>, shrinking-tail cycles are small segments of the entire sequence and they may have different lengths. Note that the definitions of both shrinking-tail block and cycle are agnostic to r, but we only refer to them during our discussion for 1 &lt; r &lt; 1.5. Instead of comparing? for idle-free and shrinking-tail directly, we compare them for each cycle (denoted as? (c) if and ? (c) st respectively). First, we observe that a shrinking-tail cycle starts with either a shrinking-tail block or an idle gap and ends with consecutive tail-increasing blocks. Second, we observe that most queries have a mismatch of 2 for both policies (e.g., Cycle 2's queries 20 to 21 and Cycle 4's queries 18 to 19 in <ref type="figure">Fig. F)</ref>, and that the second query in a cycle is always 3 due to having a shrinking-tail block or an idle-gap before it. This is the rounding effect when adding multiple fractional numbers. The difference between the two policies is thus the mismatch of the first query. For 1 &lt; r &lt; 1.5, the first query of idle-free has a mismatch of 3, while shrinking-tail has a mismatch of 2. Intuitively, given that the majority of queries are with mismatch 2, the number of queries with mismatch 3 determines the relationship between? (c) :?</p><formula xml:id="formula_10">(c) st &lt;? (c)</formula><p>if . Therefore, when the sequence length is long enough, the policy with a smaller? (c) leads to a smaller overall cumulative mismatch. Now, we present a more formal analysis on the above statement. To quantify the cycle patterns, we first quantify the number of consecutive tail-increasing blocks. Let the number of consecutive tail-increasing blocks be a and the tail of the first block covered by the cycle be b (in the case where the first block starts after an idle gap, we define b to be 0). We first observe that a = max{a |a ? r +b ? 1, a ? N } = 1?b ?r . Also, b has its own range for each policy. For idle-free, 0 ? b &lt; ? r , and for shrinking-tail, b = 0. Taking <ref type="figure">Figure F</ref> for example (? r = 0.3), Cycle 1 has a = 3 and b = 0, Cycle 2 has a = 2 and b = ? 4 , and Cycle 4 has a = 3 and b = 0.</p><p>Since a might vary from cycle to cycle, we introduce a reference quantity that is constant and can be used to measure the length of cycles. Let a 0 be the a when b = 0, i.e., a 0 = 1 ?r , and c be the length of a cycle. For idle-free policy, c = a 0 + 2 or a 0 + 1. The variable length in cycles is due to variable b between cycles. When b ? 1 ? a 0 ? r , we have the first type of cycle with length a 0 + 2 (denoted as c 1 ); when b &gt; 1 ? a 0 ? r , we have the second type of cycle of length a 0 + 1 (denoted as c 2 ). The starting cycle in a sequence is always the first type, while the ensuing cycles can be either the first or second type. Note that it is possible that all cycles are the first type. For example, when r = 1.25, each cycle resets itself and b = 0 for all cycles. For shrinking-tail policy, each cycle resets itself (whose length denoted as c 3 ). Note that c 1 , c 2 , c 3 denotes the length of the 3 types of cycles and Cycle 1, 2, 3, ... in the figures denote specific cycle instances. From the above analysis, we can see</p><formula xml:id="formula_11">c 1 = a 0 + 2, c 2 = a 0 + 1, c 3 = a 0 + 1. (6) ? (c1) if = 2 a 0 + 2 + 2,? (c2) if = 2 a 0 + 1 + 2,? (c3) st = 1 a 0 + 1 + 2.<label>(7)</label></formula><p>Therefore,?</p><formula xml:id="formula_12">(c3) st &lt;? (c1) if &lt;? (c2) if<label>(8)</label></formula><p>Next, we explain how to infer the relationship between? from that between ? (c) . To analyze the mismatch of the whole sequence, we need to inspect the boundary cases at the start and the end of the sequence, where the cycle-based analysis might not hold. As shown in <ref type="figure">Figure G</ref>, the first cycles for both policies have different mismatch patterns due to empty detection at the first two queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Idle-Free</head><p>Shrinking- <ref type="table" target="#tab_7">Tail   Mismatch   Mismatch   Input Index   Input Index   0  0  2  2  2  3  3  2  2  3  3  2  2  3</ref> Cycle Compared to regular cycles in <ref type="figure">Figure F</ref>, the first cycle has 6 and 5 less total mismatch for idle-free and shrinking-tail policy respectively. Let m 1 , m 2 , and m 3 be the number of complete cycles of type c 1 , c 2 , and c 3 in a sequence, respectively, d be the number of residual queries at the end of the sequence that do not complete a cycle, and e be the total mismatch of these d queries, then we have</p><formula xml:id="formula_13">T = m 1 c 1 + m 2 c 2 + d if (9) T = m 3 c 3 + d st<label>(10)</label></formula><formula xml:id="formula_14">? if = (m 1 c 1? (c1) if + m 2 c 2? (c2) if ? 6 + e if )/T (11) ? st = (m 3 c 3? (c3) st ? 5 + e st )/T<label>(12)</label></formula><p>Note that the above holds only when m 1 ? 1 and m 3 ? 1 (the sequence is at least one cycle long for both policies). If T is smaller or equal to one cycle, the two policies are equivalent and ? if = ? st . When T is large enough (e.g., T ? ?), the? (c) terms dominate Eq 11 and Eq 12, and due to Eq 7, we have? st &lt;? if , which shows that the shrinking-tail policy is superior. Formally, when T &gt; C(r), where C(r) is some constant depending on r,? st &lt;? if .</p><p>Summary of the theoretical analysis Considering all 3 cases, we can draw the conclusion that? st ?? if when T is large enough (greater than C(r) if ? r &lt; 0.5, and no requirement otherwise). By achieving less average mismatch, shrinking-tail outperforms idle-free.</p><p>Practical Performance of Dynamic Scheduling We apply our dynamic schedule (Alg. 1) to a wide suite of detectors under the same settings as our main experiments and summarize the results in <ref type="table" target="#tab_7">Table I</ref>. In practice, runtime is stochastic due to complicated software and hardware scheduling or running an input adaptive model, but we find the theoretical results obtained under constant runtime assumption generalizes to most of the practical cases under our experiment setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Additional Details for Forecasting</head><p>We use an asynchronous Kalman filter for our forecasting module.  <ref type="bibr" target="#b2">[3]</ref>, which assumes that the area (the product of the width and the height) varies linearly instead of that each of the width and the height varies linearly. We find that such a representation produces lower numbers in AP.</p><p>As explained in the main text, Kalman filter needs to be asynchronous and time-varying for streaming perception. Let ?t k be the time-varying intervals <ref type="table" target="#tab_7">Table I</ref>. Empirical performance comparison before and after using Alg. 1. We see that our shrinking-tail policy consistently boosts the streaming performance for different detectors and for different input scales. We also observe some failure cases (last two rows), where runtime is close to one frame duration. This is because our theoretical analysis assumes constant runtime, while it is dynamic in practice. Hence, the variance in runtime when it is a boundary value can make a noticeable difference on the performance Method AP (Before) AP (After) Runtime (ms) Runtime (frames) SSD @ s0.5 9.7 9.7 66.7 2.0 RetinaNet R50 @ s0. <ref type="bibr" target="#b4">5</ref> 10.9 11.6 54.5 1.6 RetinaNet R101 @ s0.5 9.9 9.9 66.8 2.0 Mask R-CNN R101 @ s0. <ref type="bibr" target="#b4">5</ref> 11.0 11.1 68. between updates or prediction steps, we pick the transition matrix to be:</p><formula xml:id="formula_15">F k = I 4?4 ?t k I 4?4 I 4?4<label>(13)</label></formula><p>and the process noise to be</p><formula xml:id="formula_16">Q k = ?t 2 k I 8?8<label>(14)</label></formula><p>Intuitively, the process noise is larger the longer between the updates. All forecasting modules are implemented on the CPU and thus can be parallelized while the detector runs on the GPU. Our batched (over multiple objects) implementation of the asynchronous Kalman filter takes 0.98 ? 0.39ms for the update step and 0.22 ? 0.07ms for the prediction step, which are relatively very small overheads compared to detector runtimes. For scalable evaluation, we assume zero runtime for the association and forecasting module, and implement forecasting as post-processing of the detection outputs. One might wonder that a simulated post-processing run and an actual real-time parallel execution might have different final APs. We have also implemented the latter for verification purposes. For most settings the differences are within 1%. Although for some settings the difference can reach 3%, we find such fluctuation does not affect the relative rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Additional Details for Visual Tracking</head><p>For our tracking experiments (Section 4.4), we adapt and modify the state-ofthe-art multi-object tracker <ref type="bibr" target="#b1">[2]</ref>. A component breakdown in <ref type="figure" target="#fig_11">Fig. H</ref>   The advantage of a visual tracker is that it runs faster than a detector and thus yields lower latency for streaming perception. The multi-object tracker used here is modified from <ref type="bibr" target="#b1">[2]</ref>. It is mostly the same as a two-stage detector, except that its box head uses the last known object location as input in place of region proposals. Therefore, we get a computation saving by not running the RPN head. Runtime is measured for Mask R-CNN (ResNet 50) with input scale 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Evaluation of Our Meta-Detector Streamer</head><p>Streamer is introduced in Section 4.3 in the main text. Given a detector and an input scale, Streamer automatically schedules the detector and employs forecasting to compensate for some of its latency. In the single GPU case, our dynamic schedule (Alg. 1) is used and in the infinite GPU case, idle-free scheduling <ref type="figure">(Fig. 4c)</ref> is used. Proper scheduling requires the knowledge of runtime of the algorithm, which is known in the case of benchmark evaluation. When applied in the wild, we can optionally track runtime of the algorithm on unseen data and adjust the scheduling accordingly. The forecasting module is implemented with asynchronous Kalman filter (Section B.2). Streamer has several key features. First, it enables synchronous processing for an asynchronous problem. Under the commonly studied settings (both offline and online), computation is synchronous in that the outputs and the inputs have a natural one-to-one correspondence. Therefore, many existing temporal reasoning models assume that the inputs are at a uniform rate and each input corresponds to an output <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>. In the real-time setting, however, such a relationship does not exist due to the latency of the algorithm, i.e., the number of outputs can be arbitrary. Streamer converts detectors with arbitrary runtimes into systems that output at a designated fixed rate. In short, it abstracts away the asynchronous <ref type="table" target="#tab_7">Table J</ref>. Performance boost after applying Streamer. "(B)" standards for "Before", and "(A)" standards for "After". The evaluation setting is the same as <ref type="table" target="#tab_4">Table 1</ref> in the main text. This table assumes a single GPU, and an infinite GPU counterpart can be found in <ref type="table" target="#tab_7">Table K</ref>. Under this setting, we observe significant improvement in AP, ranging from 5% to 78%, and averaging at 34%  <ref type="table" target="#tab_7">Table K</ref>. Performance boost after applying Streamer. "(B)" standards for "Before", and "(A)" standards for "After". The evaluation setting is the same as nature of the problem and therefore allows downstream synchronous processing. Second, by adopting forecasting, Streamer significantly boosts the performance of streaming perception. In Tables J and K, we evaluate the detection AP before and after applying our meta-detector. We observe relative improvement from 4% to 80% with an average of 33% in detection AP under 80 different settings (8 detectors ? 5 image scales ? 2 compute models). Note that the difference of this evaluation and benchmark evaluation in the main text is that we fix the detector and input scale here, while benchmark evaluation searches over the best configuration of detectors and input scales. For the infinite GPU settings, we discount the boost from additional compute itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Implementation Details</head><p>Detectors We experiment with a large suite of object detectors: SSD [27], RetinaNet [25], Mask R-CNN <ref type="bibr" target="#b17">[18]</ref>, Cascade Mask R-CNN <ref type="bibr" target="#b5">[6]</ref>, and HTC <ref type="bibr" target="#b7">[8]</ref>. The "optimized" and "re-optimized" rows in all tables represent the optimal configuration over all detectors and all input scales of 0.2, 0.25, 0.5, 0.75, and 1.0. We adopt mmdetection codebase <ref type="bibr" target="#b8">[9]</ref> (one of the fastest open-source implementation for Mask R-CNN) for object detectors. Note that for all detectors, the implementation has reproduced both the accuracy and runtime reported in the original papers.</p><p>Potentially better implementation We acknowledge that there are additional bells and whistles to reduce runtime of object detectors, which might further improve the results on our benchmark. We focus on general techniques instead of device-or application-specific ones. For example, we have explored The scheduling is similar as with the Kalman filter case in that both are asynchronous. The difference is that linear forecasting does not explicitly maintain a state representation but only stores two latest detection results. Association takes place immediately after a new detection result becomes available, and it links the bounding boxes in two consecutive detection results and computes a velocity estimate. Forecasting takes place right before the next time step, and it uses linear extrapolation to produce an output as the estimation of the current world state. The equations represent the computation for reporting to benchmark query at t = 4. b is a simplified representation for object location. At this time, only detection results for frame 0 and 1 are available, but through association and forecasting, the algorithm can make a better prediction for the current world state.</p><p>GPU image pre-processing, which is applicable to all GPUs. Another implementation technique is to use half-precision floating-point numbers (FP16), which we have not explored, since it will only pay off for certain GPUs that have been optimized for FP16 computation (it is reported that FP16 yields only marginal testing time speed boost on 1080 Ti <ref type="bibr" target="#b9">[10]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Forecasting Baselines</head><p>We have also tested linear extrapolation (i.e., constant velocity) and quadratic extrapolation for forecasting detection results. We include an illustration of linear forecasting in <ref type="figure" target="#fig_4">Fig. I, and</ref> the quadratic counterpart is a straight-forward extension that involves three latest detection results. Though they produce inferior results than Kalman filter, we include the results in <ref type="table" target="#tab_7">Table L</ref> for completeness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 An End-to-End Baseline</head><p>In the main text, we break down the streaming detection task into detection, tracking, and forecasting for modular analysis. Alternatively, it is also possible to train a model that directly outputs detection results in the future. F2F [28] is one such model. Building upon Mask R-CNN, it does temporal reasoning and forecasting at the level of FPN feature maps. Note that no explicit tracking is performed. In this section, we compare against this end-to-end baseline in both offline and streaming settings.</p><p>In the offline setting, the algorithm is given s frames as input history, and outputs detection results for t frames ahead. This is the same as the evaluation in <ref type="bibr">[28]</ref>. We set both s and t to be 3, as the optimal detector in our forecasting experiments ( <ref type="table" target="#tab_7">Table 2</ref>) has runtime of 2.78 frames. Since F2F forecasts at the FPN feature level, it is agnostic to second stage tasks. In our evaluation, we focus on the bounding box detection task instead of instance segmentation. <ref type="table" target="#tab_7">Table M</ref>. Standard offline forecasting evaluation for the end-to-end method F2F [28]. The goal is to forecast 3 frames into the future. Surprisingly, the more expensive F2F method performs worse than the simpler Kalman filter in terms of the overall AP ID Method AP APL APM APS AP50 AP75 Also, we conduct experiments on Argoverse-HD, consistent with the setting in our other experiments. Due to a lack of annotation, we adopt pseudo ground truth (Section A.2) for training (data from the original training set of Argoverse 1.1 <ref type="bibr" target="#b6">[7]</ref>). We implement our own version of F2F based on mmdetection (instead of Detectron as done in <ref type="bibr">[28]</ref>). We train the model for 12 epochs end-to-end (a 50% longer schedule than combined stages in [28]). For a fair comparison, we also finetuned the detectors on Argoverse with the same pseudo ground truth. For Mask R-CNN ResNet 50 at scale 0.5, it boosts the offline box AP from 19.4 to 22.9. We use this finetuned detector in our method to compare against F2F. The results are summarized in <ref type="table" target="#tab_7">Table M</ref>. We see that an end-to-end solution does not immediately boost the performance. We believe that it is still an open problem on how to effectively replace tracking and forecasting with an end-to-end solution.</p><p>In the streaming setting, F2F can be viewed as a detector that compensates its own latency. The results are summarized in <ref type="table" target="#tab_7">Table N</ref>. We see that F2F is too expensive compared with other streaming solutions, showing that forecasting can help only if it is fast under our evaluation. Note that the detectors (row 1-2) are not finetuned as in the offline case, which means that they can be further improved. <ref type="table" target="#tab_7">Table N</ref>. Streaming evaluation for the end-to-end method F2F <ref type="bibr">[28]</ref>. The setting is the same as the experiments in the main text. Rows 1 and 2 are the optimized detector and the Kalman filter forecasting solution from the main text. The underlying detectors used are Mask R-CNN ResNet 50 at scale 0.5 and scale 0.75 respectively. Row 3 suggests that F2F has a low streaming AP, due to its forecasting module being very expensive (last column, runtime in milliseconds). For diagnostics purpose, we assume F2F to run as fast as our optimized detector (row 4), and arm it with our scheduling algorithm (row 5). But even so, F2F still under-performs the simple Kalman filter solution ID Method AP APL APM APS AP50 AP75 Runtime </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2. Prior art routinely explores the trade-off between detection accuracy versus runtime. We generate the above plot by varying the input resolution of each detection network. We argue that such plots are exclusive to offline processing and fail to capture latencyaccuracy trade-offs in streaming perception. AP stands for average precision, and is a standard metric for object detection [26].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Qualitative results. Video results can be found on the project website [Link]. a) Pseudo ground truth b) Real-time latency c) Instance mask forecasting Generalization to instance segmentation. (a) The offline pseudo ground truth we adopt for evaluation is of high quality. (b) A similar latency pattern can be observed for instance segmentation as in object detection. (c) Forecasting for instance segmentation can be implemented as forecasting the bounding boxes and then warping the masks accordingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>20. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. ArXiv abs/1704.04861 (2017) 3 21. Kalman, R.E.: A new approach to linear filtering and prediction problems. Transactions of the ASME-Journal of Basic Engineering 82(Series D), 35-45 (1960) 11 22. Kosinski, R.J.: A literature review on reaction time. Clemson University 10 (2008) 1 23. Kristan, M., Leonardis, A., Matas, J., Felsberg, M., Pflugfelder, R.,?ehovin Zajc, L., Vojir, T., H?ger, G., Luke?i?, A., Eldesokey, A., Fernandez, G.: The visual object tracking VOT2017 challenge results (2017) 4 24. Li, M., Yumer, E., Ramanan, D.: Budgeted training: Rethinking deep neural network training under resource constraints. In: ICLR (2020) 20 25. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll?r, P.: Focal loss for dense object detection. In: ICCV (2017) 2, 3, 19, 37 26. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014) 4, 9, 20, 21 27. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C.Y., Berg, A.C.: SSD: Single shot multibox detector. In: ECCV (2016) 2, 3, 19, 37 28. Luc, P., Couprie, C., LeCun, Y., Verbeek, J.: Predicting future instance segmentations by forecasting convolutional features. In: ECCV (2018) 3, 38, 39 29. Lukezic, A., Voj?r, T., Zajc, L.C., Matas, J., Kristan, M.: Discriminative correlation filter with channel and spatial reliability. In: CVPR (2017) 6 30. Mao, H., Yang, X., Dally, W.J.: A delay metric for video object detection: What average precision fails to tell. In: ICCV (2019) 3 31. McLeod, P.: Visual reaction time and high-speed ball games. Perception (1987) 2 32. Mullapudi, R.T., Chen, S., Zhang, K., Ramanan, D., Fatahalian, K.: Online model distillation for efficient video inference. In: ICCV (2019) 14 33. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., Ng, A.Y.: ROS: an open-source robot operating system. In: ICRA workshop on open source software. Kobe, Japan (2009) 13 34. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: CVPR (2016) 2, 3, 19 35. Russell, S.J., Wefald, E.: Do the right thing: studies in limited rationality. MIT press (1991) 4 36. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset (2019) 21 37. Voigtlaender, P., Krause, M., Osep, A., Luiten, J., Sekar, B.B.G., Geiger, A., Leibe, B.: MOTS: Multi-object tracking and segmentation. In: CVPR (2019) 21 38. Wen, L., Du, D., Cai, Z., Lei, Z., Chang, M., Qi, H., Lim, J., Yang, M., Lyu, S.: DETRAC: A new benchmark and protocol for multi-object detection and tracking. arXiv abs/1511.04136 (2015) 21 39. Yang, L., Fan, Y., Xu, N.: Video instance segmentation. In: ICCV (2019) 7, 21 40. Zhang, X., Zhou, X., Lin, M., Sun, J.: ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In: CVPR (2018) 3 41. Zilberstein, S.: Using anytime algorithms in intelligent systems. AI magazine (1996) 4, 13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. A .</head><label>A</label><figDesc>Runtime distribution for an object detector. Note that runtime is not constant, and this variance needs to be modeled in a simulation. This plot is obtained by running RetinaNet (ResNet 50) [25] on Argoverse 1.1<ref type="bibr" target="#b6">[7]</ref> with input scale 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig</head><label></label><figDesc>. C. Mismatch is the same for the shrinking-tail policy with different runtime r1 and r2 as long as r1 = r2 , ? (r1) ? 0.5, and ? (r2) ? 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig</head><label></label><figDesc>. E. A shrinking-tail execution block (orange) increases temporal mismatch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. H .</head><label>H</label><figDesc>Multi-object visual tracker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Scheduling for linear forecasting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Two computation models considered in our evaluation. Each block represents an algorithm running on a device and its length indicates its runtime.</figDesc><table><row><cell></cell><cell></cell><cell>t</cell><cell>t</cell></row><row><cell>1 frame</cell><cell>1 frame</cell><cell>1 frame</cell><cell>1 frame</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 frame</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 frame</cell></row><row><cell></cell><cell></cell><cell></cell><cell>??.</cell></row><row><cell cols="3">(a) Single GPU model</cell><cell>(b) Infinite GPU model</cell></row><row><cell>Fig. 4.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell></row><row><cell>Fast Alg</cell><cell>frame 0</cell><cell>frame 1</cell><cell>frame 2</cell><cell>frame 3</cell><cell>Idle-Free</cell><cell>frame 0</cell><cell></cell><cell cols="2">frame 1</cell><cell>frame 2</cell><cell></cell></row><row><cell>Accurate Alg (Slow)</cell><cell cols="2">frame 0</cell><cell cols="2">frame 2</cell><cell>Dynamic (Shrinking-Tail)</cell><cell>frame 0</cell><cell></cell><cell cols="3">Sit idle and wait! frame 1 frame 3</cell><cell></cell></row><row><cell></cell><cell cols="3">(a) Fast vs Accurate</cell><cell></cell><cell></cell><cell cols="5">(b) Dynamic Scheduling</cell><cell></cell></row></table><note>row 7). We provide a theoretical analysis of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A .</head><label>A</label><figDesc>Summary of the experiments in the infinite GPU settings (in the main text) and the number of GPUs needed in practice to achieve this performance (i.e., the maximum number of concurrent jobs).</figDesc><table><row><cell cols="2">This suggest that our simulation can also</cell></row><row><cell>identify the optimal hardware configuration</cell><cell></cell></row><row><cell>Method</cell><cell># of GPUs</cell></row><row><cell>Det (Table 1, row 8)</cell><cell>4</cell></row><row><cell>Det + Associate + Forecast (Table 2, row 3)</cell><cell>4</cell></row><row><cell>Det + Visual Track (Table 3, row 4)</cell><cell>9</cell></row><row><cell>Det + Visual Track + Forecast (Table 3, row 5)</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table B .</head><label>B</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Camera Setup</cell><cell cols="5">Image Res Image FPS Annot FPS Classes Boxes</cell></row><row><cell>DETRAC</cell><cell>Survelliance</cell><cell>960 ? 540</cell><cell>30</cell><cell>6</cell><cell>4</cell><cell>1.21M</cell></row><row><cell>KITTI-MOTS</cell><cell>Ego-Vehicle</cell><cell>1242 ? 375</cell><cell>10</cell><cell>10</cell><cell>2</cell><cell>46K</cell></row><row><cell>MOTS</cell><cell>Generic</cell><cell>1920 ? 1080</cell><cell>30</cell><cell>30</cell><cell>2</cell><cell>30K</cell></row><row><cell>UAVDT</cell><cell cols="2">UAV Survelliance 1080 ? 540</cell><cell>30</cell><cell>30</cell><cell>1</cell><cell>842K</cell></row><row><cell>Waymo</cell><cell>Ego-Vehicle</cell><cell>1920 ? 1280</cell><cell>10</cell><cell>10</cell><cell>4</cell><cell>11.8M</cell></row><row><cell>Youtube-VIS</cell><cell>Generic</cell><cell>1280 ? 720</cell><cell>30</cell><cell>6</cell><cell>40</cell><cell>131K</cell></row><row><cell>Argoverse-HD (Ours)</cell><cell>Ego-Vehicle</cell><cell>1920 ? 1200</cell><cell>30</cell><cell>30</cell><cell>8</cell><cell>250K</cell></row></table><note>Comparison of 2D video object detection datasets. For surveillance camera setups, the cameras are either stationary or have limited motion. For ego-vehicle setups, the scene dynamics evolve quickly, as (1) the ego-vehicle is traveling fast, and (2) other objects are much closer to the camera and thus have a higher speed in the image space. Our contributed dataset (annotation) is a high-frame-rate and high-resolution multi-class one compared to existing datasets</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table C .</head><label>C</label><figDesc>Instance segmentation overhead compared with object detection. This table lists runtimes of several methods with and without the mask head, and their differences are the extra cost which one has to pay for instance segmentation. All numbers are milliseconds except the scale column and the last column.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">The average overhead is</cell></row><row><cell>17ms or 13%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="5">Scale w/o Mask w/ Mask Overhead Overhead</cell></row><row><cell></cell><cell>0.2</cell><cell>34.3</cell><cell>41.4</cell><cell>7.1</cell><cell>21%</cell></row><row><cell></cell><cell>0.25</cell><cell>36.1</cell><cell>44.3</cell><cell>8.2</cell><cell>23%</cell></row><row><cell>Mask R-CNN ResNet 50</cell><cell>0.5</cell><cell>56.7</cell><cell>65.6</cell><cell>8.8</cell><cell>16%</cell></row><row><cell></cell><cell>0.75</cell><cell>92.7</cell><cell>101.0</cell><cell>8.3</cell><cell>9%</cell></row><row><cell></cell><cell>1.0</cell><cell>139.6</cell><cell>147.7</cell><cell>8.1</cell><cell>6%</cell></row><row><cell></cell><cell>0.2</cell><cell>38.4</cell><cell>46.4</cell><cell>7.9</cell><cell>21%</cell></row><row><cell></cell><cell>0.25</cell><cell>40.9</cell><cell>48.7</cell><cell>7.8</cell><cell>19%</cell></row><row><cell>Mask R-CNN ResNet 101</cell><cell>0.5</cell><cell>68.8</cell><cell>76.4</cell><cell>7.6</cell><cell>11%</cell></row><row><cell></cell><cell>0.75</cell><cell>119.7</cell><cell>127.1</cell><cell>7.5</cell><cell>6%</cell></row><row><cell></cell><cell>1.0</cell><cell>183.8</cell><cell>190.8</cell><cell>7.0</cell><cell>4%</cell></row><row><cell></cell><cell>0.2</cell><cell>60.9</cell><cell>66.0</cell><cell>5.1</cell><cell>8%</cell></row><row><cell></cell><cell>0.25</cell><cell>59.2</cell><cell>69.1</cell><cell>9.9</cell><cell>17%</cell></row><row><cell>Cascade MRCNN ResNet 50</cell><cell>0.5</cell><cell>80.0</cell><cell>95.4</cell><cell>15.3</cell><cell>19%</cell></row><row><cell></cell><cell>0.75</cell><cell>118.1</cell><cell>133.8</cell><cell>15.7</cell><cell>13%</cell></row><row><cell></cell><cell>1.0</cell><cell>164.6</cell><cell>181.9</cell><cell>17.3</cell><cell>10%</cell></row><row><cell></cell><cell>0.2</cell><cell>66.4</cell><cell>71.0</cell><cell>4.6</cell><cell>7%</cell></row><row><cell></cell><cell>0.25</cell><cell>65.4</cell><cell>75.2</cell><cell>9.7</cell><cell>15%</cell></row><row><cell cols="2">Cascade MRCNN ResNet 101 0.5</cell><cell>92.2</cell><cell>106.6</cell><cell>14.4</cell><cell>16%</cell></row><row><cell></cell><cell>0.75</cell><cell>143.4</cell><cell>159.2</cell><cell>15.8</cell><cell>11%</cell></row><row><cell></cell><cell>1.0</cell><cell>208.2</cell><cell>225.1</cell><cell>16.9</cell><cell>8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table E .</head><label>E</label><figDesc>Streaming evaluation for instance segmentation with forecasting. Despite that we only forecast boxes and warp masks accordingly, we still observe significant improvement from forecasting for mask AP. The optimized algorithm for row 1 is Mask R-CNN ResNet 50 @ s0.5, and for row 2 is Mask R-CNN ResNet 50 @ s0.75</figDesc><table><row><cell>ID Method</cell><cell>AP APL APM APS AP50 AP75</cell></row><row><cell cols="2">1 Detection + Scheduling + Association + Forecasting 24.1 32.4 23.0 6.0 43.7 22.0</cell></row><row><cell>2 + Infinite GPUs</cell><cell>29.2 30.7 30.2 11.4 53.0 26.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table H .</head><label>H</label><figDesc>Streaming perception with joint detection, visual tracking, and forecasting on Tesla V100 (corresponding toTable 3in the main text). We find the similar conclusions that visual tracking with forecasting does not outperform association with forecasting in the single GPU case and achieves comparable performance in the infinite GPU case</figDesc><table><row><cell>B Solution Details</cell><cell></cell></row><row><cell cols="2">1 Detection + Scheduling + Association + Forecasting 18.2 42.7 16.1 1.1 30.9 17.7</cell></row><row><cell>2 + Re-optimize Detection</cell><cell>19.6 33.0 19.2 5.3 38.5 17.9</cell></row><row><cell>3 + Infinite GPUs</cell><cell>22.9 38.7 23.1 6.9 43.8 21.2</cell></row><row><cell>ID Method</cell><cell>AP APL APM APS AP50 AP75</cell></row><row><cell>1 Detection + Visual Tracking</cell><cell>12.6 21.5 9.0 2.2 27.1 10.5</cell></row><row><cell>2 + Forecasting</cell><cell>18.0 34.7 16.8 3.2 36.0 16.4</cell></row><row><cell>3 + Infinite GPUs w/o Forecasting</cell><cell>14.4 24.2 11.2 2.8 30.6 12.0</cell></row><row><cell>4 + Forecasting</cell><cell>22.8 38.6 23.0 6.9 43.7 21.0</cell></row></table><note>B.1 Dynamic Scheduling</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>explains how this tracker works and why it has the potential to achieve better performance under the streaming setting.</figDesc><table><row><cell>Latest</cell><cell></cell></row><row><cell>Observation</cell><cell></cell></row><row><cell>FPN</cell><cell></cell><cell>RPN Head</cell></row><row><cell></cell><cell></cell><cell>24% (19ms)</cell><cell>Box Head</cell></row><row><cell></cell><cell></cell><cell>30% (23ms)</cell></row><row><cell>Box</cell><cell></cell><cell>FPN Feature Extraction</cell></row><row><cell></cell><cell></cell><cell>46% (35ms)</cell></row><row><cell>Last Known</cell><cell>Updated</cell></row><row><cell>Object Locations</cell><cell>Object Locations</cell></row><row><cell cols="2">(a) Multi-Object Tracker</cell><cell>(b) Computation Saving</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 1</head><label>1</label><figDesc>in the main text. This table assumes infinite GPUs, and a single GPU counterpart can be found inTable J. Under this setting, we observe significant improvement in AP, ranging from 4% to 80%, and averaging at 32%</figDesc><table><row><cell>Method</cell><cell cols="7">Scale AP(B) AP(A) Boost APL(B) APL(A) Boost</cell></row><row><cell></cell><cell>0.2</cell><cell>9.9</cell><cell>10.6</cell><cell>7%</cell><cell>25.5</cell><cell>29.4</cell><cell>15%</cell></row><row><cell></cell><cell>0.25</cell><cell>9.6</cell><cell>10.7</cell><cell>12%</cell><cell>24.9</cell><cell>31.7</cell><cell>27%</cell></row><row><cell>SSD</cell><cell>0.5</cell><cell>11.3</cell><cell>14.7</cell><cell>30%</cell><cell>24.1</cell><cell>35.4</cell><cell>47%</cell></row><row><cell></cell><cell>0.75</cell><cell>8.0</cell><cell>13.3</cell><cell>66%</cell><cell>14.6</cell><cell>25.6</cell><cell>76%</cell></row><row><cell></cell><cell>1.0</cell><cell>5.5</cell><cell>9.8</cell><cell>80%</cell><cell>10.0</cell><cell>16.5</cell><cell>65%</cell></row><row><cell></cell><cell>0.2</cell><cell>6.1</cell><cell>6.3</cell><cell>4%</cell><cell>18.6</cell><cell>21.3</cell><cell>15%</cell></row><row><cell></cell><cell>0.25</cell><cell>7.1</cell><cell>7.6</cell><cell>8%</cell><cell>21.4</cell><cell>27.1</cell><cell>26%</cell></row><row><cell>RetinaNet R50</cell><cell>0.5</cell><cell>12.3</cell><cell>14.7</cell><cell>20%</cell><cell>28.1</cell><cell>40.1</cell><cell>42%</cell></row><row><cell></cell><cell>0.75</cell><cell>13.1</cell><cell>18.0</cell><cell>37%</cell><cell>24.3</cell><cell>37.8</cell><cell>56%</cell></row><row><cell></cell><cell>1.0</cell><cell>11.7</cell><cell>17.3</cell><cell>48%</cell><cell>19.5</cell><cell>31.3</cell><cell>60%</cell></row><row><cell></cell><cell>0.2</cell><cell>5.5</cell><cell>6.0</cell><cell>9%</cell><cell>15.3</cell><cell>20.1</cell><cell>32%</cell></row><row><cell></cell><cell>0.25</cell><cell>6.7</cell><cell>7.5</cell><cell>12%</cell><cell>18.8</cell><cell>26.1</cell><cell>38%</cell></row><row><cell>RetinaNet R101</cell><cell>0.5</cell><cell>11.3</cell><cell>14.0</cell><cell>24%</cell><cell>25.3</cell><cell>38.1</cell><cell>50%</cell></row><row><cell></cell><cell>0.75</cell><cell>11.8</cell><cell>17.0</cell><cell>44%</cell><cell>21.3</cell><cell>34.3</cell><cell>61%</cell></row><row><cell></cell><cell>1.0</cell><cell>10.8</cell><cell>16.3</cell><cell>51%</cell><cell>18.2</cell><cell>28.2</cell><cell>55%</cell></row><row><cell></cell><cell>0.2</cell><cell>6.7</cell><cell>7.4</cell><cell>10%</cell><cell>20.0</cell><cell>26.2</cell><cell>31%</cell></row><row><cell></cell><cell>0.25</cell><cell>7.8</cell><cell>9.2</cell><cell>17%</cell><cell>20.8</cell><cell>30.1</cell><cell>45%</cell></row><row><cell>Mask R-CNN R50</cell><cell>0.5</cell><cell>13.9</cell><cell>17.4</cell><cell>26%</cell><cell>29.0</cell><cell>42.6</cell><cell>47%</cell></row><row><cell></cell><cell>0.75</cell><cell>14.4</cell><cell>20.3</cell><cell>40%</cell><cell>24.3</cell><cell>38.5</cell><cell>59%</cell></row><row><cell></cell><cell>1.0</cell><cell>12.4</cell><cell>18.7</cell><cell>51%</cell><cell>19.4</cell><cell>31.4</cell><cell>62%</cell></row><row><cell></cell><cell>0.2</cell><cell>6.5</cell><cell>7.3</cell><cell>13%</cell><cell>17.4</cell><cell>24.3</cell><cell>40%</cell></row><row><cell></cell><cell>0.25</cell><cell>7.9</cell><cell>9.1</cell><cell>15%</cell><cell>20.5</cell><cell>28.9</cell><cell>41%</cell></row><row><cell>Mask R-CNN R101</cell><cell>0.5</cell><cell>11.9</cell><cell>16.2</cell><cell>36%</cell><cell>23.7</cell><cell>38.4</cell><cell>62%</cell></row><row><cell></cell><cell>0.75</cell><cell>12.4</cell><cell>18.5</cell><cell>49%</cell><cell>20.3</cell><cell>35.3</cell><cell>74%</cell></row><row><cell></cell><cell>1.0</cell><cell>10.6</cell><cell>16.2</cell><cell>53%</cell><cell>16.9</cell><cell>27.7</cell><cell>64%</cell></row><row><cell></cell><cell>0.2</cell><cell>7.0</cell><cell>7.9</cell><cell>13%</cell><cell>18.9</cell><cell>26.5</cell><cell>40%</cell></row><row><cell></cell><cell>0.25</cell><cell>8.5</cell><cell>9.9</cell><cell>16%</cell><cell>22.3</cell><cell>31.7</cell><cell>42%</cell></row><row><cell>Cascade MRCNN R50</cell><cell>0.5</cell><cell>12.9</cell><cell>17.6</cell><cell>37%</cell><cell>26.0</cell><cell>41.2</cell><cell>58%</cell></row><row><cell></cell><cell>0.75</cell><cell>13.2</cell><cell>19.9</cell><cell>51%</cell><cell>22.1</cell><cell>36.5</cell><cell>65%</cell></row><row><cell></cell><cell>1.0</cell><cell>12.6</cell><cell>19.8</cell><cell>57%</cell><cell>19.0</cell><cell>31.8</cell><cell>67%</cell></row><row><cell></cell><cell>0.2</cell><cell>6.8</cell><cell>7.9</cell><cell>17%</cell><cell>17.8</cell><cell>26.6</cell><cell>49%</cell></row><row><cell></cell><cell>0.25</cell><cell>8.3</cell><cell>9.8</cell><cell>18%</cell><cell>21.0</cell><cell>31.7</cell><cell>50%</cell></row><row><cell cols="2">Cascade MRCNN R101 0.5</cell><cell>12.6</cell><cell>17.0</cell><cell>35%</cell><cell>25.0</cell><cell>38.5</cell><cell>54%</cell></row><row><cell></cell><cell>0.75</cell><cell>11.4</cell><cell>17.7</cell><cell>56%</cell><cell>19.0</cell><cell>32.7</cell><cell>72%</cell></row><row><cell></cell><cell>1.0</cell><cell>10.5</cell><cell>16.6</cell><cell>59%</cell><cell>16.7</cell><cell>27.6</cell><cell>65%</cell></row><row><cell></cell><cell>0.2</cell><cell>6.3</cell><cell>8.0</cell><cell>27%</cell><cell>14.0</cell><cell>21.8</cell><cell>55%</cell></row><row><cell></cell><cell>0.25</cell><cell>7.3</cell><cell>9.8</cell><cell>34%</cell><cell>15.7</cell><cell>25.5</cell><cell>62%</cell></row><row><cell>HTC</cell><cell>0.5</cell><cell>9.2</cell><cell>13.7</cell><cell>50%</cell><cell>16.3</cell><cell>26.9</cell><cell>65%</cell></row><row><cell></cell><cell>0.75</cell><cell>8.2</cell><cell>11.4</cell><cell>39%</cell><cell>13.2</cell><cell>20.5</cell><cell>55%</cell></row><row><cell></cell><cell>1.0</cell><cell>7.4</cell><cell>9.3</cell><cell>25%</cell><cell>11.1</cell><cell>15.8</cell><cell>43%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table L .</head><label>L</label><figDesc>Comparison of different forecasting methods for streaming perception. We see that both linear and Kalman filter forecasting methods significantly improve the streaming performance. Kalman filter further outperforms the linear forecasting. The quadratic forecasting decreases the AP, suggesting that high-order extrapolation is not suitable for this task. The detection used here is Mask R-CNN ResNet 50 @ s0.5 with dynamic scheduling (Alg. 1)</figDesc><table><row><cell>ID Method</cell><cell cols="3">AP APL APM APS AP50 AP75</cell></row><row><cell>1 No Forecasting</cell><cell>13.0 26.6 9.2</cell><cell cols="2">1.1 26.8 11.1</cell></row><row><cell cols="4">2 Linear (constant velocity) 15.7 38.1 13.8 1.1 30.2 14.8</cell></row><row><cell>3 Quadratic</cell><cell>9.7 23.8 6.6</cell><cell>0.4 21.4</cell><cell>7.9</cell></row><row><cell>4 Kalman filter</cell><cell cols="3">16.7 39.8 14.9 1.2 31.2 16.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>1 None (copy last) 13.4 24.3 10.9 1.9 27.9 11.3 2 Linear 16.3 34.8 16.8 1.8 32.9 14.3 3 Kalman filter 19.1 40.3 19.8 2.6 35.8 17.7 4 F2F 18.3 41.0 20.0 2.5 33.9 17.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>1</head><label></label><figDesc>Detection 12.0 24.3 7.9 1.0 25.1 10.1 56.7 2 + Scheduling (Alg. 1) + KF 17.8 33.3 16.3 3.2 35.2 16.5 92.7</figDesc><table><row><cell>3 F2F</cell><cell>6.2 11.1 3.4 0.8 13.1 5.2</cell><cell>321.6</cell></row><row><cell>4 F2F (Simulated Fast)</cell><cell>14.1 29.1 12.7 1.9 28.9 12.0</cell><cell>92.7</cell></row><row><cell>5 + Scheduling (Alg. 1)</cell><cell>15.6 33.0 15.2 2.1 30.7 13.9</cell><cell>92.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is possible to derive 2D annotations from the provided 3D annotations, but we find that such derived annotations are highly imprecise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="42">. Zilberstein, S., Mouaddib, A.I.: Optimal scheduling of progressive processing tasks. International Journal of Approximate Reasoning (2000) 4</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was supported by the CMU Argo AI Center for Autonomous Vehicle Research and was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0051. Annotations for Argoverse-HD were provided by Scale AI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On hardware-in-the-loop simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Decision and Control</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deliberation scheduling for problem solving in timeconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Argoverse: 3D tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SimpleDet: A simple and versatile distributed framework for object detection and instance recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04567</idno>
		<title level="m">CVPR19 tracking and detection challenge: How crowded can it get</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The unmanned aerial vehicle benchmark: Object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dynamic zoom-in network for fast object detection in large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A better baseline for AVA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1807.10066</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV (2017) 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computation and action under bounded resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
