<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Alonso</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">RoPeRT group</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<postCode>DIIS -I3A</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sabater</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">RoPeRT group</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<postCode>DIIS -I3A</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferstl</surname></persName>
							<email>dferstl@magicleap.com</email>
							<affiliation key="aff1">
								<orgName type="department">Magic Leap</orgName>
								<address>
									<settlement>Z?rich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">RoPeRT group</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<postCode>DIIS -I3A</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">RoPeRT group</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<postCode>DIIS -I3A</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Bitbrain, Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents a novel approach for semi-supervised semantic segmentation. The key element of this approach is our contrastive learning module that enforces the segmentation network to yield similar pixel-level feature representations for same-class samples across the whole dataset. To achieve this, we maintain a memory bank continuously updated with relevant and high-quality feature vectors from labeled data. In an end-to-end training, the features from both labeled and unlabeled data are optimized to be similar to same-class samples from the memory bank. Our approach outperforms the current state-of-the-art for semisupervised semantic segmentation and semi-supervised domain adaptation on well-known public benchmarks, with larger improvements on the most challenging scenarios, i.e., less available labeled data. https://github.com/ Shathe/SemiSeg-Contrastive</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation consists in assigning a semantic label to each pixel in an image. It is an essential computer vision task for scene understanding that plays a relevant role in many applications such as medical imaging <ref type="bibr" target="#b31">[31]</ref> or autonomous driving <ref type="bibr" target="#b1">[2]</ref>. As for many other computer vision tasks, deep convolutional neural networks have shown significant improvements in semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>All these examples follow supervised approaches requiring a large set of annotated data to generalize well. However, the availability of labels is a common bottleneck in supervised learning, especially for tasks such as semantic segmentation, which require expensive per-pixel annotations.</p><p>Semi-supervised learning assumes that only a small subset of the available data is labeled. It tackles this limited labeled data issue by extracting knowledge from unlabeled samples. Semi-supervised learning has been applied to a <ref type="bibr">Figure 1</ref>. Proposed contrastive learning module overview. At each training iteration, the teacher network f ? updates the feature memory bank with a subset of selected features from labeled samples. Then, the student network f ? extracts features from both labeled and unlabeled samples, which are optimized to be similar to same-class features from the memory bank ?.</p><p>wide range of applications <ref type="bibr" target="#b38">[38]</ref>, including semantic segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">27]</ref>. Previous semi-supervised segmentation works are mostly based on per-sample entropy minimization <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29]</ref> and per-sample consistency regularization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b29">29]</ref>. These segmentation methods do not enforce any type of structure on the learned features to increase inter-class separability across the whole dataset. Our hypothesis is that overcoming this limitation can lead to better feature learning and performance, especially when the amount of available labeled data is low.</p><p>This work presents a novel approach for semi-supervised semantic segmentation. Our approach follows a teacherstudent scheme where the main component is a novel representation learning module <ref type="figure">(Figure 1</ref>). This module is based on positive-only contrastive learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14]</ref> and enforces the class-separability of pixel-level features across different samples. To achieve this, the teacher network produces feature candidates, only from labeled data, to be stored in a memory bank. Meanwhile, the student network learns to produce similar class-wise features from both labeled and unlabeled data. The features stored in the memory bank are selected based on their quality and learned relevance for the contrastive optimization. Besides, our module enforces the alignment of unlabeled and labeled data (memory bank) in the feature space, which is another unexploited idea in semisupervised semantic segmentation. Our main contributions are the following:</p><p>? A novel semi-supervised semantic segmentation framework. ? The use of a memory bank for high-quality pixel-level features from labeled data. ? A pixel-level contrastive learning scheme where elements are weighted based on their relevance. The effectiveness of our method is demonstrated on well-known semi-supervised semantic segmentation benchmarks, reaching the state-of-the-art on different set-ups. Besides, our approach can naturally tackle the semi-supervised domain adaptation task, obtaining state-of-the-art results too. In all cases, the improvements upon comparable methods increase with the percentage of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section summarizes relevant related work for semisupervised learning and contrastive learning, with particular emphasis on work related to semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-Supervised Learning</head><p>Pseudo-Labeling Pseudo-labeling leverages the idea of creating artificial labels for unlabeled data <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b33">33]</ref> by keeping the most likely predicted class by an existing model <ref type="bibr" target="#b22">[22]</ref>. The use of pseudo-labels is motivated by entropy minimization <ref type="bibr" target="#b13">[13]</ref>, encouraging the network to output highly confident probabilities on unlabeled data. Both pseudo-labeling and direct entropy minimization methods are commonly used in semi-supervised scenarios <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b29">29]</ref> showing great performance. Our approach makes use of both pseudo-labels and direct entropy minimization.</p><p>Consistency Regularization Consistency regularization relies on the assumption that the model should be invariant to perturbations, e.g., data augmentation, made to the same image. This regularization is commonly applied by using two different methods: distribution alignment <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b36">36]</ref>, or augmentation anchoring <ref type="bibr" target="#b34">[34]</ref>. While distribution alignment enforces the prediction of perturbed and non-perturbed samples to have the same class distribution, augmentation anchoring enforces them to have the same semantic label. To produce high-quality non-perturbed class distribution or prediction on unlabeled data, the Mean Teacher method <ref type="bibr" target="#b37">[37]</ref>, proposes a teacher-student scheme where the teacher network is an exponential moving average (EMA) of model parameters, producing more robust predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-Supervised Semantic Segmentation</head><p>One common approach for semi-supervised semantic segmentation is to make use of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b12">[12]</ref>. Hung et al. <ref type="bibr" target="#b17">[17]</ref> propose to train the discriminator to distinguish between confidence maps from labeled and unlabeled data predictions. Mittal et al. <ref type="bibr" target="#b27">[27]</ref> make use of a two-branch approach, one branch enforcing low entropy predictions using a GAN approach and another branch for removing false-positive predictions using a Mean Teacher method <ref type="bibr" target="#b36">[36]</ref>. A similar idea was proposed by Feng et al. <ref type="bibr" target="#b9">[10]</ref>, a recent work that introduces Dynamic Mutual Training (DMT). DMT uses two models and the model's disagreement is used to re-weight the loss. DMT method also followed the multi-stage training protocol from CBC <ref type="bibr" target="#b8">[9]</ref>, where pseudo-labels are generated in an offline curriculum fashion. Other works are based on data augmentation methods for consistency regularization. French et al. <ref type="bibr" target="#b10">[11]</ref> focus on applying CutOut <ref type="bibr" target="#b6">[7]</ref> and CutMix <ref type="bibr" target="#b46">[46]</ref>, while Olsson et al. <ref type="bibr" target="#b29">[29]</ref> propose a data augmentation technique specific for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Contrastive Learning</head><p>The core idea of contrastive learning <ref type="bibr" target="#b15">[15]</ref> is to create positive and negative data pairs, to attract the positive and repulse the negative pairs in the feature space. This technique has been used in supervised and self-supervised setups. However, recent self-supervised methods have shown similar accuracy with contrastive learning using positive pairs only by performing similarity maximization with distillation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14]</ref> or redundancy reduction <ref type="bibr" target="#b47">[47]</ref>.</p><p>As for semantic segmentation, these techniques has been mainly used as pre-training <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45]</ref>. Very recently, Wang et al. <ref type="bibr" target="#b40">[40]</ref> have shown improvements in supervised scenarios applying standard contrastive learning in a pixel and region level for same-class supervised samples. Van et al. <ref type="bibr" target="#b39">[39]</ref> have shown the advantages of contrastive learning in unsupervised set-ups, applying it between features from different saliency masks. Lai et al. <ref type="bibr" target="#b21">[21]</ref> proposed to use contrastive learning in a self-supervised fashion where positive samples were the same pixel from different views/crops and negative samples were different pixels from a different view, making the model invariant to context information.</p><p>In this work, we propose to follow a positive-only contrastive learning based on similarity maximization and distillation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14]</ref>. This way, we boost the performance on semi-supervised semantic segmentation in a simpler and <ref type="figure">Figure 2</ref>. Supervised and self-supervised optimization. The student f ? is optimized with Lsup for labeled data (x l , y l ). For unlabeled data xu, the teacher f ? computes the pseudo-labels?u that are later used for optimizing L pseudo for pairs of augmented samples and pseudo-labels (x a u ,?u). Finally, Lent is optimized for predictions from x a u .</p><p>more computationally efficient fashion than standard contrastive learning <ref type="bibr" target="#b40">[40]</ref>. Differently from previous works, our contrastive learning module tackles a semi-supervised scenario aligning class-wise and per-pixel features from both labeled and unlabeled data to features from all over the labeled set that are stored in a memory bank. Contrary to previous contrastive learning works <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b16">16]</ref> that saved imagelevel features in a memory bank, our memory bank saves per-pixel features for the different semantic classes. Besides, as there is not infinite memory for all dataset pixels, we propose to only save features with the highest quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Semi-supervised semantic segmentation is a per-pixel classification task where two different sources of data are available: a small set of labeled samples X l = {x l , y l }, where x l are images and y l their corresponding annotations, and a large set of unlabeled samples X u = {x u }.</p><p>To tackle this task, we propose to use a teacher-student scheme. The teacher network f ? creates robust pseudolabels from unlabeled samples and memory bank entries from labeled samples to teach the student network f ? to improve its segmentation performance. Teacher-student scheme. The learned weights ? of the student network f ? are optimized using the following loss:</p><formula xml:id="formula_0">L = ? sup L sup +? pseudo L pseudo +? ent L ent +? contr L contr .</formula><p>(1) L sup is a supervised learning loss on labeled samples (Section 3.1). L pseudo and L ent tackle pseudo-labels (Sec-tion 3.2) and entropy minimization (Section 3.3) techniques, respectively, where pseudo-labels are generated by the teacher network f ? . Finally, L contr is our proposed positive-only contrastive learning loss (Section 3.4).</p><p>Weights ? of the teacher network f ? are an exponential moving average of weights ? of the student network f ? with a decay rate ? ? [0, 1]. The teacher model provides more accurate and robust predictions <ref type="bibr" target="#b37">[37]</ref>. Thus, at every training step, the teacher network f ? is not optimized by a gradient descent but updated as follows:</p><formula xml:id="formula_1">? = ? ? + (1 ? ? )?.<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Supervised Segmentation: L sup</head><p>Our supervised semantic segmentation optimization, applied to the labeled data X l , follows the standard optimization with the weighted cross-entropy loss. Let H be the weighted cross-entropy loss between two lists of N perpixel class probability distributions y 1 , y 2 :</p><formula xml:id="formula_2">H(y 1 , y 2 ) = ? 1 N N n=1 C c=1 y (n,c) 2 log(y (n,c) 1 )? c ? n ,<label>(3)</label></formula><p>where C is the number of classes to classify, N is the number of elements, i.e., pixels in y 1 , ? c is a per-class weight, and, ? n is a per-pixel weight. Specific values of ? c and ? n are detailed in Section 4.2. The supervised loss (see top part of <ref type="figure">Figure 2</ref>) is defined as follows:</p><formula xml:id="formula_3">L sup = H (f ? (x a l ) , y l ) ,<label>(4)</label></formula><p>where x a l is a weak augmentation of x l (see Section 4.2 for augmentation details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning from Pseudo-labels: L pseudo</head><p>The key to the success of semi-supervised learning is to learn from unlabeled data. One idea our approach exploits is to learn from pseudo-labels. In our case, pseudo-labels are generated by the teacher network f ? (see <ref type="figure">Figure 2</ref>). For every unlabeled sample x u , the pseudo-labels? u are computed following this equation:</p><formula xml:id="formula_4">y u = arg max f ? (x u ) ,<label>(5)</label></formula><p>where f ? predicts a class probability distribution. Note that pseudo-labels are computed at each training iteration. Consistency regularization is introduced with augmentation anchoring, i.e., computing different data augmentation for each sample x u on the same batch, helping the model to converge to a better solution <ref type="bibr" target="#b34">[34]</ref>. The pseudo-labels loss for unlabeled data X u is calculated by the cross-entropy:</p><formula xml:id="formula_5">L pseudo = 1 A A a=1 H (f ? (x a u ) ,? u ) ,<label>(6)</label></formula><p>where x a u is a strong augmentation of x u and A is the number of augmentations we apply to sample x u (see Section 4.2 for augmentation details). . These features are projected, filtered by their quality, and then, ranked to finally only store the highest-quality features into the memory bank. Concurrently, feature vectors from input samples extracted by f ? are fed to the projection and prediction heads (see left part). Then, feature vectors are passed to a self-attention module in a class-wise fashion, getting a per-sample weight. Finally, input feature vectors are enforced to be similar to same-class features from the memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Direct Entropy Minimization: L ent</head><p>Direct entropy minimization is applied on the class distributions predicted by the student network from unlabeled samples x u as a regularization loss:</p><formula xml:id="formula_6">L ent = ? 1 A 1 N A a=1 N n=1 C c=1 f ? (x a,n,c u ) log f ? (x a,n,c u ) ,<label>(7)</label></formula><p>where C is the number of classes to classify, N is the number of pixels and A is the number of augmentations. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates our proposed contrastive optimization inspired by positive-only contrastive learning works based on similarity maximization and distillation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14]</ref>. In our approach, a memory bank is filled with high-quality feature vectors from the teacher f ? (right part of <ref type="figure" target="#fig_0">Figure 3</ref>). Concurrently, the student f ? extracts feature vectors from either X l or X u . In a per-class fashion, every feature is passed through a simple self-attention module that serves as per-feature weighting in the contrastive loss. Finally, the loss enforces the weighted feature vectors from the student to be similar to feature vectors from the memory bank. As the memory bank contains high-quality features from all labeled samples, the contrastive loss helps to create a better class separation in the feature space across the whole dataset as well as aligning the unlabeled data distribution with the labeled data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Contrastive Learning: L contr</head><p>Optimization. Let f ? ? be the student network without the classification layer and {x, y} a training sample either from {X l , Y l } or {X u ,? u }. The first step is to extract all feature vectors: V = f ? ? (x). The feature vectors V are then fed to a projection head, Z = g ? (V ), and a prediction head, P = q ? (Z), following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14]</ref>, where g ? and q ? are two different Multi-Layer Perceptrons (MLPs). Next, P is grouped by the different semantic classes in y.</p><p>Let P c = {p c } be the set of prediction vectors from P of a class c. Let Z c = {z c } be the set of projection vectors of class c obtained by the teacher, Z = g ? (f ? ? (x)) from the labeled examples stored in the memory bank.</p><p>Next, we learn which feature vectors (p c and z c ) are beneficial for the contrastive task, by assigning per-feature learned weights (Equation 8) that will serve as a weighting factor (Equation 10) for the contrastive loss function <ref type="figure">(Equation 11</ref>). These per-feature weights are computed using class-specific attention modules S c,? (see Section 4.2 for further details) that generate a single value (w ? [0, 1]) for every z c and p c feature. Following <ref type="bibr" target="#b35">[35]</ref> we L1 normalize these weights to prevent converging to the trivial all-zeros solution. For the prediction vectors P c case, the weights w pc are then computed as follows:</p><formula xml:id="formula_7">w pc = N Pc pi?Pc S c,? (p i ) S c,? (p c ),<label>(8)</label></formula><p>where N Pc is the number of elements in P c . Equation 8 is used to compute w z c too, changing Z c and z c for P c and p c . The contrastive loss enforces prediction vectors p c to be similar to projection vectors z c as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14]</ref> (in our case, projection vectors are in the memory bank). For that, we use the cosine similarity as the similarity measure C:</p><formula xml:id="formula_8">C(p c , z c ) = p c , z c p c 2 ? z c 2 ,<label>(9)</label></formula><p>where, the weighted distance between predictions and memory bank entry is computed by:</p><formula xml:id="formula_9">D(p c , z c ) = w pc w z c (1 ? C(p c , z c )),<label>(10)</label></formula><p>and, our contrastive loss is computed as follows:</p><formula xml:id="formula_10">L contr = 1 C 1 N pc 1 N z c C c=1 pc?Pc z c ?Z c D(p c , z c ).<label>(11)</label></formula><p>Memory Bank. The memory bank is the data structure that maintains the target feature vectors z c , ? for each class c, used in the contrastive loss. As there is not infinite space for saving all pixels of the labeled data, we propose to store only a subset of the feature vectors from labeled data with the highest quality. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, the memory bank is updated on every training iteration with a subset of z c ? Z generated by the teacher. To select what subset of Z is included in the memory bank, we first perform a Feature Quality Filter (FQF), where we only keep features that lead to an accurate prediction when the classification layer is applied, y = arg max f ? (x l ), having confidence higher than a threshold, f ? (x l ) &gt; ?. The remaining Z are grouped by classes Z c . Finally, instead of picking randomly a subset of every Z c to update the memory bank, we make use of the class-specific attention modules S c,? . We get ranking scores R c = S c,? (Z c ) to sort Z c and we update the memory bank only with the top-K highest-scoring vectors. The memory bank is a First In First Out (FIFO) queue per class for computation and time efficiency. This way it maintains recent high-quality feature vectors in a very efficient fashion computation-wise and time-wise. Detailed information about the hyper-parameters is included in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section describes evaluation set-up and the evaluation of our method on different benchmarks for semisupervised semantic segmentation, including a semisupervised domain adaptation, and a detailed ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>? Cityscapes <ref type="bibr" target="#b5">[6]</ref>. It is a real urban scene dataset composed of 2975 training and 500 validation samples, with 19 semantic classes. ? PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref>. It is a natural scenes dataset with 21 semantic classes. The dataset has 10582 and 1449 images for training and validation respectively. ? GTA5 <ref type="bibr" target="#b30">[30]</ref>. It is a synthetic dataset captured from a video game with realistic urban-like scenarios with 24966 images in total. The original dataset provides 33 different categories but, following <ref type="bibr" target="#b42">[42]</ref>, we only use the 19 classes that are shared with Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Architecture. We use DeepLab networks <ref type="bibr" target="#b3">[4]</ref> in our experiments. For the ablation study and most benchmarking experiments, DeepLabv2 with a ResNet101 backbone is used to have similar settings to previous works <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">27]</ref>. DeepLabv3+ with Resnet50 backbone is also used to equal comparison with <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b21">21]</ref>. ? is set from 0.995 to 1 during training in (Equation 2). The prediction and projection heads follow <ref type="bibr" target="#b14">[14]</ref>: Linear (256) ? BatchNorm <ref type="bibr" target="#b18">[18]</ref> ? Relu <ref type="bibr" target="#b28">[28]</ref> ? Linear (256). The proposed class-specific attention modules follow a similar architecture: Linear (256 )? BatchNorm ? LeakyRelu <ref type="bibr" target="#b24">[24]</ref> ? Linear (1) ? Sigmoid. We use 2?N classes attention modules since they are used in a classwise fashion. In particular, two modules per class are used because we have different modules for projection or prediction feature vectors.</p><p>Optimization. For all experiments, we train for 150K iterations using the SGD optimizer with a momentum of 0.9. The learning rate is set to 2 ? 10 ?4 for DeepLabv2 and 4 ? 10 ?4 for DeepLabv3+ with a poly learning rate schedule. For the Cityscapes and GTA5 datasets, we use a crop size of 512 ? 512 and batch sizes of 5 and 7 for Deeplabv2 and Deeplabv3+, respectively. For Pascal VOC, we use a crop size of 321 ? 321 and batch sizes of 14 and 20 for Deeplabv2 and Deeplabv3+, respectively. Cityscapes images are resized to 512 ? 1024 before cropping when Deeplabv2 is used for a fair comparison with <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">27]</ref>. The different loss weights in (Equation 1) are set as follows for all experiments: ? sup = 1, ? pseudo = 1, ? ent = 0.01, ? contr = 0.1. An exception is made for the first 2K training iterations where ? contr = 0 and ? pseudo = 0 to make sure predictions have some quality before being taken into account. Regarding the per-pixel weights (? n ) from H in <ref type="figure" target="#fig_0">(Equation 3</ref>), we set it to 1 for L sup . For L pseudo , we follow <ref type="bibr" target="#b8">[9]</ref> weighting each pixel with its corresponding pseudo-label confidence with a sharpening operation, f ? (x u ) s , where we set s = 6. As for the per-class weights ? c in <ref type="figure" target="#fig_0">(Equation 3</ref>), we perform a class balancing for the Cityscapes and GTA5 datasets by setting ? c = fm fc with f c being the frequency of class c and f m the median of all class frequencies. In semi-supervised settings the amount of labels, Y l , is usually small. For a more meaningful estimation, we compute these frequencies not only from Y l but also from? u . For the Pascal VOC we set ? c = 1 as the class balancing does not have a beneficial effect.</p><p>Other details. DeepLab's output resolution is ?8 lower than the input resolution. For feature comparison during training, we keep the output resolution and downsample the labels reducing memory requirements and computation.</p><p>The memory bank size is fixed to ? = 256 vectors per class (see Section 4.4 for more details). The confidence threshold ? for accepting features is set to 0.95. The number of vectors added to the memory bank at each iteration, for each image, and for each class is set as max(1, ? |X l | ), where |X l | is the number of labeled samples.</p><p>A single NVIDIA Tesla V100 GPU is used for all experiments. All our reported results are the mean of three different runs with different labeled/unlabeled data splits.</p><p>Following <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b37">37]</ref>, the segmentation is performed with the student f ? in the experimental validation, although the teacher would lead to a slightly better performance <ref type="bibr" target="#b34">[34]</ref>. Data augmentation. We use two different augmentation set-ups, a weak one for labeled samples and a strong set-up for unlabeled samples, following <ref type="bibr" target="#b29">[29]</ref> with minor modifications ( <ref type="table" target="#tab_0">Table 1</ref> describes the followed data augmentation scheme in our method). Besides, we set A = 2 (Equation 6) as the number of augmentations for each sample. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Benchmark Experiments</head><p>Following experiments compare our method with stateof-the-art methods in different semi-supervised scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Semi-supervised Semantic Segmentation</head><p>Cityscapes. <ref type="table" target="#tab_2">Table 4.</ref>3.1 compares different methods on the Cityscapes benchmark for different labeled-unlabeled rates: <ref type="bibr" target="#b0">1</ref> 30 , 1 8 and, <ref type="bibr">1 4</ref> . Fully Supervised (FS) scenario, where all images are labeled, is also shown as a reference. As shown in the table, our approach outperforms the current state-of-the-art by a significant margin. The performance difference is increasing as less labeled data is available, demonstrating the effectiveness of our approach. This is particularly important since the goal of semi-supervised learning is to learn with as little supervision as possible. Note that the upper bound for each method is shown in the fully supervised setting (FS). <ref type="figure" target="#fig_1">Figure 4</ref> shows a visual com-  <ref type="table">Table 3</ref>. Performance (Mean IoU) for the Pascal VOC val set for different labeled-unlabeled ratios and, in parentheses, the difference w.r.t. the corresponding fully supervised (FS) result.</p><p>parison of the top-performing methods on different relevant samples from Cityscapes. Note that Lai et al. <ref type="bibr" target="#b21">[21]</ref> have a higher FS baseline since they use a higher batch size and crop size among other differences in the set-up.</p><p>Pascal VOC. <ref type="table" target="#tab_2">Table 4</ref>.3.1 shows the comparison of different methods on the Pascal VOC benchmark, using different labeled-unlabeled rates: 1 50 , 1 20 and, <ref type="bibr">1 8</ref> . Our proposed method outperforms previous methods for most of the configurations. Like in the previous benchmark, our method presents larger benefits for the more challenging cases, i.e., only a small fraction of data is labeled <ref type="bibr">( 1 50</ref> ). This demonstrates that the proposed approach is especially effective to learn from unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Semi-supervised Domain Adaptation</head><p>Semi-supervised domain adaptation for semantic segmentation differs from the semi-supervised set-up in the availability of labeled data from another domain. That is, apart from having X l = {x l , y l } and X u = {x u } from the target domain, a large set of labeled data from another domain is also available:  Our method can naturally tackle this task by evenly sampling from both X l and X d as our labeled data when optimizing L sup and L contr . However, the memory bank only stores features from the target domain X l . In this way, both the features from unlabeled data X u , and the features from the other domain X d are aligned with those from X l .</p><formula xml:id="formula_11">X d = {x d , y d }.</formula><p>Following ASS <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b23">23]</ref>, we take the GTA5 dataset as X d , where all elements are labeled, and the Cityscapes is the target domain consisting of a small set of labeled data X l and a large set of unlabeled samples X u . <ref type="table" target="#tab_2">Table 4</ref>.3.1 compares the results of our method with previous methods <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b23">23]</ref> where all methods use ImageNet pre-training. For reference, we also show the results of our approach with no adaptation, i.e., only training on the target domain Cityscapes, as we do for the semi-supervised set up from the previous experiment <ref type="table" target="#tab_2">(Table 4</ref>.3.1). We can see that our approach benefits from the use of the other domain data (GTA5), especially where there is little labeled data available ( 1 30 ), as it could be expected. Our method outperforms ASS by a large margin in all the different set-ups. As in previous experiments, our improvement is more significant when the amount of available labeled data is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Experiments</head><p>The following experiments study the impact of the different components of the proposed approach. The evaluation is done on the Cityscapes data, since it provides more complex scenes compared to Pascal VOC. We select the challenging labeled data ratio of <ref type="bibr">1 30</ref> . Losses impact.  <ref type="table">Table 5</ref>. Ablation study on the different losses included (Equation 1). Mean IoU obtained on Cityscapes benchmark ( 1 30 available labels, Deeplabv2-ResNet101 COCO pre-trained). <ref type="bibr">1 30</ref> of the Cityscapes labeled data is available. Note that our proposed contrastive learning module L contr is able to get 54.32 mIoU even without any other complementary loss, which is the previous state-of-the-art for this set-up (see <ref type="table" target="#tab_2">Table 4</ref>.3.1). Adding the L pseudo significantly improves the performance and then, adding L ent regularization loss gives a little extra performance gain.</p><p>Note that at testing time, our approach only uses the student network f ? , adding zero additional computational cost. At training time, for the experiment of <ref type="table" target="#tab_2">Table 4</ref>.4 having an input resolution of 512 ? 512 with a forward pass cost of 372.04 GFLOPs, our method performs 1151. <ref type="bibr" target="#b19">19</ref> GFLOPs for one training step using one labeled image and one unlabeled image, compared to the 1488.16 GFLOPs from <ref type="bibr" target="#b9">[10]</ref> or 1116.12 GFLOPs from <ref type="bibr" target="#b29">[29]</ref>. The total number of GFLOPs come from 372.04 for computing labeled image predictions, 372.04 for the unlabeled image predictions, 372.04 for computing the pseudo-labels and, 35.07 for our contrastive module, which mainly include the computation of the prediction and projection heads (8.59), the class-? contr 10 4 10 2 10 1 10 0 10 ?1 10 ?2 10 ?4 mIoU 50.3 51.4 54.8 59.1 59.4 58.7 57.6  <ref type="bibr">4.4)</ref>. High values are also detrimental, probably because it acts as increasing the learning rate vastly, which hinders the optimization. The best performance is achieved when this contrastive loss weight is a little lower than the segmentation losses L sup and L pseudo (? contr = 10 ?1 ).</p><p>The effect of the size (per-class) of our memory bank is studied in <ref type="table" target="#tab_2">Table 4</ref>.4. As expected, higher values lead to stronger performances, although from 256 up they tend to maintain similarly. Because all the elements from the memory bank are used during the contrastive optimization (Equation 11) the larger the memory bank is, the more computation and memory it requires. Therefore, we select 256 as our default value. <ref type="table" target="#tab_2">Table 4</ref>.4 studies the effect of the main components used in the proposed contrastive learning module. The base configuration of the module which includes our simplest implementation of the per-pixel contrastive learning using the memory bank, still presents a performance gain compared to not using the contrastive learning module (57.4 mIoU from 4.4). Generating and selecting good quality prototypes is the most important factor. This is done both by the Feature Quality Filter (FQF), i.e., checking that the feature leads to an accurate and confident prediction, and extracting them with the teacher network f ? . Then, using the class-specific attention S c,? to weight every sample (both from the memory bank and input sample) is also beneficial, acting as a learned sampling method.</p><p>Future direction. Our proposed approach could potentially be applied to other semi-supervised tasks like object detection or instance segmentation. The straightforward way is to perform the proposed contrastive learning using  the features from the semantic head of the detection or instance segmentation networks, i.e., the part of the network that outputs the semantic class of the object or instance. The method is currently restricted by the number of classes and number of memory bank entries per class. A future step to solve this problem could be to cluster the feature vectors per class and save only cluster centers of the class features, similar to the very recent work from Zhang et. al <ref type="bibr" target="#b48">[48]</ref> for domain adaptation based on prototypical learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a novel approach for semisupervised semantic segmentation. Our work shows the benefits of incorporating positive-only contrastive learning techniques to solve this semi-supervised task. The proposed contrastive learning module boosts the performance of semantic segmentation in these settings. Our new module contains a memory bank that is continuously updated with selected features from those produced by a teacher network from labeled data. These features are selected based on their quality and relevance for the contrastive learning. Our student network is optimized for both labeled and unlabeled data to learn similar class-wise features to those in the memory bank. The use of contrastive learning at a pixellevel has been barely exploited and this work demonstrates the potential and benefits it brings to semi-supervised semantic segmentation and semi-supervised domain adaptation. Our results outperform state-of-the-art on several public benchmarks, with particularly significant improvements on the more challenging set-ups, i.e., when the amount of available labeled data is low.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Contrastive learning optimization. At every iteration, features are extracted by f ? from labeled samples (see right part)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on Cityscapes. Models are trained with 1 8 of the labeled data using Deeplabv2 with ResNet-101. From left to right: Image, manual annotations, ClassMix<ref type="bibr" target="#b29">[29]</ref>, DMT<ref type="bibr" target="#b9">[10]</ref>, our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 f</head><label>4</label><figDesc>Base f ? S c,? FQF mIoU 58? : Use teacher model f ? to extract features instead of f ? S c,? : Use class-specific attention S c,? to weight every feature FQF: Feature Quality Filter for Memory Bank update</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Strong and weak data augmentation set-ups</figDesc><table><row><cell>Parameter</cell><cell>Weak</cell><cell>Strong</cell></row><row><cell>Flip probability</cell><cell>0.50</cell><cell>0.50</cell></row><row><cell>Resize ?[0.75, 1.75] probability</cell><cell>0.50</cell><cell>0.80</cell></row><row><cell>Color jittering probability</cell><cell>0.20</cell><cell>0.80</cell></row><row><cell>Brightness adjustment max intensity</cell><cell>0.15</cell><cell>0.30</cell></row><row><cell>Contrast adjustment max intensity</cell><cell>0.15</cell><cell>0.30</cell></row><row><cell>Saturation adjustment max intensity</cell><cell cols="2">0.075 0.15</cell></row><row><cell>Hue adjustment max intensity</cell><cell>0.05</cell><cell>0.10</cell></row><row><cell>Gaussian blurring probability</cell><cell>0</cell><cell>0.20</cell></row><row><cell>ClassMix probability</cell><cell>0.20</cell><cell>0.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance (Mean IoU) for the Cityscapes val set for different labeled-unlabeled ratios and, in parentheses, the difference w.r.t. the corresponding fully supervised (FS) result. (-17.7) 64.7 (-10.2) 69.5 (-5.4) 74.9 s4GAN [27]+ 63.3 (-10.3) 67.2 (-6.4) 71.4 (-2.2) 73.6 French et al. [11]* 64.8 (-7.7)66.5 (-6.0) 67.6 (-4.9) 72.5</figDesc><table><row><cell>method</cell><cell>1/30</cell><cell>1/8</cell><cell>1/4</cell><cell>FS</cell></row><row><cell cols="3">Architecture: Deeplabv2 with ResNet-101 backbone</cell><cell></cell><cell></cell></row><row><cell>Adversarial [17]+</cell><cell>-</cell><cell cols="3">58.8 (-7.6) 62.3 (-4.1) 66.4</cell></row><row><cell>s4GAN [27]*</cell><cell>-</cell><cell cols="3">59.3 (-6.7) 61.9 -(4.9) 66.0</cell></row><row><cell cols="5">French et al. [11]* 51.2 (-16.3) 60.3 (-7.2) 63.9 (-3.6) 67.5</cell></row><row><cell>CBC [9]+</cell><cell cols="4">48.7 (-18.2) 60.5 (-6.4) 64.4 (-2.5) 66.9</cell></row><row><cell>ClassMix [29]+</cell><cell cols="4">54.1 (-12.1) 61.4 (-4.8) 63.6 (-2.6) 66.2</cell></row><row><cell>DMT [10]*+</cell><cell cols="2">54.8 (-13.4) 63.0 (-5.2)</cell><cell>-</cell><cell>68.2</cell></row><row><cell>Ours*</cell><cell cols="4">58.0 (-8.4) 63.0 (-3.4) 64.8 (-1.6) 66.4</cell></row><row><cell>Ours+</cell><cell>59.4 (-7.9)</cell><cell cols="3">64.4 (-2.9) 65.9 (-1.4) 67.3</cell></row><row><cell cols="3">Architecture: Deeplabv3+ with ResNet-50 backbone</cell><cell></cell><cell></cell></row><row><cell>Error-corr [26]*</cell><cell>-</cell><cell cols="3">67.4 (-7.4) 70.7 (-4.1) 74.8</cell></row><row><cell>Lai et al. [21]*</cell><cell>-</cell><cell cols="3">69.7 (-7.8) 72.7 (-4.8) 77.5</cell></row><row><cell>Ours*</cell><cell cols="4">64.9 (-9.3) 70.1 (-4.1) 71.7 (-2.5) 74.2</cell></row></table><note>* ImageNet pre-training, + COCO pre-training</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Mean IoU in Cityscapes val set. Central columns evaluate the semi-supervised domain adaptation task (GTA5 ? Cityscapes). The last column evaluates a semi-supervised setting in Cityscapes (no adaptation). Different labeled-unlabeled ratios for Cityscapes are compared. All methods use ImageNet pretrained Deeplabv2 with ResNet-101 backbone.</figDesc><table><row><cell>City</cell><cell cols="3">ASS [42] Liu et al. [23] Ours</cell><cell>Ours</cell></row><row><cell>Labels</cell><cell cols="2">with domain adaptation</cell><cell></cell><cell>no adaptation</cell></row><row><cell>1/30</cell><cell>54.2</cell><cell>55.2</cell><cell>59.9</cell><cell>58.0</cell></row><row><cell>1/15</cell><cell>56.0</cell><cell>57.0</cell><cell>62.0</cell><cell>59.9</cell></row><row><cell>1/6</cell><cell>60.2</cell><cell>60.4</cell><cell>64.2</cell><cell>63.7</cell></row><row><cell>1/3</cell><cell>64.5</cell><cell>64.6</cell><cell>65.6</cell><cell>65.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>.4 shows the impact of every loss used by the proposed method. We can observe that the four losses are complementary, getting a 10 mIoU increase over our baseline model, using only the supervised training when L sup L pseudo L ent L contr mIoU</figDesc><table><row><cell>49.5</cell></row><row><cell>56.7</cell></row><row><cell>52.2</cell></row><row><cell>54.4</cell></row><row><cell>57.4</cell></row><row><cell>59.0</cell></row><row><cell>57.3</cell></row><row><cell>59.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Effect of different values for the factor ?contr (Equation 1) that weights the effect of the contrastive loss Lcontr. Results on Cityscapes benchmark ( 1 30 available labels, Deeplabv2-ResNet101 COCO pre-trained). Effect of our memory bank size (features per-class), ?. attention modules(15.96) and, the distance between the input features and memory bank features (10.52). learning module.Table 4.4 shows an ablation on the influence of the contrastive learning module for different values of ? contr(Equation 1). As expected, if this value is too low, the effect gets diluted, with similar performance as if the proposed loss is not used at all (seeTable</figDesc><table><row><cell>?</cell><cell>32</cell><cell>64</cell><cell>128 256 512</cell></row><row><cell cols="4">mIoU 58.7 58.9 59.2 59.4 59.3</cell></row><row><cell cols="4">Results on Cityscapes benchmark ( 1 30 available labels, Deeplabv2-</cell></row><row><cell cols="3">ResNet101 COCO pre-trained).</cell><cell></cell></row><row><cell>Contrastive</cell><cell></cell><cell></cell><cell></cell></row></table><note>specific</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Ablation study of our contrastive learning module main components. Results on Cityscapes benchmark ( 1 30 available labels, using Deeplabv2-ResNet101 COCO pre-trained).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This work was partially funded by FEDER/ Ministerio de Ciencia, Innovaci?n y Universidades/ Agencia Estatal de Investigaci?n/RTC-2017-6421-7, PGC2018-098817-A-I00 and PID2019-105390RB-I00, Arag?n regional government (DGA T45 17R/FSE) and the Office of Naval Research Global project ONRG-NICOP-N62909-19-1-2027.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mininet: An efficient semantic segmentation convnet for real-time robotic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on CVPR</title>
		<meeting>IEEE Conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation via dynamic self-training and classbalanced curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dmt: Dynamic mutual training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
	</analytic>
	<monogr>
		<title level="m">29th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised learning by entropy minimization. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ting</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07934</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with directional context-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Domain adaptation for semantic segmentation via patch-wise contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Zebedin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11056</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">350</biblScope>
			<biblScope unit="page" from="365" to="369" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised segmentation based on error-correcting supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Antonio De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Jo?o Paulo Papa, and Christoph Palm</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and lowlevel consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive patternrecognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning to sample the most useful training patches from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12097,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06191</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11939</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09157</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Alleviating semantic-level shift: A semi-supervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pretraining for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10043</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
