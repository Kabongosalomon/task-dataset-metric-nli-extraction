<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modal Contrastive Learning for Text-to-Image Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<email>zhanghan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><forename type="middle">Yu</forename><surname>Koh</surname></persName>
							<email>jykoh@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
							<email>yinfeiy@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Modal Contrastive Learning for Text-to-Image Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN's output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, butmore importantly-people prefer XMC-GAN by 77.3% for image quality and 74.1% for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Compared to other kinds of inputs (e.g., sketches and object masks), descriptive sentences are an intuitive and flexible way to express visual concepts for generating images. The main challenge for text-to-image synthesis lies in learning from unstructured description and handling the different statistical properties between vision and language inputs. Generative Adversarial Networks (GANs) <ref type="bibr" target="#b11">[12]</ref> have shown promising results on text-to-image generation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>, using a conditional GAN formulation <ref type="bibr" target="#b10">[11]</ref>. At-tnGAN <ref type="bibr" target="#b57">[58]</ref> proposes a multi-stage refinement framework to generate fine-grained details by attending to relevant words in the description. These models generate high fidelity images on single domain datasets (e.g., birds <ref type="bibr" target="#b55">[56]</ref> and flowers <ref type="bibr" target="#b34">[35]</ref>), but struggle on complex scenes with many objects-such as those in MS-COCO <ref type="bibr" target="#b29">[30]</ref>. Recent methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref> propose object-driven, hierarchical approaches that explicitly model object instances within an image. Given the text description, they first infer a semantic layout (e.g., object bounding boxes, segmentation masks, or a combination), and then generate an image from the layout. These hierarchical methods are cumbersome to apply to real-world scenarios; generation becomes a multi-step process (box-to-mask-to-image), and the model requires much more fine-grained object labels to train.</p><p>We study contrastive learning in the context of text-toimage synthesis and demonstrate that a simple one-stage GAN without object-level annotation can outperform prior object-driven and multi-stage approaches. Besides generating realistic images, we also hope (1) the image should holistically match the description; (2) generated images should match real images when they are conditioned on the same description; <ref type="bibr" target="#b2">(3)</ref> individual image regions should be recognizable and consistent with words in the sentence. To fulfill these desiderata and achieve strong language alignment, we propose to maximize the mutual information between the corresponding pairs through contrastive learning. Our method, the Cross(X)-Modal Contrastive Generative Adversarial Network (XMC-GAN), uses image to sentence, image region to word, and image to image contrastive losses to enforce alignment between generated images and their captions ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Our primary contributions include:</p><p>? We propose XMC-GAN, a simple one-stage GAN that employs several contrastive losses. XMC-GAN produces dramatic improvements over previous models, e.g. reducing FID <ref type="bibr" target="#b14">[15]</ref> from 24.70 to 9.33 on MS-COCO and from 48.70 to 14.12 on LN-COCO (the MS-COCO portion of Localized Narratives <ref type="bibr" target="#b39">[40]</ref>). ? We conduct thorough human evaluations comparing XMC-GAN to three recent models. These show that people prefer XMC-GAN 77.3% of the time for image realism, and 74.1% for image-text alignment. ? We establish a strong benchmark on the challenging LN-OpenImages (Open Images subset of Localized Narratives). To the best of our knowledge, this is the first text-to-image results training and testing on the diverse images and descriptions for Open Images. ? We conduct a thorough analysis of contrastive losses used in XMC-GAN to provide general modeling insights for contrastive learning in conditional GANs.</p><p>XMC-GAN consistently produces images that are more coherent and detailed than previous models. In addition to greater realism (with clearer, more delineated objects), they better capture the full image description, including the presence of named objects and background compositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Text-to-image synthesis Generating images from text descriptions has been quickly improved with deep generative models, including pixelCNN <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b44">45]</ref>, approximate Langevin sampling <ref type="bibr" target="#b33">[34]</ref>, variational autoencoders (VAEs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44]</ref>. GAN-based models in particular have shown better sample quality <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24]</ref>. GAN-INT-CLS <ref type="bibr" target="#b43">[44]</ref> was the first to use conditional GANs for text to image generation. StackGAN <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref> improves this with a coarse-to-fine framework that progressively generates images at different resolutions for highresolution synthesis. AttnGAN <ref type="bibr" target="#b57">[58]</ref> introduces cross-modal attention to better capture details. DM-GAN <ref type="bibr" target="#b65">[66]</ref> adaptively refines generated images with a memory module that writes and reads text and image features. MirrorGAN <ref type="bibr" target="#b42">[43]</ref> enforces text-image consistency via caption generation on the generated images. SD-GAN <ref type="bibr" target="#b58">[59]</ref> proposes word-level conditional batch normalization and dual encoder structure with triplet loss to improve text-image alignment. Compared with the triplet loss, our contrastive loss does not require mining for informative negatives and thus lowers training complexity. CP-GAN <ref type="bibr" target="#b27">[28]</ref> proposes an objectaware image encoder and fine-grained discriminator. Its generated images obtain high Inception Score <ref type="bibr" target="#b45">[46]</ref>; however, we show it performs poorly when evaluated with the stronger FID <ref type="bibr" target="#b14">[15]</ref> metric and in human evaluations (see Sec. 6.1). To create a final high resolution image, these approaches rely on multiple generators and discriminators to generate images at different resolutions. Others have proposed hierarchical models that explicitly generate different objects after inferring semantic layouts <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>. A drawback of these is that they need fine-grained object labels (e.g., object bounding boxes or segmentation maps), so generation is a multi-step process. Compared to these multistage and multi-step frameworks, our proposed XMC-GAN only has a single generator and discriminator trained endto-end, and it generates much higher quality images.</p><p>Contrastive learning and its use in GANs Contrastive learning is a powerful scheme for self-supervised representation learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b56">57]</ref>. It enforces consistency of image representations under different augmentations by contrasting positive pairs with negative ones. It has been explored under several adversarial training scenarios <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref>. Cntr-GAN <ref type="bibr" target="#b64">[65]</ref> uses a contrastive loss as regularization on image augmentations for unconditional image generation. ContraGAN <ref type="bibr" target="#b19">[20]</ref> explores contrastive learning for class-conditional image generation. DiscoFaceGAN <ref type="bibr" target="#b8">[9]</ref> adds contrastive learning to enforce disentanglement for face generation. CUT <ref type="bibr" target="#b38">[39]</ref> proposes patch-based contrastive learning for image-to-image translation by using positive pairs from the same image location in input and output images. Unlike prior work, we use intra-modality (image-image) and inter-modality (imagesentence and region-word) contrastive learning in text-toimage synthesis ( <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive Representation Learning</head><p>Contrastive learning aims to learn useful features given different views of data <ref type="bibr" target="#b52">[53]</ref>. For example, note that v 1 and v 2 are two random variables to represent two different views of data. Feature representations are learned by measuring the mutual dependence I(v 1 ; v 2 ) between these two variables. As directly maximizing the mutual information is challenging <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b49">50]</ref>, the InfoNCE loss <ref type="bibr" target="#b35">[36]</ref> was proposed to maximize a lower bound of the mutual information I(v 1 ; v 2 ) Specifically, given a query sample v 1,i , minimizing the InfoNCE loss is to score the matching positive sam-  <ref type="figure">Figure 2</ref>: Overview of the proposed XMC-GAN.</p><formula xml:id="formula_0">ple v 2,i ? p(v 2 |v 1,i ) higher than M ?1 negative samples v 2,j ? p(v 2 ).</formula><p>The overall objective can be summarized as follows:</p><formula xml:id="formula_1">I(v 1 ; v 2 ) ? log(M ) ? L N CE , where L N CE = ?E log exp(S(v 1,i , v 2,i )) M j=1 exp(S(v 1,i , v 2,j ))</formula><p>.</p><p>Here, S(?, ?) is the score function, which usually has two parameterized feature encoders for v 1 and v 2 . The encoders can share parameters if v 1 and v 2 are from the same modality. There are many ways to construct v 1 and v 2 : different image augmentations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref>; spatially adjacent image patches <ref type="bibr" target="#b35">[36]</ref>; a video as v 1 and its aligned audio as v 2 for video representation learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative Adversarial Networks (GANs)</head><p>GANs <ref type="bibr" target="#b11">[12]</ref> are generative models that employ both a generator and a discriminator. The generator G maps a latent variable z?p(z) (usually sampled from a Gaussian distribution) to a real data distribution p data . The discriminator D is trained to distinguish whether inputs are synthesized by G or sampled from real data. The generator G is trained to synthesize images that the discriminator will classify as real.</p><p>A large amount of work has focused on designing the adversarial objective to improve training <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54]</ref>. A notable example is the hinge loss:</p><formula xml:id="formula_2">L D = ? E x?pdata [min(0, ?1 + D(x))] ? E z?p(z) [min(0, ?1 ? D(G(z)))] , L G = ? E z?p(z) [D(G(z))] .</formula><p>The hinge loss has been used in state-of-the-art GANs for image generation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b62">63]</ref>. For conditional GANs, the generator and the discriminator are provided with an additional condition c, yielding G(z, c) and D(x, c). For conditional generation, the generated sample should be both realistic and also match the condition c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>We describe the losses and components of XMC-GAN below. See <ref type="figure">Fig. 2</ref> for an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Contrastive Losses for Text-to-Image Synthesis</head><p>Text-to-image synthesis is a conditional generation task. Generated images should both be realistic and well-aligned with a given description. To achieve this, we propose to maximize the mutual information between the corresponding pairs: (1) image and sentence, (2) generated image and real image with the same description, and (3) image regions and words. Directly maximizing mutual information is difficult (see Sec. 3.1), so we maximize the lower bound of the mutual information by optimizing contrastive (i.e., In-foNCE) losses.</p><p>Image-text contrastive loss. Given an image x and its corresponding description s, we define the score function following previous work in contrastive learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_3">S sent (x, s) = cos(f img (x), f sent (s))/?,</formula><p>where cos(u, v) = u T v/ u v denotes cosine similarity, and ? denotes a temperature hyper-parameter. f img is an image encoder to extract the overall image feature vector and f sent is a sentence encoder to extract the global sentence feature vector. This maps the image and sentence representations into a joint embedding space R D . The contrastive loss between image x i and its paired sentence s i is computed as:</p><formula xml:id="formula_4">L sent (x i , s i ) = ? log exp(cos(f img (x i ), f sent (s i ))/? ) M j=1 exp(cos(f img (x i ), f sent (s j ))/? ) .</formula><p>This form of contrastive loss is also known as the normalized temperature-scaled cross entropy loss (NT-Xent) <ref type="bibr" target="#b4">[5]</ref>.</p><p>Contrastive loss between fake and real images with shared description. This contrastive loss is also defined with NT-Xent. The main difference is that a shared image encoder f img extracts features for both real and fake images. The score function between two images is S img (x,x) = cos(f img (x), f img (x))/? . The image-image contrastive loss between real image x i and generated image G(z i , s i ) is:</p><formula xml:id="formula_5">L img (x i , G(z i , s i )) = ? log exp(S img (x i , G(z i , s i ))) M j=1 exp(S img (x i , G(z j , s j )))</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive loss between image regions and words.</head><p>Individual image regions should be consistent with corresponding words in an input description. We use attention <ref type="bibr" target="#b57">[58]</ref> to learn connections between regions in image x and words in sentence s, without requiring fine-grained annotations that align words and regions. We first compute the pairwise cosine similarity matrix between all words in the sentence and all regions in the image; then, we compute the soft attention ? i,j for word w i to region r j as:</p><formula xml:id="formula_6">? i,j = exp(? 1 cos(f word (w i ), f region (r j ))) R h=1 exp(? 1 cos(f word (w i ), f region (r h ))) ,</formula><p>where f word and f region represent word and region feature encoders respectively, R is the total number of regions in the image and ? 1 is a sharpening hyper-parameter to reduce the entropy of the soft attention. The aligned region feature for the i th word is defined as c i = R j=1 ? i,j f region (r j ). The score function between all the regions in image x and all words in sentence s can then be defined as:</p><formula xml:id="formula_7">Sword(x, s) = log T h=1 exp(?2 cos(fword(w h ), c h )) 1 ? 2 /?,</formula><p>where T is the total number of words in the sentence. ? 2 is a hyper-parameter that determines the weight of the most aligned word-region pair, e.g., as ? 2 ? ?, the score function approximates to max T h=1 cos(f word (w h ), c h ). Finally the contrastive loss between the words and regions in image x i and its aligned sentence s i can be defined as:</p><formula xml:id="formula_8">L word (x i , s i ) = ? log exp(S word (x i , s i )) M j=1 exp(S word (x i , s j ))</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attentional Self-Modulation Generator</head><p>We propose a one-stage generator to directly generate the image at the desired resolution. This is much simpler than previous multi-stage generators that create images at multiple, different resolutions. We first sample noise z from a standard Gaussian distribution. We obtain the global sentence embedding e s and the word embeddings e w from a pretrained BERT <ref type="bibr" target="#b9">[10]</ref> module. e s and z are concatenated to form the global condition, which is passed through several up-sampling blocks (see appendix for details) to generate a 16 ? 16 feature map. The global condition is also used as the condition to calculate scale parameter ? and shift parameter ? in conditional batch normalization layers. This formulation is also known as self-modulation <ref type="bibr" target="#b5">[6]</ref>.</p><p>The self-modulation layer improves consistency of the hidden feature with the conditional inputs, but it lacks finer details for each sub-region. To generate fine-grained, recognizable regions, we propose the attentional self-modulation layer. Specifically, besides random noise z and global sentence embedding e s , we modify the attention mechanism <ref type="bibr" target="#b57">[58]</ref> to calculate the word-context vector as the additional modulation parameter for each sub-region. For the j th region with feature h j , the word-context vector c j is:</p><formula xml:id="formula_9">cj = T i=1? j,iew i , where?j,i = exp(?0 cos(ew i , hj)) T k=1 exp(?0 cos(ew k , hj)) ,</formula><p>where T is the total number of words in the sentence and ? 0 is a sharpening hyper-parameter. Then, the modulated feature h j for the j th region can be defined as:</p><formula xml:id="formula_10">h j = ?j(concat(z, es, cj)) hj ? ? ? + ?j(concat(z, es, cj)),</formula><p>where ? and ? are the estimated mean and standard deviation from aggregating both batch and spatial dimensions. ? j (?) and ? j (?) represent any function approximators; in our work we simply use linear projection layers. Further details of the generator can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Contrastive Discriminator</head><p>Our proposed discriminator has two roles: (1) to act as a critic to determine whether an input image is real or fake, and (2) to act as an encoder to compute global image and region features for the contrastive loss. The image is</p><formula xml:id="formula_11">Algorithm 1 XMC-GAN Training Algorithm.</formula><p>Input: generator and discriminator parameters ? G , ? D , contrastive loss coefficients ? 1 , ? 2 , ? 3 , Adam hyperparameters ? 1 , ? 2 , generator and discriminator learning rate lr G , lr D , batch size M , number of discriminator iterations per generator iteration N D 1: for number of training iterations do 2:</p><formula xml:id="formula_12">for t = 1, ..., N D do 3: Sample {z i } M i=1 ? p(z) 4: Sample {(x i , s i )} M i=1 ? p data (x, s) 5: L r sent ? 1 M M i=1 L sent (x i , s i ) 6: L r word ? 1 M M i=1 L word (x i , s i ) 7: L D GAN ? ? 1 M M i=1 min(0, ?1 + D(x i , s i ))? 1 M M i=1 min(0, ?1 ? D(G(z i , s i ), s i )) 8: L D ? L D GAN + ? 1 L r sent + ? 2 L r word 9: ? D ? Adam(L D , lr D , ? 1 , ? 2 ) 10: end for 11: Sample {z i } M i=1 ? p(z), {(x i , s i )} M i=1 ? p data (x, s) 12: L f sent ? 1 M M i=1 L sent (G(z i , s i ), s i ) 13: L f word ? 1 M M i=1 L word (G(z i , s i ), s i ) 14: L img ? 1 M M i=1 L img (G(z i , s i ), x i ) 15: L G GAN ? 1 M M i=1 ?(D(G(z i , s i ), s i )) 16: L G ? L G GAN + ? 1 L f sent + ? 2 L f word + ? 3 L img 17: ? G ? Adam(L G , lr G , ? 1 , ? 2 ) 18: end for</formula><p>passed through several down-sampling blocks until its spatial dimensions are reduced to 16?16 (see <ref type="figure">Fig. 2</ref>, bottom left). Then, a 1?1 convolution is applied to obtain region features, where the feature dimensions are consistent with the dimensions of the word embedding. The original image feature is fed through two more down-sampling blocks and a global pooling layer. Finally, a projection head computes the logit for the adversarial loss, and a separate projection head computes image features for the image-sentence and image-image contrastive loss. Note that it is important to only use the real images and their descriptions to train these discriminator projection heads. The reason is that the generated images are sometimes not recognizable, especially at the start of training. Using such generated image and sentence pairs hurts the training of the image feature encoder projection heads. Therefore, the contrastive losses from fake images are only applied to the generator. In addition to the discriminator projection layers, we use a pretrained VGG network <ref type="bibr" target="#b48">[49]</ref> as an image encoder for an additional supervisory image-image contrastive loss (see Sec. 6.2). Algorithm 1 summarizes the XMC-GAN training procedure. For simplicity, we set all contrastive loss coefficients (? 1 , ? 2 , ? 3 in Algorithm 1) to 1.0 in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data</head><p>We perform a comprehensive evaluation of XMC-GAN on three challenging datasets (summarized in <ref type="table" target="#tab_1">Table 1</ref>).</p><p>MS-COCO <ref type="bibr" target="#b29">[30]</ref> is commonly used for text-to-image synthesis. Each image is paired with 5 short captions. We follow most prior work to use the 2014 split (COCO-14) for evaluation.</p><p>Localized Narratives <ref type="bibr" target="#b39">[40]</ref> contains long form image descriptions for several image collections. We benchmark results on LN-COCO, which contains narratives for images in the 2017 split of MS-COCO (COCO-17). Narratives are four times longer than MS-COCO captions on average and they are much more descriptive (see <ref type="figure">Figure 4</ref>). Narratives also contain disfluencies since they are spoken and then transcribed. These factors make text-to-image synthesis for LN-COCO much more challenging than MS-COCO.</p><p>We also train and evaluate using LN-OpenImages, the Open Images <ref type="bibr" target="#b22">[23]</ref> split of Localized Narratives. Its images are both diverse and complex (8.4 objects on average). LN-OpenImages is also much larger than MS-COCO and LN-COCO (see <ref type="table" target="#tab_1">Table 1</ref>). To the best of our knowledge, we are the first to train and evaluate a text-to-image generation model for Open Images. XMC-GAN is able to generate high quality results, and sets a strong benchmark for this very challenging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metrics</head><p>Following previous work, we report validation results by generating images for 30,000 random captions 1 . We evaluate comprehensively using several measures.</p><p>Image quality. We use standard automated metrics for assessing image quality. Inception Score (IS) <ref type="bibr" target="#b45">[46]</ref> calculates KL-divergence between the conditional class distribution and the marginal class distribution given a pre-trained image classifier. Fr?chet Inception Distance (FID) <ref type="bibr" target="#b14">[15]</ref> is the Fr?chet distance between two multivariate Gaussians fit to Inception <ref type="bibr" target="#b50">[51]</ref> features of generated and real images. While IS and FID have both been shown to correlate with human judgements of generated image quality, IS is likely less informative as it overfits easily and can be manipulated to achieve much higher scores using simple tricks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. This is further emphasized by our results (Sec. 6.1) showing that FID correlates better with human judgments of realism.  Text-Image Alignment. Following previous work <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b26">27]</ref>, we use R-precision to assess whether a generated image can be used to retrieve its conditioning description. However, we notice that previous work computes R-precision using image-text encoders from AttnGAN <ref type="bibr" target="#b57">[58]</ref>, and many others use these encoders as part of their optimization function during training. This skews results: many generated models report R-precision scores significantly higher than real images. To alleviate this, we use an image-text dualencoder 2 <ref type="bibr" target="#b37">[38]</ref> pretrained on real images in the Conceptual Captions dataset <ref type="bibr" target="#b47">[48]</ref>, which is disjoint from MS-COCO. We find that computing R-precision with independent encoders better correlates with human judgments.</p><formula xml:id="formula_13">Model IS ? FID ? R-prec (CC) ? SOA-C ? SOA-I ? Real</formula><p>Caption retrieval metrics assess whether the entire image matches the caption. In contrast, Semantic Object Accuracy (SOA) <ref type="bibr" target="#b16">[17]</ref> evaluates the quality of individual regions and objects within an image. Like previous work, we report SOA-C (i.e., the percentage of images per class in which a desired object is detected) and SOA-I (i.e., the percentage of images in which a desired object is detected). Further details of SOA can be found in <ref type="bibr" target="#b16">[17]</ref>. SOA was originally designed for COCO-14, and can take very long to compute as it requires generating multiple samples for each MS-COCO class label. We use the official code to compute the metrics reported in <ref type="table" target="#tab_3">Table 2</ref>, but approximate results for LN-COCO and other ablation experiments where we compute results over 30,000 random samples.</p><p>Human evaluation. Automated metrics are useful while iterating on models during experimentation, but they are no substitute for human eyes. We conduct thorough human evaluations on generated images from 1000 randomly selected captions. For each caption, we request 5 independent human annotators to rank the generated images from best to worst based on (1) realism, and (2) language alignment. <ref type="bibr" target="#b1">2</ref> This model will be publicly released to facilitate future evaluations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Realism</head><p>Text Alignment <ref type="figure">Figure 3</ref>: Human evaluation on COCO-14 for image quality and text alignment. Annotators rank (anonymized and order-randomized) generated images from best to worst.</p><p>6. Experiments 6.1. Results . <ref type="figure">Figure 3</ref> shows human evaluations comparing XMC-GAN to three recent strong models: CP-GAN <ref type="bibr" target="#b27">[28]</ref>, SD-GAN <ref type="bibr" target="#b58">[59]</ref>, and OP-GAN <ref type="bibr" target="#b16">[17]</ref>. Given images (anonymized and randomly ordered) generated from the same caption by the four models, annotators are asked to rank them from best to worst. Realism and text alignment judgments are collected independently. XMC-GAN is the clear winner on both: its output is ranked best in 77.3% of realism comparisons, and 74.1% of text alignment ones. OP-GAN is a distant second, at 9.90% and 9.70%, respectively. XMC-GAN achieves this while being a simpler, onestage model, whereas OP-GAN is multi-stage and needs object bounding boxes. Visual inspection of selected images ( <ref type="figure">Fig. 4</ref>) convincingly shows the large quality improvement. XMC-GAN's images are much higher fidelity compared to others, and depict clearer objects and more coherent scenes. This also holds for more random samples (see appendix). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A small kitchen with low a ceiling</head><p>This picture shows an inner view of a restroom we see a wash basin with tap and a mirror on the wall and we see a light on it and we see a toilet seat and a frame on the wall and ...</p><p>A child eating a birthday cake near some balloons.</p><p>In this image we can see a red color train on the railway track. Here we can see platform</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A living area with a television and a table</head><p>In this picture there are two members lying on the beach in the sand under an umbrella. There are some people standing here. In the background there is water <ref type="figure">Figure 4</ref>: Generated images for selected examples from COCO-14 and LN-COCO. XMC-GAN generated images are generally of much higher quality and depict clearer scenes. More random samples are available in the appendix.  our independently trained encoders, indicating a large improvement in fidelity of generated images to the captions they are conditioned on-and consistent with human judgments. Although CP-GAN achieves higher IS and SOA scores, both our human evaluations and visual inspection of randomly selected images indicates XMC-GAN's image quality is much higher than CP-GAN's. This may be due to the issue that IS and SOA do not penalize intra-class mode dropping (low diversity within a class)-a model that generates one "perfect" sample for each class can achieve good scores on IS and SOA. Our findings are consistent with other works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref>, which suggest that FID may be a more reliable metric for measuring text-to-image synthesis quality.</p><formula xml:id="formula_14">Model IS ? FID ? R-prec ? SOA-C ? SOA-I ? Real</formula><p>LN-COCO. Localized Narratives <ref type="bibr" target="#b39">[40]</ref> contains much longer descriptions, which increases the difficulty of textto-image synthesis (see Sec. 5.1).   <ref type="bibr" target="#b57">[58]</ref>, indicating much better text alignment. This is supported by qualitative comparison of randomly selected outputs: XMC-GAN's images are decisively clearer and more coherent (see <ref type="figure">Fig. 4</ref>). We stress that TReCS exploits LN-COCO's mouse trace annotations-incorporating this training signal in XMC-GAN in future should further boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LN-OpenImages.</head><p>We train XMC-GAN on Open Images dataset, which is much more challenging than MS-COCO due to greater diversity in images and descriptions. XMC-GAN achieves an IS of 24.90, FID of 26.91, and R-precision of 57.55, and manages to generate high quality images (see appendix). To the best of our knowledge, XMC-GAN is the first text-to-image model trained and evaluated on Open Images. Its strong automated scores establish strong benchmark results on this challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablations</head><p>We thoroughly evaluate the different components of XMC-GAN and analyze their impact. <ref type="table" target="#tab_8">Table 4</ref> summarizes our ablations 3 on the COCO-14 validation set. To study the effects of each contrastive loss component used in XMC-GAN, we experiment with four losses: (1) image-sentence, (2) region-word, (3) image-image using discriminator features, and (4) image-image using VGG features. For (3), we use the discriminator encoder projection (indicated by D in <ref type="table" target="#tab_8">Table 4</ref>) to extract image features. For (4), we extract image features from a VGG-19 network <ref type="bibr" target="#b48">[49]</ref> pretrained on ImageNet.</p><p>Individual contrastive losses. <ref type="table" target="#tab_8">Table 4</ref> shows that using any of the contrastive losses improves all metrics compared to the baseline. During experimentation, we also found that including any contrastive loss greatly improves training stability. The largest improvements come from the intermodal image-sentence and region-word contrastive losses, which improve FID from 39.28 to 19.25 and 24.38, respectively. This is much larger compared to the image-image intra-modal contrastive losses, e.g., including the loss from the discriminator feature encoder (D) only improves FID to 29.71. These ablations highlight the effectiveness of intermodal contrastive losses: sentence and word contrastive losses each greatly improve the text-alignment metrics, as well as improving image quality.</p><p>Combined contrastive losses. Combining contrastive losses provides further gains. For example, using both image-sentence and region-word losses achieves better performance (FID 14.25) than alone (FID 19.25 and 24.38, respectively). This demonstrates that local and global conditions are complementary. Moreover, using both intermodal losses (sentence and words) outperforms the intramodal losses (D + VGG): FID scores are 14.25 and 21.14, respectively. These results further emphasize the effectiveness of cross-modal contrastive learning. Nevertheless, the inter-modal and intra-modal contrastive losses also complement each other: the best FID score comes from combining image-sentence, region-word, and image-image (VGG) losses. Performance on IS and text alignment further improves when using the image-image (D + VGG) loss. To     <ref type="table" target="#tab_3">(Table 2)</ref>, we train a model (with base channels dimension 96) using all 4 contrastive losses.</p><formula xml:id="formula_15">Modulation IS ? FID ? R-prec ? SOA-C ? SOA-I ? Self-</formula><p>Deeper contrastive heads. In unsupervised representation learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, adding non-linear layers generally improves performance. To study this, we increase the depth of the projection head in the discriminator. Training curves for FID and contrastive accuracy <ref type="bibr" target="#b4">[5]</ref> on fake images are in <ref type="figure" target="#fig_2">Fig. 5</ref>, across 1000 epochs. We find that using no additional projection layers gives the best FID (12.61, compared to 19.42 of the 2-layer MLP). Moreover, we also find that the contrastive accuracy increases on fake images (from 76.56% to 88.55%) when more layers are added to the projection head. We posit that the discriminator overfits to the contrastive learning task in this configuration, resulting in poorer performance on the adversarial task as a critic and hence worse as a supervisory signal for the generator.</p><p>Attentional Self-Modulation. We compare two generator setups: (1) self-modulation layers <ref type="bibr" target="#b5">[6]</ref> in all residual blocks, and (2) attentional self-modulation layers (see Sec. 4.2) for blocks with input resolution larger than 16?16. <ref type="table" target="#tab_10">Table 5</ref> shows that the proposed attentional self-modulation layer outperforms self-modulation on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss types.</head><p>A frequently used loss function in generative models is the l 2 loss over VGG <ref type="bibr" target="#b48">[49]</ref> outputs between fake images and corresponding real images. This is also commonly known as the perceptual loss <ref type="bibr" target="#b18">[19]</ref>. <ref type="table" target="#tab_11">Table 6</ref> shows that contrastive losses outperform such perceptual losses. This demonstrates that repelling mismatched samples is more effective than simply pulling together aligned samples. Given this superior performance, replacing perceptual losses with contrastive losses may help other generative tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we present a cross-modal contrastive learning framework to train GAN models for text-to-image synthesis. We investigate several cross-modal contrastive losses that enforce correspondence between image and text. With both human and automated evaluations on multiple datasets, XMC-GAN establishes a marked improvement over previous models: it generates higher quality images that better match their input descriptions, including for long, detailed narratives. It does so while being a simpler, endto-end model. We believe that these advances are strong leaps towards creative applications for image generation from natural language descriptions.</p><p>In this appendix, we share implementation details (Sec. A), architecture details (Sec. B), details about our human evaluation procedure (Sec. C), and further qualitative results (Sec. E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>All models are implemented in TensorFlow 2.0. Spectral normalization is used for all convolutional and fullyconnected layers in the discriminator. For training all models, we use the Adam optimizer with parameters ? 1 = 0.5 and ? 2 = 0.999. The learning rates for the generator and discriminator are set to 1e ?4 and 4e ?4 respectively. We use two discriminator training steps for each generator training step. During validation, we report results from the generator with exponential moving averaged weights, with a decay rate of 0.999.</p><p>Models are trained with a batch size of 256. For reporting results in our paper, models are trained for 1000 epochs, and we report the scores corresponding to the checkpoint with the best FID score on the validation set. For reporting our main results, we train a model with base channel dimensions ch = 96 (see <ref type="table">Table 8</ref>). For ablation experiments in the main paper, we train models with base channel dimensions ch = 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture Details</head><p>Detailed generator and discriminator architectures can be found in <ref type="table">Tables 8a and 8b</ref> respectively. The details of the up-sampling block and down-sampling block are shown in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Human Evaluations</head><p>The user interface shown to human evaluators is shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. Users are requested to rank 4 images from best to worst on (1) image realism and (2) alignment to a given caption. The images are displayed in a random order.  <ref type="table">Table 7</ref>: Contrastive losses applied on the generator/discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Similarities and differences between DAMSM and the proposed contrastive losses</head><formula xml:id="formula_16">Loss IS ? FID ? R-prec ? SOA-C ? SOA-I ? G</formula><p>Our proposed contrastive losses bear several similarities to the DAMSM losses of AttnGAN. However, there are several key differences which are crucial to our strong performance:</p><p>? DAMSM losses are only used to train the generator (G), while contrastive losses in XMC-GAN are designed to train the discriminator (D) also. Features for contrastive losses are calculated from the different heads of the D backbone. This allows D to learn more robust and discriminative features, so XMC-GAN is less prone to mode collapse. This is a key reason that our model does not require multi-stage training. For training G, our contrastive losses are similar to DAMSM, which enforce consistency between generated images and conditional text descriptions. <ref type="table">Table 7</ref> compares adding contrastive losses on D and G separately, which highlights the benefits of our proposed method of training the discriminator.</p><p>? Second, the motivation behind contrastive losses and DAMSM also differs. As described in Sec. 4.1, we propose maximizing the mutual information between intra-modality and inter-modality pairs. We do this by maximizing the lower bound through optimizing contrastive (InfoICE) losses, consistently using cosine distance as the similarity metric. In contrast, the DAMSM loss in AttnGAN is motivated by information retrieval. Their DAMSM module uses dot product in certain instances (Eq. 7 in AttnGAN), and requires an additional normalization step (Eq. 8 in AttnGAN).</p><p>? Last, our training procedure is completely end-to-end, while AttnGAN needs a separate pretraining step. For AttnGAN, their DAMSM module undergoes a separate pretraining step before training the main generator / discriminator models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Effect of random noise on generated images</head><p>In Sec. 6.1 of the main paper, we show that XMC-GAN generated images are largely preferred by human raters. XMC-GAN also significantly improves state-of-the-art FID scores. However, we also observe that the IS and SOA scores for CP-GAN are better than XMC-GAN. We conjecture that the issue was with IS and SOA not penalizing intra-class mode dropping (i.e. low diversity within a class or caption).</p><p>To verify this hypothesis, we conduct experiments to generate images from CP-GAN and XMC-GAN conditioned on the same caption, but with varying noise vectors z. The comparison results are shown in <ref type="figure" target="#fig_5">Fig. 8</ref>. Both the captions and noise vectors used are selected at random. As shown in the figure, XMC-GAN is able to generate diverse images (e.g., different view angles or compositions of the scene) for a fixed caption when different noise vectors are used. In contrast, CP-GAN generated images do not show much diversity despite conditioning on different noise vec-tors. This verifies our hypothesis that CP-GAN may have less diversity for the same class or caption. XMC-GAN is able to generate high quality and diverse scenes even when conditioned on a single caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Effect of captions on generated images</head><p>In <ref type="figure" target="#fig_6">Fig. 9</ref>, we present several examples of XMC-GAN generated images given different captions corresponding to the same original image.</p><p>Different MS-COCO captions. We observe that the generated images vary widely depending on the given caption, even if they are semantically similar. For example, we observe that in the first row, XMC-GAN generated images for caption #2 and caption #3 produce very different images. For caption #3, "A bus driving in a city area with traffic signs.", we observe that XMC-GAN is able to generate features of a city, with high-rise buildings in the background, and a traffic light to the left of the image. In contrast, in caption #2, which does not mention the city XMC-GAN generates an image that shows the bus next to a curb, in agreement with the caption.</p><p>MS-COCO compared to LN-COCO captions. We also observe distinct differences in generated images when conditioned on MS-COCO as compared to LN-COCO captions. LN-COCO captions are much more detailed, which increases image generation difficulty. The increase in difficulty of LN-COCO captions appears to lead to less coherent scenes in general as compared to the MS-COCO model (e.g. the third row of <ref type="figure" target="#fig_6">Fig. 9</ref>).  Random qualitative samples from COCO-14 are presented in <ref type="figure" target="#fig_0">Fig. 10</ref>. We observe that even over randomly selected captions, XMC-GAN appears to generate images that are significantly clearer and more coherent. Scenes often depict clear objects, as compared to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Random samples</head><p>LN-COCO Random qualitative samples from LN-COCO are presented in <ref type="figure" target="#fig_0">Fig. 11</ref>. The longer captions increase the challenge of realistic text-to-image synthesis, but we observe clear improvements from previous methods in most images. In particular, XMC-GAN appears to generate objects and people that are more clear and distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LN-OpenImages Random qualitative samples from LN-</head><p>OpenImages are presented in <ref type="figure" target="#fig_0">Fig. 12</ref>. As this dataset was previously untested on, we simply display the original images against XMC-GAN generated images. Despite the increase in complexity and diversity of images, XMC-GAN generates very strong results, with especially convincing scene generation capability (e.g. first column, second and third last rows). We hope that our results will inspire future work to advance on tackling this very challenging dataset.     A group of people walking down a road.</p><p>In this image, in the middle there are some people walking, in the right side there is a man standing and he is holding a umbrella, in the background there are some cars, there is a bus, there are some green color trees, in the top there is a sky which is cloudy and in white color.</p><p>People are in a parking lot beside the water, while a train is in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Colorful commuter train goes through a marina area on a cloudy day</head><p>A parking lot next to a marina next to a railroad Group of people standing beside their cars on a pier.</p><p>A train crosses as a bunch of gathered vehicles watch.</p><p>Bottom left side of the image there are two vehicles behind the vehicles there are few ships on the water and there are few people are standing. In the middle of the image there is a train on the bridge. Behind the train there are some trees and clouds. In the middle of the image there are two poles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A calculator and cell phone lay on a desk in front of a keyboard</head><p>A cell phone on top of a calculator near a computer keyboard. a table with a calculator and phone siting on it A picture of a cell phone Calculator and a computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>There is a phone on top of a calculator</head><p>In the picture we can see a calculator which is black in color and on it there is a mobile phone and it is also black in color, in the background we can see a keyboard which is white in color placed on white paper on the wooden table.     In this picture we can see a pole in front, in the bottom there are some leaves, in the background we can see a white color and black color cars, on the right side of the image we can see a tree, in the background there is a building and a hill.</p><p>In this image we can see zebra and giraffe standing in grass, And there are so many plants, lake with water, mountain with trees.</p><p>In this image we can see both of the children are standing, and smiling and cooking, in front here is the stove and pan on it, here is the spoon, at side here is the vessel, and at back here is the table, here is the wall, and here is the glass door.</p><p>In this image there are group of persons who are sitting around the table in a restaurant and having some food and there are water glasses on the table,at the background of the image there is a door,mirror and some paintings attached to the wall.</p><p>Here we can see a woman sitting in the briefcase. And this is wall.</p><p>There is a man in white color shirt, wearing a black color tie, standing. In the background, there is a yellow wall near the white ceiling.</p><p>The picture consists of food items on a white color plate like object.</p><p>In this image i can see person holding a bat and a wearing a white helmet. He is wearing blue shirt and white pant. At the back side I can see three person sitting. There is a net. The person is holding a umbrella which is in green and white color. Back Side i can see vehicle.</p><p>Here we can see a bench and this is road. There are plants and this is grass. In the background there is a wall.</p><p>This image consists of refrigerator. On that there are cans and boxes. There is light on the top. There is magnum sticker on refrigerator. <ref type="figure" target="#fig_0">Figure 11</ref>: Original and generated images for random examples from LN-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caption</head><p>Original</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XMC-GAN Caption Original XMC-GAN</head><p>In this picture I can see the cars on the grass in the top right hand side there is a vehicle. In the background there may be the buildings.</p><p>In this image I can see a mirror with some text written on it. In the background I can see a car the trees and the buildings with some text written on it.</p><p>In this image I can see a cat on a sidewalk and I can see a dark color.</p><p>In this image we can see people sitting on chairs. Also we can see packets on chairs. There are two people standing. Also we can see cupboards with books. And there is a pillar. And there is a table ...</p><p>In this image we can see vehicles a fence and a pole. At the top there is sky. At the bottom there are plants and we can see grass.</p><p>In this picture we can see a grill meat piece in black plate which is placed on the wooden table top.</p><p>In front of the image there is a person running on the track. Beside the track there is a sponsor board. At the bottom of the image there is grass on the surface.</p><p>In this image in the foreground we can see a sculpture and in the background we can see many branches of a tree. This is an aerial view and here we can see buildings and trees. At the top there is sky.</p><p>In front of the image there is an army personnel holding some objects in his hand. Behind the person there are a few army personnel. In the background of the image there are photo frames and doors on ...</p><p>In this image I can see cake on the table. There is hand of a person holding the knife also there are hands of another person holding food item in one hand. And there are some other objects.</p><p>In this picture we see a plastic glass containing the ice cream is placed on the white table. We see the tissue papers and a paper glass are placed on the table. In the background we see a grey color object is placed ...</p><p>In this image we can see a bunch of flowers to the plants. We can also see the wooden surface.</p><p>In this picture I can see few plants with leaves and I can see the flowers.</p><p>In the foreground I can see grass a fence a net light poles and wires.</p><p>In the background I can see water house plants some objects the trees and the sky.</p><p>It is an edited image with different shaped designs.</p><p>In this image there is dried grass on the ground. In the top left side of the image I can see a tree. In the background there is sky.</p><p>In this image there are birds on a pathway and I can see a duck in the water.</p><p>In this image I can see a pen which is black in color on the white colored surface.</p><p>In this image I can see the cat on the mat and I can see few objects. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Inter-modal and intra-modal contrastive losses in our proposed XMC-GAN text-to-image synthesis model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Comparison between different contrastive heads. obtain our final results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) UI for ranking image realism.(b) UI for ranking text alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>User interface for collecting human evaluations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of CP-GAN and XMC-GAN generated images for the same caption with different noise vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Generated images for varying captions from COCO-14 and LN-COCO corresponding to the same original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>CaptionOP-GAN SD-GAN CP-GAN XMC-GAN CaptionOP-GAN SD-GAN CP-GAN XMC-GAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Generated images for random examples from COCO-14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Original and generated images for random examples from LN-OpenImages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">COCO-14 train val</cell><cell cols="4">LN-COCO LN-OpenImages train val train val</cell></row><row><cell>#samples</cell><cell>82k</cell><cell cols="2">40k 134k</cell><cell>8k</cell><cell>507k</cell><cell>41k</cell></row><row><cell>caption/image</cell><cell>5</cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell></row><row><cell>avg. caption length</cell><cell cols="2">10.5</cell><cell>42.1</cell><cell></cell><cell>35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of XMC-GAN with previous models on COCO-14. R-prec (CC) are R-precision scores computed from a model trained on Conceptual Captions (see Sec. 5.2). ? indicates scores computed from images shared by the original paper authors, and ? indicates scores computed from images generated from the open-sourced models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>provides comprehensive COCO-14 results for automated metrics. XMC-GAN dramatically improves FID from 24.70 to 9.33, a 62.2% relative improvement over the next best model, OP-GAN<ref type="bibr" target="#b16">[17]</ref>. XMC-GAN also outperforms others (71% vs. 59%) for R-precision computed withMS-COCO Caption OP-GAN SD-GAN CP-GAN XMC-GAN LN-COCO Caption AttnGAN TReCS XMC-GAN a green train is coming down the tracks There is a group of people. They are standing on ski board. They are smiling. They are holding a sticks. In the center of the person is wearing a helmet. On the right side ... In this image I can see people are sitting on chairs. I can also see few of them are wearing shades. Here I can see few more chairs and tables. On this table I can see food ...</figDesc><table><row><cell>A group of skiers</cell></row><row><cell>are preparing to</cell></row><row><cell>ski down a moun-</cell></row><row><cell>tain.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of XMC-GAN on LN-COCO. SOA metrics together with others are computed from 30,000 ran- dom examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Real Images [17]</cell><cell>34.88</cell><cell>6.09</cell><cell>69.36</cell><cell>76.17</cell><cell>80.12</cell></row><row><cell></cell><cell cols="2">15.89 39.28</cell><cell>21.41</cell><cell>8.99</cell><cell>25.72</cell></row><row><cell></cell><cell cols="2">23.50 19.25</cell><cell>53.57</cell><cell>24.57</cell><cell>45.41</cell></row><row><cell></cell><cell cols="2">20.72 24.38</cell><cell>44.42</cell><cell>20.50</cell><cell>39.12</cell></row><row><cell>D</cell><cell cols="2">18.90 29.71</cell><cell>31.16</cell><cell>12.73</cell><cell>30.89</cell></row><row><cell>VGG</cell><cell cols="2">21.54 39.58</cell><cell>35.89</cell><cell>17.41</cell><cell>35.08</cell></row><row><cell cols="3">D + VGG 23.61 21.14</cell><cell>47.04</cell><cell>23.87</cell><cell>44.41</cell></row><row><cell></cell><cell cols="2">26.02 14.25</cell><cell>64.94</cell><cell>30.49</cell><cell>51.60</cell></row><row><cell>D</cell><cell cols="2">28.06 12.96</cell><cell>65.36</cell><cell>34.21</cell><cell>54.23</cell></row><row><cell>VGG</cell><cell cols="2">30.55 11.12</cell><cell>70.98</cell><cell>39.36</cell><cell>59.10</cell></row><row><cell cols="3">D + VGG 30.66 11.93</cell><cell>69.86</cell><cell>39.85</cell><cell>59.78</cell></row></table><note>shows that XMC- GAN provides massive improvements over prior work. Compared to TReCS [22], XMC-GAN improves IS andS W I IS ? FID ? R-prec ? SOA-C ? SOA-I ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Ablation results with different contrastive losses on COCO-14. S indicates the sentence-image loss. W in- dicates the region-word loss. I indicates the image-image loss, where D represents using the discriminator to extract image features, and VGG represents using a pre-trained VGG network to extract image features.FID, by 7.07 and 34.58 (absolute), respectively. It also im- proves R-precision by 23.04% absolute over AttnGAN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different modulation layers.</figDesc><table><row><cell>VGG Loss</cell><cell cols="4">IS ? FID ? R-prec ? SOA-C ? SOA-I ?</cell></row><row><cell>l 2 loss</cell><cell>12.46 52.86</cell><cell>22.62</cell><cell>8.27</cell><cell>25.48</cell></row><row><cell cols="2">Contrastive (InfoNCE) loss 21.54 39.58</cell><cell>35.89</cell><cell>17.41</cell><cell>35.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different VGG losses.</figDesc><table><row><cell>FID on Validation Set</cell><cell>15 20 25 30 35 40 45 50</cell><cell></cell><cell></cell><cell></cell><cell cols="2">No projection Linear MLP</cell><cell>Fake Sentence Contrastive Acc.</cell><cell>0.75 0.80 0.85 0.90 0.95 1.00</cell><cell></cell><cell></cell><cell></cell><cell>No projection Linear MLP</cell></row><row><cell></cell><cell>10</cell><cell>0</cell><cell>200</cell><cell>400 Epochs 600</cell><cell>800</cell><cell>1000</cell><cell></cell><cell>0.70</cell><cell>0</cell><cell>200</cell><cell>400 Epochs 600</cell><cell>800</cell><cell>1000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>this image there are some vehicles on the road and behind the vehicles one big building is there on the right side there are some persons are walking on the street and the background is little bit sunny.</figDesc><table><row><cell>Real Image</cell><cell>Caption #1</cell><cell>Caption #2</cell><cell>Caption #3</cell><cell>Caption #4</cell><cell>Caption #5</cell><cell>LN-COCO</cell></row><row><cell></cell><cell>The bus is pulling off to the side of the road.</cell><cell>A bus pulls over to the curb close to an intersection.</cell><cell>A bus driving in a city area with traf-fic signs.</cell><cell>a public transit bus on a city street</cell><cell>Bus coming down the street from the intersection</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>In the center of the image there</cell></row><row><cell></cell><cell>A group of peo-ple sitting around a table with laptops and notebooks.</cell><cell>Seven seated at table talk-people ing and working on computer devices.</cell><cell>A group of people at a table working on small laptops.</cell><cell>A group of people sitting at a table us-ing computers.</cell><cell>Several friends are visiting at a table with tablets.</cell><cell>is a table and there are people sitting around the table. We can see bottles, laptops and wires placed on the table. In the back-ground there is a man stand-ing. We can see a counter table,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>chairs and lights.</cell></row><row><cell></cell><cell>A group of peo-ple are walking and one is holding an umbrella.</cell><cell>these people are walking together down a road</cell><cell>Three young peo-ple walking behind a large crowd.</cell><cell>Three men who are walking in the sand.</cell><cell></cell><cell></cell></row></table><note>in</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We oversample the images and captions if there are less than 30,000 samples in the validation set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">All ablation results (Fig. 5,Tables 4, 5, and 6) are reported using metrics re-implemented in TensorFlow. SOA is approximated using 30,000 random samples. Ablation models use a reduced base channels dimension of 64. Implementation details are provided in the appendix.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perfect match: Improved cross-modal embeddings for audiovisual synchronisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Whan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Goo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disentangled and controllable face image generation via 3d imitative-contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial networks for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating multiple objects at spatially distinct locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semantic object accuracy for generative text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ContraGAN: Contrastive Learning for Conditional Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Text-to-image generation grounded by fine-grained user attention. WACV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Open Images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual adversarial inference for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicheng</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Pesaranghader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Di</forename><surname>Jorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Infomax-gan: Improved adversarial image generation via information maximization and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Kwot Sin Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CPGAN: Fullspectrum content-parsing generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiadong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Audiovisual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12943,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Alexey Dosovitskiy, and Jeff Clune. Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVGIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimation of entropy and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.15020</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Connecting vision and language with localized narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Geometry-contrastive gan for facial expression transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengchun</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01822</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learn, imagine and create: Text-to-image generation from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving GANs using optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding the limitations of variational mutual information estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantics-enhanced adversarial nets for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hongchen Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">AttnGAN: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stack-GAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Stack-GAN++: Realistic image synthesis with stacked generative adversarial networks. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Photographic text-to-image synthesis with a hierarchically-nested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Image augmentations for GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02595</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dmgan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
