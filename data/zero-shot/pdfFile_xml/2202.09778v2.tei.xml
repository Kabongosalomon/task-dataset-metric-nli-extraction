<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2022 PSEUDO NUMERICAL METHODS FOR DIFFUSION MODELS ON MANIFOLDS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Liu</surname></persName>
							<email>luping.liu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
							<email>linzhijie@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<email>zhaozhou@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2022 PSEUDO NUMERICAL METHODS FOR DIFFUSION MODELS ON MANIFOLDS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce final samples. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Denoising Diffusion Probabilistic Models (DDPMs) <ref type="bibr" target="#b23">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b9">Ho et al., 2020)</ref> is a class of generative models which model the data distribution through an iterative denoising process reversing a multi-step noising process. DDPMs have been applied successfully to a variety of applications, including image generation <ref type="bibr" target="#b9">(Ho et al., 2020;</ref><ref type="bibr" target="#b35">Song et al., 2020b</ref>), text generation <ref type="bibr">(Hoogeboom et al., 2021;</ref><ref type="bibr" target="#b1">Austin et al., 2021)</ref>, 3D point cloud generation <ref type="bibr">(Luo &amp; Hu, 2021</ref>), textto-speech <ref type="bibr" target="#b2">Chen et al., 2020)</ref> and image super-resolution <ref type="bibr" target="#b21">(Saharia et al., 2021)</ref>.</p><p>Unlike Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">(Goodfellow et al., 2014)</ref>, which require careful hyperparameter tuning according to different model structures and datasets, DDPMs can use similar model structures and be trained by a simple denoising objective which makes the models fit the noise in the data. To generate samples, the iterative denoising process starts from white noise and progressively denoises it into the target domain according to the noise predicted by the model at every step. However, a critical drawback of DDPMs is that DDPMs require hundreds to thousands of iterations to produce high-quality samples and need to pass through a network at least once at every step, which makes the generation of a large number of samples extremely slow and infeasible. In contrast, GANs only need one pass through a network.</p><p>There have been many recent works focusing on improving the speed of the denoising process. Some works search for better variance schedules, including <ref type="bibr" target="#b20">Nichol &amp; Dhariwal (2021)</ref> and . Some works focus on changing the inference equation, including <ref type="bibr" target="#b35">Song et al. (2020a)</ref> and <ref type="bibr" target="#b35">Song et al. (2020b)</ref>. Denoising Diffusion Implicit Models (DDIMs)  relying on a non-Markovian process accelerate the denoising process by taking multiple steps every iteration. Probability Flows (PFs) ) build a connection between the denoising process and solving ordinary differential equations and use numerical methods of differential equations to accelerate the denoising process. Additionally, we introduce more related works in Appendix A.1. <ref type="figure">Figure 1</ref>: 5, 10, 20, 50 and 100-steps generated results using DDIMs, classical numerical methods and PNDMs.</p><p>However, this direct connection between DDPMs and numerical methods (e.g., forward Euler method, linear multi-step method and Runge-Kutta method (Timothy, 2017)) has weaknesses in both speed and effect (see Section 3.1). Some numerical methods are straightforward, like the forward Euler method, but they can only trade quality for speed. Some numerical methods can accelerate the reverse process without loss of quality, like the Runge-Kutta method, but they need to propagate forward more times along a neural network at every step. Furthermore, we also notice that numerical methods can introduce noticeable noise at a high speedup rate, which makes high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs. This phenomenon is also mentioned in <ref type="bibr" target="#b22">Salimans &amp; Ho (2022)</ref>.</p><p>To figure out the reason for the performance degradation in classical numerical methods, we conduct some analyses and find that classical numerical methods may sample data far away from the main distribution area of the data, and the inference equations of DDPMs do not satisfy a necessary condition of numerical methods at the last several steps (see Section 3.2).</p><p>To tackle these problems, we design new numerical methods called pseudo numerical methods for diffusion models (PNDMs) to generate samples along a specific manifold in R n , which is the highdensity region of the data. We first compute the corresponding differential equations of diffusion models directly and self-consistently, which builds a theoretical connection between DDPMs and numerical methods. Considering that classical numerical methods cannot guarantee to generate samples on certain manifolds, we provide brand-new numerical methods called pseudo numerical methods based on our theoretical analyses. We also find that DDIMs are simple cases of pseudo numerical methods, which means that we also provide a new way to understand DDIMs better. Furthermore, we find that the pseudo linear multi-step method is the fastest method for diffusion models under similar generated quality.</p><p>Besides, we provide a detailed theoretical analysis of our new theory and give visualization results to support our theory intuitively. According to our experiments, our methods have several advantages:</p><p>? Our methods combine the benefits of DDIMs and high-order numerical methods successfully. We theoretically prove that our new methods PNDMs are second-order convergent while DDIMs are first-order convergent, which makes PNDMs 20x faster without loss of quality on Cifar10 and CelebA. ? Our methods can reduce the best FID of pre-trained models with even shorter sampling time. With only 250 steps, our new denoising process can reduce the best FID by around 0.4 points Cifar10 and CelebA. We achieve a new SOTA FID score of 2.71 on CelebA. ? Our methods work well with different variance schedules, which means that our methods have a good generalization and can be used together with those works introducing better variance schedules to accelerate the denoising process further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section, we introduce some backgrounds. Firstly, we present the classical understanding of DDPMs. Then we provide another understanding based on <ref type="bibr" target="#b35">Song et al. (2020b)</ref>, which inspires us to use numerical methods to accelerate the denoising process of diffusion models. After that, we introduce some background on numerical methods used later in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DENOISING DIFFUSION PROBABILISTIC MODELS</head><p>DDPMs model the data distribution from Gaussian distribution to image distribution through an iterative denoising process. Let x 0 be an image, then the diffusion process is a Markov process and the reverse process has a similar form to the diffusion process, which satisfies:</p><formula xml:id="formula_0">x t+1 ?N ( 1 ? ? t x t , ? t I), t = 0, 1, ? ? ? , N ? 1. x t?1 ?N (? ? (x t , t), ? ? (x t , t)I), t = N, N ? 1, ? ? ? , 1.<label>(1)</label></formula><p>Here, ? t controls the speed of adding noise to the data, calling them the variance schedule. N is the total number of steps of the denoising process. ? ? and ? ? are two neural networks, and ? are their parameters. <ref type="bibr" target="#b9">Ho et al. (2020)</ref> get some statistics estimations of ? ? and ? ? . According to the properties of the conditional Gaussian distribution, we have:</p><formula xml:id="formula_1">q(x t |x 0 ) = N ( ?? t x 0 , (1 ?? t )I), q(x t?1 |x t , x 0 ) = N (? t (x t , x 0 ),? t I).</formula><p>(2)</p><formula xml:id="formula_2">Here, ? t = 1 ? ? t ,? t = t i=1 ? i ,? t = ?? t?1?t 1??t x 0 + ? ?t(1??t?1) 1??t x t and? t = 1??t?1 1??t ? t .</formula><p>Then this paper sets ? ? =? t and designs a objective function to help neural networks to represent ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Function</head><p>The objective function is defined by:</p><formula xml:id="formula_3">L t?1 = E q ||? t (x t , x 0 ) ? ? ? (x t , t)|| 2 = E x0, || 1 ? ? t x t (x 0 , ) ? ? t ? 1 ?? t ? ? ? (x t (x 0 , ), t)|| 2 = E x0, ? 2 t ? t (1 ?? t ) || ? ? ( ?? t x 0 + ? 1 ?? t , t)|| 2 .<label>(3)</label></formula><p>Here, x t (x 0 , ) = ?? t x 0 + ? 1 ?? t , ? N (0, 1), ? is an estimate of the noise . The relationship between ? ? and ? is ? ? = 1</p><formula xml:id="formula_4">? ? t (x t ? ?t ? 1??t ? ).</formula><p>Because ? N (0, 1), we assume that the mean and variance of ? are 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">STOCHASTIC DIFFERENTIAL EQUATION</head><p>According to <ref type="bibr" target="#b35">Song et al. (2020b)</ref>, there is another understanding of DDPMs. The diffusion process can be treated as solving a certain stochastic differential equation dx = ( 1 ? ?(t) ? 1)x(t)dt + ?(t)dw. According to <ref type="bibr" target="#b0">Anderson (1982)</ref>, the denoising process also satisfies a similar stochastic differential equation:</p><formula xml:id="formula_5">dx = ( 1 ? ?(t) ? 1)x(t) ? ?(t) ? (x(t), t) dt + ?(t)dw.<label>(4)</label></formula><p>This is Variance Preserving stochastic differential equations (VP-SDEs). Here, we change the do- <ref type="bibr" target="#b35">Song et al. (2020b)</ref> also show that this equation has an ordinary differential equation (ODE) version with the same marginal probability density as Equation <ref type="formula" target="#formula_5">(4)</ref>:</p><formula xml:id="formula_6">main of t from [1, N ] to [0, 1]. When N tends to infinity, {? i } N i=1 , {x i } N i=1 become continuous functions ?(t) and x(t) on [0, 1].</formula><formula xml:id="formula_7">dx = ( 1 ? ?(t) ? 1)x(t) ? 1 2 ?(t) ? (x(t), t) dt.<label>(5)</label></formula><p>This different denoising equation with no random item and the same diffusion equation together is Probability Flows (PFs). These two denoising equations show us a new possibility that we can use numerical methods to accelerate the reverse process. As far as we know, DDIMs first try to remove this random item, so PFs can also be treated as an acceleration of DDIMs, while VP-SDEs are an acceleration of DDPMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">NUMERICAL METHOD</head><p>Many classical numerical methods can be used to solve ODEs, including the forward Euler method, Runge-Kutta method and linear multi-step method <ref type="bibr" target="#b28">(Timothy, 2017)</ref>.</p><p>Forward Euler Method For a certain differential equation satisfying dx dt = f (x, t). The trivial numerical method is forward Euler method satisfying x t+? = x t + ?f (x t , t).</p><p>Runge-Kutta Method Runge-Kutta method uses more information at every step, so it can achieve higher accuracy 2 . Runge-Kutta method satisfies:</p><formula xml:id="formula_8">? ? ? k 1 = f (x t , t) , k 2 = f (x t + ? 2 k 1 , t + ? 2 ) k 3 = f (x t + ? 2 k 2 , t + ? 2 ) , k 4 = f (x t + ?k 3 , t + ?) x t+? = x t + ? 6 (k 1 + 2k 2 + 2k 3 + k 4 ).<label>(6)</label></formula><p>Linear Multi-Step Method Linear multi-step method is another numerical method and satisfies:</p><formula xml:id="formula_9">x t+? = x t + ? 24 (55f t ? 59f t?? + 37f t?2? ? 9f t?3? ), f t = f (x t , t).<label>(7)</label></formula><p>3 PSEUDO NUMERICAL METHOD FOR DDPM In this section, we first compute the corresponding differential equations of diffusion models to build a direct connection between DDPMs and numerical methods. As a byproduct, we can directly use pre-trained models from DDPMs. After establishing this connection, we provide detailed analyses on the weakness of classical numerical methods. To solve the problems in classical numerical methods, we dive into the structure of numerical methods by dividing their equations into a gradient part and a transfer part and define pseudo numerical methods by introducing nonlinear transfer parts. We find that DDIMs can be regarded as simple pseudo numerical methods. Then, We explore the pros and cons of different numerical methods and choose the linear multi-step method to make numerical methods faster. Finally, we summarize our findings and analyses and safely propose our novel pseudo numerical methods for diffusion models (PNDMs), which combine our proposed transfer part and the gradient part of the linear multi-step method. Furthermore, we analyze the convergence order of pseudo numerical methods to demonstrate the effectiveness of our methods theoretically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FORMULA TRANSFORMATION</head><p>According to <ref type="bibr" target="#b35">Song et al. (2020a)</ref>, the reverse process of DDPMs and DDIMs satisfies:</p><formula xml:id="formula_10">x t?1 = ?? t?1 x t ? ? 1 ?? t ? (x t , t) ?? t + 1 ?? t?1 ? ? 2 t ? (x t , t) + ? t t .<label>(8)</label></formula><p>Here, ? t controls the ratio of random noise. If ? t equals one, Equation <ref type="formula" target="#formula_10">(8)</ref> represents the reverse process of DDPMs; if ? t equals zero, this equation represents the reverse process of DDIMs. And only when ? t equals zero, this equation removes the random item and becomes a discrete form of a certain ODE. Theoretically, the numerical methods that can be used on differential equations with random items are limited. And <ref type="bibr" target="#b35">Song et al. (2020b)</ref> have done enough research in this case. Empirically, <ref type="bibr" target="#b35">Song et al. (2020a)</ref> have shown that DDIMs have a better acceleration effect when the number of total steps is relatively small. Therefore, our work concentrate on the case ? t equals zero.</p><p>To find the corresponding ODE of Equation <ref type="formula" target="#formula_10">(8)</ref>, we replace discrete t ? 1 with a continuous version t ? ? according to  and change this equation into a differential form, namely, subtract x t from both sides of this equation:</p><formula xml:id="formula_11">x t?? ? x t = (? t?? ?? t ) x t ?? t ( ?? t?? + ?? t ) ? ? (x t , t) ?? t ( (1 ?? t?? )? t + (1 ?? t )? t?? )</formula><p>.</p><p>(9) Because ? is a continuous variable from 0 to t, we can now compute the derivative of the generation data x t and get that lim</p><formula xml:id="formula_12">??0 xt?x t?? ? = ?? (t) x(t) 2?(t) ? ? (x(t),t) 2?(t) ? 1??(t)</formula><p>. Here,?(t) is the continuous version of {? i } N i=1 like the definition of x(t). Therefore, the corresponding ODE when ? tends to zero of Equation <ref type="formula">(9)</ref> is:</p><formula xml:id="formula_13">dx dt = ?? (t) x(t) 2?(t) ? ? (x(t), t) 2?(t) 1 ??(t) .<label>(10)</label></formula><p>3.2 CLASSICAL NUMERICAL METHOD After getting the target ODE, the easiest way to solve it is through classical numerical methods. However, We notice that classical numerical methods can introduce noticeable noise at a high speedup rate, making high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs. This phenomenon is also mentioned in <ref type="bibr" target="#b22">Salimans &amp; Ho (2022)</ref>. To make better use of numerical methods, we analyze the differences between Equation <ref type="formula" target="#formula_0">(10)</ref> and usual differential equations and find two main problems when we directly use numerical methods with diffusion models. The first problem is that the neural network ? and Equation (10) are well-defined only in a limited area. Equation <ref type="formula">(2)</ref> shows that the data x t is generated along a curve close to an arc. According to <ref type="figure" target="#fig_0">Figure 2</ref>, most of x t is concentrated in a band with a width of around 0.1, namely the red area in <ref type="figure" target="#fig_0">Figure 2</ref>. This means that the neural network ? cannot get enough examples to fit the noise successfully away from this area. Therefore, ? and Equation <ref type="formula" target="#formula_0">(10)</ref>, which contains ? , are only well-defined in this limited area. However, all classical numerical methods generate results along a straight line instead of an arc. The generation process may generate samples away from the well-defined area and then introduce new errors. In Section 4.3 we will give more visualization results to support this.</p><p>The second problem is that Equation <ref type="formula" target="#formula_0">(10)</ref> is unbounded at most cases. We find that for most linear variance schedules ? t , Equation (10) tends to infinity when t tends to zero (see Appendix A.4), which does not satisfy the condition of numerical methods mentioned in Section 2.3. This is an apparent theoretical weakness that previous works have not explored. On the contrary, in the original DDPMs and DDIMs, the prediction of the sample x t and the noise ? in the data are more and more precise as the index t tends to zero (see Appendix A.5). This means original diffusion models do not make a significant error in the last several steps, whereas using numerical methods on Equation <ref type="formula" target="#formula_0">(10)</ref> does. This explains why DDIMs are better than higher-order numerical methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PSEUDO NUMERICAL METHOD ON MANIFOLD</head><p>The first problem above shows that we should try to solve our problems on certain manifolds. Here, target manifolds are the high-density region of the data x t of DDPMs, which is defined by <ref type="bibr" target="#b6">Ernst &amp; Gerhard (1996)</ref> show several numerical methods to solve differential equations on manifolds that have analytic expressions. Unfortunately, it's challenging to use the above expression of manifolds. Because we do not know the target x 0 in the reverse process and random items are hard to handle, too.</p><formula xml:id="formula_14">x t (x 0 , ) = ?? t x 0 + ? 1 ?? t , ? N (0, 1).</formula><p>In this paper, we design a different way that we make our new equation of denoising process more fits with the equation of original DDIMs to make their results share similar data distribution. Firstly, we divide the classical numerical methods into two parts: gradient and transfer parts. The gradient part determines the gradient at each step, while the transfer part generates the result at the next step. For example, linear multi-step method can be divided into the gradient part f = ? 24 (55f t ?59f t?? + 37f t?2? ? 9f t?3? ) and the transfer part x t+? = x t + ?f . All classical numerical methods have the same linear transfer part, while gradient parts are different.</p><p>We define those numerical methods which use a nonlinear transfer part as pseudo numerical methods. And an expected transfer part should have the property that when the result from the gradient part is precise, then the result of the transfer part is as close to the manifold as possible and the error of this result is as small as possible. We find that Equation <ref type="formula">(9)</ref> satisfies such property.</p><p>Property 3.1 If is the precise noise in x t , then the result of x t?? from Equation <ref type="formula">(9)</ref> is also precise.</p><p>And we put the proof of this property in Appendix A.5. Therefore, we use:</p><formula xml:id="formula_15">?(x t , t , t, t ? ?) = ?? t?? ?? t x t ? (? t?? ?? t ) ?? t ( (1 ?? t?? )? t + (1 ?? t )? t?? ) t<label>(11)</label></formula><p>as the transfer part and ? as the gradient part. That if ? is precise, the result of x t?? is also precise, which means that ? can determine the direction of the denoising process to generate the final results. Therefore, such a choice also satisfies the definition of a gradient part. Now, we have our gradient part ? and transfer part ?.</p><p>This combination solves the two problems mentioned above successfully. Firstly, our new transfer parts do not introduce new errors. This property also means that it keeps the results at the next step on the target manifold because generating samples away is a kind of error. This shows that we solve the first problem. Secondly, we know that the prediction of ? is more and more precise in the reverse process in the above subsection. And our new transfer part can generate precise results according to the precise prediction of ? . Therefore, our generation results are more and more precise using pseudo numerical methods, while classical numerical methods can introduce obvious error at the last several steps. This shows that we solve the second problem, too. We also find that their combination ?(x t , ? (x t , t), t, t ? 1) is just the inference equation used by DDIMs, so DDIMs is a simple case of pseudo numerical methods. Here, we define DDIMs as DDIMs*, emphasizing that it is a pseudo numerical method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">GRADIENT PART</head><p>Because we split numerical methods into two parts, we can use the same gradient part from different classical numerical methods freely (e.g., linear multi-step method), although we change the transfer part of our inference equation. Our theoretical analyses and experiments show that the gradient part from different classical methods can work well with our new transfer part (see Section 3.6, 4.2). By using the same gradient part of the linear multi-step method, we have:</p><formula xml:id="formula_16">? ? ? e t = ? (x t , t) e t = 1 24 (55e t ? 59e t?? + 37e t?2? ? 9e t?3? ) x t+? = ?(x t , e t , t, t + ?).</formula><p>(12) By using the same gradient part of Runge-Kutta method, we have:</p><formula xml:id="formula_17">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? e 1 t = ? (x t , t) x 1 t = ?(x t , e 1 t , t, t + ? 2 ) e 2 t = ? (x 1 t , t + ? 2 ) x 2 t = ?(x t , e 2 t , t, t + ? 2 ) e 3 t = ? (x 2 t , t + ? 2 ) x 3 t = ?(x t , e 3 t , t, t + ?) e 4 t = ? (x 4 t , t + ?) e t = 1 6 (e 1 t + 2e 2 t + 2e 3 t + e 4 t ) x t?? = ?(x t , e t , t, t + ?). (13) Algorithm 1 DDIMs 1: xT ? N (0, I) 2: for t = T ? 1, ? ? ? , 1, 0 do 3: xt = ?(xt+1, ? (xt+1, t + 1), t + 1, t) 4: end for 5: return x0 Algorithm 2 PNDMs 1: xT ? N (0, I) 2: for t = T ? 1, T ? 2, T ? 3 do 3: xt, et = PRK(xt+1, t + 1, t) 4: end for 5: for t = T ? 4, ? ? ? , 1, 0 do 6: xt, et = PLMS(xt+1, {ep}p&gt;t, t + 1, t) 7: end for 8: return x0</formula><p>Abbreviate Equation <ref type="formula" target="#formula_0">(12)</ref> and <ref type="formula" target="#formula_0">(13)</ref> </p><formula xml:id="formula_18">as x t+? , e t = PLMS(x t , {e p } p&lt;t , t, t + ?), x t+? , e 1 t = PRK(x t , t, t + ?).</formula><p>Here, we have provided three kinds of pseudo numerical methods. Although advanced numerical methods can accelerate the denoising process, some may have to compute the gradient part ? more times at every step, like the Runge-Kutta method. Propagating forward four times along a neural network makes the denoising process slower. However, we find that the linear multi-step method can reuse the result of ? four times and only compute ? once at every step. And theoretical analyses tell us that the Runge-Kutta and linear multi-step method have the same convergence order and similar results.  Therefore, we use the gradient part of the linear multi-step method and our new transfer part as our main pseudo numerical methods for diffusion models (PNDMs). In <ref type="table" target="#tab_1">Table 1</ref>, we show the relationship between different numerical methods. Here, we can see PNDMs combine the benefits of higher-order classical numerical methods (in the gradient part) and DDIMs (in the transfer part).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ALGORITHM</head><p>We can provide our whole algorithm of the denoising process of DDIMs now. According to <ref type="bibr" target="#b35">Song et al. (2020a)</ref>, the algorithm of the original method satisfies Algorithm 1. And our new algorithm of PNDMs uses the pseudo linear multi-step and pseudo Runge-Kutta method, which satisfies Algorithm 2. Here, we cannot use linear multi-step initially because the linear multi-step method cannot start automatically, which needs at least three previous steps' information to generate results. So we use the Runge-Kutta method to compute the first three steps' results and then use the linear multi-step method to calculate the remaining.</p><p>We also use the gradient parts of two second-order numerical methods to get another pseudo numerical method. We introduce the details of this method in Appendix A.3. We call it S-PNDMs, because its gradient part uses information from two steps at every step. Similarly, we also call our first PNDMs F-PNDMs, which use data from four steps, when we need to distinguish them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">CONVERGENCE ORDER</head><p>Change the transfer part of numerical methods may introduce unknown error. To determine the influence of our new transfer part theoretically, we compute the local and global error between the theoretical result of Equation <ref type="formula" target="#formula_0">(10)</ref> x(t + ?) and our new methods, we find that</p><formula xml:id="formula_19">x(t + ?) ? x DDIM (x + ?) = O(? 2 ) and x(t + ?) ? x S/F-PNDM (x + ?) = O(? 3 ).<label>(14)</label></formula><p>If the target ODE satisfies Lipschitz condition and local error e local = O(? k ), then there are C and h such that the global error e global satisfies e global ? C? k (1 + e h + e 2h + ? ? ? ) ? C ? k?1 . And we have that the convergence order is equal to the order of the global error. The detailed proof can be found in Appendix A.6. Therefore, we get the following property:</p><p>Property 3.2 S/F-PNDMs have third-order local error and are second-order convergent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SETUP</head><p>We conduct unconditional image generation experiments on four datasets: Cifar10 (32 ? 32) <ref type="bibr" target="#b15">(Krizhevsky et al., 2009)</ref>, CelebA (64 ? 64) <ref type="bibr" target="#b18">(Liu et al., 2015)</ref>, LSUN-church (256 ? 256) and LSUN-bedroom (256 ? 256) <ref type="bibr" target="#b31">(Yu et al., 2016)</ref>. According to the analysis in Section 3.1, we can use pre-trained models from prior works in our experiments. The pre-trained models for Cifar10, LSUNchurch and LSUN-bedroom are taken from <ref type="bibr" target="#b9">Ho et al. (2020)</ref> and the pre-trained model for CelebA is taken from <ref type="bibr" target="#b35">Song et al. (2020a)</ref>. In these models, the number of total steps N is 1000 and the variance schedule is linear variance schedule. And we also use a pre-trained model for Cifar10, which uses a cosine variance schedule from improved denoising diffusion probabilistic models (iDDPMs <ref type="bibr" target="#b20">(Nichol &amp; Dhariwal, 2021)</ref>  method and linear multi-step method). On Cifar10 and CelebA, we first provide the results of previous works DDIMs. Then, we use the same pre-trained models to test numerical methods mentioned in this paper and put the results in Cifar10 / CelebA (linear). We also use models from iDDPMs to test nonlinear variance schedules and put the results in Cifar10 (cosine). <ref type="bibr" target="#b35">Song et al. (2020b)</ref> do not provide detailed FID results of probability flows (PFs) under different steps, so we retest the results using its pretrained models by ourselves.</p><p>Efficiency Our two baselines are DDIM and PF. DDIM is a simple case of pseudo numerical methods, and PF is a case of classical numerical methods. However, PF uses a much bigger model than DDIM and uses some tricks to improve the sample quality. To ensure the experiment's fairness, we use fourth-order numerical methods on Equation (10) and the model from DDIM. In <ref type="table" target="#tab_3">Table 2</ref>, we find that the performance of FON is limited when the number of steps is small. By contrast, our new methods, including S-PNDM and F-PNDM, can improve the generated results regardless of whether the number of steps used is large or small. According to Cifar10 / CelebA (linear), F-PNDM can achieve lower FID than 1000 steps DDIM using only 50 steps, making diffusion models 20x faster without losing quality. We draw a line chart of computation cost with FID according to the results of Cifar10 (linear) above in <ref type="figure" target="#fig_1">Figure 3</ref>. Because F-PNDM uses the pseudo Runge-Kutta method to generate the first three steps, it is slower than other methods at the first several steps. Therefore, S-PNDM can achieve the best FID initially, then F-PNDM becomes the best and the acceleration is significant.</p><p>Quality When the number of steps is relatively big, the results of FON become more and more similar to that of pseudo numerical methods. This is because all the methods are solving Equation <ref type="formula" target="#formula_0">(10)</ref>, and their convergent results should be the same. However, pseudo numerical methods still work better using a large number of steps empirically. F-PNDM can improve the best FID around 0.4 using pretrained models and achieves a new SOTA FID score of 2.71 on CelebA, which shows that our work can not only accelerate diffusion models but also improve the sample quality topline. We also notice that the FID results of F-PNDM converge after more than 250 steps. The FID results will fluctuate around a value then. This phenomenon is more pronounced when we test our methods on LSUN (see <ref type="table" target="#tab_7">Table 5</ref>, 6).</p><p>According to Cifar10 (cosine), the cosine variance schedule can lower FID using a relatively large number of steps. More analyses about variance schedule can be found in Appendix A.7. What's more, we test our methods on other datasets and provide our FID results in Appendix A.9 and image results in Appendix A.10. We can draw similar conclusions on our methods' acceleration and sampling quality, regardless of the datasets and the size of the images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SAMPLE ON MANIFOLDS</head><p>Here, we design visualization experiments to show the effort of our new methods and support our analyses. Because it is hard to visualize high-dimensional data, we use the change of a global characteristic norm and a local characteristic pixel to show the change of the data under different steps. For pixel, we randomly choose two positions p 1 , p 2 . Then for a series of images x T , x T ?k , ? ? ? , x 0 derived from the reverse process, we denote y k t as the value of x t at position p k . Then we draw a polyline (y 1 t , y 2 t ) t=T,??? in R 2 . For norm, we first count the distribution of the norm of the training datasets under different steps and use this to make a heat map as the background. After that, we draw the norm of our generated results using different methods and steps above this heat map.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref>, we can see that the FON may run far away from the high-density area of the data, which explains why FON may introduce noticeable noise. However, PNDM can avoid this problem and appropriately fit the target result. More visualization results supporting our analysis can be found in Appendix A.11. What's more, we design a toy example to test our new methods without the influence of neural networks and get similar conclusions as to the real cases above. We put the detailed results in Appendix A.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this paper, we provided PNDMs, a new numerical method suitable for solving the corresponding ODEs of DDPMs. PNDMs can generate high-quality images using fewer steps without loss of quality successfully. Based on the idea of this work, further improvement can be explored in our future works: 1) find a better variance schedule for PNDMs: although we tested PNDMs on linear variance schedule and cosine variance schedule in this work, there might be another variance schedule more suitable for our proposed numerical methods. 2) Find higher-order convergent pseudo numerical methods: we analyzed the convergence order of S/F-PNDMs, which are both second-order convergent. However, F-PNDMs achieve better FID than S-PNDMs in most cases. We think this is because the result between our transfer part and target ODE has a higher-order error, which limits the convergence order of F-PNDMs. This error from the change of the transfer part is theoretical but does not influence the quality of images according to the property of Equation (11). However, making the transfer part higher-order convergent and finding the influence of such change is still exciting and needs more research. 3) Extend PNDMs to more general applications: when we proved the convergence order of PNDMs, we found that other kinds of transfer parts can keep the convergence order unchanged too, which means that pseudo numerical methods can be used on more general applications, like certain neural ODEs <ref type="bibr" target="#b3">(Chen et al., 2019;</ref><ref type="bibr" target="#b5">Dupont et al., 2019)</ref>.</p><p>improve the results of NCSNs also can be used in DDPMs. Additional, <ref type="bibr" target="#b35">Song et al. (2020b)</ref> combine DDPMs and NCSNs under the framework of neural differential equations <ref type="bibr" target="#b3">(Chen et al., 2019;</ref><ref type="bibr" target="#b5">Dupont et al., 2019)</ref>. Therefore, numerical methods widely used in neural differential equations can also be applied to accelerate DDPMs. Our work successfully combines the advantages of <ref type="bibr" target="#b35">Song et al. (2020a)</ref> and <ref type="bibr" target="#b35">Song et al. (2020b)</ref>. We use a transfer part from DDIMs and use different gradient parts from different numerical methods. Although we and <ref type="bibr" target="#b35">Song et al. (2020b)</ref> solve certain differential equations derived from DDPMs, we use different target different equations and different numerical methods, which get better results.</p><p>The application of DDPMs is not limited to unconditional image generation. Some works apply DDPMs to various types of data successfully, including text-to-speech <ref type="bibr" target="#b2">(Chen et al., 2020;</ref><ref type="bibr" target="#b16">Lam et al., 2021)</ref>, singing voice <ref type="bibr">(Liu et al., 2021)</ref>, 3D Point Cloud <ref type="bibr">(Luo &amp; Hu, 2021</ref>), text generation <ref type="bibr" target="#b1">(Austin et al., 2021)</ref>. Additionally, DDPMs can also be used to generate conditional samples, too .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CONVERGENT ORDER OF METHOD</head><p>We use the forward Euler method and linear multi-step method to show what is the order of a method. Assume that x t is precise and compute the error at x t+? . For forward Euler method, we have:</p><formula xml:id="formula_20">e t,? = x(t + ?) ? x t,? = x(t) + ?f (x(t), t) + ? 2 2 f (c) ? (x(t) + ?f (x t , t)) = ? 2 2 f (c) ? ? 2 2 M<label>(15)</label></formula><p>Here, we assume that f is continuous, so f is bounded in a close area. x(t + ?) is the precise result and x t,? is the numerical result from t to t + ?.</p><p>For linear multi-step method, we have:</p><formula xml:id="formula_21">e t,? =x(t + ?) ? x t,? = x(t) + ? 1! f (x(t), t) + ? 2 2! f (x(t), t) + ? ? ? + ? 4 4! f (3) (x(t), t) + O ? 5 ? x(t) + b 1 ?f (x(t), t) + 4 s=2 b s ? 4 k=0 (?(s ? 1)?) k (k)! f (k?1) (x(t), t) + O ? 5 =O(? 5 )<label>(16)</label></formula><p>Here, {b s } satisfy 4 s=1 b s = 1 and the following equations for j ? {1, ? ? ? , 3}:</p><formula xml:id="formula_22">(?1) j b 2 + (?2) j b 3 + (?3) j b 4 = 1 j + 1 .<label>(17)</label></formula><p>We call the error at x t,? local error, and the error at x t+M ? (M is big enough but finite) global error. Assume the local error of our method has order k+1 and the target ODE satisfies Lipschitz condition, then:</p><formula xml:id="formula_23">x(t + M ?) ? x t,M ? ?|x(t + M ?) ? x t+(M ?1)?,? | + |x t+(M ?1)?,? ? x t,M ? | ?e t+(M ?1)?,? + e L? |x(t + (M ? 1)?) ? x t,(M ?1)? |</formula><p>?e t+(M ?1)?,? + e L? e t+(M ?2)?,? + ? ? ? ?C? k+1 1 + e Lh + ? ? ? + e (i?1)Lh</p><formula xml:id="formula_24">=Ch k+1 e iL? ? 1 e L? ? 1 ? C? k+1 e iL? ? 1 L? =O(? k ).<label>(18)</label></formula><p>Therefore, the global error will be one order lower than the local error. From Equation <ref type="formula" target="#formula_0">(15)</ref>, we can see the forward Euler method has local error O(? 2 ) and global error O(?), so we call it the first-order numerical method. And the linear multi-step method has local error O(? 5 ) and global error O(? 4 ), and we call it the fourth-order numerical method.</p><p>In addition, assuming that a numerical method has kth-order global error, we can compute the convergence speed of this numerical method:</p><formula xml:id="formula_25">lim ??0 x 2? t+T ? x(t + T ) x ? t+T ? x(t + T ) = (2?) k ? k = 2 k .<label>(19)</label></formula><p>Here, x ? t+T is the result at t+T and move ? every step. This shows that the fourth-order method can converge to the exact solution faster than the first-order method when ? ? 0, which means that we can use a bigger iteration interval ? to achieve similar global error and a bigger iteration interval means that we can iterate fewer times to get results with high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PSEUDO SECOND-ORDER METHOD</head><p>We introduce two second-order numerical methods. First is improved Euler method satisfying:</p><formula xml:id="formula_26">? ? ? k 1 = f (x t , t) k 2 = f (x t + ?k 1 , t + ?) x t+? = x t + ? 2 (k 1 + k 2 )<label>(20)</label></formula><p>Second is another linear multi-step method called second-order linear multi-step method satisfying:</p><formula xml:id="formula_27">x t+? = x t + ? 2 (3f t ? f t?? )<label>(21)</label></formula><p>And the corresponding pseudo improved Euler methods satisfying:</p><formula xml:id="formula_28">? ? ? ? ? ? ? ? ? ? ? ? ? e 1 t = ? (x t , t) x 1 t = ?(x t , e 1 t , t, t + ?) e 2 t = ? (x 1 t , t + ?) e t = 1 2 (e 1 t + e 2 t ) x t+? = ?(x t , e t , t, t + ?)<label>(22)</label></formula><p>Pseudo second-order linear multi-step method satisfying:</p><formula xml:id="formula_29">? ? ? e t = ? (x t , t) e t = 1 2 (3e t ? e t?? ) x t+? = ?(x t , e t , t, t + ?)<label>(23)</label></formula><p>Similar to what we do to get F-PNDMs, We combine them to get S-PNDMs. Abbreviate Equation <ref type="formula" target="#formula_28">(22)</ref> and (23) as</p><p>x t+? , e t = P IE(x t , {e p } p&lt;t , t, t + ?),</p><p>x t+? , e 1 t = P LM S (x t , t, t + ?).</p><p>Algorithm 3 S-PNDMs 1: xT ? N (0, I) 2: for t = T ? 1 do 3: xt, et = P IE(xt+1, t + 1, t) 4: end for 5: for t = T ? 2, ? ? ? , 1, 0 do 6:</p><p>xt, et = P LM S (xt+1, {ep}p&gt;t, t +1, t) 7: end for 8: return x0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 THE EXISTENCE OF A DERIVATIVE</head><p>Because? t is usually obtained by multiplying a linear variance schedule ? t . So we hav?</p><formula xml:id="formula_30">? t = e at 2 +bt+c ,<label>(24)</label></formula><formula xml:id="formula_31">and? 0 = 1, so c = 0. Now we have lim ??0 x t?? ? x t ? = lim ??0? t?? ?? t ? x t ?? t ( ?? t?? + ?? t ) ? ? (x t , t) ?? t ( (1 ?? t?? )? t + (1 ?? t )? t?? ) = lim ??0? t?? ?? t ? x t 2? t ? ? (x t , t) 2 ? 1 ?? t?t = (e at 2 +bt ) x t 2? t ? ? (x t , t) 2 ? 1 ?? t?t =(2at + b)? t x t 2? t ? ? (x t , t) 2 ? 1 ?? t?t = 1 2 (2at + b)(x t ? ? (x t , t) ? 1 ?? t ).<label>(25)</label></formula><p>To make lim ??0</p><formula xml:id="formula_32">x t?? ?xt ? | t=0 is well-defined, b must equal to zero, or (2at + b) ? (xt,t) ?</formula><p>1??t will tend to infinity. This is a strong condition that most variance schedules do not satisfy. In practice, DDPMs can choose the variance schedule very freely. This means that treating DDPMs as ODEs directly is not proper and has theoretical weakness.</p><p>A.5 RELATIONSHIP BETWEEN t, ? AND x t Relationship between t and ? . In <ref type="figure" target="#fig_3">Figure 5</ref>, we can see that the denoising process tends to converge, whether in the ? domain or the sample/image domain when the step-index tends to zero. Therefore, we can say that the noise becomes more and more precise when step, namely t, tends to zero. Relationship between ? and x t To prove Property 3.1, assume that x t = ?? t x 0 + ? 1 ?? t , NN is the neural network and ? = NN(x t , t). Because we assume that the gradient part is precise, then we have ? = . Then for all t ? t, we have:</p><formula xml:id="formula_33">x t = ?? t x t ? ? 1 ?? t ? ?? t + ? 1 ?? t ? = ?? t ?? t x 0 + ? 1 ?? t ? ? 1 ?? t ?? t + ? 1 ?? t ? (x t , t) = ?? t x 0 + ? 1 ?? t .<label>(26)</label></formula><p>Here, we can find that x t = ?? t x 0 + ? 1 ?? t is also precise, so Property 3.1 is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 ORDER ANALYSIS OF PSEUDO METHOD</head><p>For the convenience of theoretical analysis, we generalize the problem. Let ?(x(t), , t, ?) = f (x(t), t, ?) + g(t, ?) (x(t), t) and we have the property f (x(t), t, 0) = g(t, 0) = 0. Then we have:</p><formula xml:id="formula_34">x(1) = x(0) + ??0 (y(t + ?) ? y(t)) = x(0) + ??0 (f (x(t), t, ?) + g(t, ?) (x(t), t)) = x(0) + 1 0 ?f ?? (x(t), t, 0) + ?g ?? (t, 0) (x(t), t) .<label>(27)</label></formula><p>Now, Equation <ref type="formula" target="#formula_0">(10)</ref> becomes a special case of this more general version and, in this special case, we have:</p><formula xml:id="formula_35">f (x(t), t, ?) = ?(t + ?) ? ? ? 1 x(t) g(t, ?) = 1 ? ?(t + ?) ? (1 ? ?(t))?(t + ?) ?(t)<label>(28)</label></formula><p>Now, we compute the local error of S-PNDMs. We first compute the theoretical and numerical results of different numerical methods. We have:</p><formula xml:id="formula_36">x(t + ?) =x(t) + ? ?f ?? (x(t), t, 0) + ?g ?? (t, 0) (x(t), t) + ? 2 2 ?f ?? (x(t), t, 0) + ?g ?? (t, 0) (x(t), t) + O(? 3 ) =x(t) + ? ?f ?? (x(t), t, 0) + ?g ?? (t, 0) (x(t), t) + O(? 3 )+ ? 2 2 ? 2 f ???t (x(t), t, 0) + ? 2 f ???x (x(t), t, 0) ?f ?? (x(t), t, 0) + ?g ?? (t, 0) (x(t), t) + ? 2 2 ? 2 g ???t (t, 0) (x(t), t) + ?g ?? (t, 0) (x(t), t)<label>(29)</label></formula><p>and</p><p>x S-PNDM (t + ?)</p><formula xml:id="formula_37">=x(t) + f (x(t), t, ?) + g(t, ?) 1 2 ( (x(t), t) + (x(t) + ?(..., t, ?), t + ?)) =x(t) + ? ?f ?? (x(t), t, 0) + ? 2 2 ? 2 f ?? 2 (x(t), t, 0) + O(? 3 )+ ? ?g ?? (t, 0) + ? 2 2 ? 2 g ?? 2 (t, 0) 1 2 ( (x(t), t) + (x(t + ?) + O(? 2 ), t + ?)) =x(t) + ? ?f ?? (x(t), t, 0) + ? 2 2 ? 2 f ?? 2 (x(t), t, 0) + O(? 3 )+ ? ?g ?? (t, 0) + ? 2 2 ? 2 g ?? 2 (t, 0) 1 2 ( (x(t), t) + (x(t + ?), t + ?)) =x(t) + ? ?f ?? (x(t), t, 0) + ? 2 2 ? 2 f ?? 2 (x(t), t, 0) + O(? 3 )+ ? ?g ?? (t, 0) + ? 2 2 ? 2 g ?? 2 (t, 0) 1 2 ( (x(t), t) + (x(t), t) + ? (x(t), t) ) =x(t) + ? ?f ?? (x(t), t, 0) + ? 2 2 ? 2 f ?? 2 (x(t), t, 0) + O(? 3 )+ ? ?g ?? (t, 0) (x(t), t) + 1 2 ? (x(t), t) + ? 2 2 ? 2 g ?? 2 (t, 0) (x(t), t) =x(t) + ? ?f ?? (x(t), t, 0) + ?g ?? (t, 0) (x(t), t) + O(? 3 )+ ? 2 2 ? 2 f ?? 2 (x(t), t, 0) + ?g ?? (t, 0) (x(t), t) + ? 2 g ?? 2 (t, 0) (x(t), t) .<label>(30)</label></formula><p>Then we compute the difference between the theoretical and numerical results. We have:</p><formula xml:id="formula_38">x(t + ?) ? x S-PNDM (x + ?) = ? 2 2 ( ? 2 f ???t ? ? 2 f ?? 2 )(x(t), t, 0) + ? 2 f ???x (x(t), t, 0) ?f ?? (x(t), t, 0) + ? 2 2 ( ? 2 g ???t ? ? 2 g ?? 2 )(t, 0) + ? 2 f ???x (x(t), t, 0) ?g ?? (t, 0) (x(t), t) + O(? 3 )<label>(31)</label></formula><p>In this special case, we compute the derivatives of some items needed in Equation (31). We have:</p><formula xml:id="formula_39">?g ?? (t, ?) = ? ?? 1 ? ?(t + ?) ? (1 ? ?(t))?(t + ?) ?(t) = ?? (t + ?) 2 1 ? ?((t + ?)) ? 1 ? ?(t)? (t + ?) 2 ?(t)?(t + ?) (32) ?f ?? (x(t), t, ?) = ? ?? ?(t + ?) ? ? ? 1 x(t) = ? (t + ?) 2 ?(t)?(t + ?) x(t) (33) ? 2 f ?? 2 (x(t), t, ?)| ?=0 = ? (t + ?) 2 ?(t)?(t + ?) x(t) + ?? (t + ?) 2 4 ?(t)?(t + ?) 3 x(t)| ?=0<label>(34)</label></formula><formula xml:id="formula_40">? 2 f ???t (x(t), t, 0) = ? (t) 2?(t) x(t) ? ? (t) 2 2?(t) 2 x(t)<label>(35)</label></formula><formula xml:id="formula_41">? 2 f ???x (x(t), t, 0) = ? (t) 2?(t)<label>(36)</label></formula><formula xml:id="formula_42">? 2 g ?? 2 (t, ?)| ?=0 = ?? (t + ?) 2 1 ? ?((t + ?)) ? 1 ? ?(t)? (t + ?) 2 ?(t)?(t + ?) + ?? (t + ?) 2 4 1 ? ?(t + ?) 3 + 1 ? ?(t)? (t + ?) 2 4 ?(t)?(t + ?) 3 | ?=0 = ?? (t) 2 1 ? ?((t)) ? 1 ? ?(t)? (t) 2 ?(t)?(t) + ?? (t) 2 4 1 ? ?(t) 3 + 1 ? ?(t)? (t) 2 4 ?(t)?(t) 3 (37) ? 2 g ???t (t, ?)| ?=0 = ? ?t ?? (t) 2 1 ? ?((t)) ? 1 ? ?(t)? (t) 2 ?(t)?(t) = ?? (t) 2 1 ? ?((t)) ? 1 ? ?(t)? (t) 2 ?(t)?(t) + ?? (t) 2 4 1 ? ?(t) 3 + 1 ? ?(t)? (t) 2 2?(t) 2 + ? (t) 2 4 ? 2 (t)(1 ? ?(t))<label>(38)</label></formula><p>Now we can compute the final result of Equation <ref type="formula" target="#formula_0">(31)</ref>. We split it into three parts and the values of the first two terms. We have:</p><p>(</p><formula xml:id="formula_43">? 2 f ???t ? ? 2 f ?? 2 )(x(t), t, 0) + ? 2 f ???x (x(t), t, 0) ?f ?? (x(t), t, 0) = ? (t) 2?(t) x(t) ? ? (t) 2 2?(t) 2 x(t) ? ? (t) 2?(t) x(t) + ?? (t) 2 4?(t) 2 x(t) + 1 2?(t) ? (t) 2?(t) x(t) =0<label>(39)</label></formula><p>and</p><formula xml:id="formula_44">( ? 2 g ???t ? ? 2 g ?? 2 )(t, 0) + ? 2 f ???x (x(t), t, 0) ?g ?? (t, 0) = 1 ? ?(t)? (t) 2 4?(t) 2 + ? (t) 2 4?(t) 1 ? ?(t) + ? (t) 2?(t) ?? (t) 2 1 ? ?((t)) ? 1 ? ?(t)? (t) 2?(t) = ? (t) 2 4?(t) 2 1 ? ?(t) + ? (t) 2?(t) ?? (t) 2 1 ? ?(t)?(t) =0<label>(40)</label></formula><p>Finally, we get the final result of Equation <ref type="formula" target="#formula_0">(31)</ref>:</p><formula xml:id="formula_45">x(t + ?) ? x S-PNDM (x + ?) = O(? 3 )<label>(41)</label></formula><p>And the computation of the convergence order of F-PNDMs is similar, and we ignore it here. Therefore, Property 3.2 is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 VARIANCE SCHEDULE</head><p>According to Cifar10 (cosine) in <ref type="table" target="#tab_3">Table 2</ref>, PNDMs can be used on both linear variance schedule and cosine variance schedule. However, we also notice cosine variance schedule can make FID lower when we use relatively big generation steps, but the effort is limited when the number of steps is small. F-PNDM uses information from four consecutive steps, so the smoothness of the schedule is more important for F-PNDM than DDIM. According to this experiment, our work can be used with works that pay attention to variance schedules to improve the acceleration effect further. However, a variance schedule that fits pseudo numerical methods better remains to be found in further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 TOY EXAMPLE</head><p>Here, we design a toy example to test our new methods without the influence of neural networks. We randomly generate the initial input x 1 = (m 1 , m 2 ), m i ? U (0, 1) and use a simple analytic equations ? (x) = (sin x[0], cos x[1]) to replace the neural networks in real cases. Let ? in Equation <ref type="formula" target="#formula_0">(11)</ref> is unchanged and? t = ?(t) = 1 ? t, then we get:</p><formula xml:id="formula_46">?(x t , ? (x t ), t, t ? ?) = ?? t?? ?? t x t ? (? t?? ?? t ) ?? t ( (1 ?? t?? )? t + (1 ?? t )? t?? ) t = 1 ? (t ? ?) ? 1 ? t x t ? ? ? 1 ? t (t ? ?)(1 ? t) + t(1 ? (t ? ?)) ? (x t )<label>(42)</label></formula><p>Here, we use three different numerical methods to generate x 0 .</p><p>For DDIM, we have:</p><formula xml:id="formula_47">x t?? = x t + ?(x t , ? (x t ), t, t ? ?)<label>(43)</label></formula><p>For FON, according to Equation (10), we have:</p><formula xml:id="formula_48">e t =? (t) x t 2?(t) ? ? (x t ) 2?(t) 1 ??(t) = ? x t 2(1 ? t) ? ? (x t ) 2(1 ? t) ? t x t?? = x t + ? 24 (55e t ? 59e t+? + 37e t+2? ? 9e t+3? )<label>(44)</label></formula><p>For F-PNDM, we have:</p><formula xml:id="formula_49">e = 1 24 (55 ? (x t ) ? 59 ? (x t+? ) + 37 ? (x t+2? ) ? 9 ? (x t+3? )) x t?? = x t + ?(x t , e , t, t ? ?)<label>(45)</label></formula><p>Then we draw the corresponding generation curves in <ref type="figure" target="#fig_4">Figure 6</ref>. We find that the result is similar to the real cases. The main difference here is that FON can correct its results while the real case cannot. The reason is that the gradient is well-defined everywhere, while in real cases, the gradient is meaningful on the high-density region of the data x t of DDPMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 MORE FID RESULTS</head><p>Here, We provide our more detailed FID results on Cifar10, CelebA, LSUN-church and LSUNbedroom in <ref type="table" target="#tab_5">Table 3</ref>, 4, 5, 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 MORE IMAGE RESULTS</head><p>Here, we show more generated images on Cifar10, CelebA, LSUN-church and LSUN-bedroom in <ref type="figure">Figure 7</ref>, 9, 8, 10, 11, 12.    48.8 18.8 11.7 11.0 10.1 10.0 9.84 9.83 9.85 9.88 S-PNDM 20.5 11.8 9.20 9.13 9.31 9.49 9.82 9.88 10.0 10.0 F-PNDM 14.8 8.69 9.13 9.33 9.69 9.89 10.1 9.99 10.1 10.1 .41 6.27 6.05 5.97 6.03 6.23 6.32 S-PNDM 18.1 10.2 6.50 6.02 5.74 5.81 6.29 6.44 6.69 6.75 F-PNDM 12.6 6.99 5.68 5.74 6.17 6.44 6.91 6.96 7.03 6.92 <ref type="table">Table 6</ref>: LSUN-bedroom image generation measured in FID. All of them use linear variance schedule. <ref type="figure">Figure 7</ref>: 5, 10, 20, 50, 100, 250, 500-steps generated results using DDIMs, classical numerical methods and PNDMs on Cifar10. <ref type="figure">Figure 8</ref>: 5, 10, 20, 50, 100, 250, 500-steps generated results using DDIMs, classical numerical methods and PNDMs on CelebA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21</head><p>Published as a conference paper at ICLR 2022 <ref type="figure">Figure 11</ref>: 5, 10, 20, 50, 100-steps generated results using DDIMs, classical numerical methods and PNDMs on LSUN-church. 22</p><p>Published as a conference paper at ICLR 2022 <ref type="figure" target="#fig_0">Figure 12</ref>: 5, 10, 20, 50, 100-steps generated results using DDIMs, classical numerical methods and PNDMs on LSUN-bedroom. 23</p><p>Published as a conference paper at ICLR 2022</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 MORE VISUALIZATION RESULTS</head><p>Here, we put more visualization results similar to <ref type="figure" target="#fig_2">Figure 4</ref> in <ref type="figure" target="#fig_1">Figure 13</ref>.   <ref type="table">Table 7</ref>: Image generation measured in FID on Cifar10. DDIM* means a kind of pseudo numerical method and also a retest of DDIM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>the density distribution of the norm of the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The FID results under different computation costs and different numerical methods on Cifar10. The unit of time is the computational cost of 1-step DDIM, which is 0.337s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The upper part shows the change of norm with the number of steps using different methods and different steps. The lower part shows the generation curves of two points using different methods and different steps. DDIM-n means n-step DDIM method. Experiments in this subsection all use the Cifar10 dataset and we use the 1000-step DDIM's result as our target result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The norm ? of the difference between two adjacent terms under different steps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The generation curve of our toy example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Generated images of PNDMs on Cifar10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Generated images of PNDMs on CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Visualization results under 5, 10, 20, 25, 40 and 50 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The relationship between different numerical methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Image generation measured in FID on Cifar10 and CelebA. PFs use black box ODE solvers and we use the number of score function evaluations as the step of PFs. DDIM* is a retest of DDIM.</figDesc><table><row><cell>The bold results</cell></row><row><cell>mean the best ones using the</cell></row><row><cell>same pretrained model. We</cell></row><row><cell>use the 50-step, 512 batch</cell></row><row><cell>size experiment on an RTX-</cell></row><row><cell>3090 to test the computa-</cell></row><row><cell>tional cost and the column</cell></row><row><cell>time is the average compu-</cell></row><row><cell>tational cost per step in sec-</cell></row><row><cell>onds. And we put the results</cell></row><row><cell>of standard deviation in Ap-</cell></row><row><cell>pendix A.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Cifar10 image generation measured in FID. The upper part uses linear variance schedule and the bottom half uses cosine variance schedule. The first line shows the FID provided by Song et al. (2020a). PNDM 15.2 12.2 9.45 8.42 6.50 5.69 4.03 3.72 3.30 3.19 3.01 2.99 F-PNDM 11.3 7.71 5.51 4.75 3.67 3.34 2.81 2.75 2.71 2.71 2.77 2.86</figDesc><table><row><cell>steps</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>25</cell><cell>40</cell><cell>50</cell><cell cols="2">100 125 200 250 500 1000</cell></row><row><cell>DDIM</cell><cell></cell><cell>17.3</cell><cell></cell><cell>13.7</cell><cell></cell><cell cols="2">9.17 6.53</cell><cell>3.51</cell></row><row><cell>DDIM*</cell><cell cols="8">24.4 16.9 13.4 12.3 9.99 8.95 6.36 5.74 4.78 4.44 3.75 3.41</cell></row><row><cell>FON</cell><cell cols="8">60.2 16.0 11.6 10.6 8.89 8.13 6.70 6.28 5.45 5.14 4.49 4.17</cell></row><row><cell>S-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>CelebA image generation measured in FID. All of them use linear variance schedule.</figDesc><table><row><cell>steps</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>25</cell><cell>40</cell><cell>50</cell><cell>100 125 200 250</cell></row><row><cell>DDIM</cell><cell></cell><cell cols="2">19.5 12.5</cell><cell></cell><cell></cell><cell cols="2">10.8 10.6</cell></row><row><cell>DDIM*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>LSUN-church image generation measured in FID. All of them use linear variance schedule.</figDesc><table><row><cell>steps</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>25</cell><cell>40</cell><cell>50</cell><cell>100 125 200 250</cell></row><row><cell>DDIM</cell><cell></cell><cell cols="2">17.0 8.89</cell><cell></cell><cell></cell><cell cols="2">6.75 6.62</cell></row><row><cell>DDIM*</cell><cell cols="4">51.3 16.4 8.47 7</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>A.12 FID RESULT WITH STANDARD DEVIATION Here, we report the mean and standard deviation of FID results, tested over four sampling runs. 01?.02 3.75?.04 3.67?.03 3.78?.04</figDesc><table><row><cell></cell><cell>FID step</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dataset</cell><cell></cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>250</cell><cell>1000</cell></row><row><cell></cell><cell>model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DDIM*</cell><cell cols="6">18.50?.06 10.86?.08 6.95?.04 5.49?.06 4.52?.02 4.02?.04</cell></row><row><cell>Cifar10</cell><cell>FON</cell><cell>13.00?.11</cell><cell>7.33?.06</cell><cell cols="4">5.24?.05 4.64?.04 4.12?.03 3.73?.03</cell></row><row><cell>(linear)</cell><cell cols="2">S-PNDM 11.58?.10</cell><cell>7.53?.07</cell><cell cols="4">5.15?.05 4.34?.03 3.93?.02 3.83?.03</cell></row><row><cell></cell><cell>F-PNDM</cell><cell>6.12?.07</cell><cell>5.04?.07</cell><cell>4.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To achieve higher accuracy, more information is not enough. The reason why these methods achieve higher accuracy can be found in Appendix A.2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Qinghai Zhang, Feng Wang, Shuang Hu, Yang Li, Ziyue Jiang and Rongjie Huang from Zhejiang University for their insightful discussions during the course of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/0304-4149(82)90051-5</idno>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07366</idno>
		<title level="m">Neural Ordinary Differential Equations</title>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjune</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno>abs/2108.02938:14</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01681</idno>
	</analytic>
	<monogr>
		<title level="j">Augmented Neural ODEs</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairer</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanner</forename><surname>Gerhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Series in Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="1996" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyank</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeonghun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Byoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01409</idno>
		<title level="m">Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</title>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05527</idno>
		<title level="m">Score Matching Model for Unbounded Data Score</title>
		<imprint>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00132</idno>
		<title level="m">On Fast Sampling of Diffusion Probabilistic Models</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<title level="m">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11514</idno>
		<title level="m">Bilateral Denoising Diffusion Models</title>
		<imprint>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Diffsinger: Singing voice synthesis via shallow diffusion mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02446</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Diffusion Probabilistic Models for 3D Point Cloud Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>abs/2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Denoising Diffusion Implicit Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improved Techniques for Training Score-Based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generative Modeling by Estimating Gradients of the Data Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05600</idno>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Score-Based Generative Modeling through Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Numerical Analysis. Pearson, 3 edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Sauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13469645</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Score-based Generative Modeling in Latent Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05931</idno>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to Efficiently Sample from Diffusion Probabilistic Models. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno>abs/2106.03802</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some works concentrate on improving the quality and the speed of DDPMs and making DDPMs more practical. Song et al. (2020a) introduce new inference equations to accelerate DDPMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DDPMs have been well developed in the last few years</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">choose to find better variance schedules to improve the images quality. Vahdat et al. (2021) combine the advantages of DDPMs and Variational Autoencoders and get better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ping</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">2021) try to solve an existing bottleneck that the inference equations of DDPMs are unbounded in some situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Furthermore</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">find the similarity between DDPMs and noise conditional score networks (NC-SNs (Song &amp; Ermon, 2020b)), which is that they both use a process similar to Langevin dynamics to produce samples. Therefore, some works (Song &amp; Ermon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>that can</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
