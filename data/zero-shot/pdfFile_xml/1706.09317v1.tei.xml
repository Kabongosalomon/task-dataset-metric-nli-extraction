<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alternative Semantic Representations for Zero-Shot Human Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<email>qian.wang@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
							<email>ke.chen@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Alternative Semantic Representations for Zero-Shot Human Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Zero-Shot Learning</term>
					<term>Semantic Representation</term>
					<term>Human Ac- tion Recognition</term>
					<term>Image Deep Representation</term>
					<term>Textual Description Rep- resentation</term>
					<term>Fisher Vector</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A proper semantic representation for encoding side information is key to the success of zero-shot learning. In this paper, we explore two alternative semantic representations especially for zero-shot human action recognition: textual descriptions of human actions and deep features extracted from still images relevant to human actions. Such side information are accessible on Web with little cost, which paves a new way in gaining side information for large-scale zero-shot human action recognition. We investigate different encoding methods to generate semantic representations for human actions from such side information. Based on our zero-shot visual recognition method, we conducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic representations . The results suggest that our proposed text-and image-based semantic representations outperform traditional attributes and word vectors considerably for zero-shot human action recognition. In particular, the image-based semantic representations yield the favourable performance even though the representation is extracted from a small number of images per class.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Zero-Shot Learning (ZSL) aims to recognize instances from new classes which are not seen in the training data. It is a promising alternative to the traditional supervised learning which requires labour-intensive annotation work on all the classes involved. As shown in <ref type="figure">Figure 1</ref>, in ZSL, the knowledge learned from training data is transferred to recognise unseen classes through the side information which can usually be acquired with less effort. Although most existing works in ZSL focus on the development of novel recognition models, the side information for knowledge transfer plays an equally important role in the success of ZSL. The most popular side information used in ZSL literature are attributes and word vectors. Although they have been widely used in ZSL <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, both of them have obvious drawbacks as well, especially for zero-shot human action recognition in video data.  <ref type="figure">Fig. 1</ref>. A schematic diagram of zero-shot learning framework. The work in this paper is highlighted in the dashed box. Human action classes are denoted by coloured markers (blue and black for training and unseen classes respectively) with different shapes. The training data are used to learn the mapping P and training class embedding (blue filled markers in the latent space), then the unseen class embedding (black filled markers in the latent space) is achieved by preserving the semantic distances (red lines). See Section 4.2 for more details of our ZSL method.</p><p>The definition and annotation of attributes for human actions (e.g., the attributes defined for UCF101 <ref type="bibr" target="#b9">[10]</ref> include "bodyparts-visible: face, fullbody, onehand", "body-motion: flipping, walking, diving, bending", etc.) are subjective and labour-intensive. When a large number of human actions are involved, more attributes are needed to distinguish one human action from the other. As a result, attributes based semantic representations are inappropriate for large scale zero-shot human action recognition. On the other hand, as stated in <ref type="bibr" target="#b1">[2]</ref>, using a word vector of the class label to represent a human action is far from adequate to illustrate the rich appearance variations. In addition, the word vectors are learned from textual corpus, thus suffering from the catastrophic semantic gap problem (i.e., the difference of information conveyed by visual media and texts).</p><p>To address the limitations of existing semantic representations for ZSL, we attempt to explore alternative side information towards enhanced zero-shot human action recognition. The essentials of side information for ZSL are twofold. Firstly, it should be achievable for a large number of human actions without much effort. More importantly, the side information should be able to capture the visually discriminative semantics thus benefiting the ZSL by easily bridging the semantic gap. To this end, we employ action relevant images as the side information resources to extract the semantics of human actions. With the aid of search engines, it is effortless to collect a set of action relevant images by using the action name as the key words. Although still images lack of temporal information in human actions, they provide abundant visually discriminative information which can be exploited to extract high-level semantic representations for human actions. On the other hand, we aim to enhance the word vectors by collecting and encoding textual descriptions of human actions. We believe that the contextual information in the action relevant texts (e.g., description articles of human actions from the web) will remove the ambiguity of the semantics in the original action word vectors which are based solely on the action labels.</p><p>To summarise, the contributions of this paper include:  <ref type="bibr" target="#b13">[14]</ref> Attributes, enhanced by learning from visual data <ref type="bibr" target="#b17">Qin et al. (2016)</ref>  <ref type="bibr" target="#b17">[18]</ref> Attributes, enhanced by learning from visual data <ref type="bibr" target="#b7">Fu et al. (2014)</ref>  <ref type="bibr" target="#b7">[8]</ref> Attributes, enhanced by learning from visual data  <ref type="bibr" target="#b4">[5]</ref> WordNet path length, based on WordNet ontology -We propose and implement the idea of using textual descriptions to enhance the word vector representations of human actions in ZSL. -We propose and implement the idea of using action related still images to represent semantics for video based human actions in ZSL. -Experiments are conducted to evaluate the effectiveness of the proposed semantic representations in zero-shot human action recognition, and significant performance improvement has been achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The semantic representation is key for the success of ZSL. Recently, attempts have been made to explore more effective semantic representations for objects/ actions towards improved ZSL performance. In this section, we will review the prevailing semantic representations used in ZSL <ref type="table" target="#tab_1">(Table 1)</ref>, including a variety of extensions of attributes and word vectors, as well as many other less popular approaches proposed in literature. Attributes based semantic representations were firstly proposed for ZSL in <ref type="bibr" target="#b11">[12]</ref>, thereafter, attributes have been employed for ZSL in many works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. A set of binary attributes need to be manually defined to represent the semantic properties of objects. As a result, each object class can be represented by a binary attribute vector in which the value of one and zero indicates the presence and absence of each attribute respectively. Since the attributes are shared by seen and unseen classes, the knowledge transfer is enabled. However, as mentioned above, the definition of attributes require experts with domain knowledge to discriminate different classes, and the attribute annotation for a large number of classes could be subjective and labour-intensive.</p><p>Alternatively, attributes can be mined automatically from visual features by discriminative mid-level feature learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>, but their semantic meanings are unknown, thus inappropriate for direct use in ZSL. To enhance the attributes' discriminative power and semantic meaningfulness, the manually defined attributes and the ones automatically learned from training data are usually combined. However, the data-driven attributes are usually dataset specific and probably fail on a different dataset.</p><p>The other kind of prevailing side information used in ZSL is derived from text resources. One of the most popular semantic representations is word vector (e.g., the ones generated by the word2vec tool <ref type="bibr" target="#b14">[15]</ref>) due to its convenience and effectiveness. A class label can be easily represented with the vector representation of the corresponding word or phrase. However, word vectors are deficient to discriminate different classes from the visual perspective due to the semantic gap, i.e., the gap between visual and semantic information. As a result, word vectors are usually outperformed by attributes in ZSL.</p><p>To alleviate the semantic gap problem, some attempts have been made to enhance the word vectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>. Inoue et al. <ref type="bibr" target="#b8">[9]</ref> aim to adapt the original word vectors to make two visually similar concepts close to each other in the adapted word vector space by representing a concept with a weighted sum of its original word vector and its hypernym (based on WordNet) word vectors. And the weights are learned from visual resources. Alexiou et al. <ref type="bibr" target="#b1">[2]</ref> enrich the word vector representation by mining and considering synonyms of the action class labels from multiple Internet dictionaries. Mukherjee et al. <ref type="bibr" target="#b15">[16]</ref> use Gaussian distribution instead of a single word vector to model the class labels so that the intra-class variability can be expressed properly in the semantic representations. To address the issue of polysemy, Sandouk et al. <ref type="bibr" target="#b19">[20]</ref> learn a specific vector representation for a word together with its context. That is to say, the same word could have different vector representations when it is in different contexts. Inspired by these works, our work further investigates the possible side information and enabling techniques to enhance the word vectors for ZSL.</p><p>Other than attributes and word vectors, other side information has also been investigated for knowledge transfer in ZSL, only if they are able to model the relationships among different classes and relatively easy to obtain. For example, WordNet path length is used to measure the semantic correlations between two concepts in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. The Internet together with search engines provides a natural opportunity to get side information to measure between-class semantic relationships based on hit-count on search results <ref type="bibr" target="#b18">[19]</ref>. Textual descriptions of a class rather than the single class name are employed to represent a class in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. Concept related textual descriptions (e.g., Wikipedia page) can be readily obtained from the Internet and then processed with techniques in natural language processing (NLP). Considering our focus on zero-shot human action recognition based on video data, images from the Internet can be alternative side information to texts which have been a typical choice for zero-shot image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we propose our methods of generating semantic representations for zero-shot human action recognition from text and image resources respectively. Firstly, we use search engines to collect action relevant texts and images as the side information. Some typical examples are shown in <ref type="figure">Fig.2</ref>. Once the side information are collected, we use different encoding approaches to generate the semantic representations for human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Moisturize your lips with some lip balm. Open the lip balm and run the stick across your bottom and upper lip; if it comes in a little jar, use your finger to apply it instead. This will not only help soften your lips and make them smooth, but it will also help the lip liner and lipstick go on more evenly. ? ? Learn about the keyboard. The keyboard of a piano repeats its notes from top to bottom across several octaves. This means that the notes change from low (left side) to high (right side), but don't vary in pitch.</head><p>There are twelve notes a piano can produce: seven white key notes (C, D, E, F, G, A, B) and five black ? ? Bend your knees so your shins rest on the front of the boots and lean forward slightly. The length of the skis will make falling forward unlikely. Leaning back, though tempting when you're feeling out of control, will not normally stop you and will actually make the skis even harder to control. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apply Lipstick</head><p>Playing Piano Skiing <ref type="figure">Fig. 2</ref>. Examples of collected description texts and images of three human actions from UCF101 (i.e., "Apply Lipstick", "Playing Piano" and "Skiing").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text-based Semantic Representation</head><p>Texts Collection Motivated by the fact a class label is insufficient to depict the complex concepts in the human action, we try to collect textual descriptions from the web to represent each human action. Textual descriptions of human actions can be derived from WikiHow, a website teaching people "how to do anything". Inevitably, the description texts for some actions (e.g., "pick", "sit") are not available from WikiHow, for which we turn to alternative sources including Wikipedia and Online dictionary.</p><p>Pre-Processing Once the textual descriptions for all the human actions are collected, we end up with a document for each human action class. We use natural language processing techniques to pre-process the unstructured textual data before encoding them into semantic representations. In the first step, we tokenize the documents to get all the words appearing in the documents. After removing the stop words (i.e., the words carrying little semantic meanings such as "is", "you", "of"), we have a dictionary containing d words.</p><p>Term-Document Matrix (TD) Given the documents and the dictionary containing all the terms/words in the documents, a term-document matrix M is constructed to represent the term frequency in all documents. M ij denotes the frequency of term i in document j, where i = 1, 2, ..., d and j = 1, 2, ..., C, C is the number of documents, i.e., the number of human actions in a specific dataset. Thus the column vectors in M can be used to represent the semantic representations of human actions. We denote this approach as TD in the following sections.</p><p>Average Word Vector (AWV) We aim to enhance the word vectors by incorporating the collected textual information. Taking advantage of the compositional property of word vectors, we can represent a document with the average of all the included word vectors.</p><formula xml:id="formula_0">AW V (j) = 1 n j nj i=1 v i (1)</formula><p>where n j is the number of terms in the j-th document, v i ? R D denotes the word vector of the i-th term in the document, and D is the dimensionality of word vectors.</p><p>Fisher Word Vector (FWV) In contrast to AWV using the mean of all word vectors to represent a document, FWV aims to model the distribution of word vectors in a document. Fisher Vector represents a document (i.e., a set of words) by the gradient of log likelihood with respect to the parameters of a prelearned probabilistic model (i.e., Gaussian Mixture Model) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>. A Gaussian Mixture Model (GMM) is used to fit the distribution of the word vectors involved in all documents, where the parameters ? = {? k , ? k , ? k }, k = 1, ..., K. Let V j = {v 1 , ..., v nj } be a set of word vectors from the j-th human action description document. Then the Fisher Vector of j-th document can be denoted by:</p><formula xml:id="formula_1">F W V (j) = [G V j ?,1 , ..., G V j ?,K , G V j ?,1 , ..., G V j ?,K ],<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">G V j ?,k = 1 ? ? k vi?V j ? k,i ( v i ? ? k ? k ),<label>(3)</label></formula><formula xml:id="formula_3">G V j ?,k = 1 ? 2? k vi?V j ? k,i ( (v i ? ? k ) 2 ? 2 k ? 1),<label>(4)</label></formula><formula xml:id="formula_4">? k,i = exp[? 1 2 (v i ? ? k ) T ? ?1 k (v i ? ? k )] ? K t=1 exp[? 1 2 (v i ? ? t ) T ? ?1 k (v i ? ? t )]</formula><p>.</p><p>The dimension of the Fisher Vector is 2DK, where D and K are the dimensionality of word vectors and the number of components in the GMM respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image-based Semantic Representation</head><p>Human actions are difficult to describe with texts due to the complexity and intra-class variations. Although they lack temporal information, still images can provide abundant information for the understanding of human actions. Compared to the video examples, still images are much easier to collect, annotate and store. Thus we hold the view that still images are a proper kind of side information which can benefit modelling human action relationships with little effort.</p><p>Image Collection Given a human action, we use the label as the key word and search relevant images with search engines. For most human actions we can get a collection of images each of which gives a view of the action. However, for some action names which could have multiple meanings, the additional explaining key words are needed to get reasonable searching results. For example, we use "salsa spin + dancing" and "playing + hula hoop" for the actions "salsa spin" and "hula hoop" respectively. For each human action, we get different numbers of relevant images after removing the ones of poor quality (e.g., irrelevant ones and the ones smaller than 10Kb) from the returned results. The image collection and filtering can be processed automatically without many human interventions 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>We aim to extract useful information from a set of images to represent a human action. Recently, deep convolution neural networks have been used to extract image features carrying high-level conceptual information. By feeding the images into a pre-trained CNN model, the deep image features can be obtained easily. Then each human action is represented with a set of image feature vectors F j = {f 1 , ..., f nj }. In the next two sections, we use two approaches to encode the set of image features into the action-level semantic representation.</p><p>Average Feature Vector (AFV) Similar to Eq.(1), we can use the average of multiple image features as the human action semantic representation.</p><formula xml:id="formula_6">AF V (j) = 1 n j nj i=1 f i<label>(6)</label></formula><p>Fisher Feature Vector (FFV) Similar to the processing applied on word vectors in Section 3.1, we use Fisher Vector to encode a set of image feature vectors relevant to a specific human action.</p><formula xml:id="formula_7">F F V (j) = [G F j ?,1 , ..., G F j ?,K , G F j ?,1 , ..., G F j ?,K ],<label>(7)</label></formula><p>where G F j ?,i and G F j ?,i can be calculated in the same way as Eq.(3-5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use two human action datasets to evaluate the proposed approaches for zero-shot recognition, i.e., UCF101 <ref type="bibr" target="#b21">[22]</ref> and HMDB51 <ref type="bibr" target="#b10">[11]</ref>. UCF101 is a human action recognition dataset collected from YouTube. There are 13,320 real action video clips falling into 101 action categories. In our experiments, we use 5 randomly generated 51/50 (seen/unseen) class-wise data splits. HMDB51 contains 6,766 video clips from 51 human action classes. Similarly, we use 5 randomly generated 26/25 splits in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-Shot Recognition Method</head><p>We employ our recently developed ZSL method, bidirectional latent embedding learning (BiDiLEL) <ref type="bibr" target="#b25">[26]</ref>, as a test bed in our experiments 2 . To make the paper self-contained, we will briefly describe the main idea of BiDiLEL in this section. The method employs a two-stage latent embedding algorithm to learn a latent space in which the semantic gap is bridged and zero-shot recognition can be done (see <ref type="figure">Fig.1</ref>). In bottom-up stage, we learn a projection matrix P by supervised locality preserving projection (SLPP) <ref type="bibr" target="#b3">[4]</ref>, such that the examples close to each other in the original visual space will still be close in the latent space. By exploiting the local structures and labelling information in the training data, the learned latent space preserves the data distribution and is more discriminative. The properties are expected to generalise well for test examples from unseen classes.</p><p>In the top-down embedding, the latent embedding of each seen class can be calculated by averaging the projections of all the training examples from the class and then serve as landmarks guiding the learning of latent embedding of unseen classes. We use the landmarks based Sammon mapping (LSM) <ref type="bibr" target="#b25">[26]</ref> which aims to preserve the inter-class semantic distances (measured in the semantic space). As a result, the semantic distances between seen and unseen classes as well as between any pair of unseen classes will be preserved in the latent space.</p><p>Once the latent embedding of both seen and unseen classes are obtained, we can do the zero-shot learning in the latent space using the nearest neighbour method. Specifically, given a test example, we use projection matrix P to map it into the latent space, where its distances to all the class embedding can be calculated, and it will be assigned to the closest class label. For more details, we refer the readers to <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Video Representation</head><p>C3D was proposed in <ref type="bibr" target="#b23">[24]</ref> for human action recognition. It utilizes 3D Con-vNets to learn spatio-temporal features for video streams. According to <ref type="bibr" target="#b25">[26]</ref>, the C3D video representation outperforms its counterparts in zero-shot human action recognition. We use the model pre-trained on Sports-1M dataset and follow the setting in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> to extract spatio-temporal deep features (i.e., the 4096-dimensional "fc6" activations of the deep neural network) from 16-frame segments. Finally, the visual representation of a video stream is calculated by averaging the features of all the segments from the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>In most existing ZSL works, the evaluations are based on the assumption that test examples are only from unseen classes, which is often referred as to conventional zero-short learning (cZSL). In practice, however, the test examples can be from either training classes or unseen classes. To evaluate ZSL methods in a more practical scenario, the problem of generalised ZSL has been formulated and investigated in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>. In gZSL, given a test example, the label search space consists of both seen and unseen classes. In our experiments, we follow the protocols in <ref type="bibr" target="#b26">[27]</ref> and report both conventional and generalised ZSL (cZSL and gZSL) results using per-class accuracy. In the generalised ZSL scenarios, except the examples from test classes, we also reserve 20% examples from each training class for testing and the rest 80% examples from each training class for training.</p><p>Concretely, we report the recognition accuracy of test examples from unseen classes by setting the search space in the unseen label set U for the cZSL; the accuracy is denoted by A U ?U . For gZSL, we set the search space in the whole label set T = S ? U and report three types of per-class accuracies, i.e., the recognition accuracy of test examples from unseen classes A U ?T , the recognition accuracy of test examples from seen classes A S?T and the harmonic mean,</p><formula xml:id="formula_8">H = 2 * A U ?T * A S?T /(A U ?T + A S?T ).<label>(8)</label></formula><p>The ZSL method employed in our experiments works in the inductive setting (i.e., the test example is processed individually), but can be extended to the transductive setting (i.e., all the test examples are assumed to be available as a collection when doing the recognition) easily by using the structured prediction method <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. The method of structure prediction uses Kmeans to group all the test examples into clusters (the number of clusters is set to be the number of unseen classes) and find a one-to-one map from the clusters to unseen classes. In our experiments, we will report the results of cZSL in both inductive and transductive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we present the designed experiments and the results to evaluate the effectiveness of proposed semantic representations 3 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text-based Representation</head><p>We conduct experiments of zero-shot human action recognition by utilising the proposed text-based semantic representations in Section 3.1, i.e., TD, AWV and FWV. We use the 300-dimensional word vectors pre-trained with word2vec on Google News dataset (about 100 billion words) <ref type="bibr" target="#b3">4</ref> . For FWV, we set the value of K in Eq.(2) to be {1, 2, 3, 4, 5}. The experiments aim to investigate how different text-based semantic representations perform in zero-shot human action recognition. In our experiments, we follow the protocols in <ref type="bibr" target="#b25">[26]</ref> using class-wise cross validation to find the optimal values of hyper-parameters. According to the performance on the validation data, cosine distances are employed to calculate the semantic distances for FWV, and Euclidean distances are employed for AWV.</p><p>We report the results of conventional ZSL in both inductive and transductive settings in <ref type="table" target="#tab_3">Table 2</ref>. With only the textual description sources, the simple encoding method TD can achieve the accuracy of 19.54% and 15.26% respectively on UCF101 and HMDB51, which indicates the textual descriptions collected by search engines are useful for modelling the inter-class relationships. By incorporating the pre-trained word vectors, AWV improves the accuracy to 24.38% and 21.80% respectively on UCF101 and HMDB51. On the other hand, by comparing FWV with different K values, we know that K = 1 gives the best results with an accuracy of 23.76% on UCF101 and 19.57% on HMDB51; however, it is still outperformed by AWV on both datasets regardless of inductive or transductive settings. To conclude, AWV performs the best among different text-based semantic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image-based Representation</head><p>In our experiments, we collect variant numbers of relevant images for different human actions. The average number of relevant images per class is around 200 and 100 for UCF101 and HMDB51 respectively. To extract the image features, we use the GoogLeNet [23] model pre-trained on ImageNet dataset 5 . The activations of top fully connected layer of GoogLeNet of 1024 dimensions are used as the deep image features. We evaluate the image-based semantic representations encoded with different approaches described in Section 3.2, i.e., AFV and FFV. Again, we set the values of K in Eq.(7) to be {1, 2, 3, 4, 5}. We employ the same experiment protocols as those used in the previous experiments (Section 5.1). According to the performance on the validation data, cosine distances are employed to model the semantic distances for FFV, and Euclidean distances are employed for AFV.</p><p>The experimental results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Apparently, K = 1 again gives the best performance of FFV, achieving 40.12% and 25.82% respectively on UCF101 and HMDB51 in the inductive setting, 50.67% and 31.51% respectively on UCF101 and HMDB51 in the transductive setting. Different from the text-based semantic representations, image-based semantic representations FFV encoded by Fisher Vector outperforms the AFV on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Other Semantic Representations</head><p>In this experiment, we compare the proposed semantic representations with other popular ones. From <ref type="table" target="#tab_3">Table 2</ref> and 3, we know that AWV and FFV(K=1) perform the best among the text-and image-based semantic representations respectively. So we consider AWV and FFV(K=1) as the representatives of the proposed text-and image-based semantic representations. As described in Section 4.4, we conduct the experiments in both conventional and generalised ZSL scenarios in our experiments.</p><p>We present the experimental results in <ref type="table" target="#tab_5">Table 4</ref>. Clearly, the proposed two semantic representations (i.e., AWV and FFV(K=1)) outperform word vectors and attributes consistently in terms of the conventional ZSL evaluation metric. On UCF101, the use of textual information enhances the word vectors based solely on the action labels by lifting the accuracy from 19.42% to 24.38%, even higher than that of labour-intensive attributes (21.54%). The image-based semantic representation FFV encoded with Fisher Vector gives the best accuracy of 40.12%, significantly higher than its counterparts. This is attributed to the narrower semantic gap between video representation space and image-based semantic space. The still images contain abundant visually discriminative information which can be further encoded into high-level semantic representations of human actions. On HMDB51, the same conclusions can be drawn. It is noteworthy that AWV is only slightly better than WV for HMDB51 dataset. The reason might be the existence of actions which are difficult to describe with texts in this dataset, such as, "sit", "talk", "turn", "stand", "pick", "catch", and etc. Regarding the generalised ZSL scenario, the proposed AWV and FFV perform better on the test examples from unseen classes (with 5.32% and 16.55% respectively on UCF101, 2.99% and 5.91% respectively on HMDB51), outperforming the attributes and word vectors. We also notice that FFV does not perform the best on test examples from seen classes (i.e., A S?T ), although it is significantly better than others in terms of harmonic mean (H). This is reasonable and practically preferable with the trade-off between recognition accuracy of examples from seen and unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">How Many Images Are Enough?</head><p>In the previous experiments, we use all the collected images to encode the imagebased semantic representations. In this experiment, we investigate how the number of images affects the encoded semantic representations. We use AFV and FFV(K=1) as the encoding methods and generate the semantic representations for each human action with the number of relevant images to be 5, 10, 20, 30, 40, 50, 60, 70, 80, 90 and 100 respectively (For the case when the total number of collected images for one human action is less than the expected number, we simply use all the collected images of that action in the experiment). The experiments are conducted on two human action datasets in conventional ZSL scenario under both inductive and transductive settings.</p><p>The performances of two types of image-based semantic representations with different numbers of images are shown in <ref type="figure" target="#fig_0">Fig.3</ref>. For a direct comparison, we display the baseline performance of attributes and word vectors in the figure as well. Using more images usually benefits the performance of AFV and FFV on both datasets. In specific, we can see a dramatic performance boost with the number of images increased from 5 to 40 per class for UCF101. A further increase of images does not improve the performance significantly, which is especially true in the inductive setting. For HMDB51 dataset, the similar trend of performance improvement can be observed from <ref type="figure" target="#fig_0">Fig.3</ref>, and the performance improvement stops until the number of images per class increases to around 80. In addition, the proposed image-based semantic representations using only 5 images per class can achieve better performance on UCF101 than attributes and word vectors, and the number rises to 20 for HMDB51 to beat word vectors. To summarise, we are able to use a small number of relevant images to encode the semantic representations of human actions, yet boosting the zero-shot human action recognition accuracy to a large extent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We explore the alternative side information to the existing attributes and word vectors towards improved zero-shot human action recognition. The textual descriptions of human actions from the Internet can be used as side information for knowledge transfer in ZSL. In addition, the combination with pre-trained word vectors can further improve the power of text-based semantic representations, even better than the manually annotated attributes. On the other hand, the image-based semantic representations achieve dramatic performance improvement compared with the ones based on other side information (e.g., texts and human annotations), due to the narrower semantic gap. Our experiments also show that a small number of images are enough to gain significant performance improvement.</p><p>There are quite a few directions we can follow in our future work. Firstly, we only use a very simple encoding method (TD) for text-based semantic representations in this paper, which results in an extremely high dimensionality and sparse vector representation per document. It has been chosen in this work as a proof of concept, but could be optimised by using alternative techniques such as latent Dirichlet allocation (LDA), latent semantic indexing (LSI), etc. Besides, in our methods of text-based representation encoding, only the occurrences of different words in a given document are considered, and the word orders which play an important role in text understanding have been ignored. Thus the meaning of sentences containing "not" and "but" would be destroyed. To overcome this limitation, some potential techniques recently developed in NLP (e.g., docu-ment2vec <ref type="bibr" target="#b12">[13]</ref>) would be investigated. Currently, we extract image features with deep CNN models pre-trained on large scale object classification dataset (i.e., ImageNet). Although the pre-trained models have already shown great generalization and transferability to other visual recognition tasks, better performance can be expected by fine-tuning the models with our specific human action image data. We have done some preliminary experiments on the combination of two different types of semantic representations, but only get results no better than the use of image-based semantic representation alone. We do not want to rush to the conclusion that the image-and text-based semantic representations are not complementary before further studying the combination methods in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Effects of number of images on the performance of AFV and FFV(K=1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>A survey on semantic representations in ZSL</figDesc><table><row><cell>Authors and Year</cell><cell>Semantic Representation</cell></row><row><cell>Lampert et al. (2009) [12]</cell><cell>Attributes, annotated manually</cell></row><row><cell cols="2">Sharmanska et al. (2012) [21] Attributes, enhanced by learning from visual data</cell></row><row><cell>Liu et al. (2011)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Results of different text-based semantic representations (mean?standard error of recognition accuracy %) on UCF101 and HMDB51 datasets. (Sem.Rep.-Semantic Representation, Att-Attributes, WV-Word vector) ? 0.69 22.05 ? 1.74 21.53 ? 1.75 24.14 ? 3.43 TD 19.54 ? 0.75 24.29 ? 0.65 15.26 ? 0.57 15.33 ? 1.72 AWV 24.38 ? 1.00 30.60 ? 2.67 21.80 ? 0.87 26.13 ? 1.29 FWV(K=1) 23.76 ? 0.72 28.54 ? 0.70 19.57 ? 1.21 20.41 ? 1.74 FWV(K=2) 23.61 ? 1.08 28.64 ? 1.45 18.80 ? 1.22 20.01 ? 1.74 FWV(K=3) 22.21 ? 0.96 24.33 ? 2.34 17.35 ? 1.93 21.37 ? 3.16 FWV(K=4) 22.11 ? 0.62 28.76 ? 1.03 17.07 ? 1.41 18.80 ? 2.95 FWV(K=5) 21.50 ? 0.67 27.56 ? 2.43 16.95 ? 1.19 17.20 ? 1.92</figDesc><table><row><cell>Sem. Rep.</cell><cell cols="2">UCF101 (51/50)</cell><cell cols="2">HMDB51 (26/25)</cell></row><row><cell></cell><cell cols="4">Inductive Transductive Inductive Transductive</cell></row><row><cell>Random</cell><cell>2.00</cell><cell>2.00</cell><cell>4.00</cell><cell>4.00</cell></row><row><cell>Att</cell><cell cols="2">21.54 ? 0.72 32.00 ? 2.30</cell><cell>-</cell><cell>-</cell></row><row><cell>WV</cell><cell>19.42</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of different image-based semantic representations (mean?standard error of recognition accuracy %) on UCF101 and HMDB51 datasets. ? 0.89 50.48 ? 1.35 25.55 ? 1.66 30.77 ? 3.23 FFV(K=1) 40.12 ? 1.30 50.67 ? 2.45 25.82 ? 1.19 31.51 ? 1.67 FFV(K=2) 38.01 ? 1.58 49.60 ? 1.82 25.50 ? 0.95 28.98 ? 1.94 FFV(K=3) 36.52 ? 1.38 45.48 ? 0.73 24.27 ? 1.10 26.95 ? 3.38 FFV(K=4) 35.31 ? 1.17 44.76 ? 2.40 23.22 ? 1.25 25.26 ? 2.32 FFV(K=5) 34.98 ? 0.68 45.08 ? 1.82 23.09 ? 1.12 23.93 ? 2.06</figDesc><table><row><cell>Sem. Rep.</cell><cell cols="2">UCF101 (51/50)</cell><cell cols="2">HMDB51 (26/25)</cell></row><row><cell></cell><cell cols="4">Inductive Transductive Inductive Transductive</cell></row><row><cell>Random</cell><cell>2.00</cell><cell>2.00</cell><cell>4.00</cell><cell>4.00</cell></row><row><cell>AFV</cell><cell>37.24</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>A comparison of different semantic representations on UCF101 and HMDB51 datasets (mean?standard error)%.</figDesc><table><row><cell cols="2">Dataset Sem. Rep.</cell><cell>cZSL</cell><cell></cell><cell>gZSL</cell></row><row><cell></cell><cell></cell><cell>A U ?U</cell><cell>A U ?T</cell><cell>A S?T</cell><cell>H</cell></row><row><cell></cell><cell>Random</cell><cell>2.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell>WV</cell><cell cols="4">19.42 ? 0.69 4.54 ? 0.64 84.79 ? 0.91 8.59 ? 1.17</cell></row><row><cell>UCF101</cell><cell>Att</cell><cell cols="4">21.54 ? 0.72 2.48 ? 0.62 86.39 ? 1.37 4.78 ? 1.18</cell></row><row><cell></cell><cell>AWV</cell><cell cols="4">24.38 ? 1.00 5.32 ? 1.53 86.43 ? 1.06 9.85 ? 2.66</cell></row><row><cell></cell><cell>FFV</cell><cell cols="4">40.12 ? 1.30 16.55 ? 1.30 82.38 ? 1.17 27.49 ? 1.86</cell></row><row><cell></cell><cell>Random</cell><cell>4.00</cell><cell>2.00</cell><cell>2.00</cell><cell>2.00</cell></row><row><cell>HMDB51</cell><cell>WV AWV</cell><cell cols="4">21.53 ? 1.75 2.64 ? 0.33 58.70 ? 1.40 5.05 ? 0.61 21.80 ? 0.87 2.99 ? 0.35 62.00 ? 2.57 5.69 ? 0.64</cell></row><row><cell></cell><cell>FFV</cell><cell cols="4">25.68 ? 1.07 5.91 ? 0.90 58.57 ? 1.50 10.65 ? 1.48</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The image scraper tool is available: http://staff.cs.manchester.ac.uk/?kechen/ASRHAR/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Like attributes and word vectors, our proposed semantic representations may be directly deployed in all the existing zero-shot human action recognition methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The scripts and data used in our experiments can be available on our project page: http://staff.cs.manchester.ac.uk/?kechen/ASRHAR/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://code.google.com/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.vlfeat.org/matconvnet/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors would like to thank Ubai Sandouk from MLO group at The University of Manchester for personal communication and the anonymous reviewers for their valuable comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-cue zero-shot learning with strong supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring synonyms as context in zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4190" to="4194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised kernel locality preserving projections for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="443" to="449" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring semantic interclass relationships (sir) for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3769" to="3775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Write a classifier: Zero-shot learning using purely textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2584" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning multimodal latent attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptation of word vectors using tree structure for visual semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="277" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<title level="m">Thumos challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3337" to="3344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gaussian visual-linguistic embedding for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond semantic attributes: Discrete latent attributes learning for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1667" to="1671" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What helps whereand why? semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="910" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-label zero-shot learning via concept embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sandouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00282</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Augmented attribute representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="242" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition via bidirectional latent embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="533" to="548" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
