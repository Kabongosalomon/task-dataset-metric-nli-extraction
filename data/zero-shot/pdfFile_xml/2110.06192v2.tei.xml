<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lampe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Jost</roleName><forename type="first">Tobias</forename><surname>Springenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arunkumar</forename><surname>Byravan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimrod</forename><surname>Gileadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Khosid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Fantacci</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Enrique</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhil</forename><surname>Raju</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rae</forename><surname>Jeong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neunert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Laurens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Saliceti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Casarini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Nori</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>sim-to-real</term>
					<term>offline RL</term>
					<term>manipulation</term>
					<term>stacking</term>
					<term>robot learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of robotic stacking with objects of complex geometry. We propose a challenging and diverse set of such objects that was carefully designed to require strategies beyond a simple "pick-and-place" solution. Our method is a reinforcement learning (RL) approach combined with visionbased interactive policy distillation and simulation-to-reality transfer. Our learned policies can efficiently handle multiple object combinations in the real world and exhibit a large variety of stacking skills. In a large experimental study, we investigate what choices matter for learning such general vision-based agents in simulation, and what affects optimal transfer to the real robot. We then leverage data collected by such policies and improve upon them with offline RL. A video and a blog post of our work are provided as supplementary material. 3 <ref type="figure">Figure 2</ref>: Illustration of the deformations applied for each of the 4 major axes of the RGB-Objects parametric family. Each deformation changes the stacking affordance of RGB-objects; for bottom objects the stacking support is illustrated with red-green lines showing where a stack is possible (green).</p><p>Imitation Learning and (Offline) Reinforcement Learning. For training in simulation, and subsequent training of sim-to-real policies, we use techniques from off-policy RL followed by an interactive imitation learning approach. For training in simulation we consider RL with full stateinformation, in order to avoid problems with partial observability. In this setting we use the MPO algorithm [15]; a sample efficient, state-of-the-art, off-policy actor-critic method. Initial training is then followed by our pipeline for obtaining a (general) vision policy via a DAgger <ref type="bibr" target="#b15">[16]</ref> style interactive imitation learning approach to distill the MPO policy. We found this to significantly outperform Behavior Cloning [17] / policy distillation <ref type="bibr" target="#b17">[18]</ref> due to the fact that DAgger-style training provides corrections for mistakes of the vision-based policy. We use Domain Randomization [19,<ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24</ref>] to obtain good sim-to-real transfer in the absence of real-world data; more details on this are given in the supplementary. Finally, when improving policies with offline RL from real data we consider: i) a filtered behavior cloning loss that is equivalent to CRRexp [25] but with data from a single policy source (analogous to work on combined offline and online RL <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>); ii) Behavior Cloning based on only successful trajectories. Both allow for stable offline learning without the problems standard RL algorithms exhibit in this setting <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The RGB-Stacking Benchmark</head><p>Despite stacking being addressed as a robotic learning task in prior work, previous approaches have been limited to a reduced set of objects, typically cubical in shape. We focus on the problem of stacking a variety of objects characterized by different shapes. This is obtained by defining a parametric family of RGB-objects. The design principle is to vary the grasp and stack affordances of these objects for a parallel gripper. Our choices significantly change the difficulty of the stacking task by requiring an agent to exhibit behaviors that go beyond a simple pick-and-place [28] strategy. We do so while keeping the benefits of automated learning and evaluation as in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>. In Appendix A, we analyze the difficulty of the task in various ways. We qualitatively evaluate the grasping affordance based on general principles on force closure and object funnels <ref type="bibr" target="#b29">[30]</ref>, and quantitatively based on the Ferrari-Canny grasp metric <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Similarly, we qualitatively depict the stacking affordance <ref type="figure">(Figure 2)</ref>, which is varied by having bottom objects, whose top flat surfaces differ in area, shape, and orientation. We also quantitatively evaluate both affordances by evaluating the stacking performance of human teleoperators in simulation, and of a carefully scripted agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The RGB-Objects Family</head><p>Our objects are all obtained by applying a deformation to a seed cube, a 2D vertical extrusion of a planar square. We defined 4 major axes of deformation, illustrated in <ref type="figure">Figure 2</ref>, resulting in different shapes. These shapes can be thought of as the vertical extrusion of a 2D shape. The Polygon Axis transforms the planar square into a regular polygon. The Trapezoid Axis morphs the planar square to an isosceles trapezoid. The Parallelogram Axis changes the orientation of the extrusion axis, from vertical to progressively more slanted axes. Lastly, the Rectangle Axis uniformly scales the object along the x, y or z-axis.</p><p>These deformations and their combinations form a parametric family of objects. We obtain the final set of objects by uniformly sampling, for both the major axes and all pairwise combinations, 8 objects between the seed object and a maximally deformed object. <ref type="figure">Figure 1(d)</ref> shows a subset of these in the real world, and Appendix A contains a complete depiction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a plethora of work on applying learning algorithms to solving difficult real robot manipulation problems in a general way with a large and diverse set of objects. However, the focus of existing work has primarily been on tasks like grasping <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, which do not typically require complex inter-object contact dynamics. Manipulation tasks which involve interactions between diverse objects, e.g. construction of simple structures, require skills and control strategies that are more complex than and qualitatively different from simply grasping and placing such objects. As a step in this direction, we study whether, and how, we can tackle stacking of diverse objects in the real world via a learned policy, using only information from RGB cameras and proprioception. Several works have recently considered vision-based real-robot stacking <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. However, the focus has almost exclusively been on stacking cube-like objects (which does not require reasoning about orientation, shape, or stack stability), or on a limited set of known irregular objects with limited diversity <ref type="bibr" target="#b4">[5]</ref>. Additionally, prior work reported low task success rates, pointing towards the difficulty of stacking even cuboids in the real world. In order to study stacking in a principled way, we propose a new benchmark, RGB-Stacking, see <ref type="figure" target="#fig_16">Figure 1</ref>, which consists of a set of 152 procedurally-generated and 3D-printed rigid geometric objects. These were carefully designed to pose different degrees of grasping and stacking difficulty for a parallel gripper on a 4-DoF arm. The benchmark is standardized and we release all relevant information for replicating it as supplementary material. <ref type="bibr" target="#b3">4</ref> What differentiates our work are two primary characteristics: our large variety of objects and our extensive real world evaluations.</p><p>To highlight the challenges in stacking these objects, we instantiated two RGB-stacking tasks (with corresponding versions in simulation and real world) designed to investigate a set of research questions. In the first task, we consider mastering stacking for a set of 5 specific combinations of objects.  <ref type="figure" target="#fig_16">Figure 1</ref>: The RGB-stacking tasks involve three objects colored Red, Green, and Blue to signal to a visionbased agent which one should be stacked on top of which. We present tasks for stacking red objects on top of blue ones, with the green ones as distractors. The variations in shape require successful agents to reason about contact dynamics and object orientation to form stable stacks. The benchmark defines a parametric family of RGB-objects, which allows designating held-out objects that are quantifiably different from the training objects.</p><p>These, shown in <ref type="figure" target="#fig_1">Figure 3</ref>, were chosen in terms of the challenges they present: precision in grasping and stacking, balancing, and using the top object as a tool to flip a bottom object that has a slanted face pointing up. For this task, we investigated whether it is possible to learn a single vision-based policy that can reliably stack all 5 combinations in the real world. For the second task, we considered the challenge of learning general stacking strategies from a large set of training objects and test how they transfer to held-out objects-to assess generalization. In both cases, we find that using an agent trained in simulation is the most efficient way to bootstrap data collection for offline RL.</p><p>Our contributions are as follows. <ref type="bibr">(a)</ref> We present and release a benchmark for stacking that features a diverse set of geometric objects and tens of thousands of possible stacks. (b) We show that it is possible to learn a vision-based policy that can stack multiple combinations of objects and can demonstrate a variety of stacking strategies for non-cuboid objects, emergent from RL training. (c)</p><p>We present the framework we used to obtain our results without the need for human demonstrations. Specifically, we train vision-based agents with domain randomization via an interactive distillation approach-decoupling learning the required stacking skills with RL training from mapping the skills to perception with imitation. We also ablate individual components of this pipeline, requiring more than 54 000 stacking attempts on real robots. (d) Finally, we show that an offline RL step using data gathered on the real robots can boost performance when the data is collected with sim-to-real policies; but doing so using real episodes from a scripted agent was worse than zero-shot sim-to-real.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Robotic Stacking. Several works have recently dealt with vision-based real-robot stacking either by learning a curriculum of the different stages of the task <ref type="bibr" target="#b5">[6]</ref>, by combining human demonstrations and RL <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, or by using some flavor of sim-to-real transfer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. In most prior work, the focus has almost exclusively been on cube stacking. Furrer et al <ref type="bibr" target="#b4">[5]</ref> deals with stacking 6 known stones of different shapes. These stones however all have fairly wide support, high friction, require similar pick-and-place strategies, and only 11 episodes were used for evaluation of the suggested method. In contrast, RGB-stacking provides a general, reproducible, and significantly more difficult benchmark for robotic manipulation. It involves thousands of stacking combinations and our evaluations are significantly more extensive than in prior work.</p><p>Large-Scale Deep RL for Robot Manipulation. Deep learning for robotic manipulation was in part popularized by large-scale data collection for grasping in the real world <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref>], a task that allows for automated evaluation and resetting. However, these efforts considered problems with short time horizons and limited dynamical effects (e.g. position-controlled robots with actions taking up to a second). As a result they often used simple Q-learning from collected data with direct optimization for action selection <ref type="bibr" target="#b2">[3]</ref>. In contrast we consider velocity-based control at a rate of 20 Hz leading to long episodes (400 timesteps). This is a more challenging scenario in which learning with, e.g., QT-Opt <ref type="bibr" target="#b2">[3]</ref> from recorded data alone would exhibit problems due to wrongly-estimated Q-values <ref type="bibr" target="#b11">[12]</ref>. Additionally, large-scale RL-based learning in the real world can be difficult to reproduce as the entire real-world process (which can last months <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>) is a function of the initial conditions. In contrast, our simulation-based training is inherently reproducible. In-hand object manipulation provides an alternative route for a difficult challenge, but automating episodic resets become more challenging as the objects easily fall out of the hand <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RGB-Stacking Tasks</head><p>The RGB-stacking tasks we tackle in this paper involve three objects in a basket in front of the robot. Those are colored Red, Green, and Blue to signal to a vision-based agent which one should be stacked on top of which, and which one is just a distractor. In all our experiments, red is assigned to the top object, blue to the bottom object, and green to the distractor. We note that the role of the latter is not just to distract visually but also to serve as an obstacle during stacking. A successful stack is achieved when the red object is above the blue one and their centroids are vertically aligned within some thresholds. A detailed description can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Skill Mastery on 5 Specific Triplets</head><p>The first task involves 5 RGB-object triplets, with each object in a triplet being from a major axis. The triplets were chosen to have different degrees of difficulty for a stacking agent. The task is to achieve skill mastery on these 5 fixed combinations, shown in <ref type="figure" target="#fig_1">Figure 3</ref>, with a single vision-based agent. We explain the challenges for each triplet in a loosely descending order of difficulty: All the 5 stacks shown here are performed by a single agent trained solely in simulation with domain randomization and transferred to the real robot in a zero-shot fashion. Each triplet poses its own unique challenges to the agent: Triplet 1 requires a precise grasp of the top object; Triplet 2 often requires the top object to be used as a tool to flip the bottom object before stacking; Triplet 3 requires balancing; Triplet 4 requires precision stacking (the object centroids need to align); and the top object of Triplet 5 can easily roll off if not stacked gently.</p><p>Triplet 1. The main challenge is grasping the top object, a grasp that involves the gripper closing on the slanted sides will fail. The bottom object also provides difficulty, as it has a limited stacking surface and can easily roll.</p><p>Triplet 2. In this triplet, the bottom object has sloped sides and may need to be reoriented by using the the top object as a tool before stacking. Triplet 4. An easy triplet, it has rectangular prisms for both red and blue objects; the challenge is primarily to align their centroids, required for a stack to be considered successful.</p><p>Triplet 5. In this triplet, the top object can easily roll off the bottom object as it has a large number (10) of faces and is nearly cylindrical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Skill Generalization</head><p>The second RGB-stacking task we are tackling is a transfer task. For this we designate the axes of all pairwise combined deformations (see Appendix A for details) as the training axes and the ones of single deformation-the major axes above-as the held-out axes. Based on these, we created a training object set, which consists of 103 different shapes, and a held-out set (containing 4 axes of deformation), containing 40 shapes. The 5 fixed triplets chosen for the previous task belong to the held-out axes and final performance is evaluated based on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Stacking a Diverse Set of Objects in the Real World with a General Policy</head><p>Since we have access to a simulator with a reasonably accurate model of our real robot and its cell, our approach is to solve the RGB-stacking tasks in three stages: 1) RL training of an expert in simulation using state information, 2) imitation of such an RL expert with a vision-based policy in simulation with domain randomization for both visual appearance and dynamics, and 3) a policy improvement step using data gathered on the real robots. Explicitly decoupling these steps allows for faster and cheaper iteration by enabling parallel experimentation and tuning for all three stages.</p><p>Notation. We consider the Markov Decision Process (MDP) in which a policy ?(a|s)-a probability distribution over actions a ? A that is conditioned on states s ? S-acts to maximize the discounted sum of rewards, with sparse rewards r(s, a) and discount factor ? ? (0, 1]. We use Q ? (s, a) = E ?? [</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments: Solving RGB-Stacking</head><p>We outline our efforts to solve the two stacking tasks we proposed in Section 3.2. In both tasks we evaluate our agents on 5 fixed object triplets, and we assume access to unlimited simulated data but only to a fixed budget (&lt; 100 000) of real episodes. In Skill Mastery we have access to the fixed object triplets during all stages of training. In Skill Generalization our agents only have access to the training object set; the 5 fixed triplets are now held out both in simulation and the real world.</p><p>In all experiments we use a standardized evaluation protocol: the positions of the objects are randomized at the beginning of each episode, and the agent has 20 seconds at 20 Hz to stack the red object on the blue one. We define task success as having a stack at the end of the episode. This definition was chosen to exclude cases in which a stack is briefly achieved incidentally in the middle of an episode when the red object is above the blue one but subsequently falls off. In simulation, we evaluate on the training object set by running each policy on 5000 random object triplets for 2 episodes each. For the 5 fixed test triplets, we evaluate each policy for 1000 episodes per triplet. In the real world, we only evaluate on the test triplets, as evaluating on a sufficiently large sample of training objects to obtain statistics is impractical. We use one robot per triplet (evaluating for 200 episodes), for a total of 1000 episodes per policy. Mean performance is calculated by averaging the success rate across the triplets. For state-based policies and vision-based policies trained on real data we report the average across 2 training runs (i.e. random seeds), and for the vision-based policies distilled from a teacher we report the average across 4 runs: 2 distillation runs for each of the 2 teacher seeds. Results for all runs individually are in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on the Simulation Stacking Benchmark</head><p>We first evaluate several baselines on our simulated benchmark task. We are interested in the following questions: (a) How difficult is the task in simulation? (b) Would it be preferable to train directly from vision? (c) How does interactive imitation and the design choices for it affect performance compared to the state-based teacher?</p><p>To assess the task difficulty, we tuned a scripted agent with access to the object positions and evaluated it on both our training set and our test triplets. We also hired 4 individuals with no relation to the research team to attempt the sim task via teleoperation and a game pad for 846 episodes. For details on these indicative baselines, see Appendix C. These demonstrate the difficulty of the task as in both cases the success was lower than 50% (see <ref type="table">Table 1</ref>).</p><p>Next, we evaluate our benchmark tasks in the dense reward setting with MPO from state. A first finding was that the state agents, which are given the full pose of each of the three objects, learn the task 10 times faster than the vision agent; as illustrated in Appendix D, using a vision agent to train from scratch on the robot would be impractical for these tasks. The MPO state-based agents obtain 79.3% on the test triples in the skill mastery setting and 68.8% on the training objects in the skill generalization setting, significantly outperforming the scripted agent. However, generalization from training to test objects is only slightly higher than the scripted agent's performance (47.8%). This is because these state-based agents are conditioned on object parameters, which are out of distribution for the test triplets. After obtaining vision agents via IIL-s2r we observe ( <ref type="table">Table 1</ref>, bottom) that although the training set performance drops, performance improves for the test triplets, as the agents can now generalize from visual cues to make inferences about the shape.</p><p>We next compare the interactive imitation learning setting (IIL) to learning from teacher trajectories with a behavior cloning loss (BC). Rather than explicitly constructing a fixed size dataset, we generate the data on-the-fly by executing the teacher continuously during training. As shown in <ref type="table">Table 1</ref>, learning from data generated by the student (interactive imitation) is crucial: IIL performed significantly better than the alternative in both task settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on the Real-World Stacking Benchmark</head><p>We then investigate the following questions related to RGB-stacking in the real world: (a) Is it possible to solve the challenging tasks on real robots with a single vision policy, and is that better than a scripted baseline tuned for the test triplets? (b) How well does zero-shot transfer of simtrained agents solve the tasks, and which of our decisions were important for zero-shot performance? (c) Do we get a single-step improvement with BC-IMP and CRR-IMP from using data collected in the real world? (d) And how does the data distribution affect the performance of these algorithms?  <ref type="table">Table 2</ref>: Sim-to-Real Transfer Success. Ablations of the components of the sim-to-real policy. The simulation and real-world evaluations use 20 000 and 4000 episodes for the triplet average, respectively. The choice of using binary gripper actions for the policy seemed to be the most important factor for sim-to-real transfer success, followed by the choice of executing the deterministic policy on the robots.</p><p>As described in Section 4.2, we distill a vision-based policy from a single or multiple state-based experts in simulation. These are 5 specialists for the Skill Mastery task and a single generalist for the Skill Generalization task. We can then directly execute this distilled vision policy on the real robots. As discussed previously, this decoupling allowed us to quickly iterate and investigate what aids sim-to-real transfer for these tasks. We compare, in <ref type="table">Table 2</ref>, the simulation performance and the zero-shot sim-to-real transfer performance when ablating these various choices for our method. Policies were trained up to 1 million learner steps, and the sim-to-real policy was selected to be the one with the highest performance in the training setting for each task. As our policies are stochastic, for inference we have the option to execute a random sample from the action distribution or its mode (i.e. the action with highest probability). We execute the stochastic policy in simulation and the deterministic mode of the policy on the real robots, unless otherwise specified. We are able to achieve high performance of 67.9% on the Skill Mastery task and 51.9% on the Skill Generalization task in the real world. Qualitatively, we also see in <ref type="figure" target="#fig_1">Figure 3</ref> that we can, with a single vision-based agent, show a diverse set of skills to address the challenges needed to solve the task for the 5 specific triplets we have chosen. Videos of the behavior are in the supplementary.</p><p>Ablations. We ablate different algorithmic choices in <ref type="table">Table 2</ref> (extended table in the supplementary). One of our decisions was on the BC loss for distillation, which can either be the negative log-likelihood or the mean squared error (MSE), which respectively correspond to ? log ? vis ? (a e |o(s t )) or E a?? vis ? [||a ? a e || 2 ], where a e are actions from the teacher (the two are not equivalent even for the Gaussian case as the policy variance is state dependent). We also decided to use a hybrid action space, where the 3D Cartesian and angular velocities are modeled as continuous actions (Gaussian), but we have a binary gripper action (Bernoulli). This applies to both statebased and vision-based policies. This choice seemed to be the most important factor for sim-to-real transfer success. However, note that the effects are only visible when transferring (corresponding simulation performance does not change as much). For the Skill Generalization task, the state-based expert is conditioned on the object parameters y. However, the object parameters are not passed to ? vis ? -the agent needs to infer physical object properties from image observations. We also chose to use a Transformer model <ref type="bibr" target="#b32">[33]</ref> after the ResNet vision encoder for the policy's network architecture, which gives the agent access to temporal information with an attention mechanism. The Transformer does not have a significant effect on Skill Mastery. However, it seems to be able to utilize the skill variety of a state-based expert conditioned on the object parameters a more successfully, when compared to an MLP model with the same number of weights. This is clear both in simulation and the real world (and we hence used it in all experiments). Random action execution delays of 0, 1 or 2 timesteps, and standard image augmentation (random color perturbations and image translations) did not seem to affect transfer but we decided to include them in our training procedure to increase our agents' robustness to natural perturbations.  <ref type="table">Table 3</ref>: Real-Robot Success. Different approaches for solving our RGB-stacking tasks in the real world. The kind of data used matters significantly for BC and CRR: using scripted agent data does not result in improved performance, but using data from a sim-to-real agent does. Note that the sim-to-real data used were collected by a single agent per task. For Skill Generalization, as data collection with multiple triplets from the training set was exceptionally time-consuming, we used a suboptimal agent trained earlier in our investigation. This agent was not trained with the best settings we now have for sim-to-real. See text for more details.</p><p>Our sim-to-real approach also outperforms a scripted agent tuned on the test triplets, as shown on <ref type="table">Table 3</ref>. For the Skill Mastery task, we further improve results via offline RL with BC (BC-IMP) on 32 651 successful real episodes collected by the scripted agent and with CRR (CRR-IMP) on a total of 67 446 episodes, which included the unsuccessful ones collected by the same agent. Hyperparameters and architecture choices were selected after such an investigation in simulation. As evident in the table, learning from scripted agent data is not better than sim-to-real. Qualitatively, the policies learned seem to exhibit the same "robotic" movements as the scripted policy that generated the data they were trained on. However, a single policy improvement step based on data collected by a sim-to-real agent with an average success of 69.6%: 85 213 episodes (58 979 successful) yields 74.6% (BC-IMP) and 81.6% (CRR-IMP), resulting in policies with remarkable stacking consistency.</p><p>As data collection for the training object set is particularly time-consuming, we only did a single collection (38 446 episodes, 37.4% of which were successful) with an earlier iteration of a simto-real agent for the Skill Generalization task (performance of 32.6% when evaluated on the test triplets). As before we trained BC-IMP and CRR-IMP on this data and again observed improved performance on the test triplets, this time using only the episodes collected from the training object set. Note that even though the suboptimal sim-to-real agent trained on the training set of objects was performing worse than the scripted agent, and the data in this case was more limited and of different objects, BC-IMP and CRR-IMP trained on this more diverse data lead to a significant improvement. Finally, <ref type="table">Table 3</ref> showcases the variation in difficulty of the 5 triplets: all methods perform best on triplets 4 and 5, as these can sometimes be solved with a stereotypical "pick and place" behavior. In contrast, triplets 1, 2, and 3 each require specialized, advanced, strategies as indicated by the large gap between sim-to-real performance and scripted performance in the Skill Mastery task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We studied the problem of vision-based stacking with a large variety of geometric objects that require a diverse set of stacking strategies. We propose a benchmark for studying this problem in a principled way, and make significant progress in solving two tasks involving both skill mastery on specific objects and generalization across them. We use simulation-trained agents to collect data in the real world, which in turn can be used for further performance improvement with offline RL. Our best agent is a single vision-based policy that is capable of a variety of stacking strategies and achieves high performance. Finally, we provide thorough real-world evaluations and discuss what is important for solving our tasks. Despite this success, our benchmark still poses many open challenges as, for example, shown by the gap between Skill Mastery and Skill Generalization results and we hope that it can help development of new methods for learning general policies (e.g. by further adaptation at robot deployment time).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The RGB-Stacking Benchmark</head><p>In this section, we provide more details on our proposed benchmark and analyze its difficulty in various ways. We qualitatively evaluate the grasping affordance based on general principles on force closure and object funnels <ref type="bibr" target="#b29">[30]</ref>, and quantitatively based on the Ferrari-Canny grasp metric <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Similarly, we qualitatively depict the stacking affordance ( <ref type="figure">Figure S5</ref>), which is varied by having bottom objects, whose top flat surfaces differ in area, shape, and orientation. We also quantitatively evaluate both affordances by evaluating the stacking performance of human teleoperators in simulation, and of a carefully scripted agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The RGB-objects family</head><p>The RGB-objects are all obtained by applying a certain deformation to a cube, which is our seed object. We have defined four major axes of deformation (see <ref type="figure">Figure 2</ref>) of the seed object, which result in different shapes. These shapes can also be thought of as the vertical extrusion of a 2D shape, which is also the name of each axis. As already described in the main paper but reproduced here for completion, these are:</p><p>? Polygon-Axis [N]: deformation obtained by transforming the extruded planar shape (i.e. the square) into a regular polygon.</p><p>? Trapezoid-Axis [R + ]: deformation obtained by progressively morphing the planar square to a isosceles trapezoid.</p><p>? Parallelogram-Axis [R + ]: deformation obtained by changing the orientation of the extrusion axis, from vertical (i.e. orthogonal to the plane of the planar shape) to progressively more slanted axes.</p><p>? Rectangle-Axis [R 3 + ]: deformation by uniformly scaling the object along the x, y or z-axis.</p><p>These deformations and their combinations define a parametric family of objects. <ref type="figure" target="#fig_4">Figure S1</ref> shows a representative sampling of this family. For our Skill Generalization task we designate the axes of all pairwise combined deformations as the training axes and the ones of single deformationthe major axes above-as the held-out axes. . Also note that the x-, y-, z-Rectangle axes are the same so we refer to these as a single major Rectangle axis.</p><p>Based on these 15 axes illustrated in <ref type="figure" target="#fig_4">Figure S1</ref>, we created a training object set, which consists of 103 different shapes, and a held-out set, containing 40 shapes. <ref type="figure">Figure S2</ref> shows a depiction of all the objects that are included in the benchmark and were used in our experiments. The 5 specific triplets chosen for the Skill Mastery task belong to the held-out axes with each object in a triplet being the seed or from a different axis <ref type="bibr" target="#b4">5</ref> . While final performance is evaluated on these 5 fixed test triplets for both tasks, during training for Skill Generalization we hold out not just these objects but the entire 4 axes of deformation they belong to. That is, during training we can use the 103 objects from the training object set, while performance is evaluated still on the 5 specific triplet combinations from the Skill Mastery task. <ref type="bibr" target="#b4">5</ref> Technically and for legacy reasons, although the objects for the 5 triplets are sampled from the held-out axes, not all of them are actually depicted in the held-out object set illustrated in <ref type="figure">Figure S2</ref>. As seen in their figure in the main text, 3 are the seed cube object (itself held out), and 4 out of the 15 are actually in the heldout object set. These are the 4 top objects that are not the seed cube. The rest 8 objects are almost identical to existing objects in the held-out object set shown in <ref type="figure">Figure S2</ref>. In the released set of object models, these will be in their own sub-directory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polygon</head><p>Trapezoid Parallelogram <ref type="figure">Figure S2</ref>: The RGB-objects that are included in the benchmark grouped according to each of the 15 chosen axes of deformation. The seed object is at the center; all the other objects are the result of deformations of this cube. These deformations change the grasping and stacking affordances of the objects. The held-out objects (major axes) are enclosed in the teal sector; the training objects (pairwise mixing of two major axes) are enclosed in the blue sector. Some objects cannot be grasped with a parallel gripper with 85 mm aperture (i.e. the Robotiq 2F-85); these objects are transparent and were omitted in our experiments.</p><formula xml:id="formula_0">x-, y-, z-Rectangle Polygon &amp; Trapezoid Polygon &amp; Parallelogram Polygon &amp; x-Rectangle Trapezoid &amp; Parallelogram Trapezoid &amp; x-Rectangle Trapezoid &amp; y-Rectangle Trapezoid &amp; z-Rectangle Parallelogram &amp; x-Rectangle Parallelogram &amp; y-Rectangle Parallelogram &amp; z-Rectangle x-Rectangle &amp; y-Rectangle</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rectangle Axis object funnel</head><p>Trapezoid Axis object funnel </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Benchmark Analysis</head><p>The design principles outlined above aim at varying the resulting objects' grasp and stack affordance for a parallel gripper. In this section we qualitatively and quantitatively discuss these variations. The qualitative evaluations follow from some general principles on force closure 6 and object's funnel (see Mason <ref type="bibr" target="#b29">[30]</ref> for a definition). <ref type="figure" target="#fig_1">Figure S3</ref> shows the graphical notation which we will use to qualitatively visualize the grasp affordance with respect to the gripper-object relative pose (left) and rotation (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Axes of Deformation</head><p>Seed Polygon Trapezoid Parallelogram Rectangl? Q = 0.132Q = 0.110Q = 0.041Q = 0.128Q = 0.045 <ref type="figure">Figure S4</ref>: A sketch of the design principles adopted to vary RGB-objects' grasp affordance as a function of the applied deformation. Top row: keeping in mind object funnels for the parallel gripper, different objects tolerate different 4-DoF displacements (3D Cartesian and vertical rotation) with respect to the grasping pose visualized in picture; in green: displacements leading to successful grasps, in red: displacements leading to unsuccessful grasps. Bottom row: a visualization of the grasp metric used in Mahler et al. <ref type="bibr" target="#b31">[32]</ref>, Wang et al. <ref type="bibr" target="#b33">[34]</ref>. Each grasp is visualized as a line segment penetrating the object at the grasping locations; the color of the segment corresponds to the corresponding value of the Ferrari-Canny <ref type="bibr" target="#b30">[31]</ref> epsilon grasp metric (robust grasps in green, weak grasps in red). The displayed valueQ is the average of the epsilon metric for all visualized grasps. <ref type="figure">Figure S4</ref> shows this qualitative visualization for some representative RGB-objects: the seed object, and 4 maximally deformed objects of the 4 major axes. The visualization aims at showing that the seed cube is relatively easy to grasp and forgiving of significant errors in the gripper positioning. The Polygon axis requires a precise positioning but it's relatively robust towards errors in gripper orientation. The Trapezoid axis offers two non-parallel faces which require accuracy in both positioning and orientation. The Parallelogram axis is designed to offer the same grasp affordance of a cube but quite different stack affordance. Finally, the Rectangle axis elongates one dimension above the gripper maximum aperture thus preventing some grasps at given orientations. Interestingly, these qualitative considerations are supported by a quantitative metric, based on the Ferrari-Canny <ref type="bibr" target="#b30">[31]</ref> epsilon grasp metric. This evaluation is shown for each of the objects considered, using the code provided in Mahler et al. <ref type="bibr" target="#b31">[32]</ref>, at the bottom row of <ref type="figure">Figure S4</ref>. A highQ, used to symbolize the average metric for all grasps sampled on the object, signifies that the object evaluated is easy to grasp. A lowQ, as is the case e.g. with the Trapezoid, means that the object is harder for grasping.</p><p>Qualitative considerations similar to the ones done above for the grasp-affordance hold for the stackaffordance, as shown in <ref type="figure">Figure S5</ref>. In this case, the affordance is varied by having bottom objects the top flat surfaces of which differ in area, shape and orientation. The Polygon axis introduces a deformation which reduces the top surface and increases the likelihood of the object to roll; the Trapezoid axis just reduces the top surface and in some configuration doesn't afford a grasp at all requiring a re-orientation to another configuration which affords a stack; the Parallelogram axis offers a stacking surface which has an off-set with respect to the center of mass and therefore requires the top object to be carefully placed to avoid objects from tipping over; the Rectangle axis offers an augmented stacking surface along one axis and a reduced one on another axis.</p><p>We have discussed how the shapes of objects influence their grasp and stack affordances. Other interesting affordances are needed to effectively solve these tasks: (1) the clutter affordance and <ref type="formula">(2)</ref> task-oriented affordance. Clutter influences affordance since only a subset of grasps is feasible in presence of obstacles (right panel in <ref type="figure" target="#fig_6">Figure S6</ref>). Additionally, for some objects valid grasps are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Axes of Deformation</head><p>Seed Polygon Trapezoid Parallelogram Rectangle <ref type="figure">Figure S5</ref>: A sketch of the design principles adopted to vary RGB-objects' stacking affordance. Bottom objects offer support of different shapes. From left to right: curved support, small support, slanted support and wide support. not suitable for stacking and this requires our agent to perform a task-oriented grasp (left panel in <ref type="figure" target="#fig_6">Figure S6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Publicly Released Objects</head><p>Instead of standardizing the objects purchase as e.g. in ? alli et al. <ref type="bibr" target="#b35">[36]</ref>, we standardize their manufacturing procedure. We will be releasing the RGB-Objects described above and used in our experiments, depicted in their entirety in <ref type="figure">Figure S2</ref>. We decided to choose the objects to be uniform color and eventually chose only three colors: red (top objects), green (distractor objects) and blue (bottom objects). This facilitates manufacturing since each object can be manufactured with a standard 3D printer, a single filament and no additional assembly steps. Additionally, we can provide, to interested researchers, instructions on how to have such objects 3D-printed by an external vendor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Environment Details</head><p>In the following, we are going to describe the components of the robot setup that was used to conduct experiments, as well as technical considerations such as the procedure for automated evaluations, specifics of the actions and observations exposed to the agent, and reward computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Real-World Environment</head><p>The environment in the real world consists of a robot arm with a gripper, a set of sensors and a basket ( <ref type="figure" target="#fig_7">Figure S7</ref>), chosen both for their durability to allow continuous and autonomous operation and for safe interaction with human operators.  Robot arm: assuming that exploration and manipulation require a certain level of physical interaction, we have chosen a robot capable of sensing, controlling and enduring the forces exchanged with the surroundings and the manipulated objects. We eventually selected the Sawyer from Rethink Robotics 7 , both for its force and torque sensing capabilities and its use of series elastic actuators that provide passive compliance.</p><p>Gripper: the gripper chosen is a Robotiq 2F-85 which guarantees industrial robustness while allowing additional interaction through a passive-spring retracting degree of freedom. It is outfitted with custom fingertips printed from nylon, as the stock fingertips' rubber coating tends to wear off onto the objects and interfere with tracking.</p><p>Sensors: besides the torque and position sensing offered by the Sawyer, we equipped the robot with a Robotiq FT 300 force-torque sensor at the wrist. Additional perception is guaranteed by surrounding the robot with three Basler ace RGB cameras which give a complete view over the robot playground ( <ref type="figure" target="#fig_8">Figure S8</ref>).</p><p>Playground: the basket in front of the robot, also referred to as "playground", is a laser-cut basket with slanted sides to delimit the robot's working area and to help with objects confinement. It has a 25 cm ? 25 cm bottom surface; the robot is constrained to moving its TCP inside a 20 cm-high virtual cube on top of this surface to ensure safe operation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Control Actions</head><p>While the robot's 7 DoFs are natively controlled in joint space, we implement a Cartesian controller to reduce the action space. We restrict the gripper to be oriented vertically, thus allowing only 4-DoF motions (3D Cartesian and 1D rotation). This restriction is used in a number of prior works studying vision-based manipulation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37]</ref>. A control action is fully specified by a 3D Cartesian velocity v x , v y , v z , and an angular velocity ? z around a vertical axis parallel to gravity. We use a P-controller to compute the horizontal angular velocity components ? x and ? y that keep the gripper oriented vertically, and combine it with the agent's actions to create the command for our Cartesian 6D velocity controller. Finally, a directional velocity action for the grippers single degree of freedom is added, yielding the actions summarized in <ref type="table" target="#tab_5">Table S1</ref>.</p><p>At every environment step, after choosing an action, we then solve a constrained least-squares problem to compute the joint velocities that best realize a target Cartesian 6D velocity of the TCP <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. The null-space is controlled to bias the robot's joint positions towards a nominal configuration in the center of the joint limits. Constraints are specified to prevent the robot from violating the joint position and velocity limits, as well as avoiding collisions between the robot and the playground <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. The constrained least-squares problem is solved using an off-the-shelf QP solver <ref type="bibr" target="#b41">[42]</ref>, and the computed joint velocities are forwarded to the robot's proprietary joint velocity controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Environment Observations</head><p>The robot, sensors and cameras provide various readings, which are collected at a fixed rate of 20 Hz and merged into a single observation. Not all possible observation elements are used in all stages of the system; most notably, the Cartesian object positions provided by the tracking system described in Section B.3 are only used for computing rewards, for reset, and for the scripted baselines-the learned agent cannot access this information.</p><p>We distinguish between two observation sets.</p><p>? Full: contains all values provided by the real robot. Parts are used for environment resets, reward computation, and scripted baselines.</p><p>? Evaluation: a subset of the full set, with tracker information removed.</p><p>In addition, each observation is stacked over several time steps. Observation stacking was chosen since the physical system is subject to actuation delays, and thus would not fulfil the requirements of an MDP without stacking. Camera observations are excluded from the observation stacking due to real-time and memory constraints.</p><p>The available observations, their units, and the places where each is used, are listed in <ref type="table" target="#tab_7">Table S2</ref>. Note that all of these sets denote available observations, and agents can choose to omit entries; for instance, the image observation from the back camera is omitted by our agent architecture to reduce inference time.    Furthermore, the Robotiq gripper used in our setup has a parallel mechanism that causes a non-linear relation between the motor encoder ticks used throughout this work, and the Cartesian distance between the fingertips. Since this relation can be hard to visualize intuitively, we present a number of poses that were used in <ref type="figure" target="#fig_10">Figure S9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Object Position Estimation</head><p>Several components of our setup rely on the availability of a tracking system to determine the 3D position of the objects, relative to the robot. Specifically, this is required for the scripted baseline used in this work and described in detail in Section C.2, for computing per-timestep rewards (Section B.4), and for automatically resetting the environment in automated evaluations (Section B.4.2). We therefore implemented a color-based object position estimation algorithm that provides an estimate of the 3D centroids of the red, green and blue objects. Given the critical role of this component, we calibrate (both intrinsics and extrinsics <ref type="bibr" target="#b42">[43]</ref>) and use all three cameras available in our robot setup. The position estimation algorithm works as follows:</p><p>1. Convert the RGB into YUV images: this conversion allows finer control over colors using the chrominance components UV and robustness over brightness variations trough the luminance component Y. 3. For each color, find the largest contour and evaluate its centroid using image moments <ref type="bibr" target="#b43">[44]</ref>.</p><p>4. Estimate the 3D centroids {[x c , y c , z c ]} c?{r,g,b} of the objects in the robot reference frame through triangulation <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Task Evaluation</head><p>The performance of an agent is tested on a number of trials -200, unless specified otherwise. The initial state of the objects is randomized between episodes in the following way:</p><p>? Using a modified variant of the scripted controller (Section C.2), all objects are moved to random positions inside the working area.</p><p>? The robot's TCP is moved to a random position inside the working volume, excluding the lowest 8 cm to avoid collisions with objects.</p><p>? The wrist joint is rotated to a random position within ? ? 2 , ? 2 . ? The fingers are moved to a random opening angle within [0, 100] (i.e. open or half-open;</p><p>see <ref type="figure" target="#fig_10">Figure S9</ref> for a visualization).</p><p>Each trial lasts 20 seconds, or 400 steps at a control rate of 20 Hz. Trials are prematurely terminated when the wrist force sensor senses horizontal forces of greater than 2 N or a vertical force of greater than 2.5 N. In this case, an RL agent receives a discount of zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.1 Reward Definition</head><p>We all can understand what a stacked pair of objects should look like, however it is surprisingly difficult to define for a large set of geometric shapes, for the purposes of automated evaluations.</p><p>Here we formalize the definition of "stacking" used in the main paper.</p><p>For objects to be considered "stacked", the top object's position (as estimated by the tracking system) must be inside a cylindrical volume of a 3 cm radius, starting 2. </p><formula xml:id="formula_1">r = 1, if (z top ? z bottom &gt; 0.025) ? (||(x top , y top ) ? (x bottom , y bottom )|| &lt; 0.03) ? (f &lt; 30) 0, otherwise.<label>(S1)</label></formula><p>The open-ended cylinder was chosen to accommodate objects of arbitrary sizes. For instance, the top (red) object of Triplet 3 has a length of 15 cm, so the centroid can be a considerable distance from the bottom object when standing on its long side. A visual depiction of different stacked pairs being considered successful (green) or failures (red) is given in <ref type="figure" target="#fig_4">Figure S10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Automation for Unattended Learning and Evaluation</head><p>The training and evaluation process used in this work is largely automated. In fact, the experiments were continued throughout multiple COVID-19 lockdowns. In particular, the randomization procedure described above is fully automatic, with a modified version of the PID controller developed for the scripted baseline (Section C.2) being used to move objects. Note that the lack of object orientation provided by the tracker means that pose randomization is not deliberate. Instead, we rely on incidental rotation when objects are dropped, and run evaluations for at least 200 trials to reduce variance.</p><p>A pool of 10 identical robot cells is used for data collection. To guarantee comparability of results, evaluations are always performed on the same five cells, each of which is associated with one specific test triplet. Furthermore, tare is performed on the wrist force-torque sensor between episodes, in order to prevent drift. All cameras are white-balanced and their brightness adjusted to the same level, to counteract both daytime fluctuations in lighting, and differences between individual robot cells.</p><p>Evaluation requests are enqueued for each cell, and processed in order of arrival; thus, there is no closed training-evaluation loop, but training from any robot data always has to be offline to some degree. In the absence of new evaluation requests, old ones are automatically repeated in order to gather additional data and reduce variance. A number of consistency checks between episodes ensure that all sensors report data at the expected rates, and that actuators are operational-failing these checks would trigger the only required human interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Simulation</head><p>Our simulation environment was implemented in the MuJoCo <ref type="bibr" target="#b44">[45]</ref> physics simulator. Like the equivalent real robot environment, it contains a Sawyer arm with a Robotiq 2F-85 gripper mounted behind the playground, with three cameras attached to the basket.</p><p>The simulation was designed to provide the same observations as the real robot, with the same ranges and shapes. A small number of observations were too dissimilar to be of use, notably the torques, and are thus omitted. For the full list of available observations in our simulated environment at different stages of training, see <ref type="table" target="#tab_16">Table S5</ref>. Like the observations, the simulation exposes the same 4-DoF Cartesian actions as the real robot's given in <ref type="table" target="#tab_5">Table S1</ref>. It uses the same QP controller as described in Section B.1.1 to compute joint velocities from Cartesian velocities at 20 Hz. It was also designed to have similar appearance and dynamics to the real environment. However, as <ref type="figure" target="#fig_4">Figure S11</ref> illustrates, the low-level appearance is noticeably different. Likewise, the physics differ in the way objects interact and slide off each other.</p><p>The evaluation protocol in simulation follows that of the real robot, with a few key differences.</p><p>1. We perform 1000 evaluation episodes per policy per Triplet 12 , rather than 200.</p><p>2. The entire 6-DoF pose of the objects is randomized, rather than only the position.</p><p>3. The sparse evaluation reward makes use of privileged information from the simulator, which isn't available on the real system, specifically whether objects are directly in contact with each other. This allows us to have a wider admissible cylinder of 5 cm in which the top object may be placed, and eliminates the need to check the gripper's opening angle.</p><formula xml:id="formula_2">r = ? ? ? ? ? ? ? ? ? ? ? ? ? 0, if ||(x top , y top ) ? (x bottom , y bottom )|| &gt; 0.05 if (z top ? z bottom ) &lt; 0.02</formula><p>if top object is not in contact with bottom object if top object is in contact with robot or basket 1, otherwise.</p><p>(S2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5.1 Shaped Reward</head><p>Our shaped reward, which we designed to use for training our state-based agent in simulation only, forms a curriculum leading to a successful stack. It is divided into five progressive stages: reaching</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Canonical Simulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera Observation</head><p>Real Robot  and grasping the top object, lifting it more than 10 cm above the basket, hovering the top object over the bottom object, stacking it, and leaving the objects stacked by moving the gripper away from them. Each stage generates a reward in [0, 1], and the highest-level stage to produce a reward of 0.1 or more is considered the "active" one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera Observation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Front Left Front Right Front Left Front Right</head><formula xml:id="formula_3">r = ? ? ? ? ? ? ? ? ? ? ? ? ? 4+R leave 5 , if R leave &gt; 0.1 3+R stack 5 , if (R stack &gt; 0.1) ? (R leave ? 0.1) 2+R hover 5 , if (R hover &gt; 0.1) ? (R stack ? 0.1) ? (R leave ? 0.1) 1+R lif t 5 , if (R lif t &gt; 0.1) ? (R hover ? 0.1) ? (R stack ? 0.1) ? (R leave ? 0.1) Rgrasp 5 , otherwise.<label>(S3)</label></formula><p>Intuitively this amounts to an agent being rewarded incrementally for each of the stages that are required to complete a stable stack. A detailed description of each of the stages and the definition of the equivalent rewards can be found below. An example reward trace illustrating the different stages is also shown in <ref type="figure" target="#fig_4">Figure S12</ref>. We now describe the distinct stages of the shaped reward described above, all of which produce rewards in [0, 1]. As laid out in Equation (S3), the stages are combined into a total reward that consists of the "highest" of these stages that is currently generating a reward over 0.1, plus a fixed amount for each "lower" stage.</p><p>Several of the stage rewards make use of a distance function D(a, b, s, t), which is defined as the tanh over the distance between a and b, which decays to 0.05 as the distance reaches s. If the distance is below a tolerance of t, the maximum value of 1 is returned. This distance function is illustrated in <ref type="figure" target="#fig_1">Figure S13</ref>.</p><formula xml:id="formula_4">D(a, b, s, t) = 1, if ||a ? b|| &lt; t 1 ? tanh ||a ? b|| tanh ?1 ? 0.95 s 2 , otherwise.<label>(S4)</label></formula><p>Reaching and Grasping The first stage R grasp provides reward when the tool center point (TCP) is moved close to the top object, with an additional bonus for closing the parallel gripper that is given only when already very close to the object. </p><formula xml:id="formula_5">R grasp = R reach ? 0.5 + R close gripper 2 , if R reach &gt; 0.9 0.5, otherwise.<label>(S5)</label></formula><p>The component R close gripper in turn is maximal when the grasp sensor is triggered. If not, a smaller shaped reward is given, which approaches its maximum as the gripper opening angle f reaches its maximum closing angle of 255. </p><p>Lifting The lift stage R lif t also makes use of the grasp component R close gripper , but multiplies it with a shaped reward that linearly increases as the designated top object's centroid moves between a minimum height of 5.5 cm and a maximum of 10 cm.</p><formula xml:id="formula_8">R lif t = R close gripper ? R move top up (S8) R move top up = ? ? ? ? ? 1, if z top &gt; 0.1 0, if z top &lt; 0.055 ztop?0.055 0.1?0.055 , otherwise<label>(S9)</label></formula><p>where z top is the height of the top object from the basket.</p><p>Hovering The hovering stage R hover simply provides reward for the top object being close to a position 4 cm above the bottom one. Maximum reward is given with a 1 cm tolerance around this position to account for noise in the tracking system. Outside this tolerance, the reward decays within 20 cm. (S10)</p><p>Stacking The stacking stage R stack is a sparse reward that is only non-zero when the red object's horizontal position is within 3 cm of the blue one's, and its vertical position within 1 cm of the point 4 cm above the blue one. Note that this differs from the open volume in which the red object is allowed to be (which is used for the real robot's evaluation in Equation (S1)).</p><formula xml:id="formula_9">R stack = ? ? ? 0, if ||(x top , y top ) ? (x bottom , y bottom )|| &gt; 0.03 if (z top ? z bottom + 0.04) &gt; 0.01 1, otherwise.</formula><p>(S11)</p><p>Leaving The final leaving stage R leave is identical to the stacking stage R stack , but multiplied by a shaped term that rewards moving the TCP to a position 10 cm above the red object, thus forcing the agent to let go of the object. Since it is not important whether that position is precisely reached, maximum reward is given with a tolerance of 3 cm.</p><p>R leave = R stack D(z T CP , z top + 0.1, 0.05, 0.03).</p><p>(S12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baselines C.1 Human performance</head><p>As a rough indication of task difficulty, we collected a few demonstrations of the task in simulation from human teleoperators. The demonstrations were collected by 4 individuals who were not part of the research team. They used game pads to control the robot arm, and faced the same time limit as was used for evaluation of learned or scripted agents. Unlike agents, teleoperators were given a single camera view at a high resolution. Teleoperators recorded a total of 846 episodes (the number varied from 141 to 331 per participant), with the object set randomly replaced every 10 episodes.</p><p>These demonstrations were not used to train any of the agents mentioned in the paper. Results are summarized in <ref type="table" target="#tab_10">Table S3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Scripted Agent</head><p>The scripted baseline is a classical robotic control approach using a lot of prior knowledge, coded in a finite-state-machine. It uses the same observations available to the agent, as well as the 3D positions of the blue and red objects' centroids. In the real environment, the 3D positions of the objects are obtained from a centroid estimation algorithm (Section B.3); in the simulated environment, the   positions are obtained directly from the simulator. The performance of our scripted behaviour is meant to be used as a data point to understand how far a task-solver can get when ignoring relevant information such as the objects' orientation and shapes.</p><p>The scripted baseline was implemented through the use of a finite-state machine (FSM) with 9 states and 12 unique transition functions. A state diagram representation of the FSM is shown in <ref type="figure" target="#fig_4">Figure S14</ref>. During normal execution, the FSM starts at the 0th state and continues through states 1-6 until the objects are stacked. If the objects are found to be stacked at any point during the execution of the FSM, a transition to the final state 7 ("End") is made. If any of the states fail, the FSM immediately transitions to the 8th state, which re-positions the tool-center-point (TCP) of the robot and loops back to state 1. Note that states 0, 1, 5, and 8 do not implement transitions or failure detection and are always executed until completion.</p><p>The description of each of the states in the FSM is given below: Position control of the TCP is achieved through a low-gain P-controller on the error between the desired 3D Cartesian position and the current position of the TCP. The desired position of the TCP is computed on each state individually based on the predefined behaviour of each state and the measured position of the objects through our perception pipeline. The orientation of the wrist is only actively controlled during the execution of the 1, 2, and 8th states, which execute a random angular velocity about the vertical axis after the 100th step, or zero otherwise. The outputs of the P-controller are passed directly to the first 3-DoF of the action space exposed by the environment, while the angular velocity commands (4th DoF) are set to zero during states that do not actively control the orientation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Details on Training Expert policies from State Features in Simulation</head><p>As outlined in the Section 4.1, the first step in our approach is to train a policy, in simulation, either specializing on each of the 5 fixed triplets for the Skill Mastery task, or a general one on the 1 092 727 triplets that are possible with the 103 training objects for the Skill Generalization task. As discussed,  : Observations and rewards used at different training stages. The state-based teacher is trained with privileged information in simulation, and is then distilled to a vision-based policy that has access to images and only proprioception observations that are realistic in simulation and can also be later used for zeroshot sim-to-real transfer. For this reason, we exclude velocity, force, and torque observations for distillation in simulation. For the one-step offline policy improvement, a new vision-based policy is trained from a real-world dataset. This improved policy now includes velocity observations, but excludes force and torque observations since they too noisy in the real system. We did not use the back camera in our experiments. we found training directly from state features s to be significantly faster than training from vision in this step and thus exposed the full simulation state-proprioceptive information from the robot and 6-DoF pose information about the objects-to the agent. The complete list of observations available to the state-based policy can be found on <ref type="table" target="#tab_16">Table S5</ref>. At this stage of the learning pipeline, we are mainly concerned with obtaining high-performing experts in a fast manner in simulation. Thus on top of access to full state information, we also use a shaped reward which is only available in simulation and enables fast learning. The shaped reward is described in Section B.5.1 and visualized in <ref type="figure" target="#fig_4">Figure S12</ref>. We provide a comparison between training with state features vs. training directly from vision in <ref type="figure" target="#fig_4">Figure S15</ref>. As is evident from the comparison training from vision results in a large slow-down in terms of training time.</p><p>As mentioned, any off-the-shelf RL algorithm could have been used for training our state-based policies. We opted to use MPO <ref type="bibr" target="#b14">[15]</ref>, which we found to lead to fast policy improvement while allowing for stable learning. MPO does not directly optimize the RL objective, but instead considers a KL regularized objective that is optimized with a policy iteration approach. Concretely, in iteration k we first learn a corresponding Q-function Q ? k?1 ? (s, a, y) for the policy from the last iteration (starting from a random policy ? 0 at k 0 ), which can be learned from a replay buffer D by finding the function that minimizes the squared temporal difference error:</p><formula xml:id="formula_10">arg min ? E (st,at,st+1)?D r(s t ) + ? E a ?? k?1 (?|st+1,y) Q ? k?1 ? (s t+1 , a , y) ? Q ? k?1 ? (s t , a t , y) 2 ,</formula><p>(S13) where ? are the parameters of a target network <ref type="bibr" target="#b45">[46]</ref> that are replaced with the current parameters ? for the Q-function every 200 optimization steps, and we use 20 samples from the policy to estimate the inner expectation. Instead of the single transition temporal difference error above, Abdolmaleki et al. <ref type="bibr" target="#b14">[15]</ref> also considered a n-step temporal difference target calculated via the Retrace algorithm <ref type="bibr" target="#b46">[47]</ref> and we use this target in our MPO implementation. This Q-function is then used to define the following KL constrained objective for policy optimization:</p><formula xml:id="formula_11">L MPO (q) = E s?D E a?q [Q ? k?1 (s, a, y)] , s.t. E ?? k [D KL (q(?|s, y), ? ?e (?|s, y))] &lt; E ,<label>(S14)</label></formula><p>where D KL denotes the KL divergence to the last policy, which restricts changes in the policy and induces stable learning. A solution to this problem can be found in closed form as q(a|s, y) ? ? k?1 (a|s, y) exp(Q ? k?1 (s, a, y)/?) which can be projected back to a parametric policy by finding the expert policy ? ?e as the maximizer ? ?e (a|s, y) = arg max</p><formula xml:id="formula_12">? ?e E s?D E a?? k?1</formula><p>[exp(Q ? k?1 (s, a, y)/?) log ? ?e (a|s, y)] ,</p><formula xml:id="formula_13">s.t. E ?? k [D KL (? k?1 (?|s, y), ? ?e (?|s, y))] &lt; M ,<label>(S15)</label></formula><p>which corresponds to minimizing the KL between q and ? ?e and where M specifies an additional trust-region constraint placed on the policy (we set ? k for each iteration to ? ?e after 200 optimization steps). We use a trust-region constraint that splits the influence on the mean and covariance for Gaussian policies as in Abdolmaleki et al. <ref type="bibr" target="#b47">[48]</ref>. When using the hybrid space of continuous and discrete actions, we have a separate third trust-region constraint for the Bernoulli distribution, though we found that the discrete component didn't require a trust-region for stable learning. Optimization can be carried out via Monte-Carlo estimation of the objective using samples from the policy ? k to estimate the inner expectation, and samples from the replay buffer for the outer expectation. For a full description of the algorithmic details of solving this optimization problem we refer to Abdolmaleki et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Details on Interactive Imitation Learning for Sim-to-Real Transfer</head><p>After obtaining the experts via MPO, we distill the state-based experts into a single vision-based policy ? vis ? via interactive imitation learning, as described in Section 4.2. In this step the ? vis ? uses only a subset of the available observations (vision and proprioceptive readings but no information about object positions; see <ref type="table" target="#tab_16">Table S5</ref> for a complete list). The two key decisions made for distillation are: 1) We collect data using ? vis ? while it is being trained. For this purpose we run a large number of "actor" processes (1000) in simulation with the domain randomized environment. These fetch the parameters ? from the learner process at the beginning of every episode and send data to a replay buffer D s2r . 2) We train ? vis ? based on feedback from the expert's on data sampled from the replay (this DAgger style training resulted in best performance as outlined in the experiments). We note that this is a purely supervised learning problem on a changing dataset; as is standard the influence of ? vis ? on the dataset collection process is only implicit (i.e. we do not calculate the gradient of the sampling process for data-collection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Details on Training Improved Policies from Real Data</head><p>When training improved policies from real data collected by executing ? vis ? on the real robots, we use a slightly different subset of observations for the improved vision-based policy ? imp ? (now including velocity information; see <ref type="table" target="#tab_16">Table S5</ref> for a complete list). As described in Section 4.3, we use a filtered cloning loss of the data for this purpose, with filtering function f (s t , a t , ? ) where ? corresponds to the trajectory data from the executed episode. When using BC-IMP, we simply set f (s t , a t , ? ) = r(s T ), i.e. it is 1 if the binary sparse reward of the last step in the episode (at time T ) is 1, and 0 otherwise. This sparse reward information is readily available from the recorded episodes. For the exponential advantage filter (i.e. CRR-IMP), we use f (s t , a t , ? ) = exp(A ? imp ? (o(s t ), a t )/?), in which case we need to learn an estimate of the advantage alongside the policy ? imp ? . We follow the implementation of CRR <ref type="bibr" target="#b24">[25]</ref> and learn a distributional action-value function <ref type="bibr" target="#b48">[49]</ref> from the same data that the policy is learned from by gradient descent on the objective:</p><formula xml:id="formula_14">L Q CRR-IMP (?) = E (st,at,st+1)?Dreal D r(s t ) + ? E a ?? imp ? (?|o(st+1)) [Q ? (o(s t+1 ), a )], Q ? (o(s t ), a t ) ,</formula><p>(S16) where D denotes the distributional Q-learning operator, ? denotes the parameters of a target network (that are swapped for ? every 200 optimization steps) and where Q ? (o(s t ), a t ) now is parameterized as a categorical distribution with 101 categories representing equally spaced bins of values from [?150.0, 150.0]. We learn this Q-function alongside the policy, and use Q ? to calculate policy improvement (i.e. the advantage used in the exponential filter is also fixed for 200 optimization steps at a time). We calculate the advantage A ? imp ? (o(s t ), a t ) using a Monte-Carlo estimate of</p><formula xml:id="formula_15">A ? imp ? (o(s t ), a t ) = Q ? (o(s t ), a t ) ? E a ?? imp ? (?|o(st)) [Q ? (o(s t ), a )]</formula><p>where we estimate the expectation with 20 samples from ? imp ? (?|o(s t )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Details E.1 Domain Randomization and Image Augmentation</head><p>As mentioned above, our strategy for solving the RGB-stacking tasks in the real world is simulationto-reality transfer. It is therefore of paramount importance to ensure that both stages described above will result in policies that are able to bridge the reality gap and perform well in our real-world setup. We do so by relying on (a) a simulation environment that is closely aligned to the real robot environment (in terms of camera poses, robot joint limits, etc.); and (b) a sufficient amount of domain randomization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref> and visual data augmentation <ref type="bibr" target="#b49">[50]</ref>. These ensure that the simulation-trained policies can successfully deal with the domain gap that still exists between the simulated and the real environments, and the increased stochasticity of the real world. Although we did consider learned adaptation methods as used in prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> like using domain-adversarial losses <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> and randomized-to-canonical networks <ref type="bibr" target="#b54">[55]</ref>, preliminary results did not seem to provide clear benefits on top of domain randomization and data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1 Domain Randomization</head><p>Domain Randomization (DR) has been shown to be a simple and powerful method to achieve generalization of simulation-trained policies to the real world for robotic learning problems <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In our simulated environment we randomized a number of physical properties (e.g. mass, friction, damping, armature) for all agents, as well as the delay of executing their actions on the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Randomized Simulation</head><p>Camera Observation ront Right</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Front Left Front Right Front Left Front Right Front Left Front Right</head><p>Figure S16: Visual illustration of our visual domain randomization with different sampling of the properties listed in <ref type="table" target="#tab_18">Table S6</ref> for the same set of objects -Triplet 2. Although all geometries can vary freely, the RGBobjects are restricted to a certain range "around" red, blue, and green to aid with the identification of these colors in the real world as well, as that is the way the vision-based agent knows which objects is the top, the bottom and the distractor.</p><p>We also randomized a number of visual properties (e.g. object colors, object textures, camera poses, lighting) for our vision-based agents at the distillation phase. In our randomized environments we uniformly sample, from pre-defined ranges, colors and textures for all geometries in our simulator, as well as lighting, and camera poses to create a large visual diversity. Action execution was randomly delayed 0, 1 or 2 timesteps, the equivalent of 0, 50 or 100 ms. Most physics properties did not require any particular tuning and are perturbed uniformly within ?10% of their default values in the non-randomized version of our environment. The only exceptions are the ranges for the tangential, torsional, and rolling friction of the gripper, which were tuned carefully to prevent unrealistic grasping behaviours, e.g. grasping and lifting an object by a corner. The ranges for these were determined by teleoperating the simulated robot with different friction values. A list of all properties randomized, along with the range these were uniformly sampled from, can be found on <ref type="table" target="#tab_18">Table S6</ref>. A few samples illustrating our object color and texture randomization can be seen in <ref type="figure" target="#fig_4">Figure S16</ref>. Note that MuJoCo multiplies the RGBA values if both texture and rgba properties were set, which results in undesirably dark appearance. Thus, for each geom, we alternate sampling textures or colors. Our RGB-objects were treated differently in order to maintain their basic color, as the task is defined based on the color theme of the objects. Firstly, they are never assigned a texture. Secondly, the hue range of each object is predefined in a way that maintains the color theme, and for each RGB-object we sample the color in HSV space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.2 Image Augmentation</head><p>In order to further increase the diversity of the data and the zero-shot real-world performance of our simulation-only trained agents, we applied a number of image transformations to our visual observations on top of domain randomization. In addition, we applied the same image transformations when directly training from real-world data (e.g. for policy improvement). Unlike domain randomization, image augmentation is applied directly on image observations, so it is applicable to images from both simulated data and real-world data.</p><p>The following transformations are applied for image augmentation: random brightness, random hue, random saturation, random contrast, and random translation. These transformations are applied sequentially in that order. The random translations use bilinear interpolation and "reflect" fill mode (i.e. the input is extended by reflecting about the edge of the last pixel). For temporal consistency, we sample the random augmentations and apply the same random offsets for all images within a trajectory subsequence. A list of the image augmentation properties, along with the ranges these   <ref type="table">Table S7</ref>: Image augmentation properties that are randomized and their ranges. We sample random offsets from these ranges and apply the same random offsets to the entire sampled trajectory subsequence. We resample the offsets for each subsequence in the batch. That is, the random augmentations are consistent across time, but not across the batch.</p><p>were uniformly sampled from, can be found on <ref type="table">Table S7</ref>. We chose these ranges qualitatively without any tuning for evaluation success. For the random perturbations of the hue, we chose ranges small enough so that the red, green, and blue objects stay reasonably close to their respective colors. A few samples illustrating the effect of image augmentation can be seen in <ref type="figure" target="#fig_4">Figure S18</ref> for images from the real robots, <ref type="figure" target="#fig_4">Figure S19</ref> for canonical images from simulation, and <ref type="figure" target="#fig_9">Figure S20</ref> for domainrandomized images from simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Additional Network Architecture Details</head><p>The inputs to the networks are preprocessed in the same way for all the networks.  In both the state-based and vision-based agents, we use an input normalization layer for the nonimage observations. This input normalization layer consists of a linear layer, layer norm layer, and a tanh non-linearity. The size of the input normalizer refers to the number of units of the linear layer.</p><p>We use an output distribution layer for the output of the actor networks. The output distribution layer for the actor network outputs an independent joint distribution of multivariate normal (MVN) distribution with diagonal variance for the continuous action dimensions, and a Bernoulli distribution for the binary action dimension. The mean of the MVN is the output of a linear layer and the diagonal standard deviation is the output of a fully-connected layer with softplus non-linearity plus a bias of ? min . This distribution is not constrained to output normalized actions in [?1, 1]; instead, we clip samples from this distribution depending on the context. The logits vector of the Bernoulli distribution is the output of a linear layer with output size 2. This distribution is scaled accordingly to output normalized actions in {?1, 1}.</p><p>State-based agents. The actor network consists of an input normalization layer, MLP, and output distribution layer. The critic network starts with an input normalization layer for the observations and clipping of the actions to [?1, 1], then both streams are concatenated, and followed by an MLP and a linear layer with 1 output. These MLPs use exponential linear unit (ELU) activations. See <ref type="table" target="#tab_20">Table S8</ref> for a full list of network architecture hyperparameters used for the state-based agents.</p><p>Vision-based agents. The actor network consists of an observation encoder, MLP, Transformer, another MLP, and output distribution layer. When using a critic (i.e. in CRR), the critic network starts with an observation encoder for the observations and clipping of the actions to [?1, 1], then both streams are concatenated, and followed by an MLP and discrete-valued output distribution layer.</p><p>The actor and critic networks use the same architecture for their observation encoders, but their parameters are not shared. The observation encoder consists of two parallel streams-a ResNet stack for the image observations and an MLP with a final activation for the proprioception observationsand the outputs are merged by concatenation. The ResNet stack consists of a pair of ResNet encoders (one for each of the two images), activation, flattening and concatenation (of encodings from both images), and MLP with a final activation. Each ResNet encoder consists of 3 ResNet group modules. Each group module first applies a convolution followed by downsampling with a max-pooling layer, and then applies residual blocks modules twice. Each residual block consists of 2 convolution layers interleaved with non-linear activations.</p><p>The output distribution layer for the critic network outputs a discrete-valued distribution with support [v min , v max ] that is uniformly spaced among n atoms atoms or bins. The logits vector of this distribution is the output of a linear layer with output size n atoms .</p><p>See <ref type="table">Table S9</ref> for a full list of network architecture hyperparameters used for the vision-based agents.</p><p>We found in preliminary experiments that having each image processed by a ResNet encoder led to better sim-to-real transfer performance compared to stacking the two images along the channel dimension and processing this stacked image with a single ResNet encoder. We also found that sharing the parameters for the ResNets of both images led to even better transfer performance. This parameter sharing also has an additional advantage of computational speedups (this can be achieved by applying the ResNet in a single pass to both images concatenated along the batch dimension).  In addition to perform a single attention operation, it is beneficial to project Q, K, V h times with learned linear projections respectively, where h is number of heads:</p><formula xml:id="formula_16">MultiHeadAttention(Q, K, V ) = Concat(head 1 , ..., head h )W o (S18) head i = Attention(QW Q i , KW K i , V W V i )</formula><p>(S19) Following the multi-head attention module, a residual connection and layer normalization are applied. To leverage the order of the sequence, a positional-encoding layer is added to the input embeddings. A fixed positional encoding using sine and cosine functions of various frequencies are used in this work: SelfAttention(Q, K, V ) = LayerNorm(MultiHeadAttention(Q, K, V ) + PositionalEncoding(V )) (S20) On top of the self-attention layer, a fully-connected feed-forward network is applied to the output of each timestep separately. The MLP consists of two linear layers with a ReLU activation in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Additional Training Details</head><p>We use a trust-region constraint for the policy update in MPO and CRR. Similarly to Abdolmaleki et al. <ref type="bibr" target="#b14">[15]</ref>, the mean and standard deviation of the multivariate normal (MVN) of the actor distribution have separate trust-region constraints. However, we do not use a trust-region constraint for the Bernoulli distribution component as we found it was not necessary for stable learning.</p><p>We use different learning rates for optimizing the actor, critic, and dual variables. We anneal the learning rates for the actor and the critic, following an exponential decay schedule denoted as range(i start , i end , i step ), where i start and i end indicate the gradient step iteration at which the annealing starts and ends, respectively, and i step indicates the interval at which the learning rate is multiplied by the decay factor.</p><p>See <ref type="table" target="#tab_5">Table S10 and Table S11</ref> for a full list of training hyperparameters used for state-based and vision-based agents, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Detailed Real-Robot Results</head><p>In <ref type="table">Table 2 and Table 3</ref> we provide, for each setting, averages of multiple runs. These are 4 runs for sim-trained distilled agents: 2 seeds for each of the 2 state-based teacher policies we distilled these  from, and 2 runs for vision agents trained from real data. Here, we provide the results for each of the runs in each setting, and also present the results for each specific triplet. We hope that this provides a better sense of the variance in each setting. Entries in <ref type="table" target="#tab_5">Table S12 correspond to Table 2</ref>, whereas entries in <ref type="table" target="#tab_5">Table S13 correspond to Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Qualitative Analysis</head><p>In the main text we described different challenges posed by the test triplets. Aside from the quantitative success scores above, it is therefore also interesting to look at the resulting policies qualitatively, and examine whether our agent visibly learns to overcome these challenges. Even if such an approach is naturally to be taken only as anecdotal, it nevertheless provides an indication of open challenges that are worth pursuing further.</p><p>We therefore performed a number of evaluations with the best-performing agent, the Skill Mastery CRR-IMP policy trained on sim-to-real agent data (see <ref type="table" target="#tab_5">Table S13</ref>). We adversarially moved the objects to generate challenging situations and observed the agent's behaviour.</p><p>Triplet 1. The main challenge of this triplet is the need to precisely orient the gripper, since closing them on the slanted sides of the object will fail. The agent exhibits this behaviour, waiting to close the gripper until the wrist is properly aligned <ref type="figure" target="#fig_0">(Figure S17(a)</ref>).</p><p>Triplet 2. The bottom object in this triplet can be oriented in such a way that its top surface is slanted, making it impossible to stack without first tipping it over. The agent can be seen to perform this kind of behaviour, although not perfectly reliably; if the object is already oriented so that it can be tilted by pushing it against the basket's slope, it will do so ( <ref type="figure" target="#fig_4">Figure S17(b)</ref>). However, if the  <ref type="table" target="#tab_5">Table S12</ref>: Sim-to-Real Transfer Success. Ablations of the components of the sim-to-real policy. This table gives a full account of all evaluations for the equivalent <ref type="table">Table 2</ref> in the main paper. We execute the stochastic and deterministic policies in simulation and on the robots, respectively, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-Robot Success</head><p>Triplet Triplet Triplet Triplet Triplet Triplet Avg. 1 2 3 4 5 <ref type="table" target="#tab_5">Table S13</ref>: Real-Robot Success. Different approaches for solving our RGB-stacking tasks in the real world. This table gives a full account of all evaluations for the equivalent <ref type="table">Table 3</ref> in the main paper, except for IIL-s2r which are given in <ref type="table" target="#tab_5">Table S12</ref>. (c) Failed attempt to stack onto a sloped bottom object.</p><p>(d) Objects are aligned even after forced into off-center grasps.</p><p>(e) No attempt to adjust off-center stack.</p><p>(f) Agent supports the top object for the rest of the episode. bottom object's orientation is unfavourable, it will not rotate it to achieve this ( <ref type="figure" target="#fig_4">Figure S17(c)</ref>), and instead try to naively stack the objects.</p><p>Triplet 3. The main challenge in this triplet lies in the asymmetry of the top object, and needing to balance it onto the bottom one in such a way that their centroids are aligned. Even if disturbed into an off-center grasp, the agent still aligns both objects precisely ( <ref type="figure" target="#fig_4">Figure S17(d)</ref>).</p><p>Triplet 4. Considered the easiest triplet, the main challenge is to align the centroids, even when the top object can be placed far off-center on the larger bottom object. Perhaps surprisingly, while the agent usually places the object in the center, it shows no attempts to recover from occasional off-center stacks ( <ref type="figure" target="#fig_4">Figure S17(e)</ref>).</p><p>Triplet 5. Due to the rounded cross-section of the top object, there is a high risk of it rolling off after stacking. As a testament to this, the agent will often stay close to the object after stacking, sometimes (but not always) even supporting it with the gripper until the end of the episode when the bottom object is on a slope and a stack thus otherwise impossible ( <ref type="figure" target="#fig_4">Figure S17(f)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Related Work</head><p>Our work deals with real-world vision-based stacking with a diverse set of objects and a learned policy. We therefore did not discuss, in the main text, prior work on e.g. stacking from extracted features or in simulation. For completeness, we discuss such works here.</p><p>Furrer et al <ref type="bibr" target="#b4">[5]</ref> is a very interesting paper on pick-and-place strategies with classical robotics methods that we have now included in our main text discussion. We should highlight here that it deals with only 6 specific stones that offer wide support and have high friction -in contrast to the 152 objects with diverse geometry we propose in our benchmark. To the best of our understanding, their method would neither be able to handle Triplet 2 (flipping the bottom object if needed), nor generalize to unseen objects, as is the case for our "Skill Generalization" task. We should also note that the evaluation on that work was done on a total of 11 episodes (or a maximum of 33 possible stacks in that setup) -in contrast to the more than 54,000 episodes we have evaluated our various choices with.</p><p>Duan et al <ref type="bibr" target="#b59">[60]</ref> deals with cube stacking in simulation only with policies that have access to task demonstrations, the state information of each cube and no visual input. Later work by Li et al <ref type="bibr" target="#b60">[61]</ref> in the same environment shows that reinforcement learning without demonstrations can learn to stack the cubes from state. Our stacking task is also related to other manipulation tasks, such as dry stacking <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref> where rocks of irregular shapes must be stacked to form a wall, but these methods do not address. However, while these methods deal with high-level planning and goal understanding, our benchmark task requires dealing with low-level contact dynamics and perception to make real object stacking possible with strategies that emerge from RL training in simulation.</p><p>Noseworthy et al <ref type="bibr" target="#b63">[64]</ref> also deals with high-level planning for stacking cubes, this time in the real world. The challenge in this work is that each cube was created with a slightly different center of mass, requiring precise stacks. However, this is not a vision-based task: each cube is also ARtagged and therefore privileged information about each cube are known during evaluation. Similarly, Macias et al <ref type="bibr" target="#b64">[65]</ref> use a combination of binary markers on objects as well as high level planning to perform pick and place stacking.</p><p>Some vision-based methods have addressed a related but distinct problem of predicting stack stability <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>. Lerer et al <ref type="bibr" target="#b66">[67]</ref> deal with intuition around physics and identifying whether a tower block will collapse, and even the trajectory the blocks will take, focusing primarily on simulation experiments. Similarly, Hamrick et al <ref type="bibr" target="#b68">[69]</ref> deal with identifying stability of simulated block towers, with the aid of Graph Neural Networks and without using vision. They also attempt to address which parts of the tower to "glue" in order to fix an unstable tower. Groth et al <ref type="bibr" target="#b67">[68]</ref> classify structures as stable or unstable from visual inputs and learn "stackability affordances" for objects of various shapes. However, this line of work does not address the dynamic aspects of manipulation, as there is no physical robot interacting with the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Supplementary Material</head><p>In this appendix we attempted to provide as much detail as we could regarding our benchmark, our environments, and our implementation and experimental details. As part of the material that supplement this paper, we also include a video that shows our agents in action <ref type="bibr" target="#b12">13</ref> , as well as designs and instructions for recreating the real robotic cell 14 and the STL files for the RGB-Objects in <ref type="figure">Figure S2</ref>  <ref type="bibr" target="#b14">15</ref> .</p><p>Here is a list of what is included in the supplementary material as part of this submission:</p><p>? Video that supplements the main text.</p><p>? BOM (Bill of materials, list of things and quantity) to build: -Cell -Basket</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Building instructions:</head><p>-Cell -Basket -Wiring diagram</p><p>We will further release, post-submission under an Apache Licence, the following:</p><p>? 3D Assembly drawing to complete / integrate assembly instruction ? 3D models and Manufacturing drawing for all the parts (cell and Basket) that need to be: -Machined -3D printed -Laser Cut ? All STL files for the training set, held-out set, and the specific triplets, in separate folders.</p><p>? A version of the simulated environment.   <ref type="table">Table S7</ref>. Note that we randomly sample different color and translation perturbations for each sequence in a batch, but we use the same random perturbations for all the images within a sequence. Although in our experiments we always use image augmentation in combination with domain randomization, here we show example sequences without domain randomization for visualization clarity. See <ref type="figure" target="#fig_9">Figure S20</ref> for examples with domain randomization. <ref type="figure" target="#fig_9">Figure S20</ref>: Image sequences from the domain-randomized simulation without and with the image augmentations listed in <ref type="table">Table S7</ref>. Note that we randomly sample different color and translation perturbations for each sequence in a batch, but we use the same random perturbations for all the images within a sequence. See <ref type="figure" target="#fig_4">Figure S19</ref> for examples without domain randomization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Simulation environment (b) Real-robot environment (c) Successful stack example (d) RGB-objects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Stacking Skill Mastery for our 5 Specific Triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Triplet 3 .</head><label>3</label><figDesc>The challenge with this triplet is to have a secure central grasp for the elongated object and balance it on top of the slanted bottom object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>C Baselines 26 C. 1 37 F Additional Related Work 39 G</head><label>2613739</label><figDesc>Human performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.2 Scripted Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D Methods 28 D.1 Details on Training Expert policies from State Features in Simulation . . . . . . 28 D.2 Details on Interactive Imitation Learning for Sim-to-Real Transfer . . . . . . . . 30 D.3 Details on Training Improved Policies from Real Data . . . . . . . . . . . . . . 31 E Experimental Details 31 E.1 Domain Randomization and Image Augmentation . . . . . . . . . . . . . . . . 31 E.2 Additional Network Architecture Details . . . . . . . . . . . . . . . . . . . . . 33 E.3 Additional Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 E.4 Detailed Real-Robot Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 E.5 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Supplementary Material 40</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S1 :</head><label>S1</label><figDesc>Illustration of the deformations applied for each of the 15 axes of the RGB-Objects parametric family (major axes and their unique pairwise combinations). Each deformation changes the stacking affordance of RGB-objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S3 :</head><label>S3</label><figDesc>A visualization of parallel-gripper grasp-affordance. Left: a visualization of how the rectangleobject grasp-funnel (beam with square section) varies with the object-gripper relative orientation: successful grasps are invariant to significant orientation differences. Right: a visualization of how the trapezoid-object (trapezoid section) grasp-funnel varies with the object-gripper relative position: small differences in the relative pose hamper a successful grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S6 :</head><label>S6</label><figDesc>Left: a visualization of good stacking-oriented gripper orientations (in green) and good onlyfor-pick&amp;place gripper orientations (in yellow) for the slanted cylinder. Right: a sketch to visualize how affordances vary in the clutter; only a subset of grasps are possible in the clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S7 :</head><label>S7</label><figDesc>Overview of the physical system. Left: Front view of the basket. Top right: Neutral pose towards which the Cartesian controller's null-space is biased. Bottom right: Detail view of the endpoint tooling: forcetorque sensor and gripper with custom fingertips.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S8 :</head><label>S8</label><figDesc>Example image observations provided to the agent. These are captured from the basket cameras, then cropped and sub-sampled to 128 ? 128. From left to right: front left view, front right view, and back left view. In this work, we only used the front views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>(a) 0 :</head><label>0</label><figDesc>Maximum opening. (b) 30: Minimum required opening to generate task reward.(c) 100: Minimum opening for random initial episode states.(d) 255: Fully closed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure S9 :</head><label>S9</label><figDesc>Overview of various relevant gripper positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure S10 :</head><label>S10</label><figDesc>Various success conditions. From left to right: 1. Successful stack with gripper open and the top object in the cylinder region. 2. The objects are centered but the gripper is closed. 3. The top object is off-center, with its centroid outside the admissible cylinder. 4. The top object is below the cylinder region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure S11 :</head><label>S11</label><figDesc>Real and simulated environments. Equivalent real and simulated environments with the camera observations used during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure S12 :</head><label>S12</label><figDesc>Reward trace for 300 steps of an episode. From left to right: 1. Approach during R reach part of Rgrasp stage. 2. R close gripper component of Rgrasp stage becomes active as the gripper is closed while realigning it to the graspable object faces. 3. Transition to R lif t stage as the object is slightly lifted. 4. R hover increases as the object is moved closer to the target. 5. Objects are precisely enough placed to be considered stacked as per R stack . 6. Gripper has moved far enough away to enter final R leave stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure S13 :</head><label>S13</label><figDesc>Examples of the distance function used in several reward terms, with the x-axis showing the distance between two entities a and b. Note how the value always decays to 0.05 (dashed line) as the distance reaches the shaping tolerance s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>The reaching component R reach is a shaped distance between the TCP position pos T CP = (x T CP , y T CP , z T CP ) and that of the top object pos top = (x top , y top , z top ), decaying within 15 cm and with no tolerance. The positions are provided in meters with respect to the robot frame of reference, centered around the arm's base.R reach = D(pos T CP , pos top , 0.15, 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>R close gripper = 1 ,</head><label>1</label><figDesc>if grasp sensor triggered D(f,255,255,0) 2 , otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>R</head><label></label><figDesc>hover = D (pos top , pos bottom + (0.0, 0.0, 0.04), 0.2, 0.01) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure S14 :</head><label>S14</label><figDesc>(Left) Example scripted baseline run for the test object set 1. Each figure shows the state ID on the top right, and the current step counter on the bottom left. (Right) State diagram for the finite-state machine used in the scripted baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure S15 :</head><label>S15</label><figDesc>State-based vs Vision-based MPO training. Comparison of average reward (left) and average task success on the training set (right) for the Skill Generalization task when training from all available state information (State-based Agent) vs training from vision and proprioception (Vision-based Agent)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>( a )</head><label>a</label><figDesc>Agent aligns gripper to the graspable faces of the object.(b) The bottom object is tipped over during approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure S17 :</head><label>S17</label><figDesc>Agent behaviour during challenging situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>( a )</head><label>a</label><figDesc>Image sequences from the simulation without image augmentations.(b) Image sequences from the simulation with image augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure S19 :</head><label>S19</label><figDesc>Image sequences from simulation without and with the image augmentations listed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The RGB-objects family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 Benchmark Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.3 Publicly Released Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Real-World Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.2 Environment Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3 Object Position Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.4 Task Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.5 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23</figDesc><table><row><cell>Table of Contents</cell><cell></cell></row><row><cell>A The RGB-Stacking Benchmark</cell><cell>14</cell></row><row><cell>A.1 B Environment Details</cell><cell>18</cell></row><row><cell>B.1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S1 :</head><label>S1</label><figDesc>Ranges and units of the different components of the agent action.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S2 :</head><label>S2</label><figDesc></figDesc><table /><note>Observations provided by the real-world robot setup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>5 cm or higher above the bottom one, and the gripper must be fully opened. Specifically, for object centroids [x top , y top , z top ] and [x bottom , y bottom , z bottom ], and for finger-opening angle f , we define the sparse, binary, stack reward:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S3 :</head><label>S3</label><figDesc>Average success rate and cumulative sparse reward for each of the test object sets, from human teleoperators.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>0. Init: No-op state that initializes the FSM and immediately transitions to the next state. This state always executes until completion; 1. Move open gripper to top object centroid: Opens the gripper and moves the TCP towards the position of the red object. Executes a non-zero angular velocity if step &gt; 100, zero otherwise. Completes if the TCP is within a pre-defined threshold of the red object. This state always executes until completion; Move closed gripper to safe height: Moves the TCP up while maintaining the gripper closed. Completes if the TCP is above 20 cm. Fails if a grasp is not detected, or if the distance between the TCP and the red object becomes too large; 4. Move closed gripper to bottom object hover position: Moves the TCP to a point at an absolute height of 20 cm directly above the blue object while maintaining the gripper closed. Completes if the TCP is above the blue object. Fails if a grasp is not detected, or if the distance between the TCP and the red object becomes too large; 5. Move closed gripper to bottom object stack position: Moves the TCP to a point 3 cm above the bottom object while maintaining the gripper closed. Completes if the TCP is within a pre-defined threshold of this point. This state always executes until completion; 6. Open gripper at bottom object stack position: Opens the gripper while maintaining the TCP position 3 cm above the bottom object. Fails if the objects are not stacked after opening the gripper; 7. End: Opens and lifts the gripper to an absolute height of 30 cm. Final state. Fails if the objects are not stacked; 8. Move open gripper to top object hover position: Opens the gripper and moves the TCP to</figDesc><table /><note>2. Grasp top object: Closes the gripper while maintaining the TCP position close to the red object. Executes a non-zero angular velocity if step &gt; 100, zero otherwise. Completes if a grasp is detected based on readings from force reading. Fails if the gripper closes and no grasp is detected; 3.a point at an absolute height of 20 cm directly above the red object. Executes a non-zero angular velocity if step &gt; 100, zero otherwise. Completes if the TCP is above the red object at a pre-defined height. This state always executes until completion and will result in different "random" grasp orientations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S4 :</head><label>S4</label><figDesc>Scripted baseline performance success rate and average cumulative reward for each of the test object sets in the simulated and real environment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S4</head><label>S4</label><figDesc>summarizes the average performance of the scripted approach on the test sets. Each test set was evaluated for 1000 episodes in simulation, and for at least 800 episodes in the real setup. The agent achieved a success rate of 43% on the training set over 10 000 episodes in simulation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table S5</head><label>S5</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table S6 :</head><label>S6</label><figDesc>Domain randomization properties that are randomized in simulation and their ranges. These properties were sampled uniformly at the beginning of every episode.</figDesc><table><row><cell>Property</cell><cell>Range</cell></row><row><cell>Brightness</cell><cell>[? 32 /255, 32 /255]</cell></row><row><cell>Hue</cell><cell>[? 1 /24, 1 /24]</cell></row><row><cell>Saturation</cell><cell>[0.5, 1.5]</cell></row><row><cell>Contrast</cell><cell>[0.5, 1.5]</cell></row><row><cell>Translation (horizontal and vertical)</cell><cell>[?4, 4] pixels</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>The image observations are normalized to [0, 1], whereas the non-image observations are flattened and concatenated into a single vector. The actions are normalized to [?1, 1]. The networks operate with normalized actions, i.e. critic networks processes normalized actions as inputs, and actor networks output action distributions in the normalized space. The agent scales back the actions to the original space when executing them in the environment.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>actor network</cell><cell></cell></row><row><cell>input normalizer size</cell><cell>512</cell></row><row><cell>MLP sizes</cell><cell>(512, 512, 256, 256)</cell></row><row><cell>MVN distribution ?min</cell><cell>10 ?4</cell></row><row><cell>activations</cell><cell>ELU</cell></row><row><cell>critic network</cell><cell></cell></row><row><cell>input normalizer size</cell><cell>512</cell></row><row><cell>MLP sizes</cell><cell>(512, 512, 256)</cell></row><row><cell>activations</cell><cell>ELU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table S8 :</head><label>S8</label><figDesc>Network Architecture Hyperparameters for State-Based Agents.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table S10 :</head><label>S10</label><figDesc></figDesc><table /><note>Hyperparameters for Training State-Based Agents.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table S11 :</head><label>S11</label><figDesc>Hyperparameters for Training Vision-Based Agents. The critic hyperparameters are only applicable to the methods that use a critic, i.e. CRR. We found that CRR doesn't need a tight trust-region for stable learning, so we chose a loose constraint of 0.1 without further tuning. The number of environment frames per gradient step and replay buffer size are only applicable in the online setting, which uses the simulated environment. The dataset size is only applicable in the offline setting, and the dataset consists of real-world episodes collected on the robots. The dataset for Skill Mastery only has the test triplets and the dataset for Skill Generalization only has the training objects.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>8% 74.7% 47.5% 82.5% 74.4% 84.8% 68.1% 74.5% 49.0% 59.5% 87.0% 70.5% IIL-s2r (deterministic) Teacher 2 -Seed 1 N/A 71.2% 74.4% 45.6% 81.6% 69.0% 85.3% 73.7% 78.0% 64.0% 69.5% 86.0% 71.0% IIL-s2r (deterministic) Teacher 1 -Seed 2 N/A 71.4% 75.2% 45.3% 79.9% 72.6% 84.2% 66.1% 67.5% 49.0% 57.5% 82.0% 74.5% 76.0% 44.1% 80.9% 71.8% 84.7% 63.7% 71.0% 33.0% 57.5% 85.1% 80.6% 50.7% 82.1% 74.9% 87.2% 66.7% 71.0% 47.0% 58.5% 86.3% 72.3% 49.1% 82.6% 75.7% 86.7% 63.6% 60.5% 40.0% 62.5% 80.7% 77.1% 54.4% 81.2% 74.5% 86.6% 70.1% 81.0% 48.5% 63.0% 83.7% 74.2% 51.0% 82.6% 74.4% 86.2% 69.1% 80.5% 54.5% 54.0% 88.0% 68.5% 3% 74.9% 52.9% 82.6% 73.6% 87.5% 72.8% 73.5% 52.5% 65.5% 86.5% 86.0% 5% 70.4% 51.5% 81.3% 73.2% 86.2% 66.6% 72.5% 40.5% 57.0% 83.5% 79.5% 4% 78.5% 48.6% 80.9% 73.0% 85.9% 70.2% 72.5% 51.0% 64.0% 86.5% 77.0% 1% 76.3% 48.8% 82.1% 72.2% 86.0% 67.6% 69.5% 38.0% 58.0% 87.5% 85.0% 1% 79.4% 50.2% 81.4% 73.3% 86.1% 66.6% 77.5% 44.0% 69.5% 81.0% 61.0% 7% 58.9% 38.1% 38.5% 44.7% 82.5% 90.5% 54.8% 30.0% 42.5% 42.0% 93.0% 66.5% IIL-s2r Teacher 1 -Seed 2 64.4% 59.2% 35.2% 42.3% 48.0% 80.8% 89.8% 49.6% 22.4% 44.5% 29.5% 91.5% 60.0% IIL-s2r Teacher 2 -Seed 1 64.8% 53.8% 15.7% 40.9% 40.7% 81.4% 90.5% 54.6% 27.0% 41.0% 39.5% 89.0% 76.5% IIL-s2r Teacher 2 -Seed 2 63.8% 52.1% 12.5% 40.8% 36.4% 80.0% 91.0% 48.4% 20.5% 30.5% 35.0% 91.0% 65.0% No Transformer Teacher 1 -Seed 1 61.5% 52.7% 22.9% 38.3% 34.4% 79.2% 88.6% 43.6% 30.0% 22.5% 14.0% 82.5% 69.0% No Transformer Teacher 1 -Seed 2 57.0% 45.1% 11.7% 35.7% 16.7% 75.5% 85.7% 44.4% 24.5% 28.5% 16.0% 79.0% 74.0% No Transformer Teacher 2 -Seed 1 64.4% 54.2% 15.3% 41.1% 40.9% 81.2% 92.3% 47.8% 31.5% 36.0% 17.5% 88.5% 65.5% No Transformer Teacher 2 -Seed 2 63.6% 53.2% 17.8% 39.4% 38.3% 80.6% 90.3% 45.7% 24.5% 26.5% 27.5% 87.5% 62.5% No object parameters Teacher 1 -Seed 1 65.0% 50.8% 29.9% 15.0% 38.3% 82.6% 88.0% 32.1% 23.0% 18.5% 24.0% 36.0% 59.0% No object parameters Teacher 1 -Seed 2 65.0% 58.0% 29.9% 41.0% 49.9% 81.9% 87.4% 29.3% 24.5% 21.0% 21.5% 27.5% 52.0% No object parameters Teacher 2 -Seed 1 61.7% 52.2% 24.0% 33.3% 39.0% 78.5% 86.0% 53.5% 29.5% 42.0% 29.0% 87.5% 79.5% No object parameters Teacher 2 -Seed 2 62.3% 53.8% 24.0% 38.7% 38.6% 80.6% 86.9% 49.6% 30.0% 33.5% 30.0% 84.0% 70.5%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Simulation Success</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Real-Robot Success</cell></row><row><cell>Method</cell><cell>Run</cell><cell cols="12">Training Triplet Triplet Triplet Triplet Triplet Triplet Triplet Triplet Triplet Triplet Triplet Triplet</cell></row><row><cell></cell><cell></cell><cell>Objects</cell><cell>Avg.</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Avg.</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Skill Mastery</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IIL-s2r (deterministic)</cell><cell>Teacher 1 -Seed 1</cell><cell>N/A</cell><cell cols="11">72.5%</cell></row><row><cell>IIL-s2r (deterministic)</cell><cell>Teacher 2 -Seed 2</cell><cell>N/A</cell><cell cols="11">71.5% 71.5%</cell></row><row><cell>IIL-s2r (stochastic)</cell><cell>Teacher 1 -Seed 1</cell><cell>N/A</cell><cell cols="11">75.0% 71.0%</cell></row><row><cell>IIL-s2r (stochastic)</cell><cell>Teacher 2 -Seed 1</cell><cell>N/A</cell><cell cols="11">73.5% 74.5%</cell></row><row><cell>IIL-s2r (stochastic)</cell><cell>Teacher 1 -Seed 2</cell><cell>N/A</cell><cell cols="11">74.5% 74.5%</cell></row><row><cell cols="14">IIL-s2r (stochastic) 73.No Transformer Teacher 2 -Seed 2 N/A Teacher 1 -Seed 1 N/A 74.No Transformer Teacher 2 -Seed 1 N/A 72.No Transformer Teacher 1 -Seed 2 N/A 73.No Transformer Teacher 2 -Seed 2 N/A 73.No image augmentation Teacher 1 -Seed 1 N/A 74.No image augmentation Teacher 2 -Seed 1 N/A 71.4% 76.5% 47.8% 81.6% 65.0% 86.2% 60.1% 67.5% 32.0% 44.5% 86.0% 70.5%</cell></row><row><cell>No image augmentation</cell><cell>Teacher 1 -Seed 2</cell><cell>N/A</cell><cell cols="11">74.1% 74.4% 51.0% 82.4% 72.9% 89.6% 70.3% 78.0% 52.0% 61.5% 90.0% 70.0%</cell></row><row><cell>No image augmentation</cell><cell>Teacher 2 -Seed 2</cell><cell>N/A</cell><cell cols="11">71.6% 76.2% 45.8% 80.6% 70.2% 85.1% 56.9% 63.0% 26.5% 48.5% 79.0% 67.5%</cell></row><row><cell>No action delay</cell><cell>Teacher 1 -Seed 1</cell><cell>N/A</cell><cell cols="11">74.5% 79.9% 50.6% 83.9% 72.8% 85.3% 71.1% 74.5% 52.5% 71.5% 83.5% 73.5%</cell></row><row><cell>No action delay</cell><cell>Teacher 2 -Seed 1</cell><cell>N/A</cell><cell cols="11">72.1% 75.2% 47.8% 81.3% 70.7% 85.3% 67.2% 73.0% 45.5% 56.5% 89.5% 71.5%</cell></row><row><cell>No action delay</cell><cell>Teacher 1 -Seed 2</cell><cell>N/A</cell><cell cols="11">74.0% 77.5% 51.3% 83.6% 73.7% 84.1% 68.9% 75.0% 43.5% 69.5% 80.5% 76.0%</cell></row><row><cell>No action delay</cell><cell>Teacher 2 -Seed 2</cell><cell>N/A</cell><cell cols="11">73.3% 74.8% 52.3% 82.6% 71.3% 85.4% 67.6% 66.5% 57.5% 58.5% 81.5% 74.0%</cell></row><row><cell cols="2">MSE &amp; no binary gripper Teacher 1 -Seed 1</cell><cell>N/A</cell><cell cols="11">72.2% 74.9% 53.6% 78.4% 71.3% 82.8% 30.8% 44.5% 23.5% 7.0% 20.0% 59.0%</cell></row><row><cell cols="2">MSE &amp; no binary gripper Teacher 2 -Seed 1</cell><cell>N/A</cell><cell cols="2">60.0% 74.8%</cell><cell cols="7">0.3% 72.6% 65.4% 81.8% 24.2% 36.0% 0.0%</cell><cell cols="2">0.0% 24.5% 60.5%</cell></row><row><cell cols="2">MSE &amp; no binary gripper Teacher 1 -Seed 2</cell><cell>N/A</cell><cell cols="11">72.4% 73.8% 55.5% 78.0% 71.0% 83.7% 24.4% 45.0% 24.0% 0.5% 13.5% 39.0%</cell></row><row><cell cols="2">MSE &amp; no binary gripper Teacher 2 -Seed 2</cell><cell>N/A</cell><cell cols="9">59.1% 73.18% 0.2% 75.0% 66.3% 81.0% 23.2% 27.0% 0.0%</cell><cell cols="2">0.5% 24.5% 64.0%</cell></row><row><cell>No binary gripper</cell><cell>Teacher 1 -Seed 1</cell><cell>N/A</cell><cell cols="11">74.4% 77.2% 53.1% 81.4% 73.4% 87.0% 43.6% 61.5% 36.5% 0.0% 61.5% 58.5%</cell></row><row><cell>No binary gripper</cell><cell>Teacher 2 -Seed 1</cell><cell>N/A</cell><cell cols="2">57.8% 72.2%</cell><cell cols="4">1.0% 68.0% 66.8% 80.9%</cell><cell cols="3">8.8% 10.0% 0.0%</cell><cell cols="2">0.5% 19.0% 14.5%</cell></row><row><cell>No binary gripper</cell><cell>Teacher 1 -Seed 2</cell><cell>N/A</cell><cell cols="10">74.5% 76.5% 52.5% 82.2% 74.3% 86.9% 16.4% 55.5% 12.5% 0.0%</cell><cell>8.0%</cell><cell>6.0%</cell></row><row><cell>No binary gripper</cell><cell>Teacher 2 -Seed 2</cell><cell>N/A</cell><cell cols="2">57.7% 70.2%</cell><cell cols="7">1.0% 69.9% 67.5% 80.0% 13.9% 24.0% 0.0%</cell><cell cols="2">0.5% 29.0% 16.0%</cell></row><row><cell>Skill Generalization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IIL-s2r</cell><cell>Teacher 1 -Seed 1</cell><cell>65.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">A two-contact stable grasp is achieved if and only if Murray et al.<ref type="bibr" target="#b34">[35,</ref> Theorem 5.6] the line connecting the contact points lies inside both friction cones (Figure S4).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The Sawyer is now developed and retailed by the Hahn Group.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The gripper does not allow setting velocities in natural units, but a byte value that is mapped to a corresponding percentage of the maximum speed, which is nominally 150 mm/s.<ref type="bibr" target="#b8">9</ref> The actual values provided by the sensor are 1 for no grasp, 2 for an inward grasp, and 3 for an outward grasp that is not possible with non-hollow objects.<ref type="bibr" target="#b9">10</ref> Measured relative to the base of the robot.<ref type="bibr" target="#b10">11</ref> This contains the previous 7-DoF joint action sent to the robot, which is distinct from the 4-DoF Cartesian action selected by the agent, and reduces ambiguity in the state.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. Apply red, green and blue color masking using UV components. It is worth noting that we used the same ranges across all robot cells used in this work, while regularly applying white balancing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">When evaluating on the training object set we evaluate 2 episodes for 5000 triplets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Video: https://dpmd.ai/robotics-stacking-YT. 14 Real cell documentation: https://github.com/deepmind/rgb_stacking/tree/main/real_cell_ documentation. 15 RGB-objects: https://github.com/deepmind/dm_robotics/tree/main/py/manipulation/ props/rgb_objects.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Image sequences from the domain-randomized simulation without image augmentations.(b) Image sequences from the domain-randomized simulation with image augmentations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Christopher Schuster, Nathan Batchelor, Serkan Cabi, Dave Barker, Jean-Baptiste Regli, Yusuf Aytar, Dushyant Rao and many others of the DeepMind team for their support and feedback during our project and the preparation of this manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Note that although the observation encoders use rectified linear unit (ReLU) activations throughout, the subsequent MLPs use ELU activations.</p><p>Transformer Architecture. While image augmentation and domain randomization helps with bridging visual and physics domain gap, the gap of transition dynamics remains. A potential approach is to give agents access to temporal information, which encourages them to "reason" about the transition dynamics. It has been shown in prior work on simulation-to-reality transfer <ref type="bibr" target="#b12">[13]</ref>, that doing so can bridge the reality gap further, and might even enable the agents to be performing system identification that can help with transfer in previously unseen environments. The Transformer architecture has been widely adopted for natural language processing <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b32">33]</ref> as well as for computer vision <ref type="bibr" target="#b57">[58]</ref>. However its application to control problems remains limited. Thereby we explore the Transformer model, which demonstrated huge power of sequence based data processing with attention mechanism, to encode temporal information. In this work, we adapted to the Transformer-XL <ref type="bibr" target="#b32">[33]</ref>. The transformer network has stacked self-attention module that apply to the input sequence repeatedly. The transformer module consists of 1) a multi-head attention submodule followed by 2) a multi-layer perceptron network.</p><p>The transformer torso takes encoded observations as input. The multi-head attention module applies scaled dot-production attention for every timestep:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. CoRR, abs/1509.06825</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509.06825" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<idno>abs/1603.02199</idno>
		<ptr target="http://arxiv.org/abs/1603.02199" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. CoRR, abs/1806.10293</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.10293" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using simulation and domain adaptation to improve efficiency of deep robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4243" to="4250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autonomous robotic stone stacking with online next best object target pose planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wermelinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gramazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA.2017.7989272</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="2350" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning by playing -solving sparse reward tasks from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neunert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>De Wiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<idno>abs/1802.10567</idno>
		<ptr target="http://arxiv.org/abs/1802.10567" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reinforcement and imitation learning for diverse visuomotor skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<idno type="DOI">10.15607/RSS.2018.XIV.009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems<address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konyushkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>?o?na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vecerik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12200</idno>
		<title level="m">A framework for data-driven robotics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-supervised sim-to-real adaptation for visual robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khosid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09470</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive curriculum generation from demonstrations for sim-to-real visuomotor control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amiranashvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6498" to="6505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Good Robot!&quot;: Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6724" to="6731" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Off-policy deep reinforcement learning without exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/fujimoto19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning dexterous in-hand manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep dynamics models for learning dexterous manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1101" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Maximum a posteriori policy optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A framework for behavioural cloning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sammut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Intelligence 15</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="103" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06295</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Policy distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cad2rl: Real single-image flight without a single real image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04201</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1703.06907</idno>
		<ptr target="http://arxiv.org/abs/1703.06907" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sim-to-real transfer of robotic control with dynamics randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1710.06537</idno>
		<ptr target="http://arxiv.org/abs/1710.06537" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<idno>abs/1707.02267</idno>
		<ptr target="http://arxiv.org/abs/1707.02267" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Asymmetric actor critic for image-based robot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1710.06542</idno>
		<ptr target="http://arxiv.org/abs/1710.06542" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain randomization and generative models for robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Critic regularized regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Awac: Accelerating online reinforcement learning with offline datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09359</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning dexterous manipulation from suboptimal experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galashov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analysis and observations from the first amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Causo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Wurman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASE.2016.2600527</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<title level="m">Robotics benchmarks for learning with low-cost robots</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The mechanics of manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mason</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROBOT.1985.1087242</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1985 IEEE International Conference on Robotics and Automation</title>
		<meeting>1985 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1985-03" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="544" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Planning optimal grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROBOT.1992.219918</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1992 IEEE International Conference on Robotics and Automation</title>
		<meeting>1992 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2290" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dexnet 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1703.09312</idno>
		<ptr target="http://arxiv.org/abs/1703.09312" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial grasp objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danielczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ichnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Automation Science and Engineering (CASE)</title>
		<meeting>IEEE Conf. on Automation Science and Engineering (CASE)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Mathematical Introduction to Robotic Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zexiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC Press, Inc., USA</publisher>
		</imprint>
	</monogr>
	<note>1st edition. ISBN 0849379814</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Benchmarking in manipulation research: The YCB object and model set and benchmarking protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
		<idno>abs/1502.03143</idno>
		<ptr target="http://arxiv.org/abs/1502.03143" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauz?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fazeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Dafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Holladay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Morona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<idno>abs/1710.01330</idno>
		<ptr target="http://arxiv.org/abs/1710.01330" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A versatile generalized inverted kinematics implementation for collaborative working humanoid robots: The stack of tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mansard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Evrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kheddar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Robotics</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Opensot: a whole-body control library for the compliant humanoid robot coman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Tsagarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference in Robotics and Automation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A local collision avoidance method for non-strictly convex polyhedra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kanehiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lamiraux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kanoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laumond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient selfcollision avoidance based on focus of interest for humanoid robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Tsagarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Humanoid Robots</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">OSQP: an operator splitting solver for quadratic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stellato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Banjac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goulart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bemporad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12532-020-00179-2</idno>
		<ptr target="https://doi.org/10.1007/s12532-020-00179-2" />
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="637" to="672" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<idno type="DOI">10.1006/cviu.1997.0547</idno>
		<ptr target="https://doi.org/10.1006/cviu.1997.0547" />
	</analytic>
	<monogr>
		<title level="j">Triangulation. Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="157" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual pattern recognition by moment invariants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="187" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NeurIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NeurIPS&apos;16<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
	<note>ISBN 9781510838819</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Relative entropy regularized policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bohez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1812.02256</idno>
		<ptr target="http://arxiv.org/abs/1812.02256" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/bellemare17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rl-cyclegan: Reinforcement learning aware simulation-to-real</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khansari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11157" to="11166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Retinagan: An object-aware approach to sim-to-real transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03148</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12627" to="12637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<idno>abs/1812.07252</idno>
		<ptr target="http://arxiv.org/abs/1812.07252" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">One-shot imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Towards practical multi-object manipulation using relational reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4051" to="4058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deep q-learning for dry stacking irregular objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shamsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Napp</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2018.8593619</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1569" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">From rocks to walls: A modelfree reinforcement learning approach to dry stacking with irregular rocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2057" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Active learning of abstract plan feasibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Vision guided robotic block stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2014.6942647</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="779" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2014.2359435</idno>
		<idno>doi:10. 1109/TPAMI.2014.2359435</idno>
		<title level="m">3d reasoning from blocks to stability. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="905" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning physical intuition of block towers by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v48/lerer16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Shapestacks: Learning vision-based physical intuition for generalised object stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="702" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Relational inductive bias for physical construction in humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<ptr target="https://mindmodeling.org/cogsci2018/papers/0341/index.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Cognitive Science Society</title>
		<editor>C. Kalish, M. A. Rau, X. J. Zhu, and T. T. Rogers</editor>
		<meeting>the 40th Annual Meeting of the Cognitive Science Society<address><addrLine>CogSci; Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-25" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">cognitivesciencesociety.org</note>
	<note>Scripted agent N/A 51.2% 36.3% 23.0% 34.4% 84.9% 77.6% Skill Mastery BC. scripted agent data) Seed 1 50.8% 25.5% 22.5% 35.0% 85.5% 85</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title/>
		<idno>9% 43.5% 31.0% 41.5% 83.5% 80.0%</idno>
	</analytic>
	<monogr>
		<title level="j">BC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">55</biblScope>
		</imprint>
	</monogr>
	<note>scripted agent data</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">5% -Sim-to-real agent for test triplets data N/A 69</title>
		<idno>6% 76.4% 52.7% 60.4% 86.5% 72.0%</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
		<respStmt>
			<orgName>CRR</orgName>
		</respStmt>
	</monogr>
	<note>scripted agent data</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">BC-IMP (sim-to-real agent data) Seed 1 75</title>
		<idno>1% 76.0% 59.5% 70.0% 90.5% 79.5%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">BC-IMP (sim-to-real agent data) Seed 2 74</title>
		<idno>1% 75.0% 62.0% 71.5% 85.0% 77.0%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">CRR-IMP (sim-to-real agent data) Seed 1 81</title>
		<idno>0% 88.0% 66.5% 74.0% 88.0% 88.5%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Skill Generalization -Suboptimal agent for training set data N/A 32</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crr-Imp</surname></persName>
		</author>
		<idno>6% 21.5% 16.5% 17.0% 60.0% 48.0%</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">sim-to-real agent data</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bc-Imp</surname></persName>
		</author>
		<idno>2% 23.0% 33.0% 37.0% 82.0% 66.0%</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
	<note>suboptimal agent data</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bc-Imp</surname></persName>
		</author>
		<idno>8% 23.0% 45.5% 41.5% 73.0% 66.0%</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
	<note>suboptimal agent data</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crr-Imp</surname></persName>
		</author>
		<idno>6% 27.5% 42.0% 41.0% 79.5% 83.0%</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
	<note>suboptimal agent data</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crr-Imp</surname></persName>
		</author>
		<idno>5% 35.0% 40.5% 43.0% 83.5% 80.5%</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
	<note>suboptimal agent data</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Real-world image sequences without image augmentations. (b) Real-world image sequences with image augmentations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Real-world image sequences without and with the image augmentations listed in Table S7. Note that we randomly sample different color and translation perturbations for each sequence in a batch, but we use the same random perturbations for all the images within a sequence</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
