<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-to-Image Translation with Low Resolution Conditioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ihsen</forename><surname>Hedhli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Lalonde</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Gagne</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universit? Laval</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CIFAR AI Chair</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image-to-Image Translation with Low Resolution Conditioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most image-to-image translation methods focus on learning mappings across domains with the assumption that images share content (e.g., pose) but have their own domain-specific information known as style. When conditioned on a target image, such methods aim to extract the style of the target and combine it with the content of the source image. In this work, we consider the scenario where the target image has a very low resolution. More specifically, our approach aims at transferring fine details from a high resolution (HR) source image to fit a coarse, low resolution (LR) image representation of the target. We therefore generate HR images that share features from both HR and LR inputs. This differs from previous methods that focus on translating a given image style into a target content, our translation approach being able to simultaneously imitate the style and merge the structural information of the LR target. Our approach relies on training the generative model to produce HR target images that both 1) share distinctive information of the associated source image; 2) correctly match the LR target image when downscaled. We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality, diversity and coverage. Qualitative and quantitative results show that when dealing with intra-domain image translation, our method generates more realistic samples compared to state-of-the-art methods such as Stargan v2 <ref type="bibr" target="#b7">[8]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-to-image translation (i2i) methods seek to learn a mapping between two related domains, thereby "translating" an image from a domain to the other while preserving some information from the original. These methods have been applied to various applications in computer vision, such as colorization <ref type="bibr" target="#b34">[35]</ref>, super-resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref>, medical imaging and photorealistic image synthesis <ref type="bibr" target="#b39">[40]</ref>.</p><p>In recent years, i2i methods have achieved promising results in terms of visual quality and diversity. Cycle-GAN <ref type="bibr" target="#b44">[45]</ref> learns a one-to-one mapping between domains <ref type="figure">Figure 1</ref>: Illustration of the proposed approach on CelebA-HQ dataset: our method learned to map the inputs HR image (source) and LR (target) image to generate a HR output that preserves the identity of the source image and stays faithful to the structure of the LR target. by introducing the cycle consistency constraint that enforces the model to preserve the content information while changing the style of the image. This idea inspired many works such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref> to disentangle the feature space into 1) a domain-specific space for the style; and 2) a shared space for the content-this allowed more diverse multimodal generation. Recent methods such as StarGAN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> unified the process in a single framework to achieve compelling results both in terms of visual quality and diversity. In the problem of reference-guided image synthesis, such methods are capable of extracting the style of a target image and merging it with the content of a source image to generate an image that shares information from both images. An inherent assumption of these methods is that both style and content images share the same spatial resolution.</p><p>In this paper, we consider the scenario where the target image is of much lower resolution than the source image. In this case, the target image only contains low frequency information such as the general shape, pose, and color of the subject. We therefore aim to learn a mapping that can leverage the high frequency information present in the HR source image, and combine it with low frequency information preserved in the LR target. As illustrated in <ref type="figure">fig. 1</ref>, such a mapping can learn to generate an HR image that shares distinctive features from both HR and LR inputs in an unsupervised manner.</p><p>This scenario also bears resemblance to the image superresolution problem, which aims at generating an HR image that corresponds to a LR input. Both scenarios are highly ill-posed since there exists a large number of HR images which all downscale to exactly the same LR imagenumber which grows exponentially with each downscaling factor <ref type="bibr" target="#b1">[2]</ref>. We also demonstrate that our framework can be applied to super-resolve a very low resolution image given a high resolution exemplar as guidance.</p><p>The main contribution of this paper is to define a novel framework that deals with resolution mismatch between target and source for image-to-image translation. We demonstrate that the approach can effectively be used when dealing with very low resolution targets where details are completely blurred to the point of being visually unrecognizable. When evaluating our approach on the CelebA-HQ and AFHQ datasets, we show that our framework results in more realistic samples than state-of-the-art image-toimage translation methods such as Stargan-v2 <ref type="bibr" target="#b7">[8]</ref>. We validated our findings by reporting FID and LPIPS scores on both dataset, and also provide more evidence by reporting density/coverage metrics <ref type="bibr" target="#b31">[32]</ref>. These extensive experiments demonstrate that our method can generate results that are photo-realistic, and that convincingly fuses information from both the HR source and LR target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref> have demonstrated promising results in various applications in computer vision, including image generation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, super-resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref> and image-to-image translation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Recent work has striven to improve sample quality and diversity, notably through theoretical breakthroughs in terms of defining loss functions which provide more stable training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref> and encourage diversity in the generated images <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39]</ref>. Architectural innovations also played a crucial role in these advancements <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>. For instance, <ref type="bibr" target="#b24">[25]</ref> makes use of an attention layer that allows it to focus on long-range dependencies present in the image. Spectral normalization <ref type="bibr" target="#b29">[30]</ref> stabilizes the network, which also translates into having high quality samples. Since our work sits at the intersection of reference-guided image synthesis and super-resolution, the remainder of this section focuses on these two branches exclusively. Reference-guided image synthesis Also dubbed "conditional" <ref type="bibr" target="#b23">[24]</ref>, reference-guided i2i translation methods seek to learn a mapping from a source to a target domain while being conditioned on a specific image instance belonging to the target domain. In this case, some methods attempt to preserve the "content" of the source image (identity, pose) and apply the "style" (hair/skin color) of the target. Inspired by the mapping network of StyleGAN <ref type="bibr" target="#b18">[19]</ref>, recent methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16</ref>] make use of a style encoder to extract the style of a target image and feed it to the generator, typically via adaptive instance normalization (AdaIN) <ref type="bibr" target="#b14">[15]</ref>. All these methods have the built-in assumption that the resolution of the source and target images are the same. In this work, we explore the scenario where the target (style) image has much lower resolution than the source. Super-resolution Our approach is also related to superresolution methods, which are aiming to learn a mapping from LR to HR images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Other approaches leverage knowledge about the class of images for super-resolution. For example, when dealing with faces, explicit facial priors can be leveraged <ref type="bibr" target="#b5">[6]</ref>. Of particular interest, PULSE <ref type="bibr" target="#b27">[28]</ref> leverages a pretrained StyleGAN <ref type="bibr" target="#b18">[19]</ref> and, through an iterative optimization procedure, searches the space of latent style vectors for a face which downscales correctly to a given LR image. Our method bears resemblance to PULSE, but allows for a guided and therefore more controlled generation procedure with a single forward pass in the network.</p><p>More closely related are the so-called reference-guided super-resolution methods, which, in addition to the LR input image, also accept additional HR images for guidance. Here, the reference images need to contain similar content (e.g. textures) as the LR image. Representative recent methods propose to transfer information using cross-scale warping layers <ref type="bibr" target="#b43">[44]</ref> or with texture transfer <ref type="bibr" target="#b41">[42]</ref>. In contrast, our method frames super-resolution in an i2i context, relying on a specific instance (e.g. a specific person for faces) to guide the super-resolution process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Given a LR target image x ? R m?n , we define the associated R x subspace of LR images as:</p><formula xml:id="formula_0">R x = ?i, x i ? R m?n : x i ? x p ? .<label>(1)</label></formula><p>We consider an close to zero, so that each subspace contains only the LR images x i that are highly similar to the LR target x according to a given norm p (here p = 1). We are also defining a subspace M X in the HR image manifold that includes all HR images that are included in R x when downscaled as LR images:</p><formula xml:id="formula_1">M X = ?j, X j ? R M ?N : DS(X j ) ? R x ,<label>(2)</label></formula><p>where X j is an image in the HR space, X ? R M ?N is the one in the HR space corresponding to x, and DS(?) (a) HR subspaces M X and M Y and their corresponding LR subspaces Rx and Ry, related by a downscaling function DS(?).  is used for extracting structural information in the source image, which is merged in the decoder (in blue) with the incoming features from the decoding part with LR targets.</p><formula xml:id="formula_2">(b) Translation of HR source image Y from M Y to M X guided by LR target x. (c) Translation of HR source image X from M X to M Y guided by LR target y.</formula><p>is a downscaling procedure. Therefore, for each subspace M X on the HR image manifold, there exists a corresponding subspace R x in the LR space related by DS(?). As illustrated in <ref type="figure" target="#fig_0">fig. 2</ref>, given two HR-LR image pairs</p><formula xml:id="formula_3">(X ? M X , x ? R x ) and (Y ? M Y , y ? R y )</formula><p>, our goal is to learn a function G that translates the HR images from one subspace M X to another subspace M Y . These two subspaces are identified by their LR counterpart (i.e., R x and R y respectively), using LR images as additional information to specify the target HR space:</p><formula xml:id="formula_4">G : M Y ? R x ? M X , G(Y, x; ?) , G : M X ? R y ? M Y , G(X, y; ?) .<label>(3)</label></formula><p>Following conventional GAN terminology <ref type="bibr" target="#b10">[11]</ref>, parameterized function G(?, ?; ?) is a generator (simplified as G(?, ?) hereinafter). The goal of G is to translate a HR image from a HR subspace to another HR subspace while preserving some of the original image information (i.e., high-frequency content). For training G, we are using a discriminator D that plays two roles: 1) to classify whether the generated images are fake or real, pushing G to sample from the natural image manifold; and 2) to judge whether the translated high resolution image is part of the right LR subspace when downscaled, guiding G into the correct subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Objectives</head><p>For achieving these two discrimination roles, the GAN is trained according to the following minimax optimization:</p><formula xml:id="formula_5">min G max D L adv + ? cyc L cyc ,<label>(4)</label></formula><p>where L adv is an adversarial loss used to ensure that we are generating plausible natural images, L cyc is the cycle consistency loss that is getting the translated images are kept on the correct subspace while carrying the right information, and ? cyc is a hyperparameter to achieve the right balance between these two losses. At each training iteration, four forward passes are done with the generator, the first two for translating from a subspace to the other one in both ways, G(X, y) and G(Y, x), while the other two forward passes are for the cycle consistency constraint, G(G(X, y), x) and G(G(Y, x), y). The discriminator is used to make sure that the generated samples are from the designated subspace. Adversarial loss Following <ref type="bibr" target="#b3">[4]</ref>, we provide the discriminator with the absolute difference d X?M Y ? R m?n between the downscaled version of the generated image DS(G(X, y)) and the LR target y:</p><formula xml:id="formula_6">d X?M Y = |y ? r DS(G(X, y)) | r ,<label>(5)</label></formula><p>where r is the color resolution. As in <ref type="bibr" target="#b3">[4]</ref>, we round the downscaled image to its nearest color resolution (r = 2/255, since pixel values are in [?1, 1]) to avoid unstable optimization caused by exceedingly large weights to measure small pixel value differences. A straight through estimator <ref type="bibr" target="#b2">[3]</ref> is employed to pass the gradient through the rounding operation in eq. 5. The discriminator therefore takes as inputs:</p><formula xml:id="formula_7">D(Y, 0) for real samples, D (G(X, y), d X?M Y ) otherwise.<label>(6)</label></formula><p>Here, 0 is an all-zeros m ? n image difference, since the downscaled version of Y is exactly y. However, for fake samples, the absolute difference d X?M Y depends on how close is the generator to the designated subspace, M Y in our example. Both networks G and D are trained via the resulting adversarial loss:</p><formula xml:id="formula_8">L adv = E X [log D(X, 0)] + E Y [log D(Y, 0)] + E X,y [log (1 ? D(G(X, y), d X?M Y ))] . (7)</formula><p>Cycle consistency loss To make sure that the generator G preserves the high frequency information available in the source HR image, we employ the cycle consistency constraint <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref> in both directions, each time by changing the LR target to specify the designated subspace:</p><formula xml:id="formula_9">L cyc = E X,x,y X ? G(G(X, y), x) 1 + E Y,x,y Y ? G(G(Y, x), y) 1 .<label>(8)</label></formula><p>This cycle consistency loss encourages the generator to identify for the shared/invariant information between each two subspaces and preserve it during translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>Most of the recent image-to-image translation models (e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>) rely on Adaptive Instance Normalization (AdaIN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> to transfer the style from a reference image to a source image. In our work, however, the hypothesis of content and style is not suitable since the LR image contains information on both style (e.g., colors) and content (e.g., pose). Thus, our generator adapts HR source image to the content and style of the LR image through the use of spatially adaptive instance normalization (SPAdaIN) <ref type="bibr" target="#b35">[36]</ref>.</p><p>Generator G ( <ref type="figure" target="#fig_1">fig. 3</ref>) is U-shaped, with skip connections (moments shortcuts <ref type="bibr" target="#b22">[23]</ref>) between the encoding and decoding part. The encoder takes the input HR image, passes it through a series of downsampling residual blocks (Res-Blks) <ref type="bibr" target="#b12">[13]</ref>. Each ResBlks is equipped with instance normalization (IN) to remove the style of the input, followed by 2D convolution layers and a positional normalization (Pono) <ref type="bibr" target="#b22">[23]</ref>. The mean ? and variance ? are subtracted and passed as a skip connection to the corresponding block in the decoder. Pono and moments shortcuts plays a crucial role in transferring the needed structural information from the HR to the decoding part of the network. These blocks, dubbed Pono ResBlks, are illustrated in detail in <ref type="figure" target="#fig_3">fig. 4a</ref>.</p><p>For the decoder blocks ( <ref type="figure" target="#fig_3">fig. 4b</ref>), we use SPAdaIN <ref type="bibr" target="#b35">[36]</ref> conditioned on the LR image, where the LR image is first upsampled to the corresponding resolution of the Pono SPAdaIN ResBlk using bilinear upsampling. It is then followed by 2D convolution layers and a dynamic moment shortcut layer, where, instead of reinjecting ? and ? as is, we use a convolutional layer that takes ? and ? as inputs to generate the ? and ? used as moment shortcuts. Using the dynamic version of moment shortcuts allows the network to adapt and align the shape of the incoming structural information to its LR counterpart.</p><p>We use the StarGAN v2 <ref type="bibr" target="#b7">[8]</ref> discriminator architecture minus the domain-specific layers since we do not have predefined domains. We also concatenate the image difference (eq. 5) at the corresponding layer (same height and width).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Baseline We compare our method with Stargan-v2 <ref type="bibr" target="#b7">[8]</ref>, the state-of-the-art for image-to-image generation on CelebA-HQ and AFHQ.</p><p>Datasets We evaluate our method on the CelebA-HQ <ref type="bibr" target="#b17">[18]</ref> and AFHQ <ref type="bibr" target="#b7">[8]</ref> datasets. However, for CelebA-HQ we do not separate the two domains into female and male, since both domains are close to each other. Also, we are not using any extra information (e.g. facial attributes of CelebA-HQ). As for AFHQ, we train our network on each domain separately, since the amount of information shared between these is much lower. Average pooling is used as downscaling operator to generate the LR images, as in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Evaluation metrics Baseline results are evaluated according to the metrics of image-to-image translation used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. Specifically, diversity and visual quality of samples produced by different methods are evaluated both with the Fr?chet inception distance (FID) <ref type="bibr" target="#b13">[14]</ref> and the learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b40">[41]</ref>. Since the FID score entwine diversity and fidelity in a single metric <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, we also experiment with the density and coverage metrics proposed in <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Setup</head><p>For our experiments, we fixed the LR image resolution to 8 ? 8 and experimented with 128 ? 128 and 256 ? 256 for the HR image resolution-we ablate the effect of LR image resolution in sec. 4.4. We train our networks with Adam <ref type="bibr" target="#b20">[21]</ref> and TTUR <ref type="bibr" target="#b13">[14]</ref>, with a learning rate of 10 ?3 for the generator and 4 ? 10 ?3 for the discriminator. We also used R 1 regularization <ref type="bibr" target="#b28">[29]</ref> with ? = 0.5, with a batch size of 8. Spectral normalization <ref type="bibr" target="#b30">[31]</ref> was used in all the layers of both G and D. In eq. 1, we use = 0 to push the downscaled version of the generated image to be as close as possible to the LR target. We set ? cyc = 1 when trained on 128 ? 128, and to ? cyc = 0.1 for 256 ? 256.   <ref type="figure">Fig. 6</ref> compares images obtained with our framework with those obtained with Stargan-v2 <ref type="bibr" target="#b7">[8]</ref> using referenceguided synthesis on CelebA-HQ. Since our method focuses on generating images that downscale to the given LR image, the generator learns to merge the high frequency information presents in the source image with the low frequency information of the LR target, while preserving the identity of the person and other distinctive features. Differently from traditional i2i methods that only change the style of the source image while preserving its content, our method adapts the source image to the pose of the LR target. More qualitative samples obtained with our technique are shown in <ref type="figure">fig. 5</ref>, where the first row of HR images are used as source images and the first column is the LR target. We also display the real HR target to show that our model is capable of generating diverse images that are different from the target. <ref type="figure">Fig. 8</ref> displays generated samples on AFHQ. Visually, we notice that our model is capable of merging most of the high frequency information coming from HR source image with low frequency information present in the LR target. The degree of this transfer depends on how much information is shared between the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluation</head><p>In table 1, we report FID and LPIPS scores on the results obtained on CelebA-HQ, using two different resolutions, 128 ? 128 and 256 ? 256. Results with the 256 ? 256 resolution show a significantly lower FID of our method compared to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>, while being similar to Stargan-v2 <ref type="bibr" target="#b7">[8]</ref>. We notice a better FID score with the lower 128 ? 128 resolution, being then significantly better than Stargan-v2. This HR image res.</p><p>128  <ref type="table">Table 1</ref>: Quantitative comparison on the CelebA dataset, comparing our method to other reference-guided i2i methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. We follow same procedure as in Stargan-v2 <ref type="bibr" target="#b7">[8]</ref>, but for our method we sample ten HR images for each LR image.</p><p>is due to the fact that the task is harder with higher scale factor since we need to hallucinate more detailed textural information missing from the LR target. For a deeper insight on the differences between our method and Stargan-v2, we used the density and coverage metrics of <ref type="bibr" target="#b31">[32]</ref>. The density measures the overlap between the real data and generated samples, while the coverage measures their diversity, by measuring the ratio of real samples that are covered by the fake samples <ref type="bibr" target="#b31">[32]</ref>. Following <ref type="bibr" target="#b31">[32]</ref>, we used the feature space embedding of both the real and fakes images with a pretrained VGG16 <ref type="bibr" target="#b33">[34]</ref> on ImageNet. The density metric is then obtained from the k-nearest neighbours (with k = 5 as in <ref type="bibr" target="#b31">[32]</ref>) on the 4096 features obtained from the VGG network's second fully connected layer. Diversity results reported in <ref type="table" target="#tab_2">table 2</ref> show higher density values for our method, meaning that their samples are closer to the real data distribution than Stargan-v2. Higher coverage measures are also obtained for our method, meaning a better coverage of the data dis-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR LR</head><p>Real HR Generated images <ref type="figure">Figure 5</ref>: Qualitative reference-guided image synthesis results on CelebA-HQ. Our method takes the HR source images (top row), and translates them according to the LR target (left column). We also add the real HR target (not seen by the network) for visual comparison. See supplementary material for more results.</p><p>tribution modes. This is noticeable for the 128 ? 128 resolution, since the coverage is close to maximum value (the domain is [0, 1]), being almost 10% higher than Stargan-v2 while this gap doubles when the HR value is increased to 256 ? 256. This indicates that our model produces more realistic results by exploiting HR information from the source image, while being more diverse by staying faithful to the LR target image.</p><p>We also report quantitative results on AFHQ <ref type="bibr" target="#b7">[8]</ref> in ta-ble 3, where we train our model on each domain separately given the higher differences of the domains distributions. Indeed, we found that our method excels on domains where images are structurally similar and share information, such as the "cat" domain. However, the "dog" and "wild" domains show a wider variety of races and species, meaning less information shared between images from the same domain. This reduces the amount of shared information that can be transferred from the HR source, forcing to halluci-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Stargan-v2 LR HR <ref type="figure">Figure 6</ref>: Comparison between our method and reference guided Stargan-v2 <ref type="bibr" target="#b7">[8]</ref> on CelebA-HQ. For both methods, we use the same HR image as source (left column). For the reference image, our method uses the LR images (top row) while Stargan-v2 <ref type="bibr" target="#b7">[8]</ref> uses the corresponding HR (bottom row).</p><p>nate more details out of LR targets. This is confirmed by the lower LPIPS results obtained by our approach compared to Stargan-v2, while keeping similar FID scores. <ref type="table" target="#tab_4">Table 4</ref> illustrates the impact of LR resolution on CelebA-HQ. As the LR target resolution increases, the model exploits the information in the LR target more and more over the information provided by the HR source image. This is confirmed by sustained decrease the LPIPS score when the resolution increases from 8 ? 8 to 32 ? 32, while maintaining similar FID score. The effect of color in the LR target images is also ablated in the supp. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison to Super-Resolution</head><p>We now compare our method to PULSE <ref type="bibr" target="#b27">[28]</ref>, which super-resolves a face image by optimizing on the latent space of a pretrained StyleGAN model. <ref type="figure">Fig. 7</ref> presents results that illustrate similarities and differences between PULSE and our method. Both methods generate images that are realistic and faithful to the given LR image. From a super-resolution standpoint, our method would be considered reference-guided-but as opposed to image synthesis which is guided by the LR image, the super-resolution reference is another HR face image. We find this provides a   <ref type="table">Table 3</ref>: Quantitative comparison on the AFHQ dataset. We compare our method to the reference-guided i2i methods Stargan-v2 <ref type="bibr" target="#b7">[8]</ref> where the target and the source image are from the same domain.</p><p>certain amount of control over the diversity of the results, which is not possible with PULSE. Our generated samples are therefore much closer to the HR reference image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>This paper proposes a novel framework for referenceguided image synthesis targeted towards the scenario where the reference (target) has very low resolution. Our method Comparison between our method and PULSE <ref type="bibr" target="#b27">[28]</ref> on CelebA-HQ. While both methods use the same LR image (top row), ours also leverages the reference images (left column) to guide the generation process. We also show the corresponding HR (bottom row) for reference only-neither method sees this HR image.  attempts to realistically fuse information from both the high resolution source (such as identity and HR facial features) and the low resolution target (such as overall color distribution and pose). Our experiments show that our method allows for the generation of a wide variety of realistic images given LR targets. We validate our method on two wellknown datasets, CelebA-HQ and AFHQ, compare it to the leading i2i methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>, and demonstrate advantages in terms of diversity and visual quality. <ref type="figure">Figure 8</ref>: Qualitative results on the AFHQ dataset. Our method takes the HR images, and translate them to the corresponding HR subspace of the given LR target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR Generated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real HR Target</head><p>Limitations and Future Work As with recent work on exemplar-based super-resolution <ref type="bibr" target="#b41">[42]</ref>, our method works best when the LR and HR images come from the same domain (human faces, for example). In the case where the target LR image comes from a different domain than the source (e.g. tiger vs lion), the generated image attempts to match the LR target at the expense of "forgetting" more information from the source image. In addition, we also note that results sometimes lack diversity for a given LR target (rows in <ref type="figure">fig. 5</ref>, for example)-this is a consequence of having to perfectly match the LR image. A potential solution to mitigate both of the above problems would be to soften that constraint, for instance by increasing the distance in eq. 1, or by modifying the discriminator inputs (eq. 6) to tolerate larger differences. Finally, the framework has so far only been tested on faces (humans and animals). Extending it to handle more generic scenes, where the LR target would capture higher level information such as layout, makes for an exciting future research direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Conceptual illustration of the proposed framework: the generator G is trained to map a HR image Y (resp. X) from its domain M Y (resp. M X ) to the other domain M X (resp. M Y ), conditioned on a LR version x of X (resp. y of Y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Generator architecture: the encoding part (in red)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Residual blocks used in the generator G: (a) Encoding pono residual blocks, used to extract and compress the features coming from the source image and pass it to the decoding part of the generator through moments shortcuts and as compressed features ; and (b) Decoding pono SPAdaIn residual blocks takes as input extracted features from the encoding part, and also upsample the LR target image and pass it to the SPAdaIn layer<ref type="bibr" target="#b35">[36]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 7: Comparison between our method and PULSE [28] on CelebA-HQ. While both methods use the same LR image (top row), ours also leverages the reference images (left column) to guide the generation process. We also show the corresponding HR (bottom row) for reference only-neither method sees this HR image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Diversity and coverage metrics<ref type="bibr" target="#b31">[32]</ref> comparison on CelebA-HQ with different HR resolutions. LPIPS ? FID ? LPIPS ? FID ? LPIPS ?</figDesc><table><row><cell>Category</cell><cell></cell><cell>Cats</cell><cell></cell><cell>Dogs</cell><cell></cell><cell>Wild</cell></row><row><cell cols="2">Metric FID ? Stargan-v2 25.2</cell><cell>0.42</cell><cell>56.5</cell><cell>0.34</cell><cell>19.87</cell><cell>0.46</cell></row><row><cell>Ours</cell><cell>22.8</cell><cell>0.40</cell><cell>67.3</cell><cell>0.47</cell><cell>20.61</cell><cell>0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>LR target res. 4 ? 4 8 ? 8 16 ? 16 32 ? 32</figDesc><table><row><cell>FID ?</cell><cell cols="2">15.13 15.34</cell><cell>19.45</cell><cell>13.55</cell></row><row><cell>LPIPS ?</cell><cell>0.30</cell><cell>0.34</cell><cell>0.14</cell><cell>0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of the LR resolution on the FID and LPIPS metrics for the CelebA-HQ dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Creating High Resolution Images with a Latent Adversarial Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02365</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledig</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theis</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huszar</forename><surname>Ferenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caballero</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Aitken</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejani</forename><surname>Alykhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Totz</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wenzhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Learning Representations (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kilian Q Weinberger, and Serge Belongie. Positional normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mode seeking generative adversarial networks for diverse image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PULSE: Self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Muhammad Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Instanceaware image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural pose transfer by spatially adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyun</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision Workshops (ECCVW)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Xudong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Haoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lau</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diversity-sensitive conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiangchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Learning Representations (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image super-resolution by neural texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Crossnet: An end-to-end reference-based super resolution network using cross-scale warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
