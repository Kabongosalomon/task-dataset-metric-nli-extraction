<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Zhang</surname></persName>
							<email>zhangziyinjupt@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
							<email>zhangfeng01@njupt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
							<email>bhanu@ee.ucr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California at Riverside</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://github.com/ZiyiZhang27/Dite-HRNet.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A high-resolution network exhibits remarkable capability in extracting multi-scale features for human pose estimation, but fails to capture long-range interactions between joints and has high computational complexity. To address these problems, we present a Dynamic lightweight High-Resolution Network (Dite-HRNet), which can efficiently extract multi-scale contextual information and model long-range spatial dependency for human pose estimation. Specifically, we propose two methods, dynamic split convolution and adaptive context modeling, and embed them into two novel lightweight blocks, which are named dynamic multi-scale context block and dynamic global context block. These two blocks, as the basic component units of our Dite-HRNet, are specially designed for the highresolution networks to make full use of the parallel multi-resolution architecture. Experimental results show that the proposed network achieves superior performance on both COCO and MPII human pose estimation datasets, surpassing the stateof-the-art lightweight networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation is a task that requires both accuracy and efficiency, especially when it encounters real-time applications on devices with limited resources. In human pose estimation, high-resolution representation is necessary for accuracy, which brings difficulties to achieve high efficiency.</p><p>While High-Resolution Network (HRNet)  achieves remarkable performance in human pose estimation, it has high computational complexity. To make it lightweight, Small HRNet  reduces the width and depth of the network, but such reduction introduces performance degradation. Lightweight High-Resolution Network (Lite-HRNet) <ref type="bibr" target="#b5">[Yu et al., 2021]</ref> shows that lightweight high-resolution network can achieve satisfactory performance by replacing each residual block in Small HRNet with an efficient CNN block. However, it remains unclear whether * Contact Author such static block that is independent of the inputs can provide the optimal trade-off between the network performance and complexity for high-resolution networks, as some operations may have different effects at different locations of the network and with inputs of different sizes <ref type="bibr" target="#b5">[Cui et al., 2021]</ref>. The problem is that input-dependent component units may perform more efficiently in high-resolution networks than the input-independent counterparts. Our work focuses on designing dynamic blocks to develop a lightweight high-resolution network with dynamic representations.</p><p>Recent research  has demonstrated the benefits of long-range spatial dependency in human pose estimation. The general concept of long-range spatial dependency is the global understanding of spatial information in a large field of view. Previous high-resolution networks <ref type="bibr" target="#b4">Cheng et al., 2020;</ref><ref type="bibr" target="#b5">Yu et al., 2021]</ref> mainly rely on the deeply stacked convolution layers in parallel branches that extract multi-scale features to build spatial dependency, but the capacity of lightweight networks can be severely affected by the restricted widths and depths. Hence, another problem is how to enhance lightweight high-resolution networks with more efficient methods for modeling long-range spatial dependency.</p><p>To address the above problems, we present a Dynamic lightweight High-Resolution Network (Dite-HRNet). We adopt the architecture similar to <ref type="bibr">HRNet [Sun et al., 2019]</ref> and develop two dynamic lightweight blocks to improve the overall efficiency. First, we propose a Dynamic Split Convolution (DSC), which can dynamically extract multi-scale contextual information and optimize the trade-off between its capacity and complexity by two hyper-parameters, and thus is more effective and flexible than the standard convolution. Then, we introduce long-range spatial dependency into the high-resolution network by designing a novel Adaptive Context Modeling (ACM) method that enables the model to learn both local and global patterns of human poses. Finally, we embed DSC and ACM into two dynamic lightweight blocks, which are specially designed for the high-resolution network to make full use of the parallel multi-resolution architecture, as the basic component units of our Dite-HRNet.</p><p>The main contributions of this paper can be summarized as follows: (1) We propose a novel lightweight network named Dite-HRNet that can efficiently extract multi-scale contextual information and model long-range spatial dependency for hu-man pose estimation. (2) We design a DSC method to build multi-scale contextual relationship and an ACM method to further strengthen long-range spatial dependency. In addition, we accomplish a joint embedding of DSC and ACM into two dynamic lightweight blocks, which are designed as the basic components of Dite-HRNet. (3) In our experiments, Dite-HRNet achieves superior trade-off between the network performance and complexity on both COCO and MPII human pose estimation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Lightweight Human Pose Estimation. Recent studies <ref type="bibr" target="#b5">Yu et al., 2021]</ref> focus on improving the efficiency of huamn pose estimation networks. Small HRNet  reduces the width and depth of the original large version of HRNet . <ref type="bibr">Lite-HRNet [Yu et al., 2021]</ref> exploits the potential of lightweight highresolution network by replacing each residual block in the Small HRNet with a conditional channel weighting block, which adopts channel attention mechanism <ref type="bibr" target="#b6">[Hu et al., 2020]</ref> by replacing high-cost 1?1 convolutions in shuffle block <ref type="bibr" target="#b7">[Ma et al., 2018]</ref> with channel weighting operations. Efficient CNN Blocks. Efficient CNN blocks have been widely used in many efficient CNN architectures <ref type="bibr" target="#b5">[Howard et al., 2017;</ref>, which aim to maximize the capacity of models under limited computational cost. MobileNetV2 <ref type="bibr" target="#b7">[Sandler et al., 2018]</ref> introduces the inverted residual block, which improves both accuracy and efficiency over traditional bottleneck blocks. Shuffle block in ShuffleNetV2 <ref type="bibr" target="#b7">[Ma et al., 2018]</ref> performs convolutions only on half of the channels with a competitive performance, due to the channel split operation that splits the features by channels and the channel shuffle operation that enhances the information exchange across channels. MixNet [Tan and Le, 2019] combines multiple convolution layer into a mixed depth-wise convolution, which is a drop-in replacement of vanilla depth-wise convolution in CNN blocks. Sandglass block  flips the structure of the inverted residual block, reducing the information loss without any additional computational cost. Spatial Dependency Modeling. Long-range spatial dependency can be spontaneously captured in the repeated convolution layers, which have better performance with large convolution kernels. However, deeply stacked convolution layers and large convolution kernels are both costly and inflexible to be integrated in lightweight networks. Non-local network  adopts a self-attention mechanism to model pixel-wise spatial relations in a single layer, which has better flexibility but still high complexity. Global context network <ref type="bibr" target="#b1">[Cao et al., 2019]</ref> simplifies the non-local network to a lightweight structure with almost no performance loss. Dynamic CNN Architectures. There are two main stream methods for dynamic CNN architecture design. One is dynamic network structure, such as PP-LCNet <ref type="bibr" target="#b5">[Cui et al., 2021]</ref>, which dynamically conducts different operations based on the inputs of different sizes in different positions of the network. And the other is dynamic convolution kernel, such as dynamic convolution <ref type="bibr" target="#b3">[Chen et al., 2020]</ref>   et al., 2019], which mix different convolution kernels by generating weights through the attention mechanism. In order to exploit more efficient representations, our proposed Dite-HRNet embodies its dynamic state not only in the network structure but also in the convolution kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dite-HRNet</head><p>As shown in <ref type="figure">Figure 1</ref>, Dite-HRNet is a 4-stage network, consisting of one high-resolution main branch with the highest resolution and three high-to-low resolution branches that are added to the network one by one in parallel at the beginning of each new stage. Each newly added branch has half the resolution and twice the number of channels compared to the previously added branch. As shown in <ref type="table" target="#tab_1">Table 1</ref>, among all four stages of Dite-HRNet, the first stage, also regarded as the stem, contains a 3 ? 3 strided convolution and a Dynamic Global Context (DGC) block on the main branch. Each subsequent stage consists of a series of cross-resolution modules which are composed of two Dynamic Multi-scale Context (DMC) blocks and a multi-scale fusion layer that exchanges information across all branches. The main branch with the highest resolution maintains a high-resolution representation, which provides the final output of the backbone network for the subsequent pose estimation. For fair comparisons, we presents two instantiations of our network, Dite-HRNet- features extracted by different layers. One difference between the two blocks is that the DMC block applies a sequence of layers on half of the channels, while the DGC block applies two different sequences of layers on all two groups of channels. The sequence of layers in the DMC block contains one Dense Context Modeling (DCM) operation, one DSC and one Global Context Modeling (GCM). Both the DCM and GCM are the instantiations of the ACM method. In the DGC block, one 3 ? 3 strided depth-wise convolution, one GCM and one 1 ? 1 convolution are performed on one group of channels, while one 3 ? 3 depth-wise convolution, one GCM, one 1 ? 1 convolution and one 3 ? 3 strided depth-wise convolution are performed on the other group of channels. Each convolution in the DGC block and the DSC layer generates the convolution kernel via Dynamic Kernel Aggregation (DKA).</p><p>Configuration. In order to demonstrate the superior efficiency of our method, we optimize our networks, which are characterized by two hyper-parameters in DSC, to have similar model sizes and computational cost like in Lite-HRNet <ref type="bibr" target="#b5">[Yu et al., 2021]</ref>. We configure the hyper-parameters separately on different resolution branches. Our default configuration choice is shown in <ref type="table" target="#tab_8">Table 5</ref> and used for the subsequent comparison experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dynamic Split Convolution (DSC)</head><p>Split-Concat-Shuffle (SCS) Module. Large convolution kernels can bring broad receptive fields, thereby enhancing the long-range spatial dependency. However, using large kernel sizes on all filters in the network not only leads to a high computational cost, but also fails to capture the local pixelwise information. Therefore, we introduce the SCS module to extract contextual information by multiple kernels of different sizes and integrate them in a single convolution layer.</p><p>In the SCS module, channels are first split into multiple groups equally, and depth-wise convolutions with different kernel sizes are applied to each group of channels in parallel. The output of the convolution on each group is formally defined as follows:</p><formula xml:id="formula_0">Y i = DW Conv(K i ? K i |C/G)(X i ), K i = 2i + 1, i ? [1, G],<label>(1)</label></formula><p>where X i and Y i denote the input and output of the depthwise convolution on the i th group of channels, respectively.</p><formula xml:id="formula_1">DW Conv(K i ? K i |C/G)(?)</formula><p>is the depth-wise convolution with the kernel size K i ? K i and channel dimension C/G, where C denotes the total number of channels among the groups and G denotes the number of groups.</p><p>After the depth-wise convolutions, the grouped features are concatenated together. To further integrate the separated information at different scales, the channel shuffle operation <ref type="bibr" target="#b7">[Ma et al., 2018]</ref> is used at the bottom of the SCS module.</p><p>Note that the SCS module does not expand the width of the network, as it only splits channels into different groups and performs different convolution operations on them in parallel. The computational efficiency is acceptable and can be optimized by the hyper-parameter G.</p><p>Dynamic Kernel Aggregation (DKA). To make the SCS module learn rich contextual information even with small convolution kernels, we introduce a DKA operation which strengthens the input-dependency of convolution kernels by dynamically aggregating multiple kernels via kernel-wise attention weights based on the input images.</p><p>Standard convolution kernel is defined by a weight matrix w with 4 dimensions which decide the kernel size and input/output channels, respectively. Instead of concatenating the output features of different convolutions, we aggregate the kernel weight matrices {w i } before calculating the convolution results, to dynamically generate different convolution kernels for different inputs. The DKA operation computes attention weights over different convolution kernels, and then applies the element-wise product to the attention weights and the kernel weights. We define the DKA operation as follows:</p><formula xml:id="formula_2">Y = W T (X)X, W (X) = N i a i (X)w i ,<label>(2)</label></formula><p>where a i (X) is the attention weight for the i th convolution kernel, and W (X) is the aggregated weight matrix of N convolution kernels. The input-dependent attention weights a(X) are computed from the input X as follows:</p><p>a(X) = Sigmoid(F C(ReLU (F C(GAP (X))))), <ref type="formula">(3)</ref> where GAP (?) represents the global average pooling, and F C(?) represents the full-connected layer. Two functions Sigmoid(?) and ReLU (?) are used right after two fullconnected layers for non-linear activation. Since the DKA operation takes place before the calculation of convolution results, the aggregated kernel performs only one convolution operation on each input feature map without expanding the network width. For a standard convolution with kernel size K ?K and input size C ?H ?W , DKA of N kernels only introduces minor additional computational cost HW C + C 2 /4 + CN/4 from the standard convolution cost HW C 2 K 2 . The number of aggregated kernels N is another hyper-parameter for the efficiency optimization. Complementation. The high computational efficiency makes the DKA operation an excellent complementation to the SCS module. Therefore, we combine the above two methods into one single convolution, which is named DSC. With two hyper-parameters G and N from the SCS module and the DKA operation, respectively, it is relatively easier for DSC to optimize the trade-off between the capacity and complexity of the convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Context Modeling (ACM)</head><p>The ACM method can be abstracted into following three steps: (a) adaptive context pooling, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, which creates a shortcut connection with a 1 ? 1 channel pooling convolution and a softmax layer to form a context mask, and then applies the mask on the feature maps through a series of resolution-adaptive transformations to obtain spatial contextual features; (b) context shifting, which realigns the spatially related contextual features together by two 1 ? 1 convolutions with non-linear activation; (c) context weighting, which adopts the element-wise weighting operation between input features and shifted context features to model the contextual relationships of the corresponding features. This abstraction of the ACM method can be defined as:</p><formula xml:id="formula_3">Y = W eight(X, Shif t(ACP ool(H, W )(X))),<label>(4)</label></formula><p>where ACP ool(H, W )(X) denotes the adaptive context pooling which pools the input features X to a certain output size H ? W , Shif t(?) denotes the context shifting, and W eight(?) denotes the context weighting. For the high-resolution network, we propose two instantiations of the ACM method, DCM and GCM, which take the advantage of the parallel multi-resolution architecture. Dense Context Modeling (DCM). We introduce a DCM operation to densely model the spatial contextual relationships of the features from all resolution branches of one stage. At the n th stage, the input features from all n branches are pooled to the lowest resolution H n ? W n . Then, all pooled features are concatenated together, so that the context shifting can be densely performed on the parallel contextual features. In the end, the shifted contextual features are upsampled to the corresponding resolutions and distributed back to the corresponding branches for the subsequent context weighting. This instantiation implements the ACM as:</p><formula xml:id="formula_4">? ? ? ? ? ? ? ? ? X k = ACP ool(H n , W n )(X k ), X = Shif t(Cat([ X 1 , ..., X n?1 , X n ])), Y k = W eight(X k , U psamp( X k )), 1 ? k ? n ? 1, W eight(X k , X k ), k = n,<label>(5)</label></formula><p>where Cat(?) and U psamp(?) denote the feature concatenation and upsampling, respectively. X k represents the input tensor with the k th highest resolution. X represents the pooled tensor from the k th branch. X k represents the shifted tensor, which is distributed to the k th branch as X k . Y k represents the corresponding k th output tensor. Global Context Modeling (GCM). To separately model the global spatial dependency at each resolution, we apply a GCM operation on each branch of the network. It is a instantiation of the ACM when the adaptive context pooling has the output size 1 ? 1. The output features of the GCM operation on the k th branch is defined as follows:</p><formula xml:id="formula_5">Y k = W eight(X k , Shif t(ACP ool(1, 1)(X k ))), 1 ? k ? n.<label>(6)</label></formula><p>The GCM operation captures the spatial relationship of all features with the same resolution in the global aspect containing abundant contextual information, while the DCM operation captures the spatial relationships of all features with different resolutions in a moderate aspect containing more pixelwise information. Meanwhile, both two operations increase the information exchange across features, so that can be better substitutes for the 1 ? 1 convolutions in the shuffle block <ref type="bibr" target="#b7">[Ma et al., 2018]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Datasets and Evaluation Metrics. The COCO dataset  has images over 200K and 250K person instances, each with a label of 17 keypoints. We train our networks on the train2017 set (contains 57K images and 150K person instances), and evaluate them on the val2017 set (contains 5K images) and test-dev2017 set (contains 20K images) by the Average Precision (AP) and Average Recall (AR) scores based on Object Keypoint Similarity (OKS). To further validate our networks, we also perform experiments on the MPII Human Pose dataset <ref type="bibr" target="#b0">[Andriluka et al., 2014]</ref>, which contains about 25K images with 40K person instances, and evaluate the accuracy by the head-normalized Probability of Correct Keypoint (PCKh) score. Training. The presented Dite-HRNet is trained on 8 GeForce RTX 3090 GPUs, with 32 samples per GPU. All parameters are updated by Adam optimizer with a base learning rate 2e ?3 . As for the data processing, we expand all human detection boxes to a fixed aspect ratio 4 : 3, and then crop the images with the detection boxes, which are resized to 256?192 or 384?288 for the COCO dataset, and 256?256    for the MPII dataset. All images are used with data augmentations, including random rotations with factor 30, random scales with factor 0.25, and random flippings for both the COCO and MPII datasets. In addition, extra half body transformations are performed for the COCO dataset.</p><p>Testing. The two-stage top-down paradigm, which produces the person detection boxes and then predicts the person keypoints, is adopted for our testing process. The person boxes are predicted by the person detectors provided by SimpleBaseline  for the COCO dataset, while the standard testing strategy for the MPII dataset uses the provided person boxes. The heatmaps are estimated via 2D Gaussian, and then averaged for the original and flipped images. The highest heatvalue locations in the heatmaps are adjusted by a quarter offset in the direction from the highest response to the second-highest response, to obtain the keypoint locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Results on the COCO val2017 Set. As shown in <ref type="table" target="#tab_3">Table  2</ref>, we compare the results of our Dite-HRNet-18 and Dite-HRNet-30 with other state-of-the-art pose estimation meth-ods, including both large and small networks. Our networks are trained with two different input sizes 256 ? 192 and 384 ? 288, achieving better trade-off between the accuracy and complexity than other methods. As compared to other small networks, our Dite-HRNet-30 achieves the highest AP score of 71.5 with the input size 384 ? 288. In particular, Dite-HRNet exceeds Small HRNet  by 10 points of AP with only 40% GFLOPs. As compared to the large pose networks, our Dite-HRNet also achieves comparable or even higher accuracy with far smaller model sizes and lower computational complexity. Benefiting from the abundant contextual information, Dite-HRNet outperforms Lite-HRNet <ref type="bibr" target="#b5">[Yu et al., 2021]</ref> with the equivalent parameters and GFLOPs, while having the same network depths and input sizes.</p><p>Results on the COCO test-dev2017 Set. In <ref type="table" target="#tab_4">Table 3</ref> Note that the accuracy improvements of Dite-HRNet-18 over Lite-HRNet-18 are more significant than that of Dite-HRNet-30 over Lite-HRNet-30 on all three sets. Moreover, our small version of Dite-HRNet achieves the proximate performance to the large version of Lite-HRNet. Therefore, our proposed methods are more effective for small networks, and also much more efficient than increasing the network depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Hyper-parameters in DSC. To explore the best configuration of two hyper-parameters G, N in DSC, we conduct a 16-group ablation study on the COCO val2017 set. The results of our Dite-HRNet-18 with different configurations are shown in <ref type="table" target="#tab_8">Table 5</ref>. On each branch of Dite-HRNet-18, parameters and GFLOPs increase as N , G increase. However, large N for the upper branches as well as large G for the lower branches have less impact on the model size and computational complexity, and also achieve more improvement on accuracy. It shows a better trade-off between the accuracy   and complexity by setting G to 1, 1, 2, 4 and N to 4, 4, 2, 1 for the highest to lowest resolution branches, respectively. Effectiveness of ACM and DSC. To further verify the effectiveness of our proposed methods, we perform ablation studies on the COCO val2017 and MPII val sets. We first use Lite-HRNet-18 <ref type="bibr" target="#b5">[Yu et al., 2021]</ref> as a baseline, to which our ACM and DSC are applied separately or together, and then compare the results with our Dite-HRNet-18. <ref type="table" target="#tab_9">Table 6</ref> shows that our ACM and DSC methods bring considerable accuracy improvements to Lite-HRNet-18 with negligible additional complexity for the overall model. Further, our lightweight blocks helps Dite-HRNet-18 achieve the best results and reduce the model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, in order to address the problems that previous high-resolution networks were input-independent and lacked long-range information, we presented a Dite-HRNet to dynamically extract feature representations for human pose estimation. Our network achieved impressive efficiency on both COCO and MPII human pose estimation datasets, due to the effectiveness of the DMC and DGC blocks, which performed input-dependent convolutions by embedding DSC and captured long-range information by embedding ACM. The proposed dynamic lightweight blocks can further expand to other networks that have multi-scale representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>18 and Dite-HRNet-30, which have the network widths and depths corresponding to Lite-HRNet-18 [Yu et al., 2021] and Lite-HRNet-30 [Yu et al., 2021], respectively. Block Design. As shown in Figure 2, our DMC block and DGC block share similar overall structure, applying the channel split, feature concatenation and channel shuffle operation from ShuffleNetV2 [Ma et al., 2018] to aggregate different Structures of Dynamic Multi-scale Context (DMC) block and Dynamic Global Context (DGC) block. Dynamic Kernel Aggregation (DKA) is applied to Dynamic Split Convolution (DSC) in the DMC block and each convolution in the DGC block. DCM (Dense Context Modeling) and GCM (Global Context Modeling) are two instantiations of our proposed Adaptive Context Modeling (ACM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Procedure of adaptive context pooling. C denotes the number of channels. H and W denote the current height and width of features. H and W denote the target height and width of features. ? denotes matrix multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>than the channel weighting operations in Lite-HRNet [Yu et al., 2021].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Structure of Dite-HRNet. DGC = Dynamic Global Context. DMC = Dynamic Multi-scale Context. # = number of. ? denotes repetitions of each operator in one cross-resolution module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>GFLOPs AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Pretrain Input size #Params (M) Large networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8-stage Hourglass [Newell et al., 2016] Hourglass</cell><cell>N</cell><cell>256 ? 192</cell><cell>25.1</cell><cell>14.3</cell><cell>66.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CPN [Chen et al., 2018]</cell><cell>ResNet-50</cell><cell>Y</cell><cell>256 ? 192</cell><cell>27.0</cell><cell>6.2</cell><cell>68.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SimpleBaseline [Xiao et al., 2018]</cell><cell>ResNet-50</cell><cell>Y</cell><cell>256 ? 192</cell><cell>34.0</cell><cell>8.9</cell><cell cols="2">70.4 88.6</cell><cell>78.3</cell><cell cols="3">67.1 77.2 76.3</cell></row><row><cell>HRNet [Sun et al., 2019]</cell><cell>HRNet-W32</cell><cell>N</cell><cell>256 ? 192</cell><cell>28.5</cell><cell>7.1</cell><cell cols="2">73.4 89.5</cell><cell>80.7</cell><cell cols="3">70.2 80.1 78.9</cell></row><row><cell>UDP [Huang et al., 2020]</cell><cell>HRNet-W32</cell><cell>Y</cell><cell>256 ? 192</cell><cell>28.7</cell><cell>7.1</cell><cell cols="2">75.2 92.4</cell><cell>82.9</cell><cell cols="3">72.0 80.8 80.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Small networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MobileNetV2 1? [Sandler et al., 2018] MobileNetV2</cell><cell>N</cell><cell>256 ? 192</cell><cell>9.6</cell><cell>1.4</cell><cell cols="2">64.6 87.4</cell><cell>72.3</cell><cell cols="3">61.1 71.2 70.7</cell></row><row><cell>ShuffleNetV2 1? [Ma et al., 2018]</cell><cell>ShuffleNetV2</cell><cell>N</cell><cell>256 ? 192</cell><cell>7.6</cell><cell>1.2</cell><cell cols="2">59.9 85.4</cell><cell>66.3</cell><cell cols="3">56.6 66.2 66.4</cell></row><row><cell>Small HRNet [Wang et al., 2021]</cell><cell>HRNet-W18</cell><cell>N</cell><cell>256 ? 192</cell><cell>1.3</cell><cell>0.5</cell><cell cols="2">55.2 83.7</cell><cell>62.4</cell><cell cols="3">52.3 61.0 62.1</cell></row><row><cell>Lite-HRNet [Yu et al., 2021]</cell><cell>Lite-HRNet-18</cell><cell>N</cell><cell>256 ? 192</cell><cell>1.1</cell><cell>0.2</cell><cell cols="2">64.8 86.7</cell><cell>73.0</cell><cell cols="3">62.1 70.5 71.2</cell></row><row><cell></cell><cell>Lite-HRNet-30</cell><cell>N</cell><cell>256 ? 192</cell><cell>1.8</cell><cell>0.3</cell><cell cols="2">67.2 88.0</cell><cell>75.0</cell><cell cols="3">64.3 73.1 73.3</cell></row><row><cell>Dite-HRNet (Ours)</cell><cell>Dite-HRNet-18</cell><cell>N</cell><cell>256 ? 192</cell><cell>1.1</cell><cell>0.2</cell><cell cols="2">65.9 87.3</cell><cell>74.0</cell><cell cols="3">63.2 71.6 72.1</cell></row><row><cell></cell><cell>Dite-HRNet-30</cell><cell>N</cell><cell>256 ? 192</cell><cell>1.8</cell><cell>0.3</cell><cell cols="2">68.3 88.2</cell><cell>76.2</cell><cell cols="3">65.5 74.1 74.2</cell></row><row><cell cols="2">MobileNetV2 1? [Sandler et al., 2018] MobileNetV2</cell><cell>N</cell><cell>384 ? 288</cell><cell>9.6</cell><cell>3.3</cell><cell cols="2">67.3 87.9</cell><cell>74.3</cell><cell cols="3">62.8 74.7 72.9</cell></row><row><cell>ShuffleNetV2 1? [Ma et al., 2018]</cell><cell>ShuffleNetV2</cell><cell>N</cell><cell>384 ? 288</cell><cell>7.6</cell><cell>2.8</cell><cell cols="2">63.6 86.5</cell><cell>70.5</cell><cell cols="3">59.5 70.7 69.7</cell></row><row><cell>Small HRNet [Wang et al., 2021]</cell><cell>HRNet-W18</cell><cell>N</cell><cell>384 ? 288</cell><cell>1.3</cell><cell>1.2</cell><cell cols="2">56.0 83.8</cell><cell>63.0</cell><cell cols="3">52.4 62.6 62.6</cell></row><row><cell>Lite-HRNet [Yu et al., 2021]</cell><cell>Lite-HRNet-18</cell><cell>N</cell><cell>384 ? 288</cell><cell>1.1</cell><cell>0.4</cell><cell cols="2">67.6 87.8</cell><cell>75.0</cell><cell cols="3">64.5 73.7 73.7</cell></row><row><cell></cell><cell>Lite-HRNet-30</cell><cell>N</cell><cell>384 ? 288</cell><cell>1.8</cell><cell>0.7</cell><cell cols="2">70.4 88.7</cell><cell>77.7</cell><cell cols="3">67.5 76.3 76.2</cell></row><row><cell>Dite-HRNet (Ours)</cell><cell>Dite-HRNet-18</cell><cell>N</cell><cell>384 ? 288</cell><cell>1.1</cell><cell>0.4</cell><cell cols="2">69.0 88.0</cell><cell>76.0</cell><cell cols="3">65.5 75.5 75.0</cell></row><row><cell></cell><cell>Dite-HRNet-30</cell><cell>N</cell><cell>384 ? 288</cell><cell>1.8</cell><cell>0.7</cell><cell cols="2">71.5 88.9</cell><cell>78.2</cell><cell cols="3">68.2 77.7 77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of results on the COCO val2017 set. Pretrain = pretrain the backbone on the ImageNet classification task. Bold indicates the best result and underline indicates the highest score with the lowest #Params or FLOPs. GFLOPs AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Input size #Params (M) Large networks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimpleBaseline [Xiao et al., 2018]</cell><cell>ResNet-50</cell><cell>256 ? 192</cell><cell>34.0</cell><cell>8.9</cell><cell>70.0 90.9</cell><cell>77.9</cell><cell>66.8 75.8 75.6</cell></row><row><cell>CPN [Chen et al., 2018]</cell><cell cols="2">ResNet-Inception 384 ? 288</cell><cell>-</cell><cell>-</cell><cell>72.1 91.4</cell><cell>80.0</cell><cell>68.7 77.2 78.5</cell></row><row><cell>HRNet [Sun et al., 2019]</cell><cell>HRNet-W32</cell><cell>384 ? 288</cell><cell>28.5</cell><cell>16.0</cell><cell>74.9 92.5</cell><cell>82.8</cell><cell>71.3 80.9 80.1</cell></row><row><cell>UDP [Huang et al., 2020]</cell><cell>HRNet-W32</cell><cell>384 ? 288</cell><cell>28.7</cell><cell>16.1</cell><cell>76.1 92.5</cell><cell>83.5</cell><cell>72.8 82.0 81.3</cell></row><row><cell>DARK [Zhang et al., 2020]</cell><cell>HRNet-W48</cell><cell>384 ? 288</cell><cell>63.6</cell><cell>32.9</cell><cell>76.2 92.5</cell><cell>83.6</cell><cell>72.5 82.4 81.1</cell></row><row><cell></cell><cell></cell><cell>Small networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MobileNetV2 1? [Sandler et al., 2018] MobileNetV2</cell><cell>384 ? 288</cell><cell>9.8</cell><cell>3.3</cell><cell>66.8 90.0</cell><cell>74.0</cell><cell>62.6 73.3 72.3</cell></row><row><cell>ShuffleNetV2 1? [Ma et al., 2018]</cell><cell>ShuffleNetV2</cell><cell>384 ? 288</cell><cell>7.6</cell><cell>2.8</cell><cell>62.9 88.5</cell><cell>69.4</cell><cell>58.9 69.3 68.9</cell></row><row><cell>Small HRNet [Wang et al., 2021]</cell><cell>HRNet-W18</cell><cell>384 ? 288</cell><cell>1.3</cell><cell>1.2</cell><cell>55.2 85.8</cell><cell>61.4</cell><cell>51.7 61.2 61.5</cell></row><row><cell>Lite-HRNet [Yu et al., 2021]</cell><cell>Lite-HRNet-18</cell><cell>384 ? 288</cell><cell>1.1</cell><cell>0.4</cell><cell>66.9 89.4</cell><cell>74.4</cell><cell>64.0 72.2 72.6</cell></row><row><cell></cell><cell>Lite-HRNet-30</cell><cell>384 ? 288</cell><cell>1.8</cell><cell>0.7</cell><cell>69.7 90.7</cell><cell>77.5</cell><cell>66.9 75.0 75.4</cell></row><row><cell>Dite-HRNet (Ours)</cell><cell>Dite-HRNet-18</cell><cell>384 ? 288</cell><cell>1.1</cell><cell>0.4</cell><cell>68.4 89.9</cell><cell>75.8</cell><cell>65.2 73.8 74.4</cell></row><row><cell></cell><cell>Dite-HRNet-30</cell><cell>384 ? 288</cell><cell>1.8</cell><cell>0.7</cell><cell>70.6 90.8</cell><cell>78.2</cell><cell>67.4 76.1 76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of results on the COCO test-dev2017 set. #Params and FLOPs are computed for pose estimation, and those for human detection are not included. Bold indicates the best result and underline indicates the highest score with the lowest #Params or FLOPs.</figDesc><table><row><cell>Method</cell><cell cols="3">#Params (M) GFLOPs PCKh</cell></row><row><cell>MobileNetV2 1? [Sandler et al., 2018]</cell><cell>9.6</cell><cell>1.9</cell><cell>85.4</cell></row><row><cell>MobileNetV3 1? [Howard et al., 2019]</cell><cell>8.7</cell><cell>1.8</cell><cell>84.3</cell></row><row><cell>ShuffleNetV2 1? [Ma et al., 2018]</cell><cell>7.6</cell><cell>1.7</cell><cell>82.8</cell></row><row><cell>Small HRNet [Wang et al., 2021]</cell><cell>1.3</cell><cell>0.7</cell><cell>80.2</cell></row><row><cell>Lite-HRNet-18 [Yu et al., 2021]</cell><cell>1.1</cell><cell>0.2</cell><cell>86.1</cell></row><row><cell>Lite-HRNet-30 [Yu et al., 2021]</cell><cell>1.8</cell><cell>0.4</cell><cell>87.0</cell></row><row><cell>Dite-HRNet-18 (Ours)</cell><cell>1.1</cell><cell>0.2</cell><cell>87.0</cell></row><row><cell>Dite-HRNet-30 (Ours)</cell><cell>1.8</cell><cell>0.4</cell><cell>87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of results on the MPII val set. Bold indicates the best result and underline indicates the second-best result.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc> while having only 5% parameters and 8% GFLOPs. Despite the accuracy gap with other large networks, our networks are much more competitive in terms of the model size and computational complexity.Results on the MPII val Set. The results of our networks compared with other lightweight networks are reported in Table 4. Our Dite-HRNet-18 improves PKCh@0.5 score by 0.9 points over Lite-HRNet-18 [Yu et al., 2021] with the equivalent model complexity, has the same score but only half of GFLOPs compared to Lite-HRNet-30 [Yu et al., 2021], and outperforms MobileNetV2 [Sandler et al., 2018], Mo-bileNetV3 [Howard et al., 2019], ShuffleNetV2 [Maet al.,  2018]  and Small HRNet with much lower parameters and GFLOPs. With the same model size as Lite-HRNet-30, our Dite-HRNet-30 achieves the best result of 87.6 PKCh@0.5 among the lightweight networks.</figDesc><table><row><cell>, we</cell></row><row><cell>report the results of comparisons of our networks and other</cell></row><row><cell>state-of-the-art methods. Our Dite-HRNet achieves higher ef-</cell></row><row><cell>ficiency than both large and small networks. As compared to</cell></row><row><cell>Lite-HRNet-18 [Yu et al., 2021], Dite-HRNet-18 improves</cell></row><row><cell>AP by 1.5 points and AR by 1.8 points with the equivalent</cell></row><row><cell>model complexity. Our Dite-HRNet-30 achieves the highest</cell></row><row><cell>AP of 70.6 among the small networks, outperforming Simple-</cell></row><row><cell>Baseline [</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results of Dite-HRNet-18 with different configurations of two hyper-parameters in DSC, the number of groups G and the number of kernels N . The numbers in the header of each row or column denote the values of G or N on the highest to lowest resolution branches, respectively. The results of the model with the default configuration we choose are marked with *. The GFLOPs is computed with the input size 256 ? 192.</figDesc><table><row><cell>Method</cell><cell>#Params (M)</cell><cell cols="4">COCO MFLOPs AP MFLOPs PCKh MPII</cell></row><row><cell>Lite-HRNet-18 [Yu et al., 2021]</cell><cell>1.1</cell><cell>205.2</cell><cell>64.8</cell><cell>273.4</cell><cell>86.1</cell></row><row><cell>with ACM</cell><cell>1.0</cell><cell>210.6</cell><cell>65.1</cell><cell>280.6</cell><cell>86.5</cell></row><row><cell>with DSC</cell><cell>1.2</cell><cell>211.0</cell><cell>65.3</cell><cell>281.1</cell><cell>86.5</cell></row><row><cell>with ACM &amp; DSC</cell><cell>1.1</cell><cell>215.5</cell><cell>65.6</cell><cell>287.2</cell><cell>86.7</cell></row><row><cell>Dite-HRNet-18 (Ours)</cell><cell>1.1</cell><cell>209.8</cell><cell>65.9</cell><cell>279.5</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies on the COCO val2017 and MPII val sets. The MFLOPs is computed with the input size 256 ? 192 for the COCO val2017 set and 256 ? 256 for the MPII val set, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Copyright International Joint Conferences on Artificial Intelligence Organization (IJCAI-ECAI-22 Conference). All rights reserved.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andriluka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Mykhaylo Andriluka</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11027" to="11036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HigherHRNet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mo-bileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.15099</idno>
		<idno>arXiv:1704.04861</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
	<note>Searching for MobileNetV3</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>BMVC</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CondConv: Conditionally parameterized convolutions for efficient inference</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1305" to="1316" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lite-Hrnet ;</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A lightweight high-resolution network</title>
		<editor>Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7091" to="7100" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking bottleneck structure for efficient mobile network design</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="680" to="697" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
