<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Domain Generalization Baselines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Cappio Borlino</surname></persName>
							<email>francesco.cappio@polito.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>D&amp;apos;innocente</surname></persName>
							<email>dinnocente@diag.uniroma1.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Rome Sapienza</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Italian Institute of Technology</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
							<email>tatiana.tommasi@polito.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Italian Institute of Technology</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Domain Generalization Baselines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite being very powerful in standard learning settings, deep learning models can be extremely brittle when deployed in scenarios different from those on which they were trained. Domain generalization methods investigate this problem and data augmentation strategies have shown to be helpful tools to increase data variability, supporting model robustness across domains. In our work we focus on style transfer data augmentation and we present how it can be implemented with a simple and inexpensive strategy to improve generalization. Moreover, we analyze the behavior of current state of the art domain generalization methods when integrated with this augmentation solution: our thorough experimental evaluation shows that their original effect almost always disappears with respect to the augmented baseline. This issue open new scenarios for domain generalization research, highlighting the need of novel methods properly able to take advantage of the introduced data variability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The real world offers such a large diversity that the standard machine learning assumption of collecting train and test data under the same conditions, thus from the same domain/distribution, is broadly violated. Domain adaptation and domain generalization methods tackle this problem under different points of view. In the first case, unlabeled test data are considered available at training time, allowing the learning model to peek into the characteristics of the target set and adapt to it <ref type="bibr" target="#b0">[1]</ref>. Domain generalization is a more challenging task because target data are fed to the system only during deployment <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. In this last setting it is crucial to train robust model, possibly exploiting multiple available sources. Towards this goal, most of the existing domain generalization strategies try to incorporate the observed data invariances, capturing them at feature <ref type="bibr" target="#b3">[4]</ref> or model (metalearning <ref type="bibr" target="#b4">[5]</ref> and self-supervision <ref type="bibr" target="#b5">[6]</ref>) level, in the hypothesis that analogous invariances hold for future test domains. An alternative solution consists in extending the source domains by synthesizing new images. This is usually done by learning generative models with the specific constraint of preserving the object content but varying the global image appearance, with the aim of better spanning the data space and include a larger variability in the training set. Thanks to the developments in generative learning, it is becoming more and more evident that their integration into domain generalization approaches is effective <ref type="bibr" target="#b6">[7]</ref>. However their performance tends to grow together with the complexity of the learning procedure which may involve one or multiple generator modules and adversarial training. We also noticed a particular trend in the most recent domain generalization research. Several papers discuss the merit of the proposed data augmentation solutions in comparison with feature and model-based generalization techniques <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Still, newly introduced feature and model-based approaches avoid benchmarks against data augmentation strategies, probably considering them unfair competitors due to the extended training set <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We believe that the field needs some clarification and we dedicate our work on this topic. Specifically our main contributions are:</p><p>? A simple and effective style transfer data augmentation approach for domain generalization. We show how the method AdaIN <ref type="bibr" target="#b10">[11]</ref>, that is able to perform style transfer in real time, can be re-purposed for data augmentation, combining semantic and texture information of the available source data (see <ref type="figure">Figure 1</ref>). The extended training set allows to get top target results, outperforming existing state of the art approaches.</p><p>? We designed tailored strategies to integrate for the first time style transfer data augmentation with the current state of the art approaches. The obtained results indicate that the original advantage of those methods almost always disappears when compared with the data augmented baseline.</p><p>The scenario described by this analysis clearly suggests the need of rethinking domain generalization baselines. On one side simple data augmentation strategies should be envisaged to increase source data variability compatible with orthogonal feature and model generalization approaches. On the other, new cross-source adaptive strategies should be designed to build over images generated by style transfer approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The literature of domain generalization (DG) grew fast in the last years. Existing methods can be roughly divided into four main groups. Feature Alignment approaches inherit the standard strategy adopted in domain adaptation which consists in measuring domain distances and learning a representation that reduce them. In the DG setting, this condition is applied among the available sources through MMD discrepancy constraints <ref type="bibr" target="#b3">[4]</ref> or using metric learning (contrastive loss) <ref type="bibr" target="#b11">[12]</ref> and adversarial domain classifiers <ref type="bibr" target="#b12">[13]</ref>.</p><p>Meta-Learning solutions separate the sources in meta-train and meta-test: a model is learned on the former with the real goal of reducing the error on the latter. In this way it is possible to get ready to the domain shift that will be experienced on the actual target. Two among the most well known approaches exploit episodic training with <ref type="bibr" target="#b13">[14]</ref>, or without <ref type="bibr" target="#b4">[5]</ref> an ad hoc gradient descent update rule. Another meta-learning strategy presented in <ref type="bibr" target="#b14">[15]</ref> formulates a novel regularization function. Self-supervised learning has recently shown to support generalization. In <ref type="bibr" target="#b15">[16]</ref> the jigsaw puzzle task was solved as auxiliary objective together with supervised object classification, helping it to focus on the object parts and their shape rather than on domain specific texture. A similar solution was also adopted in <ref type="bibr" target="#b16">[17]</ref> using rotation recognition as side task for cross-domain detection. Before self-supervision, unsupervised learning already demonstrated a beneficial effect on generalization through reconstruction <ref type="bibr" target="#b17">[18]</ref> and clustering <ref type="bibr" target="#b18">[19]</ref> tasks.</p><p>Data Augmentation strategies allow to increase the source diversity: a model learned on those data gains robustness against specific features of the seen domains. Several approaches have been proposed to generate new samples, from the simple random changing of color or background in case of synthetic objects and robotics applications <ref type="bibr" target="#b19">[20]</ref>, to the most complex use of adversarial gradients <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Domain Mixup can also be included in the data augmentation methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>: pairs of examples from different domains are interpolated together with their label to learn on a more continuous domaininvariant data distribution. Finally, style transfer approaches can be used to define a specific form of data augmentation. Those methods were originally defined to match the style and content from two different images and produce a new combined visual sample. Some approaches involve complex GAN-based architectures <ref type="bibr" target="#b23">[24]</ref>, while others simply rely on data statistics and can be easily re-purposed for domain generalization <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SOURCE AUGMENTATION BY STYLE TRANSFER</head><p>We focus on the multi-source domain generalization setting where S = {S 1 , . . . , S n } denotes the n available data sources with the respective {x s i , y s i } Ns i=1 ? S samples, where y i specifies the object classification label of its x i image. The main goal is to generalize to an unknown target database</p><formula xml:id="formula_0">{x t i , y t i } Nt i=1 ? T ,</formula><p>where T shares with S the same set of categories, while each source and the target are drawn from different marginal distributions.</p><p>We indicate with C(x s , ? c ) a basic deep learning classifier parametrized by ? c and trained on the source data by minimizing the standard cross-entropy loss L(C(x s , ? c ), y s ). To increase data variability we study how to augment each sample x s by keeping its semantic content and changing the image style, borrowing it from the other available source data. The stylized samplex s obtained from x s inherits its label y s and enriches the training set, possibly making the model learned by optimizing L(C(x s , ? c ), y s ) more robust to domain shifts. Thus, our analysis will consider a two step process, where a deep model A parametrized by ? a is first learned on the source data to perform style transfer x s ?x s = A(x s , ? a ), and then it is used to perform data augmentation at runtime while learning to classify the image object content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training the Style Transfer Model</head><p>To implement A we use AdaIN <ref type="bibr" target="#b10">[11]</ref>, a simple and effective encoder-decoder-based approach that allows style transfer in real time. The encoder E extracts representative features f c , f s respectively from the content and the style image, the first are then re-normalized to have the same channel-wise mean and standard deviation of the second as follows:</p><formula xml:id="formula_1">f cs = ?(f s ) f c ? ?(f c ) ?(f c ) + ?(f s ) .<label>(1)</label></formula><p>Finally, the obtained feature f cs is mapped back to the image space through the decoder D minimizing two losses:</p><formula xml:id="formula_2">L A = L c + ?L s .<label>(2)</label></formula><p>Both the losses measure the distance between the features reextracted through the encoder E(D(f cs )) from the stylized output image, and f cs . Specifically L c focuses on the content information considering the whole final feature output, while L s focuses on the style information, measuring the difference of mean and standard deviation of the Relu output of several encoder layers. The method has two main hyperparameters ? a = {?, ?}. The first controls the degree of the style transfer during training by adjusting the importance of the style loss and is generally kept fixed at ? = 10. The second allows a content-style trade-off at test time by interpolating between the feature maps that are fed to the decoder with f cs? = D((1 ? ?)f c + ?f cs ). When ? = 0 the network tries to reconstruct the content image, while when ? = 1 it produces the most stylized image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Style Transfer as Data Augmentation</head><p>When training our object classifier C the data batches contain samples extracted from all the source domains. The samples are augmented by randomly applying the style augmentation as depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. Each sample in a batch has the role of content image and any of the remaining instances in the same batch can be selected randomly to work as style provider. In this scenario stylization can happen both from images of the same source domain (e.g. two photos) or from images of different domains (e.g. a photo and a painting). To regulate this process we use a stochastic approach with the transformed imagex s replacing its original version x s with probability p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We designed our experimental analysis with the aim of running a thorough evaluation of the impact of style transfer data augmentation on domain generalization. Besides observing how this data augmentation can improve the standard learning baseline model, and how it compares with the most recent state of the art DG methods, we are also interested in the effectiveness of their combination. In the following we provide details on the chosen data testbeds and sota models, describing how the data augmentation strategy is integrated in each approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We consider three standard benchmark datasets which differ in number of classes and covered domains. a) PACS <ref type="bibr" target="#b24">[25]</ref>: contains images of 7 object classes spanning 4 visual domains: Photo, Art Painting, Cartoon, Sketch. Given that the visual domains go from real world representations to artistic images, the style variability is quite large. We follow the original experimental protocol by training on the train splits of three source domains (using the validation splits for model selection), and then testing on the whole left out domain which acts as unknown target. b) OfficeHome <ref type="bibr" target="#b25">[26]</ref>: is similar to PACS, it covers 4 domains (Art, Clipart, Product and Real-World) but shows a much larger set of 65 object classes. We adopt the same experimental protocol of <ref type="bibr" target="#b26">[27]</ref>: a random 90-10 train-val split is used to select the training images for the 3 source domains (once again the validation images are used for model selection) and testing is performed on the whole left out target domain. c) VLCS <ref type="bibr" target="#b27">[28]</ref>: is built upon 4 different datasets: PAS-CAL VOC 2007, Labelme, Caltech and SUN and contains 5 object categories. Differently from the other considered testbeds, all the domains are composed of real world photos with the shift mainly due to camera type, illumination conditions, point of view, etc. Moreover, while Caltech is composed by object-centered images, the other three domains contain scene images. We apply the same experimental protocol of <ref type="bibr" target="#b15">[16]</ref>: the predefined full training data is randomly partitioned in train and validation sets with a 90-10 ratio. The training is performed on the train splits of the 3 source domains while the validation splits are used for model selection. At the end the model is tested on the predefined test split of the left out domain. This split has been defined randomly by selecting 30% of images of the overall dataset.</p><p>All our results are obtained by performing an average over 3 runs. In the case of both OfficeHome and VLCS the random 90-10 train-val split was repeated for each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison methods</head><p>For our study we consider as main Baseline a classification model learned on all the source data and na?vely applied on the target. We indicate with Original the standard data augmentation with horizontal flippling and random cropping, while we use Stylized to specify the cases where we add style transfer data augmentation. The behavior of four among the most recent DG methods is evaluated under both these augmentation settings. We dedicate a particular attention to the integration of the style transfer data augmentation strategy with each of the considered approaches. The goal is getting the most out of them without undermining their nature. In particular, considering that the style transfer leads to domain mixing, it is important to not integrate it in procedures that need a separation among source domains. a) DG-MMLD <ref type="bibr" target="#b18">[19]</ref>: this approach exploits clustering and domain adversarial feature alignment. Since it does not need the source domain labels, the integration of the proposed style transfer data augmentation is straightforward: styles of random images are applied to each content images (inside a batch) with probability p, exactly as done for the Baseline.</p><p>b) Epi-FCR <ref type="bibr" target="#b4">[5]</ref>: is a meta-learning method which splits the network in two modules, each one is trained by pairing it with a partner that is badly tuned for the domain considered in the current learning episode. The modules are the feature extractor and the classifier which alternatively cover the two roles of learning part and bad reference. After this phase, a final model is learned by integrating the trained modules together with a random classifier used as regularizer. In the first stage, knowing the source domain labels is crucial to choose and set the two network modules, thus mixing the domains with style transfer augmentation could degrade its performance. In the ending stage instead, all the source data are considered together: we applied here the style data augmentation.</p><p>c) DDAIG: <ref type="bibr" target="#b6">[7]</ref> is a data augmentation strategy based on a transformation network which is trained so that every synthesized sample keeps the same label of the original image, but fools a domain classifier. In the learning procedure the transformation module, the label classifier and the domain classifier are iteratively updated. In particular the label classifier is trained on all the source data, both original and synthetic: we further extended this set with style transfer augmented data. d) Rotation <ref type="bibr" target="#b5">[6]</ref>: it has been shown that self-supervised knowledge supports domain generalization when combined with supervised learning in a multi-task model. In particular we focused on rotation recognition, where the orientation angle of each image should be recognized among {0 ? , 90 ? , 180 ? , 270 ? }. The model minimizes a linear combination of the supervised and self-supervised loss with weight ? generally kept lower than 1 to let the supervised model guide the learning process. In this case the domain labels are not used during training, so the application of the source augmentation by style transfer is straightforward.</p><p>An approach related to data augmentation, originally defined to improve generalization in standard in-domain learning, is Mixup <ref type="bibr" target="#b21">[22]</ref>: it interpolates samples and their labels, regularizing a neural network to favor a simple linear behavior between training examples. Its hyper-parameter ? ? {0, ?} controls the strength of interpolation between data pairs, recovering the Baseline for ? = 0. In our study we consider Mixup as further reference, and in particular we tested data mixing both at pixel and at feature level <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training setup</head><p>Our style transfer model A is trained on source data before training the classification model C. As already mentioned, A is implemented by AdaIN <ref type="bibr" target="#b10">[11]</ref> and is therefore based on a VGG backbone. It is trained for 20 epochs with a learning rate equal to 5e-5. The hyperparameters ? and p used in each experiment are specified in the caption of the respective result tables and in depth analysis on the sensitivity of the method to them is presented in Section IV-E. For the classification model C we use AlexNet and ResNet18 backbones. Specifically, Baseline, Rotation and Mixup are trained using SGD with 0.9 momentum for 30k iterations. We set the batch size to 32 images per source domain: since in all the testbed there are three source domains each data batch contains 96 images. The learning rate and the weigh decay are respectively fixed to 0.001 and 0.0001. Regarding the hyperparameters of the individual algorithms, we empirically set the Rotation auxiliary weight to ? = 0.5 and for Mixup ? = 0.4.</p><p>We implement Rotation by adding a rotation recognition branch to our Baseline. For DG-MMLD, Epi-FCR and DDAIG, we use the code provided by the authors integrating different datasets/backbones where needed. The training setup for these experiments is the one defined in their papers for both the Original and Stylized version. We report the previously published results whenever possible. In the following we will indicate with a star ( * ) the results we obtained by running the authors' code. <ref type="table" target="#tab_1">Table I</ref> shows results on PACS benchmark with both AlexNet and ResNet18 backbones. We get two main outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results analysis</head><p>(1) There is an evident improvement of more than 5 percentage points in the Baseline performance when using the stylized augmented source data with respect to the original case. Looking at the results for the different domains we can see that improvement is higher for Art Painting, Cartoon and Sketch, than in Photo. (2) All the considered state of the art DG methods benefit from the source augmentation. Indeed in absolute terms their performance grows, but at the same time they lose in effectiveness as they cannot outperform the Baseline any more. <ref type="table" target="#tab_1">Table II</ref> shows results on OfficeHome dataset with ResNet18 backbone. Even if in this case the improvement produced by the source augmentation by style transfer is more limited, the results confirm what we have already observed for PACS. The Stylized Baseline obtains the best accuracy 0.25 0.5 0.75 0.9 1 p = 0.1 p = 0.25 p = 0.5 p = 0.75 p = 0.9 outperforming the competitor state of the art methods, even when those are improved using the same source augmentation. <ref type="table" target="#tab_1">Table III</ref> reports results on VLCS benchmark with AlexNet backbone. This dataset is particularly challenging and shows a fundamental limit of tackling DG through style transfer data augmentation. Since the domain shift is not originally due to style differences in this testbed, source augmentation by style transfer does not support generalization.</p><p>As a final remark, we focus on Mixup. The results over all the considered datasets show that it is not able to generalize across domains and it might perform even worse that the Original Baseline. Between the two considered pixel and feature variants, only the second shows some advantage on PACS, so we focused on it in the other tests. Still, its results remain lower than those obtained by the DG methods both with and without style based data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analysis of AdaIN hyperparameters</head><p>In <ref type="figure">Figures 3 and 4</ref> we see how the PACS AlexNet results change when varying either ? or p by keeping the other fixed. With a low value of ? the style transfer is too weak to produce an effective appearance change of the source sample and introduce extra variability. In general the best results are obtained using ? = 1 regardless of the specific value of p.</p><p>For what concerns the value of p we can see that, if ? is high enough, even a small p allows to obtain good performance with the best results obtained with p = 0.5 or p = 0.75.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Style transfer from external data vs source data</head><p>The described procedure for the application of AdaIN differs from what appeared in previous works. Indeed, both the original approach <ref type="bibr" target="#b10">[11]</ref> and its use for data augmentation in <ref type="bibr" target="#b7">[8]</ref>, exploit the style transfer model trained on MS-COCO <ref type="bibr" target="#b28">[29]</ref> as content images, and paintings mostly collected from WikiArt <ref type="bibr" target="#b29">[30]</ref> as style images. In our study we did not allow extra datasets besides those directly involved in the domain generalization task as source domains. The reason is twofold: first, we want to keep the method as simple as possible, without the need of relying on external data; second, to perform a fair benchmark with the competitors DG methods all of them should have access to the same source information.</p><p>Still, the interested reader may wonder what would be the effect of using the original AdaIN model trained on MSCOCO and WikiArt. <ref type="figure" target="#fig_2">Figure 5</ref> shows one example obtained in this way. Specifically we consider a dog image drawn from the PACS Photo domain and we analyse the images obtained by borrowing the style form the Art Painting guitar image. We compare the stylized sample produced with the MSCOCO-WikiArt AdaIN model against the outcomes of the four AdaIN variants trained on the source with every one of the four domains used as target.</p><p>As can be observed, the obtained results in terms of image quality are not so different. We also run a quantitative analysis: in <ref type="table" target="#tab_1">Table IV</ref> we compare the performance of the our Stylized Baseline on PACS AlexNet with the analogous Baseline trained using the augmented data produced with the AdaIN MSCOCO-WikiArt pretrained model. The last one shows a slightly better accuracy which is though not significant if we consider the related standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>Among the current state of the art domain generalization methods some are based on data augmentation and use complex generative approaches, while other propose source feature adaptation and meta-learning strategies. Despite being orthogonal among each other, no previous work tried to integrate them. We investigated here a simple and effective style transfer data augmentation strategy for domain generalization and we showed how it overcomes its competitors. Moreover we designed proper combination of this approach with the most relevant existing DG approaches. Our experimental analysis indicates that the performance of the considered methods improves over the respective versions not including the style data augmentation, but surprisingly the methods lose their original effectiveness, not showing any improvement over the new data augmented baseline.</p><p>As other concurrent technical reports <ref type="bibr" target="#b30">[31]</ref>, our work suggests the need of shading new light on domain generalization and calls for novel strategies able to take advantage of the data variability introduced by cross-domain style transfer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Classifier's training pipeline. Each training sample is augmented by borrowing the style from other images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Average accuracy on PACS AlexNet with different values of p when varying ?. Average accuracy on PACS AlexNet with different values of ? when varying p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Example of application of style transfer using AdaIN. The top left image comes from the PACS Photo domain and is used as content while the top center image comes from PACS Art Painting domain and is used as style image. On top right there is the translation performed using AdaIN trained on MS-COCO and WikiArt images. In the second row we see the translations performed using our AdaIN models trained on source data only, respectively when the Art Paintings, Cartoon, Sketch and Photo domains are used as style sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PACS</head><label>I</label><figDesc>CLASSIFICATION ACCURACY (%). WE USED ADAIN WITH ? = 1.0 AND p = 0.75 FOR ALEXNET-BASED EXPERIMENTS AND ADAIN WITH ? = 1.0 AND p = 0.90 FOR THOSE BASED ON RESNET18.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell></row><row><cell></cell><cell></cell><cell cols="2">AlexNet</cell><cell></cell><cell></cell><cell>74</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="3">Painting Cartoon Sketch Photo 66.83 70.85 59.75 89.78</cell><cell>Average 71.80</cell><cell>72</cell></row><row><cell>Original</cell><cell>Rotation DG-MMLD</cell><cell>65.66 69.27</cell><cell cols="2">71.89 62.15 89.88 72.83 66.44 88.98</cell><cell>72.39 74.38</cell><cell>70</cell></row><row><cell></cell><cell>Epi-FCR</cell><cell>64.70</cell><cell cols="2">72.30 65.00 86.10</cell><cell>72.03</cell><cell>68</cell></row><row><cell></cell><cell>DDAIG*</cell><cell>62.77</cell><cell cols="2">67.06 58.90 86.82</cell><cell>68.89</cell></row><row><cell></cell><cell>Baseline</cell><cell>71.96</cell><cell cols="2">72.47 76.47 88.34</cell><cell>77.31</cell></row><row><cell></cell><cell>Rotation</cell><cell>71.74</cell><cell cols="2">73.39 75.98 89.22</cell><cell>77.59</cell><cell>?</cell></row><row><cell>Stylized</cell><cell>DG-MMLD</cell><cell>70.50</cell><cell cols="2">70.84 75.39 88.43</cell><cell>76.29</cell></row><row><cell></cell><cell>Epi-FCR</cell><cell>65.19</cell><cell cols="2">69.54 71.97 83.43</cell><cell>72.53</cell></row><row><cell></cell><cell>DDAIG</cell><cell>69.35</cell><cell cols="2">71.10 70.99 87.70</cell><cell>74.79</cell></row><row><cell>Mixup</cell><cell>pixel-level feature-level</cell><cell>66.03 67.04</cell><cell cols="2">68.00 51.18 88.90 69.10 55.40 88.88</cell><cell>68.53 70.11</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet18</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Baseline</cell><cell>77.28</cell><cell cols="2">73.89 67.01 95.83</cell><cell>78.50</cell></row><row><cell></cell><cell>Rotation</cell><cell>78.16</cell><cell cols="2">76.64 72.20 95.57</cell><cell>80.64</cell></row><row><cell>Original</cell><cell>DG-MMLD</cell><cell>81.28</cell><cell cols="2">77.16 72.29 96.06</cell><cell>81.83</cell></row><row><cell></cell><cell>Epi-FCR</cell><cell>82.10</cell><cell cols="2">77.00 73.00 93.90</cell><cell>81.50</cell></row><row><cell></cell><cell>DDAIG*</cell><cell>79.41</cell><cell cols="2">74.81 69.29 95.22</cell><cell>79.68</cell></row><row><cell></cell><cell>Baseline</cell><cell>82.73</cell><cell cols="2">77.97 81.61 94.95</cell><cell>84.32</cell></row><row><cell></cell><cell>Rotation</cell><cell>79.51</cell><cell cols="2">79.93 82.01 93.55</cell><cell>83.75</cell></row><row><cell>Stylized</cell><cell>DG-MMLD Epi-FCR</cell><cell>80.85 80.68</cell><cell cols="2">77.10 77.69 95.11 78.87 76.57 92.50</cell><cell>82.69 82.15</cell></row><row><cell></cell><cell>DDAIG</cell><cell>81.02</cell><cell cols="2">78.75 79.67 95.07</cell><cell>83.63</cell></row><row><cell>Mixup</cell><cell>pixel-level feature-level</cell><cell>78.09 81.20</cell><cell cols="2">71.08 66.58 93.85 76.41 69.67 96.31</cell><cell>77.40 80.90</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="6">OFFICEHOME CLASSIFICATION ACCURACY (%). WE USED ADAIN WITH</cell></row><row><cell></cell><cell cols="4">PARAMETERS ? = 1.0 AND p = 0.1.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet18</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Art Clipart Product Real World</cell><cell>Average</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">57.14 46.96 73.50</cell><cell>75.72</cell><cell>63.33</cell></row><row><cell></cell><cell>Rotation</cell><cell cols="2">55.94 47.26 72.38</cell><cell>74.84</cell><cell>62.61</cell></row><row><cell>Original</cell><cell cols="3">DG-MMLD* 58.08 49.32 72.91</cell><cell>74.69</cell><cell>63.75</cell></row><row><cell></cell><cell>Epi-FCR*</cell><cell cols="2">53.34 49.66 68.56</cell><cell>70.14</cell><cell>60.43</cell></row><row><cell></cell><cell>DDAIG*</cell><cell cols="2">57.79 48.32 73.28</cell><cell>74.99</cell><cell>63.59</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">58.71 52.33 72.95</cell><cell>75.00</cell><cell>64.75</cell></row><row><cell></cell><cell>Rotation</cell><cell cols="2">57.24 52.15 72.33</cell><cell>73.66</cell><cell>63.85</cell></row><row><cell>Stylized</cell><cell>DG-MMLD</cell><cell cols="2">59.24 49.30 73.56</cell><cell>75.85</cell><cell>64.49</cell></row><row><cell></cell><cell>Epi-FCR</cell><cell cols="2">52.97 50.14 67.03</cell><cell>70.66</cell><cell>60.20</cell></row><row><cell></cell><cell>DDAIG</cell><cell cols="2">58.21 50.26 73.81</cell><cell>74.99</cell><cell>64.32</cell></row><row><cell>Mixup</cell><cell>feature-level</cell><cell cols="2">58.33 39.76 70.96</cell><cell>72.07</cell><cell>60.28</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="6">VLCS CLASSIFICATION ACCURACY (%). WE USED ADAIN WITH</cell></row><row><cell></cell><cell cols="4">PARAMETERS ARE ? = 1.0 AND p = 0.75.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">AlexNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">CALTECH LABELME PASCAL SUN</cell><cell>Average</cell></row><row><cell></cell><cell>Baseline</cell><cell>94.89</cell><cell>59.14</cell><cell>71.31 64.64</cell><cell>72.49</cell></row><row><cell></cell><cell>Rotation</cell><cell>94.50</cell><cell>61.27</cell><cell>68.94 63.28</cell><cell>72.00</cell></row><row><cell>Original</cell><cell>DG-MMLD*</cell><cell>96.94</cell><cell>59.10</cell><cell>68.48 62.06</cell><cell>71.64</cell></row><row><cell></cell><cell>Epi-FCR*</cell><cell>91.43</cell><cell>61.36</cell><cell>63.44 60.07</cell><cell>69.07</cell></row><row><cell></cell><cell>DDAIG*</cell><cell>95.75</cell><cell>60.18</cell><cell>65.48 60.78</cell><cell>70.55</cell></row><row><cell></cell><cell>Baseline</cell><cell>96.86</cell><cell>60.77</cell><cell>68.18 63.42</cell><cell>72.31</cell></row><row><cell></cell><cell>Rotation</cell><cell>96.86</cell><cell>60.77</cell><cell>68.18 63.42</cell><cell>72.31</cell></row><row><cell>Stylized</cell><cell>DG-MMLD</cell><cell>97.49</cell><cell>61.02</cell><cell>64.23 62.37</cell><cell>71.28</cell></row><row><cell></cell><cell>Epi-FCR</cell><cell>92.69</cell><cell>58.18</cell><cell>62.59 57.87</cell><cell>67.83</cell></row><row><cell></cell><cell>DDAIG</cell><cell>97.48</cell><cell>60.48</cell><cell>65.19 62.57</cell><cell>71.43</cell></row><row><cell>Mixup</cell><cell>feature-level</cell><cell>94.73</cell><cell>62.15</cell><cell>69.82 62.98</cell><cell>72.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">OF ADAIN TRAINING STRATEGIES</cell><cell></cell></row><row><cell></cell><cell cols="2">Art Painting Cartoon Sketch Photo</cell><cell>Average</cell></row><row><cell>Stylized Baseline</cell><cell>71.96</cell><cell cols="2">72.47 76.47 88.34 77.31 ? 1.1</cell></row><row><cell>MSCOCO-WikiArt Baseline</cell><cell>73.00</cell><cell>73.78 76.37 89.04</cell><cell>78.05 ? 0.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Computational resources provided by hpc@polito: (http://hpc.polito.it).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Domain Adaptation in Computer Vision Applications, ser. Advances in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised domain adaptation for computer vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="156" to="694" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep domainadversarial image generation for domain generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning robust shape-based features for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="63" to="748" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One-shot unsupervised cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Borlino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Domain Separation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain generalization with domainspecific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Painter by numbers, WikiArt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/painter-by-numbers" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">In search of lost domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01434</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
