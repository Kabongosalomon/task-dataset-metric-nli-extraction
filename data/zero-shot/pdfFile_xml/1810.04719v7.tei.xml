<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FULLY SUPERVISED SPEAKER DIARIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aonan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
							<email>quanw@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
							<email>zyzhu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
							<email>jpaisley@columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
							<email>chongw@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FULLY SUPERVISED SPEAKER DIARIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speaker diarization</term>
					<term>d-vector</term>
					<term>clustering</term>
					<term>recur- rent neural networks</term>
					<term>Chinese restaurant process</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a fully supervised speaker diarization approach, named unbounded interleaved-state recurrent neural networks (UIS-RNN). Given extracted speaker-discriminative embeddings (a.k.a. d-vectors) from input utterances, each individual speaker is modeled by a parameter-sharing RNN, while the RNN states for different speakers interleave in the time domain. This RNN is naturally integrated with a distance-dependent Chinese restaurant process (ddCRP) to accommodate an unknown number of speakers. Our system is fully supervised and is able to learn from examples where time-stamped speaker labels are annotated. We achieved a 7.6% diarization error rate on NIST SRE 2000 CALLHOME, which is better than the state-of-the-art method using spectral clustering. Moreover, our method decodes in an online fashion while most state-of-the-art systems rely on offline clustering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Aiming to solve the problem of "who spoke when", most existing speaker diarization systems consist of multiple relatively independent components <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, including but not limited to: (1) A speech segmentation module, which removes the non-speech parts, and divides the input utterance into small segments; (2) An embedding extraction module, where speaker-discriminative embeddings such as speaker factors <ref type="bibr" target="#b3">[4]</ref>, i-vectors <ref type="bibr" target="#b5">[5]</ref>, or d-vectors <ref type="bibr" target="#b6">[6]</ref> are extracted from the small segments; (3) A clustering module, which determines the number of speakers, and assigns speaker identities to each segment; (4) A resegmentation module, which further refines the diarization results by enforcing additional constraints <ref type="bibr" target="#b0">[1]</ref>.</p><p>For the embedding extraction module, recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">7]</ref> has shown that the diarization performance can be significantly improved by replacing i-vectors <ref type="bibr" target="#b5">[5]</ref> with neural network embeddings, a.k.a. d-vectors <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>. This is largely due to the fact that neural networks can be trained with big datasets, such that the model is sufficiently robust against varying speaker accents and acoustic conditions in different use scenarios.</p><p>However, there is still one component that is unsupervised in most modern speaker diarization systems -the clustering module. Examples of clustering algorithms that have been used in diarization systems include Gaussian mixture models <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>, mean shift <ref type="bibr" target="#b10">[10]</ref>, agglomerative hierarchical clustering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">11]</ref>, k-means <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">12]</ref>, Links <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">13]</ref>, and spectral clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>The first author performed this work as an intern at Google. The implementation of the algorithms in this paper is available at: https://github.com/google/uis-rnn Since both the number of speakers and the segment-wise speaker labels are determined by the clustering module, the quality of the clustering algorithm is critically important to the final diarization performance. However, the fact that most clustering algorithms are unsupervised means that, we will not able to improve this module by learning from examples when the time-stamped speaker labels ground truth are available. In fact, in many domain-specific applications, it is relatively easy to obtain such high quality annotated data.</p><p>In this paper, we replace the unsupervised clustering module by an online generative process that naturally incorporates labelled data for training. We call this method unbounded interleaved-state recurrent neural network (UIS-RNN), based on these facts: (1) Each speaker is modeled by an instance of RNN, and these instances share the same parameters; (2) An unbounded number of RNN instances can be generated; (3) The states of different RNN instances, corresponding to different speakers, are interleaved in the time domain. Within a fully supervised framework, our method in addition handles complexities in speaker diarization: it automatically learns the number of speakers within each utterance via a Bayesian non-parametric process, and it carries information through time via the RNN.</p><p>The contributions of our work are summarized as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASELINE SYSTEM USING CLUSTERING</head><p>Our diarization system is built on top of the recent work by Wang et al. <ref type="bibr" target="#b2">[3]</ref>. Specifically, we use exactly the same segmentation module and embedding extraction module as their system, while replacing their clustering module by an unbounded interleaved-state RNN.</p><p>As a brief review, in the baseline system <ref type="bibr" target="#b2">[3]</ref>, a text-independent speaker recognition network is used to extract embeddings from sliding windows of size 240ms and 50% overlap. A simple voice activity detector (VAD) with only two full-covariance Gaussians is used to remove non-speech parts, and partition the utterance into nonoverlapping segments with max length of 400ms. Then we average window-level embeddings to segment-level d-vectors, and feed them into the clustering algorithm to produce final diarization results. The workflow of this baseline system is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The text-independent speaker recognition network for computing embeddings has three LSTM layers and one linear layer. The network is trained with the state-of-the-art generalized end-to-end loss <ref type="bibr" target="#b6">[6]</ref>. We have been retraining this model for better performance, which will be later discussed in Section 4.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">UNBOUNDED INTERLEAVED-STATE RNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of approach</head><p>Given an utterance, from the embedding extraction module, we get an observation sequence of embeddings X = (x1, x2, . . . , xT ), where each xt ? R d . Each entry in this sequence is a real-valued d-vector corresponding to a segment in the original utterance. In the supervised speaker diarization scenario, we also have the ground truth speaker labels for each segment Y = (y1, y2, . . . , yT ). Without loss of generality, let Y be a sequence of positive integers by the order of appearance. For example, Y = (1, 1, 2, 3, 2, 2) means this utterance has six segments, from three different speakers, where yt = k means segment t belongs to speaker k.</p><p>UIS-RNN is an online generative process of an entire utterance (X, Y), where 1</p><formula xml:id="formula_0">p(X, Y) = p(x1, y1) ? T t=2 p(xt, yt|x [t?1] , y [t?1] ).</formula><p>(1)</p><p>To model speaker changes, we use an augmented representation</p><formula xml:id="formula_1">p(X, Y, Z) = p(x1, y1)? T t=2 p(xt, yt, zt|x [t?1] , y [t?1] , z [t?1] ),<label>(2)</label></formula><p>where Z = (z2, . . . , zT ), and zt</p><formula xml:id="formula_2">= 1(yt = yt?1) ? {0, 1}</formula><p>is a binary indicator for speaker changes. For example, if Y = (1, 1, 2, 3, 2, 2), then Z = (0, 1, 1, 1, 0). Note that Z is uniquely determined by Y, but Y cannot be uniquely determined by a given Z, since we don't know which speaker we are changing to. Here we leave z1 undefined, and factorize each product term in Eq. (2) as three parts that separately model sequence generation, speaker assignment, and speaker change:</p><formula xml:id="formula_3">p(xt, yt, zt|x [t?1] , y [t?1] , z [t?1] ) = p(xt|x [t?1] , y [t] ) sequence generation ? p(yt|zt, y [t?1] ) speaker assignment ? p(zt|z [t?1] ) speaker change .<label>(3)</label></formula><p>For the first entry of the sequence, we let y1 = 1 and there is no need to model speaker assignment and speaker change. In Section 3.2, we introduce these components separately. <ref type="bibr" target="#b0">1</ref> We denote an ordered set (1, 2, . . . , t) as [t].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Details on model components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Speaker change</head><p>We assume the probability of zt ? {0, 1} follows:</p><formula xml:id="formula_4">p(zt = 0|z [t?1] , ? ? ?) = g ? ? ? (z [t?1] ),<label>(4)</label></formula><p>where g ? ? ? (?) is a function paramaterized by ? ? ?. Since zt indicates speaker change at time t, we have</p><formula xml:id="formula_5">p(yt = yt?1|zt, y [t?1] ) = 1 ? zt.<label>(5)</label></formula><p>In general, g ? ? ? (?) could be any function, such as an RNN. But for simplicy, in this work, we make it a constant value</p><formula xml:id="formula_6">g ? ? ? (z [t?1] ) = p0 ? [0, 1]</formula><p>. This means {zt} t? <ref type="bibr">[2,T ]</ref> are independent binary variables parameterized by ? ? ? = {p0}:</p><formula xml:id="formula_7">zt ? iid. Binary(p0).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Speaker assignment process</head><p>One of the biggest challenges in speaker diarization is to determine the total number of speakers for each utterance. To model the speaker turn behavior in an utterance, we use a distance dependent Chinese restaurant process (ddCRP) <ref type="bibr" target="#b15">[15]</ref>, a Bayesian nonparametric model that can potentially model an unbounded number of speakers. Specifically, when zt = 0, the speaker remains unchanged. When zt = 1, we let</p><formula xml:id="formula_8">p(yt = k|zt = 1, y [t?1] ) ? N k,t?1 , p(yt = Kt?1 + 1|zt = 1, y [t?1] ) ? ?.<label>(7)</label></formula><p>Here Kt?1 := max y [t?1] is the total number of unique speakers up to the (t ? 1)-th entry. Since zt = 1 indicates a speaker change, we have k ? [Kt?1] \ {yt?1}. In addition, we let N k,t?1 be the number of blocks for speaker k in y [t <ref type="bibr">?1]</ref> . A block is defined as a maximum-length subsequence of continuous segments that belongs to a single speaker. For example, if y <ref type="bibr" target="#b6">[6]</ref> = (1, 1, 2, 3, 2, 2), then there are four blocks (1, 1)|(2)|(3)|(2, 2) separated by the vertical bar, with N1,5 = 1, N2,5 = 2, N3,5 = 1. The probability of switching back to a previously appeared speaker is proportional to the number of continuous speeches she/he has spoken. There is also a chance to switch to a new speaker, with a probability proportional to a constant ?. The joint distribution of Y given Z is</p><formula xml:id="formula_9">p(Y|Z, ?) = ? K T ?1 K T k=1 ?(N k,T ) T t=2 ( k?[K t?1 ]\{y t?1 } N k,t?1 + ?) 1(z t =1)</formula><p>. (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Sequence generation</head><p>Our basic assumption is that, the observation sequence of speaker embeddings X is generated by distributions that are parameterized by the output of an RNN. This RNN has multiple instantiations, corresponding to different speakers, and they share the same set of RNN parameters ? ? ?. In our work, we use gated recurrent unit (GRU) <ref type="bibr" target="#b16">[16]</ref> as our RNN model, to memorize long-term dependencies. At time t, we define ht as the state of the GRU corresponding to speaker yt, and</p><formula xml:id="formula_10">mt = f (ht|? ? ?)<label>(9)</label></formula><p>as the output of the entire network, which may contain other layers. Let t := max{0, s &lt; t : ys = yt} be the last time we saw speaker yt before t, then:  <ref type="figure">Fig. 2</ref>. Generative process of UIS-RNN. Colors indicate labels for speaker segments. There are four options for y7 given x <ref type="bibr" target="#b6">[6]</ref> , y <ref type="bibr" target="#b6">[6]</ref> .</p><formula xml:id="formula_11">ht = GRU(x t , h t |? ? ?),<label>(10)</label></formula><p>where we can assume x0 = 0 and h0 = 0, meaning all GRU instances are initialized with the same zero state. Based on the GRU outputs, we assume the speaker embeddings are modeled by:</p><formula xml:id="formula_12">xt|x [t?1] , y [t] ? N (? ? ?t, ? 2 I),<label>(11)</label></formula><p>where ? ? ?t = ( t s=1 1(ys = yt)) ?1 ? ( t s=1 1(ys = yt)ms) is the averaged GRU output for speaker yt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Summary of the model</head><p>We briefly summarize UIS-RNN in <ref type="figure">Fig. 2</ref>, where Z and ? ? ? are omitted for a simple demonstration. At the current stage (shown in solid lines) y <ref type="bibr" target="#b6">[6]</ref> = (1, 1, 2, 3, 2, 2). There are four options for y7: 1, 2, 3 (existing speakers), and 4 (a new speaker). The probability for generating a new observation x7 (shown in dashed lines) depends both on previous label assignment sequence y <ref type="bibr" target="#b6">[6]</ref> , and previous observation sequence x <ref type="bibr" target="#b6">[6]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MLE Estimation</head><p>Given a training set (X1, X2, . . . , XN ) containing N utterances together with their labels (Y1, Y2, . . . , YN ), we maximize the following log joint likelihood: </p><p>Here we include all hyper-parameters, and each term in Eq. (12) can be factorized exactly as Eq. <ref type="formula" target="#formula_1">(2)</ref>.</p><p>The estimation of ? ? ? depends on how g ? ? ? (?) is defined. When we simply have g ? ? ? (z [t?1] ) = p0, we have a closed-form solution:</p><formula xml:id="formula_14">p * 0 = N n=1 Tn t=2 1(yn,t = yn,t?1) N n=1 Tn ? N ,<label>(13)</label></formula><p>where Tn denotes the sequence length of the nth utterance. For ? ? ? and ? 2 , there is no closed-form update. We use stochastic gradient ascent by randomly selecting a subset B (? ) ? [N ] of |B (? ) | = b utterances. For ? ? ?, we update:</p><formula xml:id="formula_15">? ? ? (? ) =? ? ? (? ?1) + N ? (? ) b n?B (? ) ? ? ? ? ln p(Xn| Yn, Zn, ? ? ?, ?),<label>(14)</label></formula><p>since ? ? ? is independent of (Yn, Zn). Eq. (15) also applies to ? 2 by replacing ? ? ? with ? 2 . For ?, we update where p(Yn| Zn, ?, ?) is given in Eq. <ref type="bibr" target="#b8">(8)</ref>. In our experiments, we run multiple iterations with a constant step size ? (? ) = ? until convergence.</p><formula xml:id="formula_16">? (? ) = ? (? ?1) + N ? (? ) b n?B (? ) ?? ln p(Yn| Zn, ?, ?),<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">MAP Decoding</head><p>Since we can decode each testing utterance in parallel, here we assume we are given a testing utterance X test = (x1, x2 . . . , xT ) without labels. The ideal goal is to find</p><formula xml:id="formula_17">Y * = arg max Y ln p(X test , Y).<label>(16)</label></formula><p>However, this requires an exhaustive search over the entire combinatorial label space with complexity O(T !), which is impractical. Instead, we use an online decoding approach which sequentially performs a greedy search, as shown in Alg. 1. This will significantly reduce computational complexity to O(T 2 ). We observe that in most cases the maximum number of speakers per-utterance is bounded by a constant C. In that case, the complexity will further reduce to O(T ). In practice, we apply a beam search <ref type="bibr" target="#b17">[17]</ref> on the decoding algorithm, and adjust the number of look-ahead entries to achieve better decoding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Speaker recognition model</head><p>We have been retraining the speaker recognition network with more data and minor tricks (see next few paragraphs) to improve its performance. Let's call the text-independent speaker recognition model in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b18">18]</ref> as "d-vector V1". This model is trained with 36M utterances from 18K US English speakers, which are all mobile phone data based on anonymized voice query logs.</p><p>To train a new version of the model, which we call "d-vector V2" <ref type="bibr" target="#b19">[19]</ref>, we added: (1) non-US English speakers; (2) data from far-field devices; (3) public datasets including LibriSpeech <ref type="bibr" target="#b20">[20]</ref>, VoxCeleb <ref type="bibr" target="#b21">[21]</ref>, and VoxCeleb2 <ref type="bibr" target="#b22">[22]</ref>. The non-public part contains 34M utterances from 138K speakers, while the public part is added to the training process using the MultiReader approach <ref type="bibr" target="#b6">[6]</ref>.</p><p>Another minor but important trick is that, the speaker recognizer model used in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b6">[6]</ref> are trained on windows of size 1600ms, which causes performance degradation when we run inference on smaller windows. For example, in the diarization system, the window size is only 240ms. Thus we have retrained a new model "dvector V3" by using variable-length windows, where the window size is drawn from a uniform distribution within [240ms, 1600ms] during training.</p><p>The speaker verification Equal Error Rate (EER) of the three models on two testing sets are shown in <ref type="table">Table 1</ref>. On speaker verification tasks, adding more training data has significantly improved <ref type="table">Table 1</ref>. Speaker verification EER of the three speaker recognition models. en-ALL represents all English locales. The EER=3.55% for d-vector V1 on en-US phone data is the same as the number reported in <ref type="table">Table 3</ref> of <ref type="bibr" target="#b6">[6]</ref>.</p><p>Model EER (%) on en-US EER (%) on en-ALL phone data phone + farfield data d-vector V1</p><p>3.55 6.14 d-vector V2</p><p>3.06 2.03 d-vector V3</p><p>3.03 1.91 the performance, while using variable-length windows for training also slightly further improved EER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">UIS-RNN setup</head><p>For the speaker change, as we have stated in Section 3.2.1, we assume {zt} t?[2,T ] follow independent identical binary distributions for simplicity. Our sequence generation model is composed of one layer of 512 GRU cells with a tanh activation, followed by two fully-connected layers each with 512 nodes and a ReLU <ref type="bibr" target="#b23">[23]</ref> activation. The two fully-connected layers corresponds to Eq. <ref type="bibr" target="#b9">(9)</ref>.</p><p>For decoding, we use beam search of width 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation protocols</head><p>Our evaluation setup is exactly the same as <ref type="bibr" target="#b2">[3]</ref>, which is based on the pyannote.metrics library <ref type="bibr" target="#b24">[24]</ref>. We follow these common conventions of other works:</p><p>? We evaluate on single channel audio.</p><p>? We exclude overlapped speech from evaluation.</p><p>? We tolerate errors less than 250ms in segment boundaries.</p><p>? We report the confusion error, which is usually directly referred to as Diarization Error Rate (DER) in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Datasets</head><p>For the evaluation, we use 2000 NIST Speaker Recognition Evaluation (LDC2001S97), Disk-8, which is usually directly referred to as "CALLHOME" in literature. It contains 500 utterances distributed across six languages: Arabic, English, German, Japanese, Mandarin, and Spanish. Each utterance contains 2 to 7 speakers. Since our approach is supervised, we perform a 5-fold cross validation on this dataset. We randomly partition the dataset into five subsets, and each time leave one subset for evaluation, and train UIS-RNN on the other four subsets. Then we combine the evaluation on five subsets and report the averaged DER.</p><p>Besides, we also tried to use two off-domain datasets for training UIS-RNN: (1) 2000 NIST Speaker Recognition Evaluation, Disk-6, which is often referred to as "Switchboard"; (2) ICSI Meeting Corpus <ref type="bibr" target="#b25">[25]</ref>. We first tried to train UIS-RNN purely on off-domain datasets, and evaluate on CALLHOME; we then tried to add the offdomain datasets to the training partition of each of the 5-fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results</head><p>We report the diarization performance results on 2000 NIST SRE Disk-8 in <ref type="table" target="#tab_3">Table 2</ref>. For each version of the speaker recognition model, we compare UIS-RNN with two baseline approaches: k-means and spectral offline clustering. For k-means and spectral clustering, the number of speakers is adaptively determined as in <ref type="bibr" target="#b2">[3]</ref>. For UIS-RNN, we show results for three types of evaluation settings: <ref type="bibr" target="#b0">(1)</ref>   From the table, we see that the biggest improvement in DER actually comes from upgrading the speaker recognition model from V2 to V3. This is because in V3, we have the window size consistent between training time and diarization inference time, which was a big issue in V1 and V2.</p><p>UIS-RNN performs noticeably better than spectral offline clustering, when using the same speaker recognition model. It is also important to note that UIS-RNN inference produces speaker labels in an online fashion. As discussed in <ref type="bibr" target="#b2">[3]</ref>, online unsupervised clustering algorithms usually perform significantly worse than offline clustering algorithms such as spectral clustering.</p><p>Also, adding more data to train UIS-RNN also improved DER, which is consistent with our expectation -UIS-RNN benefits from learning from more examples. Specifically, while large scale offdomain training already produces great results in practice (Disk-6 + ICSI), the availability of in-domain data can further improve the performance (5-fold + Disk-6 + ICSI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we presented a speaker diarization system where the commonly used clustering module is replaced by a trainable unbounded interleaved-state RNN. Since all components of this system can be learned in a supervised manner, it is preferred over unsupervised systems in scenarios where training data with high quality time-stamped speaker labels are available. On the NIST SRE 2000 CALLHOME benchmark, using exactly the same speaker embeddings, this new approach, which is an online algorithm, outperforms the state-of-the-art spectral offline clustering algorithm.</p><p>Besides, the proposed UIS-RNN is a generic solution to the sequential clustering problem, with other potential applications such as face clustering in videos. One interesting future work direction is to directly use accoustic features instead of pre-trained embeddings as the observation sequence for UIS-RNN, such that the entire speaker diarization system becomes an end-to-end model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The baseline system architecture<ref type="bibr" target="#b2">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>max ? ? ?,?,? 2 ,? ? ? N n=1 ln p(Xn, Yn, Zn| ? ? ?, ?, ? 2 , ? ? ?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>indomain training (5-fold); (2) off-domain training (Disk-6 + ICSI); and (3) in-domain plus off-domain training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Data: X test = (x test 1 , x test 2 , . . . , x test T ) Result: Y * = (y * 1 , y * 2 , .. . , y * T ) initialize x0 = 0, h0 = 0; for t = 1, 2, . . . , T do (y * t , z * t ) = arg max (y t ,z t ) ln p(zt) Online greedy MAP decoding for UIS-RNN.</figDesc><table><row><cell></cell><cell>Eq. (6)</cell></row><row><cell>+ ln p(yt|zt, y  *  [t?1] )</cell><cell>Eq. (5, 7)</cell></row><row><cell>+ ln p(xt|x [t?1] , y  *  [t?1] , yt)</cell><cell>Eq. (11)</cell></row><row><cell>update N k,t?1 and GRU hidden states;</cell><cell></cell></row><row><cell>end</cell><cell></cell></row><row><cell>Algorithm 1:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>DER on NIST SRE 2000 CALLHOME, with comparison to other systems in literature. VB is short for Variational Bayesian resegmentation<ref type="bibr" target="#b0">[1]</ref>. The DER=12.0% for d-vector V1 and spectral clustering is the same as the number reported inTable 2of<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table><row><cell>d-vector</cell><cell>Method</cell><cell>Training data</cell><cell>DER (%)</cell></row><row><cell></cell><cell>k-means</cell><cell>-</cell><cell>17.4</cell></row><row><cell></cell><cell>spectral</cell><cell>-</cell><cell>12.0</cell></row><row><cell>V1</cell><cell>UIS-RNN</cell><cell>5-fold</cell><cell>11.7</cell></row><row><cell></cell><cell>UIS-RNN</cell><cell>Disk-6 + ICSI</cell><cell>11.7</cell></row><row><cell></cell><cell cols="2">UIS-RNN 5-fold + Disk-6 + ICSI</cell><cell>10.6</cell></row><row><cell></cell><cell>k-means</cell><cell>-</cell><cell>19.1</cell></row><row><cell></cell><cell>spectral</cell><cell>-</cell><cell>11.6</cell></row><row><cell>V2</cell><cell>UIS-RNN</cell><cell>5-fold</cell><cell>10.9</cell></row><row><cell></cell><cell>UIS-RNN</cell><cell>Disk-6 + ICSI</cell><cell>10.8</cell></row><row><cell></cell><cell cols="2">UIS-RNN 5-fold + Disk-6 + ICSI</cell><cell>9.6</cell></row><row><cell></cell><cell>k-means</cell><cell>-</cell><cell>12.3</cell></row><row><cell></cell><cell>spectral</cell><cell>-</cell><cell>8.8</cell></row><row><cell>V3</cell><cell>UIS-RNN</cell><cell>5-fold</cell><cell>8.5</cell></row><row><cell></cell><cell>UIS-RNN</cell><cell>Disk-6 + ICSI</cell><cell>8.2</cell></row><row><cell></cell><cell cols="2">UIS-RNN 5-fold + Disk-6 + ICSI</cell><cell>7.6</cell></row><row><cell></cell><cell cols="2">Castaldo et al. [4]</cell><cell>13.7</cell></row><row><cell></cell><cell cols="2">Shum et al. [9]</cell><cell>14.5</cell></row><row><cell></cell><cell cols="2">Senoussaoui et al. [10]</cell><cell>12.1</cell></row><row><cell></cell><cell cols="2">Sell et al. [1] (+VB)</cell><cell>13.7 (11.5)</cell></row><row><cell cols="3">Garcia-Romero et al. [2] (+VB)</cell><cell>12.8 (9.9)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diarization resegmentation in the factor analysis subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4794" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speaker diarization using deep neural network embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mccree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4930" to="4934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speaker diarization with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Andrew</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopz</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5239" to="5243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stream-based speaker segmentation using speaker factors and eigenvoices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Colibro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Dalmasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Vair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="4133" to="4136" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?da</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized end-to-end loss for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Papir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4879" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speaker diarization using convolutional neural network for statistics accumulation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbyn?k</forename><surname>Zaj?c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Hr?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lud?k</forename><surname>M?ller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end text-dependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5115" to="5119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised methods for speaker diarization: An integrated and iterative approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?da</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2015" to="2028" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A study of the cosine distancebased mean shift for telephone speech diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Senoussaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="227" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speaker diarization with plda i-vector scoring and unsupervised calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="413" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Developing on-line speaker diarization system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Fousek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2739" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Links: A highdimensional online clustering method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Andrew</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A spectral clustering approach to speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>IN-TERSPEECH</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distance dependent chinese restaurant processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter I</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2488" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech understanding systems: Report of a steering committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Medress</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forgie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">P</forename><surname>O&amp;apos;malley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Neuburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="316" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML)</title>
		<meeting>the 27th international conference on machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">metrics: a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">hypothesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The icsi meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Janin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
