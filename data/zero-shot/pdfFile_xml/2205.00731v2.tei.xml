<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>July 11-15, 2022. July 11-15, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhi</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qika</forename><surname>Lin</surname></persName>
							<email>qikalin@foxmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudai</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
							<email>zhanglling@xjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhi</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qika</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudai</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University Xi&apos;an</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Shaanxi Province Key Laboratory of Satellite and Terrestrial Network Tech. R&amp;D, National Engineering lab for Big Data Analytics Xi&apos;an</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<address>
									<addrLine>Xi&apos;an Jiaotong University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<address>
									<addrLine>Xi&apos;an Jiaotong University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<address>
									<addrLine>Xi&apos;an Jiaotong University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 45th International ACM SIGIR Con-ference on Research and Development in Information Retrieval (SIGIR &apos;22)</title>
						<meeting>the 45th International ACM SIGIR Con-ference on Research and Development in Information Retrieval (SIGIR &apos;22) <address><addrLine>Madrid, Spain; Madrid, Spain; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">11</biblScope>
							<date type="published">July 11-15, 2022. July 11-15, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3477495.3532016</idno>
					<note>ACM ISBN 978-1-4503-8732-3/22/07. . . $15.00. 2022. Logi-former: A Two-Branch Graph Transformer Network for Interpretable Log-ical Reasoning. In</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Question answering</term>
					<term>Language models</term>
					<term>Information extraction KEYWORDS logical reasoning, machine reading comprehension, graph trans- former</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine reading comprehension has aroused wide concerns, since it explores the potential of model for text understanding. To further equip the machine with the reasoning capability, the challenging task of logical reasoning is proposed. Previous works on logical reasoning have proposed some strategies to extract the logical units from different aspects. However, there still remains a challenge to model the long distance dependency among the logical units. Also, it is demanding to uncover the logical structures of the text and further fuse the discrete logic to the continuous text embedding. To tackle the above issues, we propose an end-to-end model Logiformer which utilizes a two-branch graph transformer network for logical reasoning of text. Firstly, we introduce different extraction strategies to split the text into two sets of logical units, and construct the logical graph and the syntax graph respectively. The logical graph models the causal relations for the logical branch while the syntax graph captures the co-occurrence relations for the syntax branch. Secondly, to model the long distance dependency, the node sequence from each graph is fed into the fully connected graph transformer structures. The two adjacent matrices are viewed as the attention biases for the graph transformer layers, which map the discrete logical structures to the continuous text embedding space. Thirdly, a dynamic gate mechanism and a question-aware self-attention module are introduced before the answer prediction to update the features. The reasoning process provides the interpretability by employing the logical units, which are consistent * The corresponding author.</p><p>with human cognition. The experimental results show the superiority of our model, which outperforms the state-of-the-art single model on two logical reasoning benchmarks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine reading comprehension <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref> has been one of the major focuses in the field of Natural Language Processing (NLP) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> in recent years. A large number of models have achieved competitive performances in some famous datasets, such as SQuAD <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, RACE <ref type="bibr" target="#b13">[14]</ref>. However, these models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref> lack the capability of logical reasoning. To facilitate the machine for human intelligence, the task of logical reasoning MRC <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref> was proposed previously. Similar to the traditional MRC, the task of logical reasoning also requires the models to predict the answers depending on the given text inputs. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an logical reasoning example from ReClor dataset <ref type="bibr" target="#b36">[37]</ref>. The inputs include the context, question and a set of options. One of the unique characteristics of the text is the rich logical structures. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the logical structure of the context can be uncovered in a certain way. We define the split short sentences as logical units (e.g., U1-U6). The logical units contain the independent and complete semantics, which are not kept in the token-level text features. The understanding of the text requires the global semantics of each logical unit, as well as the interactions among them based on some logical relations (e.g., causal and co-occurrence). Therefore, the main challenges for the task of logical reasoning can be summarized as the following two aspects.</p><p>Firstly, it remains a challenge to model the long distance dependency <ref type="bibr" target="#b10">[11]</ref> of the extracted logical units. Some previous methods, such as DAGN <ref type="bibr" target="#b11">[12]</ref>, have proposed to split the text into discourse nodes <ref type="bibr" target="#b31">[32]</ref> and constructed a sequential chain graph for reasoning. However, it neglects the natural long-distance dependency among logical units. For example in <ref type="figure" target="#fig_0">Figure 1</ref>, the first and the last sentences share the same subject (Paula) and predicate (visit the dentist), though they are distant in the graph space. The chain structure limits the information update. In a word, the simple graph structure built for the logical text would fail to provide the efficient one-hop interaction <ref type="bibr" target="#b24">[25]</ref>. Pretrain-based transformer structures <ref type="bibr" target="#b29">[30]</ref> have the natural advantage of modeling the long text and show excellent performance on the popular tasks. To enhance the logical perception ability of the language models, previous works have attempted to employ additional segment embedding at the beginning. However, it is still limited to the token-level interactions, which sacrifices the global semantics of logical units. Take the first sentence of the context in <ref type="figure" target="#fig_0">Figure 1</ref> as an instance, two extracted units of Paula will visit the dentist tomorrow morning(U1) and Bill goes golfing in the morning(U2) express the causal relations within this sentence. The token-aware models would stress more on the text semantics and fail to capture such logical information.</p><p>Secondly, it is intractable to bridge the gap between the discrete logical structures and the continuous text embedding space. Take a closer observation of the logical units in <ref type="figure" target="#fig_0">Figure 1</ref>, the units are not completely separated. In this work, we pay much attention to the two explicit relationships (causal and co-occurrence). We summarize them into the logical branch and syntax branch respectively. From the logical branch, connectives (e.g., if, unless, Therefore) play a significant role in covering the logical relations. For example in <ref type="figure" target="#fig_0">Figure 1</ref>, the logical units U1 and U2 are connected with the connective if while U3 and U4 share the connective unless. From the syntax branch, the logical units are not independent but have some repeating occurrences. For example, the units of Bill goes golfing in the morning and Bill will not go golfing belong to the co-occurrence information, which have strong correlations. Thus, these syntactic relationships help make the corresponding logical units closer in structure. The above connection structures from two branches are discrete, which is incompatible to the continuous text representation. Some early works simply feed the text to the Pretrained Language Models (PLMs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref> and rely on the context to learn the logical semantics. But it includes much noise in the text embedding and lacks the potential of interpretability. Previously, LReasoner <ref type="bibr" target="#b30">[31]</ref> proposes a method to transform the logical expressions to text based on the templates and feeds the extended text into PLM. However, it still embeds the logic in an implicit form and fails to make up for the weakness of PLMs in logical reasoning. Some works like FocalReasoner <ref type="bibr" target="#b20">[21]</ref> uncovers the logical structure in only one aspect (e.g., capture the co-occurrence between units). It leads to the weak capability of the model to capture logical relationships.</p><p>In light of the above challenges, we propose a novel model named Logiformer which is an end-to-end architecture with graph transformer for interpretable logical reasoning of text. By employing the fully connected graph transformer structure to enhance the direct interactions, we tackle the issue of long distance dependency among the logical units. To encode the discrete logical structures to the continuous text embedding space, we apply the attention biases from both the logical and syntax branches. The whole reasoning is on the basis of logical units and the built graphs, which are consistent with the human cognition. The explicit relations among units and the weighted attention maps provide the interpretability for the logical reasoning. In details, firstly, Logiformer split the text into logical units and construct the logical graph based on the causal relations for the logical branch. For the syntactic branch, the split nodes and a syntax graph are also obtained. Secondly, we feed the node sequences and two graph topology to the fully connected graph transformers respectively. The respective adjacent matrices are viewed as the attention biases to encode the logical structures to each graph transformer. Thirdly, we combine the updated features from two branches with a dynamic gate mechanism. With the additional token-level embedding, we can map the features to the same space. By means of the question-aware self-attention, the final feature can be utilized to predict the answers.</p><p>The main contributions are listed as follows:</p><p>? A two-branch graph transformer network named Logiformer is proposed to model the long distance dependency of the logical units and encode the discrete logical structure to the continuous text embedding. As far as we know, we are the first to tackle both issues in the logical reasoning task. ? In light of drawbacks of chain-type text graphs, we take the fully connected structures into consideration, containing the awareness of both logic and syntax simultaneously. Two graphs are constructed based on the extracted logical units and their topology is utilized as the attention biases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we will introduce the current researches on MRC and logical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Machine Reading Comprehension</head><p>Recent years have witnessed the rapid growth of MRC <ref type="bibr" target="#b17">[18]</ref>, where the model is required to infer the answers based on the given context and a question. A variety of datasets have been proposed to check the performances of MRC models. Among them, SQuAD <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> focuses on the span extractions on the factual questions. HotpotQA <ref type="bibr" target="#b33">[34]</ref> and OpenBookQA <ref type="bibr" target="#b19">[20]</ref> require the multi-hop reasoning capability of the models. A couple of multiple choice datasets like RACE <ref type="bibr" target="#b13">[14]</ref> cover the examinations for middle or high school students. Some representative models achieve great success on these datasets. Retro-Reader <ref type="bibr" target="#b40">[41]</ref> applies a two-state strategy to solve the questions. But it mainly investigates the overall interactions of the context and question, which fails to deal with the complex logic within the text. SG-Net <ref type="bibr" target="#b39">[40]</ref> integrates the syntax information into the selfattention module to improve the performance, but it does not show the potential on tackling the logical information. Generally speaking, the datasets mentioned above rely much on the token-level matching, which can be well tackled with large-scale pretraining models like BERT <ref type="bibr" target="#b5">[6]</ref> and GPT-3 <ref type="bibr" target="#b1">[2]</ref>. To make the models closer to the human intelligence, it is necessary to introduce more challenging tasks requiring logical reasoning. Previously, the task of Natural Language Inference(NLI) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> is proposed to motivate the models to infer the relations(i.e., Contradiction, Entailment and Neutral) between two sentences. Nevertheless, it is limited by the fixed inputs and outputs and fails to extend the task to more complex settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Logical Reasoning</head><p>To improve the reasoning ability of the models, several datasets on multiple choice have been proposed previously. ReClor <ref type="bibr" target="#b36">[37]</ref>, which is extracted from standardized graduate admission examinations and law school admission test, has aroused wide concerns. For better evaluation, it separates the biased examples into EASY set and the challenging ones into HARD set. LogiQA <ref type="bibr" target="#b16">[17]</ref> is also one of the representatives, which also aims to improve the logical reasoning capability. It is sourced from expert-written questions and covers multiple types of deductive reasoning. Experiments show that previous SOTA models on traditional MRC perform bad on the two datasets. Under such circumstances, some of the recent works attempt to enhance logical reasoning from different perspectives. DAGN <ref type="bibr" target="#b11">[12]</ref> proposes a reasoning network based on the discourse units extracted from the text. But it simply forms a chaintype discourse network and weakens the relations between two distant units. FocalReasoner <ref type="bibr" target="#b20">[21]</ref> stresses that fact units in the form of subject-verb-object are significant for logical reasoning. It constructs a supergraph on top of the fact units and updates the node Paula will visit the dentist tomorrow morning <ref type="bibr" target="#b1">2</ref> Bill goes golfing in the morning <ref type="bibr" target="#b2">3</ref> Bill will not go golfing <ref type="bibr" target="#b3">4</ref> Damien agrees to go golfing too <ref type="bibr" target="#b4">5</ref> Damien has decided not to go golfing features relying on Graph Neural Network. However, it ignores the relation connectives from the text and lacked the logical modeling.</p><p>LReasoner <ref type="bibr" target="#b30">[31]</ref> focuses on capturing symbolic logic from the text and puts forward a context extension framework based on logical equivalence laws. However, it relies heavily on the language models for token-level embedding and neglects the sentence-level interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>This section will introduce the proposed end-to-end model Logiformer. The architecture of Logiformer is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. The left part of the model is an example of the logical reasoning task. The understanding of text will be divided into two branches: logical branch (upper) and syntactic branch (lower). This architecture mainly includes the following three parts: a) graph construction from the text; b) logical-aware and syntax-aware graph transformers for feature updates; c) the decoder including a dynamic gate mechanism and a question-aware self-attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formulation</head><p>Given a dataset D for logical reasoning, which consists of examples totally. The inference of the ? question can be formulated as follows:^= arg max</p><formula xml:id="formula_0">, ? , | , , ; ,<label>(1)</label></formula><p>where , , represent the context, question sentence and candidate set respectively. The number of options in is , ? [0, ? 1] and , ? represents the ? option.^denotes the predicted option. denotes the trainable parameters.</p><p>Since the current methods mainly focus on the token-level representation of the text with the help of PLMs, they will naturally ignore some global semantics for each sentence or phrase. To capture the global feature within each sentence, we first obtain the text fragments, which are split by connectives or punctuations. We define the text fragment that reflect the complete semantic of an event or argument as the logical unit with the symbol . Take the context in <ref type="figure" target="#fig_2">Figure 2</ref> as an instance, we split the text by both connectives and punctuations and obtain a set of logical units shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Considering that there exist explicit causal relations between units, we further introduce the conditional connective '?'. And in some cases, it is required to reverse logical units for the negation expression, we also employ the operation '?'. Combining the logical units and causal connections in the form of conjunction, we can derive the logical expression of the text: Obviously, there exist two key components in the logical expression: i) logical units ; ii) logical connectives, i.e., ? and ?. The former one focuses on the syntactic information, while the latter one is more related to logical structure of the context.</p><formula xml:id="formula_1">( 2 ? 1 ) ? ( 4 ? ? 3 ) ? ? 5 .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Construction</head><p>Given the ? inputs, Logiformer first concatenates the context with each option , respectively to form the input sequences. According to the previous analysis, Logiformer will tackle the inputs from two branches (i.e., logical branch and syntax branch) and build two graphs (i.e., logic graph and syntax graph) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Logical Graph. For the logical branch, Logiformer mainly concentrates on the causal relations. Considering that the causal relation often appears with explicit logical words such as if, unless, because, we can leverage the explicit logical words as the basis of split. Therefore, we include 100 commonly used logical words according to PDTB 2.0 <ref type="bibr" target="#b21">[22]</ref>.</p><p>Combining the explicit logical words and punctuations, we can separate the text sequence into logical units. Each unit serves as a node for future updates. Especially, we pick out the nodes pairs connected by the explicit causal relation words and name them as (orange nodes) and (blue nodes). Meanwhile, we classify the common nodes which do not contain causal relations into (blue nodes). Thus, we obtain the node set from the perspective of logic.</p><p>According to the extracted causal node pairs, we can create directed connection from each condition node to result node . This kind of connection is reflected in the adjacent matrix M ? R ? of the logical graph as M [ ? 1, ? 1] = 1. Also, to avoid the semantic reverse brought by the negation, we mark the nodes with the explicit negation words (e.g., not, no). The node with negation semantics are expressed in the adjacent matrix as M [ ? 1, ? 1] = ?1.</p><p>Therefore, the logical graph has the perception of the causal relations and negations. And the obtained adjacent matrix M ? R ? of the logical graph is asymmetric. 3.2.2 Syntax Graph. The main purpose of the syntactic understanding is to capture the inner relations between the logical units . Noticing that some logical units share the common words or phrases in <ref type="figure" target="#fig_2">Figure 2</ref>, e.g., Bill, Damien and go golfing. It illustrates that the text has a strong characteristics of co-occurrence. Also, cooccurrence usually exists between two complete sentences. Therefore, we consider to split the text sequence only by punctuations and obtain a set of sentence nodes with no original connection. It is required to extract the co-occurrence between the sentence nodes. As each node consists of its related tokens, we propose a simple strategy to capture the co-occurrence, shown in Algorithm 1.</p><p>Assume the total number of the nodes to be . The input for the algorithm is the sentence node , corpus containing redundant stop words and hyper-parameter . The output is an adjacent matrix M ? R ? , which reflects the co-occurrence relations between nodes. As for any two nodes, we transform them into two token sets and separately, without order and duplicate elements (Line 3 &amp; Line 5 in Algorithm 1). We define ( ) to be the number of tokens in a set. Further, let the token overlap ratio </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Transformer</head><p>Some previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> point out the drawbacks of graph neural network, such as the issue of over-smooth <ref type="bibr" target="#b14">[15]</ref>. Therefore, we take the novel architecture of graph transformer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref> into account. After the extraction of nodes and the construction of two graphs, we feed them into the logical-aware and syntax-aware graph transformer structures respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.1</head><p>Logical-aware Graph Transformer. The simple illustration of the logical-aware graph transformer is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. First of all, it is necessary to get the original feature embedding for each node. Given the concatenated input sequence of the ? question:</p><formula xml:id="formula_2">Input( , , ) = [ ] [ ] , [ ],<label>(3)</label></formula><p>we employ the RoBERTa model <ref type="bibr" target="#b18">[19]</ref> as the encoder for the tokenlevel features. For the token sequence { </p><formula xml:id="formula_3">v = 1 ?? =1 v ( ) .<label>(4)</label></formula><p>To keep the original order information of nodes in the text, positional embedding is added to the node representation.</p><formula xml:id="formula_4">V i = V o + (V o ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">V o = [v 1 ; v 2 ; ...; v ], V o ? R ? ,</formula><p>is the dimension of the hidden state, and is the number of nodes.</p><p>(?) provides a d-dimensional embedding for each node in the input sequence.</p><p>We feed the node representation V i into the logical-aware graph transformer. Firstly, V i is projected to three matrices , and of the self-attention module:</p><formula xml:id="formula_6">= V i ? W Q , = V i ? W K , = V i ? W V ,<label>(6)</label></formula><p>where W Q , W K , W V ? R ? are projection matrices, and the obtained matrices , , ? R ? . Then, we compute the attention based on the query, key and value matrices.</p><formula xml:id="formula_7">= T ?? , ( , , ) = softmax( ) ? ,<label>(7)</label></formula><p>where ? R ? is a weight matrix for node pairs. From the equations, the transformer structure provides a fully connected setting to all nodes, which ignores the inner causal relations. Therefore, Logiformer employs the obtained topology information M ? R ? of the logical graph as an attention bias. The representation of the weight matrix is adjusted as follows:</p><formula xml:id="formula_8">? = T ?? + M .<label>(8)</label></formula><p>To improve the robustness and capability of the attention module, we apply the multi-head attention mechanism with the head number :</p><formula xml:id="formula_9">( , , ) = [ 1 ; ...; ] ? W H ,<label>(9)</label></formula><p>where W H ? R ( * )? is the linear projection matrix, = ( , , ), the input query, key and value matrices are obtained by the linear projections of W Q i , W K i , W V i ? R ? respectively. For simplicity, we assume = and omit the bias term of the linear projection.</p><p>Repeating the multi-head attention for layers, we take out the hidden states of the last two layers. To enhance the robustness of the model, we make a fusion of them as the updated node features:</p><formula xml:id="formula_10">V = V ( ?1) + V ( ) ,<label>(10)</label></formula><p>where V ? R ? , and V ( ?1)</p><p>, V ( ) ? R ? represents the hidden states of the last two layers respectively. Note that there are lots of ways of feature fusion, we only present the simple addition for illustration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Syntax-aware Graph Transformer. The general idea of the syntax-aware graph transformer is similar to that of the logicalaware graph transformer. The initialization of node features are also obtained by averaging the embedding of each token. And the positional information is kept to provide the perception of original text orders. After obtaining the weight matrix from the self-attention module, we apply the adjacent matrix M to provide the attention bias for the weights. The final feature of syntax-branch node sequence is also obtained through the fusion of last two layers, represented as V ? R ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoder</head><p>So far, we have obtained the token-level representation V ? R ? , syntax branch node representation V ? R ? and logical branch node representation V ? R ? . To make comprehensive use of the advantages of the three features, it is required to ensure dimension consistency. Therefore, we broadcast the feature of each node to all the tokens it contains. The updated features are</p><formula xml:id="formula_11">transformed to V , V ? , V ? ? R ? .</formula><p>The above three features with the same dimension do not necessarily make equal contributions to the final prediction. It will be beneficial to automatically assign the importance. Thus, Logiformer will dynamically generate the fusion weights for V The gate parameter is expressed as follows:</p><formula xml:id="formula_12">= softmax([V ? ; V ? ]W + ),<label>(11)</label></formula><p>where W ? R 2 ?1 and ? R ?1 are the weight term and bias term for the linear projection. V ? and V ? are concatenated at the last dimension. The obtained gate parameter ? R ?1 is a vector. That is to say, each token will be provided with one specific weight. The final feature V can be represented in the following expression:</p><formula xml:id="formula_13">V = (V + ? V ? + (1 ? ) ? V ? ),<label>(12)</label></formula><p>where (?) denotes the layer normalization operation. Since we do not employ the global node in the graph transformer, the global feature will not updated. To this end, Logiformer integrates the local token-level features and gets the updated global information:</p><formula xml:id="formula_14">V = (V , + 1 ? 1 ?1 ?? =1 (V ? , + V ? , )),<label>(13)</label></formula><p>where V , is the first token of the original token-level embedding. V ? , and V ? , represent the ? token embedding of the syntactic branch and logical branch feature respectively. We utilize the global feature V to replace the first token feature (i.e., [cls] feature) of V. V can be expressed as the concatenation of V ,</p><formula xml:id="formula_15">V and V , that is V = [V ; V ; V ].</formula><p>To conduct the reasoning, the feature of the question V is also of great significance. Logiformer applies a simple self-attention module for the global feature V and question V . The updated question embedding is expressed as:</p><formula xml:id="formula_16">V ? = softmax( V V T ? ) ? V.<label>(14)</label></formula><p>For simplicity, the linear projections for the self-attention are omitted. At last, we concatenate the V , V , V and V ? to get the final feature V ? R ? :</p><formula xml:id="formula_17">V = [V ; V ; V ; V ? ].<label>(15)</label></formula><p>For each option in one example, we can get one specific final feature. They are fed into the feed forward network to obtain the scores, and we take the highest one as the predicted answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, extensive experiments are conducted to compare our model with SOTA single model methods in both ReClor and LogiQA datasets. Ablation studies are followed to verify the effectiveness of the proposed modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Baselines</head><p>4.1.1 Datasets. In this paper, we conduct the experiments on two logical reasoning datasets ReClor <ref type="bibr" target="#b36">[37]</ref> and LogiQA <ref type="bibr" target="#b16">[17]</ref>. ReClor consists of 6,138 examples sourced from some standardized tests, while LogiQA includes totally 8,678 questions collected from National Civil Servants Examinations of China. The detailed splits of both datasets are included in <ref type="table" target="#tab_2">Table 2</ref>. It can be concluded that ReClor is more diverse in the number of logical reasoning types, while LogiQA contains more examples. Both of them are challenging for the task of logical reasoning. ? Human Performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>: For ReClor, human performance is defined as the average score of different graduate students in a university on the test split. For LogiQA, the result is the average score of three post-graduate students on 500 randomly selected instances from the test split. ? DAGN <ref type="bibr" target="#b11">[12]</ref>: It proposed a discourse-aware network, which took RoBERTa-Large <ref type="bibr" target="#b18">[19]</ref> as the token encoder and employed GNN for the feature update. ? FocalReasoner <ref type="bibr" target="#b20">[21]</ref>: It focused on the fact units extracted from the text and built a supergraph for the reasoning. Similar to DAGN, it also leveraged RoBERTa-Large and GNN <ref type="bibr" target="#b25">[26]</ref> for the token embedding and node update respectively. ? LReasoner <ref type="bibr" target="#b30">[31]</ref>: It captured the symbolic logic from the text and further extended them into natural language based on several logical equivalence laws. For the fair comparison, we take the results of the single model with RoBERTa encoder into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All of the experiments are conducted with a single GPU of Tesla V100. For the fair comparison, the RoBERTa-large model <ref type="bibr" target="#b18">[19]</ref> is utilized as the encoder for text during the experiments and the hidden size is set to 1024. During the training process, the epoch number is fixed to 12 and the batchsize is set to 2 for both ReClor and LogiQA datasets. We take Adam <ref type="bibr" target="#b12">[13]</ref> with linearly-decayed learning rate and warm up and select peak learning rate as 5e-6. We select the model with best accuracy on the validation split to conduct the test. The details of important hyper-parameters and their search scopes are attached in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Results</head><p>Logiformer is evaluated on two logical reasoning datasets. The main results on the validation split and test split of ReClor dataset are shown in <ref type="table">Table 4</ref>. And the results on LogiQA dataset are shown in <ref type="table" target="#tab_5">Table 5</ref>. The test split of ReClor is organized into easy fold and hard fold, presented as 'Test-E' and 'Test-H' respectively in the table.</p><p>For the fair comparison, we consider the results with single model and with the encoder of RoBERTa for all the baselines. Compared with the SOTA results on two logical reasoning benchmarks, our proposed model Logiformer shows excellent improvements.</p><p>On the ReClor dataset, we witness the improvements of 2.20% and 1.10% on the validation and test split over previous SOTA model LReasoner. Since LReasoner does not make the results on Test-E <ref type="table">Table 4</ref>: Experimental results on ReClor dataset. The percentage signs (%) of accuracy values are omitted. The optimal and sub-optimal results are marked in bold and underline respectively (same for the following tables).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Valid Test Test-E Test-H Random 25.00 25.00 25.00 25.00 Human Performance <ref type="bibr" target="#b36">[37]</ref> -63.00 57.10 67.20 BERT-Large <ref type="bibr" target="#b36">[37]</ref> 53.80 49.80 72.00 32.30 XLNet-Large <ref type="bibr" target="#b36">[37]</ref> 62.00 56.00 75.70 40.50 RoBERTa-Large <ref type="bibr" target="#b36">[37]</ref> 62.60 55.60 75.50 40.00 DAGN <ref type="bibr" target="#b11">[12]</ref> 65  and Test-H splits public, we omit the comparison. Compared with FocalReasoner on the validation split, Logiformer shows strong generalization capability with 1.6% and 4.6% improvements on the validation and test split. Especially on the Test-H split, 6.61% improvement proves our superiority for the more difficult logical reasoning questions. The most important observation is that Logiformer is the first single model with RoBERTa encoder to beat the human performance by 0.50% on the ReClor dataset. Although the machine still falls behind humans on more challenging questions, our proposed method is positively narrowing the gaps. On the LogiQA dataset, Logiformer outperforms the previous SOTA model Logiformer by 1.13% and 2.30% on the validation and test split respectively. It proves the excellent generalization capability of Logiformer. However, we also discover the huge gap between humans and the machine. In view that the context in LogiQA dataset is organized in a more structural form, humans are easier to capture the inner logic. The deep learning based models are good at capturing the semantic changes and lack the perception of fixed logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Considering that the architecture of Logiformer is mainly divided into three parts: a) graph construction, b) graph transformer and c) decoder, the ablation studies are also laid out from these three aspects. The experimental results are shown in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>Firstly, in the part of graph construction, we build syntax graph and logical graph based on the different node extraction strategies. We ablate the effects of the two graphs in turn. That is to say, we only consider one of the branches each time. From the results, the logical graph contributes more to the performance on the ReClor dataset, which improves 4.80% and 3.60% on the validation and test  split respectively. The syntax graph also shows 2.00% and 2.30% improvements on ReClor. As is mentioned above, we are the first to model the causal relations within the context in the task of logical reasoning. The effectiveness of logical graph also verifies our proposed method. Secondly, we explore the impact of two attention biases on the model performance. Thus, we ablate the effects of one or both of attention bias matrices. From the results, co-occurrence bias and causal bias have different effects on the two splits of ReClor dataset, where the former one contributes more to the test split and the latter one is more helpful to the validation split. Meanwhile, positive effects are witnessed by applying both of the attention biases to the graph transformer, leading to 1.90% and 2.61% on the test split of ReClor and LogiQA respectively. Combining the ablation results for graph construction module, the fully connected structure of the logical units itself also has a positive role in the model performance.</p><p>Thirdly, we focus on the effectiveness of two important parts in the decoder. For the proposed dynamic gate mechanism, we set each element of the gate parameter vector ? R ?1 to 0.5 to ablate the effect of gates. The results show that dynamic gate mechanism contributes 1.6% improvement to the test split of ReClor, but does not have effects on that of LogiQA. It may result from the characteristics of LogiQA dataset, which require the equal contribution of syntax and logical information. For the question-aware attention, we remove the self-attention module and use the original tokenlevel representation of the question to form the final vector. The ablation results illustrate that the update of the question feature contributes a lot to the model performance, especially for the Re-Clor dataset. Considering that the question types are various on the ReClor dataset, the awareness of the question sentence is of great help.</p><p>Additionally, we present the detailed results of ReClor test split on different question types and also list the corresponding ablation results of two graphs in <ref type="figure">Figure 7</ref>. The majority of the types witness the significant improvements, especially for Principle, Dispute and Role. It illustrates that Logiformer has the advantages of inferring the hidden fact or truth within the context. A few types, such as Explain or Resolve and Identify a Flaw, show a downward trend. We blame this issue to the lack of modeling on negation. For example, the type of Identify a Flaw requires the model to figure out the most weakness one from the options, which is sentimentally opposite to the most of the types. The feature distribution obtained from the current language model is insufficient to clearly distinguish the implicit opposite semantics. Therefore, the modeling of sentimentally negative questions is worth exploring in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Supplementary Analysis</head><p>During the experiments, hyper-parameters are utilized in many places. Limited by the space, we only select one of them to conduct the analysis, which is the overlap threshold for the extraction of co-occurrence. The results of different values of are shown in <ref type="figure" target="#fig_6">Figure 4</ref>.</p><p>Results illustrate that the best performance is achieved when is equal to 0.5. When the hyper-parameter drops, it means that more co-occurrence pairs will be extracted, leading to much extra noise. When reaches 0.7, the number of co-occurrence relations is limited, the performance of the model still maintains at a high level. It further proves the robustness of Logiformer.   <ref type="figure" target="#fig_7">Figure 5</ref>. The green bar represents the accuracy and the yellow bar is the number of examples. From the statistics, the examples with 9-12 logical units account for the largest proportion. Most importantly, the accuracy remains stable for the different number of logical units. On one hand, it proves the effectiveness of the split of logical units. On another hand, we attribute the results to the employment of the graph transformer. Fully connected structure tackles the issue of long distance dependency, which reduces the impact of the increase in the number of logical units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY</head><p>In <ref type="figure">Figure 6</ref>, we present a successful case on the validation split of the ReClor dataset to illustrate the logical reasoning process in Logiformer. The basis of the reasoning process is the two constructed graphs from the logical branch and the syntax branch. In this case, Logiformer extracts 11 logical units (named from to ) based on punctuations and relation connectives for the logical branch. The split results are consistent with our expectation, and among them 4 pairs of causal units ( ? , ? , ? , ? ) are detected. For the syntax branch, the concatenated text is split into 7 sentence nodes (named from to ). Among them, 3 logical units ( , , ) are detected as the co-occurrence relations. The topology of the two graphs provides the explicit understanding of the text, which is a key point to the interpretability of Logiformer. In addition, we present the attention maps in the final layer of the graph transformers from both branches (blue one for logical branch, orange one for syntax branch). The data in the attention matrices is mapped to the range of [0,1] for better illustration. Darker color indicates the stronger correlations between two logical units. The weighted attention maps well reflect the relations and provide a boarder view for interpretability.</p><p>Meanwhile, we observe a drawback in this case. he gets married and his wedding are two similar expressions in semantic, indicating the similar meanings. However, they have no overlap of words and are not detected as the co-occurrence relation. This detail is worthy of studying in the future, which is beneficial to the fine-grained understanding of the logical text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>We propose a two-branch graph transformer network for logical reasoning of text, which is named as Logiformer. Firstly, we introduce two different strategies to construct the logical graph and syntax graph respectively. Especially for the logical graph, we are the first to model both causal relations and negations in the logical reasoning task. Secondly, we feed the extracted node sequences to the fully connected graph transformer for each graph. The topology of the graph is utilized to form the attention bias for the self-attention layers. Thirdly, a dynamic gate mechanism is applied to make a fusion of the features from two branches. To improve the awareness of different question types, the question feature is updated based on the self-attention module. Finally, the concatenated text sequence is passed through the feed forward layer and obtains the answer prediction. The whole reasoning process provides the interpretability, reflected by logical units with explicit relations and the visualization of the attention maps.</p><p>In the future, we will explore the role of question to further improve the interpretability <ref type="bibr" target="#b28">[29]</ref>. Also, we are interested in extending the logical expressions based on contrastive learning, like <ref type="bibr" target="#b15">[16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of the logical reasoning task and some detailed illustrations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>The extraction of the logical units and the explicit relations are consistent with the human cognition. The uncovered logical structures and the weighted attention maps of the logical units provide the excellent interpretability for the logical reasoning process.? Extensive experiments show that Logiformer outperforms the state-of-the-art (SOTA) results with single model on two logical reasoning datasets. Furthermore, ablation studies prove the effectiveness of each module in our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of Logiformer. The left part is an input example of the dataset. The graph construction modules (a1,a2) split the text into logical units and build two graphs from two branches respectively. The graph transformer structures (b1,b2) update the text features combined with the logical and syntactic relations. Finally, the decoder module (c) is utilized to conduct the feature fusion and predict the answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 ,</head><label>2</label><figDesc>...,( )  } with the length of each node , the obtained token embedding is represented as {v ( )1 , v ( ) 2 , ..., v ( ) }.We take the average embedding of tokens as the original feature for node :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The illustration of logical-aware graph transformer. The inputs are the node sequence as well as the topology and the outputs are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The model performances on the ReClor dataset under different .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The model performances on under different numbers of logical units. Also in Logiformer, logical units are important parts. Thus, we will study the influence of the number of logical units on the accuracy. We conduct the analysis on the validation split of ReClor dataset. The number of logical units are obtained based on the split of relation connectives and punctuations. The results are presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The set of logical units from the example context (split by connectives and punctuations).</figDesc><table><row><cell>Symbol</cell><cell>Logical Units</cell></row><row><cell>1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Co-occurrence Extraction Input: Sentence Nodes ( ? {1, 2, ..., }), hyper-parameter , stop words corpus Output: Co-occurrence Matrix M 1 Initialize co-occurrence matrix M ? R Thus, we can determine the co-occurrence relation between and if the overlap score exceeds the threshold.So far, the connections within the sentence nodes have been explored based on the co-occurrence relations. Thus, the syntax graph is constructed reflected by the obtained adjacent matrix M ? R</figDesc><table><row><cell></cell><cell></cell><cell>.,</cell><cell>do</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell cols="4">Include all the tokens of</cell><cell cols="2">into</cell><cell>. Meanwhile,</cell></row><row><cell></cell><cell cols="4">exclude stop words from</cell><cell></cell><cell>based on</cell></row><row><cell>4</cell><cell cols="3">for = + 1, + 2, ...,</cell><cell></cell><cell>do</cell></row><row><cell>5</cell><cell cols="5">Include all the tokens of</cell><cell>into</cell><cell>and exclude</cell></row><row><cell></cell><cell cols="3">stop words from</cell><cell></cell><cell cols="2">based on</cell></row><row><cell></cell><cell cols="6">/* Ensure the two sets not empty</cell><cell>*/</cell></row><row><cell>6</cell><cell>if</cell><cell>(</cell><cell cols="2">) &gt; 0 and</cell><cell>(</cell><cell>) &gt; 0 then</cell></row><row><cell>7</cell><cell></cell><cell>?</cell><cell>{ (</cell><cell></cell><cell cols="2">), (</cell><cell>)}</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell>?</cell><cell>(</cell><cell>&amp;</cell><cell>)/</cell></row><row><cell>9</cell><cell>if</cell><cell></cell><cell cols="3">&gt; then</cell></row><row><cell>12</cell><cell cols="2">end</cell><cell></cell><cell></cell><cell></cell></row><row><cell>13</cell><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>14</cell><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">15 end</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? to zero.2 for = 1, 2, ..10 M [ ? 1, ? 1] ? 111 M [ ? 1, ? 1] ? 116 return Co-occurrence Matrix M of two sets be the co-occurrence metric (Line 7 &amp; Line 8).? .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Detailed Splits of ReClor and LogiQA.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Train #Valid #Test #Reason Type</cell></row><row><cell>ReClor</cell><cell>4,638</cell><cell>500</cell><cell>1,000</cell><cell>17</cell></row><row><cell>LogiQA</cell><cell>7,376</cell><cell>651</cell><cell>651</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The tuned hyper-parameters with search scopes. Baselines. To prove the superiority of our model, we mainly employ the following baselines, including the SOTA method of single model.</figDesc><table><row><cell>Name of Parameter</cell><cell>Search Scope</cell><cell>Best</cell></row><row><cell>training batchsize</cell><cell>{1,2,4,8}</cell><cell>2</cell></row><row><cell>#epoch</cell><cell>{9,10,11,12,13}</cell><cell>12</cell></row><row><cell>#head in graph transformer</cell><cell>{4,5,6,7,8}</cell><cell>5</cell></row><row><cell>#layer in graph transformer</cell><cell>{4,5,6,7,8}</cell><cell>5</cell></row><row><cell>max sequence length</cell><cell>{128,256,512}</cell><cell>256</cell></row><row><cell>learning rate for RoBERTa</cell><cell cols="2">{4e-6. 5e-6, 6e-6, 5e-5} 5e-6</cell></row><row><cell>4.1.2</cell><cell></cell><cell></cell></row></table><note>? Random: The results are based on the random predictions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on LogiQA dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Valid Test</cell></row><row><cell>Random</cell><cell cols="2">25.00 25.00</cell></row><row><cell>Human Performance[17]</cell><cell>-</cell><cell>86.00</cell></row><row><cell>BERT-Large [17]</cell><cell cols="2">34.10 31.03</cell></row><row><cell>RoBERTa-Large [17]</cell><cell cols="2">35.02 35.33</cell></row><row><cell>DAGN [12]</cell><cell cols="2">36.87 39.32</cell></row><row><cell>FocalReasoner [21]</cell><cell cols="2">41.01 40.25</cell></row><row><cell>Logiformer</cell><cell cols="2">42.24 42.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation Studies. The improvements on the accuracy are marked in red.</figDesc><table><row><cell>Model</cell><cell>Valid</cell><cell>?</cell><cell cols="2">ReClor Test ?</cell><cell cols="3">Test-E Test-H Valid</cell><cell cols="2">LogiQA ? Test</cell><cell>?</cell></row><row><cell>Logiformer</cell><cell>68.40</cell><cell>-</cell><cell>63.50</cell><cell>-</cell><cell>79.09</cell><cell>51.25</cell><cell>42.24</cell><cell>-</cell><cell>42.55</cell><cell>-</cell></row><row><cell>a) Graph Construction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o syntax graph</cell><cell cols="5">66.40 -2.00 61.20 -2.30 77.50</cell><cell>48.39</cell><cell cols="4">38.56 -3.68 38.71 -3.84</cell></row><row><cell>w/o logical graph</cell><cell cols="5">63.60 -4.80 59.90 -3.60 75.00</cell><cell>48.04</cell><cell cols="4">38.25 -3.99 37.63 -4.92</cell></row><row><cell>b) Graph Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o co-occurrence bias</cell><cell cols="5">66.80 -1.60 62.80 -0.70 77.05</cell><cell>51.61</cell><cell cols="3">41.94 -0.30 42.55</cell><cell>-</cell></row><row><cell>w/o causal bias</cell><cell cols="5">65.20 -3.20 63.30 -0.20 76.82</cell><cell>52.68</cell><cell cols="4">39.94 -2.30 41.47 -1.08</cell></row><row><cell>w/o both of attention biases</cell><cell cols="5">66.20 -2.20 61.60 -1.90 75.23</cell><cell>50.89</cell><cell cols="4">41.63 -0.61 39.94 -2.61</cell></row><row><cell>c) Decoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o dynamic gates</cell><cell cols="5">67.00 -1.40 61.90 -1.60 76.14</cell><cell>50.71</cell><cell cols="3">41.32 -0.92 42.55</cell><cell>-</cell></row><row><cell>w/o question-aware attention</cell><cell cols="5">66.60 -1.80 60.40 -3.10 76.36</cell><cell>47.86</cell><cell cols="4">41.63 -0.61 42.09 -0.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The details of ReClor Test Split on different question types. 61.70 51.33 41.30 66.67 51.79 59.52 55.38 43.33 59.38 63.25 65.75 ? -6.14 -3.19 -4.42 -4.34 -8.33 -14.28 -2.38 -13.85 -26.67 -15.62 +5.13 +5.48</figDesc><table><row><cell>NA: Necessary Assumption, S:Strengthen, W:Weaken,</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph transformer for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Annual review of information science and technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gobinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="51" to="89" />
		</imprint>
	</monogr>
	<note>Natural language processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Advances in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="261" to="266" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language question answering: the view from here</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">natural language engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="275" to="300" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Problems of morphemic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="321" to="343" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DAGN: Discourse-Aware Graph Network for Logical Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5848" to="5855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<title level="m">Race: Large-scale reading comprehension dataset from examinations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive Graph Representations for Logical Formulas Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qika</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudai</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08124</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural machine reading comprehension: Methods and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3698</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02789</idno>
		<title level="m">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siru</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10334</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">2021. Fact-driven Logical Reasoning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Penn Discourse TreeBank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simple and effective graph autoencoders with one-hop linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Salha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07614</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks (TNN)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Storks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce Y</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01172</idno>
		<title level="m">Recent advances in natural language inference: A survey of benchmarks, resources, and approaches</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mokanarangan</forename><surname>Thayaparan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Valentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00389</idno>
		<title level="m">2020. A survey on explainability in machine reading comprehension</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03659</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14142</idno>
		<title level="m">Discourse-aware neural extractive text summarization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Do Transformers Really Perform Bad for Graph Representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01686</idno>
		<title level="m">Machine reading comprehension: a literature review</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SG-Net: Syntax-guided machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sufeng</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9636" to="9643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Retrospective Reader for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14506" to="14514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
