<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laborotory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laborotory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
							<email>jiali@buaa.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laborotory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">AI Application Research Center</orgName>
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
							<email>yhtian@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laborotory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the generalization behavior of Convolutional Neural Networks (CNN) is gradually transparent through explanation techniques with the frequency components decomposition. However, the importance of the phase spectrum of the image for a robust vision system is still ignored. In this paper, we notice that the CNN tends to converge at the local optimum which is closely related to the high-frequency components of the training images, while the amplitude spectrum is easily disturbed such as noises or common corruptions. In contrast, more empirical studies found that humans rely on more phase components to achieve robust recognition. This observation leads to more explanations of the CNN's generalization behaviors in both robustness to common perturbations and out-of-distribution detection, and motivates a new perspective on data augmentation designed by re-combing the phase spectrum of the current image and the amplitude spectrum of the distracter image. That is, the generated samples force the CNN to pay more attention to the structured information from phase components and keep robust to the variation of the amplitude. Experiments on several image datasets indicate that the proposed method achieves state-of-the-art performances on multiple generalizations and calibration tasks, including adaptability for common corruptions and surface variations, out-of-distribution detection, and adversarial attack. The code is released on github/iCGY96/APR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past few years, deep learning has achieved even surpassed human-level performances in many image recognition/classification tasks <ref type="bibr" target="#b15">[16]</ref>. However, the unintuitive generalization behaviors of neural networks, such as the vulnerability towards adversarial examples <ref type="bibr" target="#b11">[12]</ref>, common corruptions <ref type="bibr" target="#b20">[21]</ref>, the overconfidence for out-of-distribution * Corresponding author <ref type="figure">Figure 1</ref>. More empirical studies found that humans rely on more phase components to achieve robust recognition However, CNN without effective training restrictions tends to converge at the local optimum related to the amplitude spectrum of the image, leading to generalization behaviors counter-intuitive to humans.</p><p>(OOD) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>, are still confused in the community. It also leads that current deep learning models depend on the ability of training data to faithfully represent the data encountered during deployment.</p><p>To explain the generalization behaviors of neural networks, many theoretical breakthroughs have been made progressively by different model or algorithm perspectives <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">44]</ref>. Several works <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b25">26]</ref> investigate the generalization behaviors of Convolutional Neural Network (CNN) from a data perspective in the frequency domain, and demonstrate that CNN benefits from the high-frequency image components which are not perceivable to humans. Furthermore, a quantitative study is provided in <ref type="figure" target="#fig_2">Figure 4</ref> to indicate that the predictions of CNN are more sensitive to the variation of the amplitude spectrum. The above phenomena indicate that CNN tends to converge at the local optimum which is closely related to the high-frequency components of the training images. Although it is helpful when the test and training samples come from the identical distribution, yet the robustness of the CNN will be affected due of the amplitude spectrum is easily disturbed such as noises or common corruptions. On the other hand, earlier empirical arXiv:2108.08487v1 [cs.CV] <ref type="bibr" target="#b18">19</ref> Aug 2021 studies <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref> indicate that humans rely on more the components related to the phase to recognize an object. As is known, the human eye is much more robust than CNN, and this fact encourages us to rethink the influence of amplitude and phase on CNN's generalizability. A visualized example is shown in <ref type="figure">Figure 2</ref> to validate the importance of phase spectrum in <ref type="bibr" target="#b33">[34]</ref> to explain one counter-intuitive behavior of CNN. By replacing the amplitude spectrum of one Revolver with the amplitude spectrum of one Jigsaw Puzzle, the CNN classifies the fused image as Jigsaw Puzzle while humans could still recognize it as Revolver. In this example, the prediction outcomes of CNN are almost entirely determined by the amplitude spectrum of the image, which is barely perceivable to humans. On the other hand, even if the amplitude spectrum is replaced, the human is able to correctly recognize the identical object in the original picture. Moreover, we found that this phenomenon not only exists in training data (in-distribution) but also in OOD data as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. In these images, after exchanging the amplitude spectrum, the prediction of CNN is also transformed with the label of the amplitude spectrum. However, humans could still observe the object structure of the original images in the converted images.</p><p>Motivated by the powerful generalizability of the human, we argue that a robust CNN should be insensitive to the change of amplitude and pay more attention to the phase spectrum. To achieve this goal, a novel data augmentation method, called Amplitude-Phase Recombination (APR), is proposed. The core of APR is to re-combine the phase spectrum of the current image and the amplitude spectrum of the distracter image to generate a new training sample, whose label is set to the current image. That is, the generated samples force the CNN to capture more structured information from phase components rather than amplitude. Specifically, the distracter image of the current image comes in two ways: other images and its augmentations generated by existing data augmentation methods such as rotate and random crop, namely APR for the pair images (APR-P) and APR for the single image (APR-S) respectively.</p><p>Extensive experiments on multiple generalizations and calibration tasks, including adaptability for common corruptions and surface variations, OOD detection, and adversarial attack, demonstrate the proposed APR outperforms the baselines by a large margin. Meanwhile, it provides a uniform explanation to the texture bias hypothesis <ref type="bibr" target="#b9">[10]</ref> and the behaviors of both robustness to common perturbations and the overconfidence of OOD by the CNN's overdependence on the amplitude spectrum. That is, the various common perturbations change the high-frequency amplitude components significantly, while has little influence on the components related to the phase spectrum. Hence, the attack sample could confuse the CNN but is easily recognized by humans. On the other hand, the OOD samples of- <ref type="figure">Figure 2</ref>. An example of the importance of phase spectrum to explain the counter-intuitive behavior of CNN. The recombined image with the phase spectrum of Revolver and the amplitude spectrum of Jigsaw Puzzle is recognized as Jigsaw Puzzle by CNN. However, the human can still clearly recognize it as a Revolver.</p><p>ten exhibit totally different image structures but may share some similarities in the high-frequency amplitude components, which makes the CNN hard to distinguish.</p><p>Our main contributions are summarized as follows: 1) We propose that a robust CNN should be robust to the amplitude variance and pay more attention to the components related to the phase spectrum by a series of quantitative and qualitative analysis, 2) a novel data augmentation method APR is proposed to force the CNN pay more attention to the phase spectrum and achieves state-of-the-art performances on multiple generalizations and calibration tasks, including adaptability for common corruptions and surface variations, OOD detection, and adversarial attack, and 3) a unified explanation is provided to the behaviors of both robustness to common perturbations and the overconfidence of OOD by the CNN's over-dependence on the amplitude spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Frequency-Based Explanation for CNN. Recently, several works provide new insights into neural network behaviors from the aspects of the frequency domain. <ref type="bibr" target="#b49">[49]</ref> shows that high-frequency components play significant roles in promoting CNN's accuracy, unlike human beings. Based on this observation, <ref type="bibr" target="#b49">[49]</ref> concludes that smoothing the CNN kernels helps to enforce the model to use features of low frequencies. <ref type="bibr" target="#b12">[13]</ref> proposes an adversarial attack only targeting the low-frequency components in an image, which shows that the model does utilize the features in the lowfrequency domains for predictions instead of only learning from high-frequency components. <ref type="bibr" target="#b45">[45]</ref> demonstrates that state-of-the-art defenses are nearly as vulnerable as undefended models under low-frequency perturbations, which implies current defense techniques are only valid against adversarial attacks in the high-frequency domain. On the other side, <ref type="bibr" target="#b31">[32]</ref> demonstrates that CNNs can capture extra implicit features of the phase spectrum which are beneficial to face forgery detection. However, there are not works to give a qualitative study of the roles of amplitude and phase  spectrums for the generalization behavior of CNN. Data Augmentation. Data augmentation has been widely used to prevent deep neural networks from overfitting to the training data <ref type="bibr" target="#b0">[1]</ref>, and greatly improve generalization performance. The majority of conventional augmentation methods generate new data by applying transformations depending on the data type or the target task <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr" target="#b55">[55]</ref> proposes mixup, which linearly interpolates between two input data and utilizes the mixed data with the corresponding soft label for training. Then, CutMix <ref type="bibr" target="#b53">[53]</ref> suggests a spatial copy and paste based mixup strategy on images. AutoAugment <ref type="bibr" target="#b5">[6]</ref> is a learned augmentation method, where a group of augmentations is tuned to optimize performance on a downstream task. AugMix <ref type="bibr" target="#b20">[21]</ref> helps models withstand unforeseen corruptions by simply mixing random augmentations. However, many methods substantially degrade accuracy on non-adversarial images <ref type="bibr" target="#b35">[36]</ref> or need adaptive and complex parameters to different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Secret of CNN in the Frequency Domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Qualitative Study on the Frequency Domain</head><p>Beyond the examples in <ref type="figure">Figure 2</ref> and 3, here more qualitative analyses are given to measure the contributions of amplitude and phase. Several experiments are conducted on CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> to evaluate the performances of the CNNs which are trained with the inversed images by various types of amplitude and phase spectra. For the image x, its frequency domain F x is composed by amplitude A x and phase P x as:</p><formula xml:id="formula_0">Fx = Ax ? e i?Px ,<label>(1)</label></formula><p>where ? indicates the element-wise multiplication of two matrices. Here, four types of amplitude spectra, P x , P L x , P I x , and P H x are combined with four types of amplitude spectra, including A x , A L x , A I x , and A H x , respectively.</p><formula xml:id="formula_1">Here A L x , A I x ,</formula><p>A H x and P L x , P I x , P H x represent the amplitude spectrum and phase of low-frequency, intermediatefrequency and high-frequency by low-pass H l , high-pass H h , and band-pass H b filters, respectively. Noted in Eq.(1), if one element of A x is zero, then the corresponding element of F x would be zero, and the phase spectrum P x is not able to be considered. To alleviate the influence of this, we define the transfer function as:</p><formula xml:id="formula_2">z = 1, z = 0 z, otherwise.</formula><p>Finally, P x ,P L x ,P I x , andP H x are combined with A x ,? L x , A I x , and? H x , respectively. For quantitative evaluation, we trained the ResNet-18 with the inversed images by the above each pair of ampli- tude and phase spectra:</p><formula xml:id="formula_3">arg min ? l(f (iDF T (Ax ? e i?Px ); ?), y),<label>(2)</label></formula><p>where iDF T is the inverse Discrete Fourier Transform (DFT), and f (?) means the CNN model with the learnable parameters ?.</p><p>The test accuracies of the model trained by each pair are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. It is clear that the combination of phase and amplitude in the corresponding frequency domain achieves better performance in their various combinations, which indicates the CNN can capture effective information from both amplitude and phase spectrum. Moreover, when fixing the amplitude spectrum and phase spectrum respectively, the range of change without amplitude is larger than the case without phase according to the two directions of the arrow. It indicates that the convergence of the CNN more relies on the amplitude spectrum and neglects the phase spectrum.</p><p>Furthermore, we randomly select 1000 samples from CIFAR-10. Firstly, we generate 1000 corrupted samples by Gaussian noise and show the distribution of corrupted samples and original samples as shown in <ref type="figure" target="#fig_3">Figure 5</ref>(a). We could observe the amplitude spectrum in high-frequency of two types of samples is so different while the corrupted sample is just added invisible noise. Hence, CNN would make the wrong prediction when the amplitude spectrum is changed. This is also consistent with the conclusion that CNN captured high-frequency information in <ref type="bibr" target="#b49">[49]</ref>. Therefore, we propose an assumption (referred to as A1) that presumes: Assumption 1. CNN without effective training restrictions tends to perceive more amplitude spectrum instead of the phase spectrum.</p><p>Then, we can formulate another formal statement for the robustness of CNN as:</p><formula xml:id="formula_4">Corollary 1.</formula><p>With the assumption A1, there exists a sample x, y with its amplitude A x and phase P x , that the model f (?) without effective training restrictions cannot predict</p><formula xml:id="formula_5">robustly forx = iDF T ((A x + ) ? e i?Px )</formula><p>where is the upper bound of the perturbation allowed.</p><p>Secondly, we randomly select 1000 OOD samples from CIFAR-100. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>(b), it is not able to distinguish the amplitude spectrum in high-frequency of indistribution and out-of-distribution, even these samples are from different categories. As a result, CNN would be overconfident for some distributions when similar amplitude information appears. Therefore, we first attempt to provide an assumption (referred to as A2) for the behaviors of the robustness to common perturbations and the overconfidence of OOD: Assumption 2. The behaviors of the sensitivity to common perturbations and the overconfidence of OOD may be all due to CNN's over-dependence on the amplitude spectrum.</p><p>Meanwhile, we can extend our main argument for OOD to a new formal statement:</p><formula xml:id="formula_6">Corollary 2.</formula><p>With the assumptions A1 and A2, there exists a in-distribution sample x 1 , y and an out-of-distribution sample x 2 with their amplitude A x1 , A x2 and phase P x1 , P x2 , that the model without effective training restrictions would give a high confidence of the y forx = iDF T (A x1 ? e i?Px 2 ).</p><p>The proof is a direct outcome of the previous discussion and thus omitted. The Corollary 1 has been proved in previous works <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">44]</ref> and Corollary 2 can also be verified empirically (e.g., in <ref type="figure">Figure 2</ref> and 3), therefore we can safely state that these two corollaries can serve as the alternative explanations to the generalization behavior of CNN. Meanwhile, we provide more examples for proof in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Role of the Phase Spectrum</head><p>Previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11]</ref> have shown many of the important features of a signal are preserved if only the phase spectrum is retained. Meanwhile, several works of image saliency <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref> shown the connection of phase spectrum with the fixation of the human visual system. Further, we wish to explore why this important information of the image is retained in the phase spectrum. Here, we reinterpret the concept of discrete Fourier transforms from the perspective of template-based contrast computation <ref type="bibr" target="#b29">[30]</ref>.</p><p>Give a gray image x with resolution N ?N , its complexvalued Fourier coefficient at (u, v) can be computed as:</p><formula xml:id="formula_7">Fx(u, v) = N n=1 N m=1 x(n, m) ? e i? , = N n=1 N m=1 x(n, m) ? (cos ? + i ? sin ?),</formula><p>where ? = ?2?(un+vm)/N . Then, the real and the imaginary parts of F x (u, v) can be rewritten as:</p><formula xml:id="formula_8">Rx(u, v) = cos ??0 cos ? ? x(n, m) + cos ?&lt;0 cos ? ? x(n, m), Ix(u, v) = sin ??0 sin ? ? x(n, m) + sin ?&lt;0 sin ? ? x(n, m).</formula><p>The frequency in (u, v) by Fourier transform can be interpreted as computing by four template-based contrasts:</p><formula xml:id="formula_9">T R+ u,v (x) = max(cos ?, 0), T R? u,v (x) = max(? cos ?, 0), T I+ u,v (x) = max(sin ?, 0), T I? u,v (x) = max(? sin ?, 0).<label>(3)</label></formula><p>Moreover, we can define 4 ? N ? N templates for an image x based on the signs of the real-part and the imaginary-part. A template-based example is shown in <ref type="figure" target="#fig_4">Figure 6</ref>. More examples for templates are shown in Appendix. Meanwhile, the phase spectrum P x (u, v) for the image x is equal to arctan( Ix(u,v) Rx(u,v) ), which can be reinterpreted as:</p><formula xml:id="formula_10">Px(u, v) = arctan( x ? T I+ u,v ? x ? T I? u,v x ? T R+ u,v ? x ? T R? u,v</formula><p>). <ref type="bibr" target="#b3">(4)</ref> In Eq.(4), first, we can observe that the above four templates are encoded in the spectral phase. Hence, all 4?N ?N tem-plates are contained in the phase spectrum. This templatebased contrast can help to explain the importance of the phase spectrum. Once the templates containing more targets without distractors are correctly estimated, the model can highly effectively locate the target objects <ref type="bibr" target="#b29">[30]</ref>. On the other hand, these templates in the phase spectrum could help to recover the structural information of the original image even without the original amplitude spectrum as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The robustness human visual system can also rely on this visible structured information for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Amplitude-Phase Recombination</head><p>Motivated by the powerful generalizability of the human, we argue that reducing the dependence on the amplitude spectrum and enhancing the ability to capture phase spectrum can improve the robustness of CNN. Therefore, we introduce a none-parameter data augmentation routine, termed as Amplitude-Phase Recombination (APR), constructing more effective training examples based on the single sample or pair samples.</p><p>APR for the Pair Samples (APR-P). Firstly, (x i , y i ) and (x j , y j ) are two examples drawn at random from our training data. The main principle of APR is to change the amplitude spectrum as much as possible while keeping the phase spectrum and the corresponding labels unchanged. Hence, the APR-P could be defined as:</p><formula xml:id="formula_11">AP RP (xi, xj) = iDF T (Ax j ? e i?Px i ).<label>(5)</label></formula><p>Then, the inversed training pair samples (AP RP (xi, xj), yi) and (AP RP (xj, xi), yj) are generated. Note that we use labels of phase as targets to allow the model to find the effective structured information in the phase spectrum. Meanwhile, through a variety of spectrum changes, the model gradually ignores the information from the imperceptible amplitude spectrum. It can be implemented by the way as Mixup <ref type="bibr" target="#b14">[15]</ref> that uses a single data loader to obtain one minibatch, and then APR-P is applied to the original minibatch and the minibatch after random shuffling. APR for the Single Sample (APR-S). For a single training sample, we consider a set S consisting of K different (random or deterministic) transformations, denoted S = {S 1 , S 2 , . . . S K }. Here, we attempt to consider that the sample (x, y) and its transformed samplex are two different samples with the same label. The process of APR-S could be denoted as:</p><formula xml:id="formula_12">AP RS(S(xi),S(xi)) = iDF T (AS (x i ) ? e i?P S(x i ) ),<label>(6)</label></formula><p>whereS and S are transformations set based on different random seeds or sequences. Moreover, these two ways of amplitude-phase recombination could be used in combination and generate different gains for different data. Several examples from APR-P and APR-S are shown in <ref type="figure" target="#fig_5">Figure 7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets. CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b27">[28]</ref> datasets contain small 32x32x3 color natural images, both with 50,000 training images and 10,000 testing images. CIFAR-10 has 10 categories, and CIFAR-100 has 100. The larger and more difficult ImageNet <ref type="bibr" target="#b6">[7]</ref> dataset contains 1,000 classes of approximately 1.2 milion large-scale color images.</p><p>In order to measure a model's resilience to common corruptions and surface variations, we evaluate methods on the CIFAR-10-C, CIFAR-100-C, and ImageNet-C datasets <ref type="bibr" target="#b17">[18]</ref>. These datasets are constructed by corrupting the original CIFAR and ImageNet testsets. For each dataset, there are a total of 15 noise, blur, weather, and digital corruption types, each appearing at 5 severity levels or intensities. Since these datasets are used to measure network behavior under data shift, these 15 corruptions are not introduced into the training procedure.</p><p>To measure the ability for OOD detection, we consider CIFAR-10 as in-distribution and the following datasets as OOD: SVHN <ref type="bibr" target="#b32">[33]</ref>, resized LSUN and ImageNet <ref type="bibr" target="#b30">[31]</ref>, CIFAR-100 <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">CIFAR-10 and CIFAR-100</head><p>Training Setup. For a model's resilience to common corruptions and surface variations, we adopt various architectures including an All Convolutional Network <ref type="bibr" target="#b39">[40]</ref>, a DenseNet-BC (k = 2, d = 100) <ref type="bibr" target="#b24">[25]</ref>, a 40-2 Wide ResNet <ref type="bibr" target="#b54">[54]</ref>, and a ResNeXt-29 (32x4) <ref type="bibr" target="#b51">[51]</ref>. All networks use an initial learning rate of 0.1 which decay every 60 epochs. All models require 200 epochs for convergence. We optimize with stochastic gradient descent using Nesterov momentum <ref type="bibr" target="#b46">[46]</ref>. All input images are processed with "Standard" random left-right flipping and cropping prior to any augmentations. For the data augmentations of APR-S, we adopt those used in <ref type="bibr" target="#b20">[21]</ref> which is shown in Appendix. For the OOD detection, we use ResNet-18 <ref type="bibr" target="#b16">[17]</ref> with the same training strategies above. The data augmentations are set up the same as the above. We report the Area Under the Receiver Operating Characteristic curve (AUROC) <ref type="bibr" target="#b19">[20]</ref> as a threshold-free evaluation metric for a detection score. We divide all methods into two categories, one is to add one augmentation on the basis of standard augmentations (random left-right flipping, and cropping), and the other is to add a combination of multiple augmentations as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Common Corruptions and Surface Variations. We first evaluate all methods with common corruptions and surface variations, such as noise, blur, weather, and digital. Compared to the Mixup or CutMix based on pair images, our APR-P with exchanging amplitude spectrum in pair images achieves 6% lower absolute corruption error for CIFAR-100 as shown in <ref type="table" target="#tab_0">Table 1</ref>. For methods based on a combination of multiple augmentations, our APR-S of the single image with just Cross-Entropy loss (CE) performs better than AugMix with simply mixing random augmentations and using the Jensen-Shannon loss substantially. When combining our method for single and pair images, the APR-SP achieves 5% performance improvement compared with AugMix in CIFAR-100. In addition to surpassing numerous other data augmentation techniques, <ref type="table" target="#tab_0">Table 1</ref> also demonstrates that these gains come from simple recombination of amplitude and phase without a complex mixup strategy. More comparisons and results about test accuracy are shown in Appendix.</p><p>Out-of-Distribution Detection. We compare APR with those augmentations (Cutout, and Mixup) and those several training methods, the cross-entropy, supervised contrastive learning (SupCLR) <ref type="bibr" target="#b26">[27]</ref>, and state-of-the-art method contrasting shifted instances (CSI) <ref type="bibr" target="#b47">[47]</ref>. Since our goal is to calibrate the confidence, the maximum softmax probability is used to detect OOD samples. <ref type="table" target="#tab_1">Table 2</ref> shows the results. Firstly, APR-P consistently improves 2% AU-ROC than Cutout on CIFAR-10 while maintaining test accuracy. Then, after combining APR based on single and pair images, APR-SP exceeds CSI and gains in almost all OOD tasks. APR promotes CNN to pay more attention to the phase spectrum so that some OOD samples that affect CNN's decision-making in amplitude spectrum could be detected effectively.</p><p>Adversarial Attack. Moreover, the phenomenon of CNN focusing on amplitude spectrum leads to a question of whether APR can improve the adversarial robustness of models. Here, we evaluate several augmentations against one adversarial attack, AutoAttack <ref type="bibr" target="#b3">[4]</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the AutoAttack <ref type="bibr" target="#b3">[4]</ref> performance by combining different methods with revisiting adversarial training method of FSGM [50] on CIFAR10. The cutout is not able to effectively against adversarial attacks compared with the baseline with revisiting adversarial training method of FSGM <ref type="bibr" target="#b50">[50]</ref>. On the contrary, APR could effectively against AutoAttack while maintaining test accuracy. Compared with APR-P, APR-S for single images achieves more improvement on AutoAttack. Furthermore, the combination of these two strategies achieves better performance. It is evident that APR-SP improves the ability of the original model not only on clean images but also against adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet Classsification</head><p>Training Setup. ResNet-50 <ref type="bibr" target="#b16">[17]</ref> is trained with an initial learning rate of 0.1 which decay every 30 epochs. It is optimized with stochastic gradient descent using momentum 0.9 <ref type="bibr" target="#b34">[35]</ref>, and requires 100 epochs for convergence. All input images are pre-processed with standard random cropping horizontal mirroring. For the data augmentations of APR-S, we adopt those used in <ref type="bibr" target="#b20">[21]</ref> without augmentations such as contrast, color, brightness, sharpness, and Cutout, which may overlap with the corruptions of ImageNet-C. Following <ref type="bibr" target="#b20">[21]</ref>, we utilize the convention of normalizing the corruption error by the corruption err of AlexNet <ref type="bibr" target="#b28">[29]</ref>. Corruption Error (CE c ) is computed as CE c = Results. Our method APR-SP achieves 15% improvement than the baseline 80.6% mCE c while maintaining test accuracy. Other methods such as AutoAugment and Aug-Mix require a more complex combination strategy, while ours does not. Meanwhile, APR improves corruption robustness <ref type="bibr" target="#b20">[21]</ref> and uncertainty estimates across almost every individual corruption and severity level while the per-  <ref type="figure">Figure 8</ref>. The Gradient-weighted Class Activation Mapping <ref type="bibr" target="#b42">[43]</ref> of the baseline and the proposed APR-SP for images with frog noise. Best viewed in color. APR-SP still is able to focus on the parts of the target object even in a heavy fog.</p><p>formance of zoom blur is comparable with most methods. APR-SP gets about 5% improvement than APR-S and APR-P, and APR-SP with DeepAugment improves 6% than the reproduced DeepAugment <ref type="bibr" target="#b18">[19]</ref>. As shown in <ref type="figure">Figure 8</ref>, the CNN trained with APR-SP is able to focus on the parts of the target object for classification even in a heavy fog. These results demonstrate that scaling up APR from CIFAR to ImageNet also leads to state-of-the-art results in robustness and uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Labeled by Amplitude or Phase?</head><p>For our proposed APR-P, we utilize the labels of phase spectrum in the pair samples. Naturally, we wish to explore the impact of using labels amplitude and phase separately. Here, we add a linear classifier layer in ResNet-18 to predict the labels of the amplitude spectrum. The model is trained for the samplex combined by the phase spectrum P xi and the amplitude spectrum A xj by optimizing: arg min ? ?l(fP (x; ?), yi) + (1 ? ?) ? l(fA(x; ?), yj).</p><p>Then, the final prediction is defined as? = ?f P +(1??)f A . The recognition ability of the model to different distribution changes with ? as shown in <ref type="figure" target="#fig_7">Figure 9</ref>. With the enhancement of the weight of phase prediction, the accuracy of the model is improved, especially for common corruptions and surface variations, and OOD detection. Meanwhile, the detection ability of the model for OOD samples becomes stronger with the increase of phase attention. This result could fur- ther prove the correctness of our corollaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion &amp; Outlook</head><p>This paper proposes a series of quantitative and qualitative analyses to indicate that a robust CNN should be robust to the amplitude variance and pay more attention to the components related to the phase spectrum. Then, a novel data augmentation method APR is proposed to force the CNN to pay more attention to the phase spectrum and achieves state-of-the-art performances on multiple generalizations and calibration tasks. Also, a unified explanation is provided to the behaviors of both adversarial attack and the overconfidence of OOD by the CNN's over-dependence on the amplitude spectrum. Looking forward, more research directions about phase could be exploited in the future era of computer vision research. One possible direction is to explore how to represent part-whole hierarchies <ref type="bibr" target="#b22">[23]</ref> in neural networks that rely on the phase spectrum. On the other hand, more CNN models <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37]</ref> or convolution operations to capture more phase information are worth exploring.</p><p>In the supplemental material, we firstly visualize the distributions of the corrupted samples, adversarial samples, and OOD samples in the frequency domain to validate the Assumption 2 in the main text in Section A. Then, several typical templates of the phase spectrum are shown in Section B to intuitionally explain the rationality of the proposed APR method, and the implementation details of the data augmentation of APR-S are listed in Section C. In Section D, the Fourier analysis is provided to demonstrate the various gains from APR-P and APR-S, and the clean error analysis and OOD detection on ImageNet are also listed to clarify the excellent scalability of our method. A. More Studies on the Frequency Domain.</p><p>Here, we show more studies of the different types of the amplitude spectrum. For all samples in CIFAR, we generate the low-frequency, intermediate-frequency and high-frequency counterparts with r for non-zero parts set to [0, 8], <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> and <ref type="bibr">[16, 16 ? 2]</ref>, respectively. In <ref type="figure" target="#fig_8">Figure 10</ref>, we show the amplitude spectrum distributions of low-frequency, intermediate-frequency, and high-frequency from original samples, their corrupted samples, adversarial samples, and OOD samples respectively.</p><p>Firstly, for corrupted samples and adversarial samples from a single category, we could observe that their amplitude spectrum in high-frequency and intermediatefrequency has different distribution with the original samples even if only invisible noises are introduced. Moreover, the amplitude spectrums in low-frequency of corrupted samples and adversarial samples are indistinguishable from the original images. It also explains that CNN captures the high-frequency image components for classification <ref type="bibr" target="#b49">[49]</ref>. Hence, CNN would make a wrong prediction for 'similar' images (corruption and adversarial samples) when the parts of the amplitude spectrum are changed.</p><p>Then, for the OOD samples from CIFAR-100, it is evident that any type of the amplitude spectrum of indistribution and out-of-distribution could be not able to distinguish. CNN focusing on the amplitude spectrum has a huge risk that any OOD sample with a similar amplitude part of in-distribution samples would be classified as an indistribution sample. Hence, CNN would be overconfident for some out-of-distributions when similar amplitude information appears.</p><p>Overall, the above analyses explain our Assumption 2 in the main text, that the counter-intuitive behaviors of the sensitivity to common perturbations and the overconfidence of OOD maybe both be related to CNN's over-dependence on the amplitude spectrum. We do not focus on the high frequency only, because the OOD samples may come from the similarity of any amplitude part as shown in <ref type="figure" target="#fig_8">Figure 10</ref>. As a result, the focusing of some parts of the amplitude spectrum may create an invisible way to attack CNN, such as the adversarial attack and various corruptions (Corollary 1), and the amplitude attack or OOD attack (Corollary 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Templates from the Phase Spectrum</head><p>To better reveal the role of the phase spectrum, we analyze the phase spectrum in the different frequency domains. When DFT is applied on a gray image, there are totally 4 ? N 2 templates which are used to compute 2 ? N 2 contrast scores (as shown in <ref type="figure">Figure 11</ref>). Consequently, the frequency spectrum stores contrast values obtained at multiple scales and directions. The classification or other visual tasks could benefit from capturing the difference between targets and distractors by these templates. The templates of the lowest frequencies divide images into large regions, which are "coarse" partitions. Then, the templates of the highest frequencies provide "fine" partitions that achieve only high responses to noises and textures. In addition, the templates of the intermediate frequencies provide "moderate" partitions which may include the target object, and it has also been proved that it's beneficial for fixation prediction in <ref type="bibr" target="#b29">[30]</ref>. These templates in the phase spectrum could help to recover the structural information of the original image even without the original amplitude spectrum <ref type="bibr" target="#b33">[34]</ref>. The ro-  <ref type="figure">Figure 11</ref>. Fourier transform can be interpreted as dividing an image with 4 ? N 2 templates for contrast computation. bustness human visual system can also rely on this visible structured information for recognition <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Augmentation Operations</head><p>The augmentation operations used in APR-S are same with <ref type="bibr" target="#b20">[21]</ref> as shown in <ref type="figure" target="#fig_10">Figure 12</ref>. We do not use contrast, color, brightness, sharpness and Cutout as they may overlap with the corruptions of CIFAR-C and ImageNet-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Fourier Analysis</head><p>In order to better understand the reliance of our methods on different frequencies, here we measure the model sensitivity to the additive noise at differing frequencies. We add a total of 33 ? 33 Fourier basis vector to the CIFAR-10 test set, one at a time, and record the resulting error rate after adding each Fourier basis vector. Each point in the 33 ? 33 sensitivity heatmap shows the error rate on the CIFAR-10 test set after it has been perturbed by a single Fourier basis vector. Points corresponding to the low-frequency vectors are shown in the center of the heatmap, whereas the highfrequency vectors are farther than the center.</p><p>In <ref type="figure" target="#fig_1">Figure 13</ref>, we observe that the standard model is robust to the low-frequency perturbations but severely lacks robustness to the high-frequency perturbations, where the error rates exceed 80%. Then, the model trained by APR-P is more robust to all frequencies, especially to the low  <ref type="table" target="#tab_4">Table 5</ref> reports clean error <ref type="bibr" target="#b20">[21]</ref> of CIFAR-10 by different methods, and the proposed method achieves the best performances on various backbone networks. APR-SP not only improves the model adaptability to the common cor-  <ref type="figure" target="#fig_1">Figure 13</ref>. Model (Wide ResNet) sensitivity to the additive noise aligned with different Fourier basis vectors on CIFAR-10 validation images. We fix the additive noise to have L2 norm 15 and evaluate four methods: a standard trained model, APR-P, APR-S, APR-SP. Error rates are averaged over 1000 randomly sampled images from the test set. The standard trained model is highly sensitive to the additive noise in all but the lowest frequencies. APR-SP could substantially improve robustness to most frequency perturbations. ruptions, surface variations and OOD detection, but also improves the classification accuracy of the clean images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Clean Error</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. OOD Detection on ImageNet.</head><p>We conduct OOD experiments on the larger and more difficult ImageNet-1K dataset <ref type="bibr" target="#b37">[38]</ref>. ImageNet-O <ref type="bibr" target="#b21">[22]</ref> is adopted as the out-of-distribution dataset of ImageNet-1K. ImageNet-O includes 2K examples from ImageNet-22K <ref type="bibr" target="#b37">[38]</ref> excluding ImageNet-1K. The ResNet 50 <ref type="bibr" target="#b16">[17]</ref> is trained on ImageNet-1K and tested on both ImageNet-1K and ImageNet-O.</p><p>In order to evaluate the accuracy of in-distribution and the ability of OOD detection simultaneously, we introduce Open Set Classification Rate (OSCR) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref> as an evaluation metric. Let ? is a score threshold. The Correct Classification Rate (CCR) is the fraction of the samples where the correct class k has maximum probability and has a probability greater than ?: where D k I is the interest in-distribution classes that the neural network shall identify. The False Positive Rate (FPR) is the fraction of samples from OOD data D O that are classified as any in-distribution class k with a probability greater than ?: A larger value of OSCR indicates a better detection performance. As shown in <ref type="table">Table 6</ref>, APR-SP performs better than the standard augmentations even on the large and difficult dataset. Especially, APR-SP achieves about 22% improvement on AUROC. From the OSCR, APR-SP improves the performances of the OOD detection while maintaining test accuracy. These results indicate the excellent scalability of APR in larger-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. More CAM Visualization Examples</head><p>We also list more visualization examples with various corruptions in <ref type="figure" target="#fig_2">Figure 14</ref> and 15, the CNN trained by APR-SP is able to focus on the target objects for classification even with different common corruptions and surface variations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) In-distribution samples of airplane and frog (b) In-distribution samples of cat and bird (c) Out-of-distribution samples of 5 and 6 (d) Out-of-distribution samples of 2 and 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Four pairs of testing samples selected from in-distribution CIFAR-10<ref type="bibr" target="#b27">[28]</ref> and OOD SVHN that help explain that CNN captures more amplitude spectrum than phase spectrum for classification: First, in (a) and (b), the model (All Convolutional Network) correctly predicts the original image (1 st column in each panel), but the predicts are also exchanged after switching amplitude spectrum (3 rd column in each panel) while the human eye can still give the correct category through the contour information. Secondly, the model is overconfident for the OOD samples in (c) and (d). Similarly, after the exchange of amplitude spectrum, the label with high confidence is also exchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>We test the classification power of CNNs trained with various combinations of the amplitude and phase spectrum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The T-SNE<ref type="bibr" target="#b48">[48]</ref> distribution of the amplitude spectrum of high-frequency. Red represents the original image or indistribution (ID) samples in CIFAR-10, and gray represents the corrupted samples from CIFAR-10 or OOD samples from CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The four template-based contrasts for (u, v) in Eq.(3). Each Fourier coefficient is computed by dividing an image into two pairs of regions by the signs of real-part and the imaginarypart. These signs are encoded in spectral phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The two ways of Amplitude-Phase Recombination: APR-P and APR-S. The inversed images by APR-S are less different from the original image, compared with samples through ARP-P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>.</head><label></label><figDesc>The average of the 15 corruption errors is as the Mean Corruption Error (mCE c ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>The performance of ResNet-18 for various distribution as different attention weights for the amplitude and phase spectrum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>(a) A L of CS (b) A I of CS (c) A H of CS (d) A L of AA (e) A I of AA (f) A H of AA (g) A L of OOD (h) A I of OOD (i) A H of OODThe T-SNE<ref type="bibr" target="#b48">[48]</ref> visualization of the different types of the amplitude spectrum. Red represents the original image or indistribution (ID) samples in CIFAR-10, and gray represents Corrupted Samples (CS), the samples generated by Adversarial Attacks (AA) from CIFAR-10, or OOD samples from CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Positive Real Part T R+ (b) Negative Real Part T R? (c) Positive Imaginary Part T I+ (d) Negative Imaginary Part T I?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Illustration of augmentation operations applied to the same image. and intermediate frequencies. Moreover, the model trained by APR-S maintains robustness to low-frequency perturbations and improves robustness to the high-frequency perturbations, but is still sensitive to the additive noise in the intermediate frequencies. This further explains the experiments in Section 5.1.2 (main text) that APR-P improves performances of both OOD detection and defense adversarial attacks tasks. From Appendix A, the adversarial samples are more different from original samples in intermediate and high frequencies, while the OOD samples may share similarities with the original samples in any frequencies. The gains of APR-P and APR-S to different frequency domains bring the gains to different tasks. Furthermore, APR-SP (combining APR-S and APR-P) could substantially improve robustness to most frequency perturbations. The weak sensitivity to the intermediate frequencies is reasonable because of the gains for target prediction from intermediate frequencies in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>CCR(?) = |{x ? D k I ? argmax k P (k|x) =k ? P (k|x) ? ?} |D k I | .(A.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>F</head><label></label><figDesc>P R(?) = |{x|x ? DO ? max k P (k|x) ? ?}| |DO| . (A.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .Figure 15 .</head><label>1415</label><figDesc>The Gradient-weighted Class Activation Mapping<ref type="bibr" target="#b42">[43]</ref> of the baseline (the third column in each panel) and the proposed APR-SP (the fourth column in each panel) for images with different common corruptions and surface variations (the second column in each panel). The original images are in the first column in each panel. Best viewed in color. APR-SP still is robust even in various corruptions.(a) Weather: Fog (b) Digital: JPEG Compression (c) Digital: Elastic Transform (d) Digital: Pixelate The Gradient-weighted Class Activation Mapping [43] of the baseline (the third column in each panel) and the proposed APR-SP (the fourth column in each panel) for images with different common corruptions and surface variations (the second column in each panel). The original images are in the first column in each panel. Best viewed in color. APR-SP still is robust even in various corruptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The adaptability test of common corruptions and surface variations. Average classification error as percentages. All values are percentages and the best results are indicated in bold.</figDesc><table><row><cell>-SP</cell></row></table><note>Standard Cutout Mixup CutMix Adv Training APR-P AutoAugment AugMix APR-S ARP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The experiment of distinguishing in-and various OOD data for image classification. The best results are indicated in bold.</figDesc><table><row><cell>Method</cell><cell>Test acc.</cell><cell cols="6">CIFAR-10 ?? SVHN LSUN ImageNet LSUN(FIX) ImageNet(FIX) CIFAR100</cell><cell>Mean</cell></row><row><cell>Cross Entropy (CE)</cell><cell>93.0</cell><cell>88.6</cell><cell>90.7</cell><cell>88.3</cell><cell>87.5</cell><cell>87.4</cell><cell>85.8</cell><cell>88.1</cell></row><row><cell>CE w/ Cutout [8]</cell><cell>95.8</cell><cell>93.6</cell><cell>94.5</cell><cell>90.2</cell><cell>92.2</cell><cell>89.0</cell><cell>86.4</cell><cell>91.0</cell></row><row><cell>CE w/ Mixup [15]</cell><cell>96.1</cell><cell>78.1</cell><cell>80.7</cell><cell>76.5</cell><cell>80.7</cell><cell>76.0</cell><cell>74.9</cell><cell>77.8</cell></row><row><cell>CE w/ APR-P</cell><cell>95.0</cell><cell>98.1</cell><cell>93.7</cell><cell>95.2</cell><cell>91.4</cell><cell>91.1</cell><cell>88.9</cell><cell>93.1</cell></row><row><cell>SupCLR [27]</cell><cell>93.8</cell><cell>97.3</cell><cell>92.8</cell><cell>91.4</cell><cell>91.6</cell><cell>90.5</cell><cell>88.6</cell><cell>92.0</cell></row><row><cell>CSI [47]</cell><cell>94.8</cell><cell>96.5</cell><cell>96.3</cell><cell>96.2</cell><cell>92.1</cell><cell>92.4</cell><cell>90.5</cell><cell>94.0</cell></row><row><cell>CE w/ APR-S</cell><cell>95.1</cell><cell>90.4</cell><cell>96.1</cell><cell>94.2</cell><cell>90.9</cell><cell>89.1</cell><cell>86.8</cell><cell>91.3</cell></row><row><cell>CE w/ APR-SP</cell><cell>95.6</cell><cell>97.7</cell><cell>97.9</cell><cell>96.3</cell><cell>93.7</cell><cell>92.8</cell><cell>89.5</cell><cell>94.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance of ResNet-18 against with AutoAttack [4].</figDesc><table><row><cell>Method</cell><cell>Clean</cell><cell>AutoAttack[4] l inf ( = 8/255)</cell></row><row><cell>FSGM [50]</cell><cell>83.3</cell><cell>43.2</cell></row><row><cell>FSGM w/ Cutout</cell><cell>81.3</cell><cell>41.6</cell></row><row><cell>FSGM w/ APR-P</cell><cell>85.3</cell><cell>44.1</cell></row><row><cell>FSGM w/ APR-S</cell><cell>83.5</cell><cell>45.0</cell></row><row><cell>FSGM w/ APR-SP</cell><cell>84.3</cell><cell>45.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Test Error, Corruption Error (CEc), and mCEc values for various methods with ResNet-50 on ImageNet-C. All values are percentages and the best results are indicated in bold. Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG</figDesc><table><row><cell cols="4">Method Gauss Shot Standard Test Err. Noise 23.9 79 80</cell><cell>82</cell><cell>82</cell><cell>Blur 90</cell><cell>84</cell><cell>80</cell><cell>86</cell><cell>Weather 81 75</cell><cell>65</cell><cell>79</cell><cell>Digital 91</cell><cell>77</cell><cell>mCE 80 80.6</cell></row><row><cell>Patch Uniform</cell><cell>24.5</cell><cell>67</cell><cell>68</cell><cell>70</cell><cell>74</cell><cell>83</cell><cell>81</cell><cell>77</cell><cell>80</cell><cell>74 75</cell><cell>62</cell><cell>77</cell><cell>84</cell><cell>71</cell><cell>71 74.3</cell></row><row><cell>AutoAugment(AA)</cell><cell>22.8</cell><cell>69</cell><cell>68</cell><cell>72</cell><cell>77</cell><cell>83</cell><cell>80</cell><cell>81</cell><cell>79</cell><cell>75 64</cell><cell>56</cell><cell>70</cell><cell>88</cell><cell>57</cell><cell>71 72.7</cell></row><row><cell>Random AA</cell><cell>23.6</cell><cell>70</cell><cell>71</cell><cell>72</cell><cell>80</cell><cell>86</cell><cell>82</cell><cell>81</cell><cell>81</cell><cell>77 72</cell><cell>61</cell><cell>75</cell><cell>88</cell><cell>73</cell><cell>72 76.1</cell></row><row><cell>MaxBlur pool</cell><cell>23.0</cell><cell>73</cell><cell>74</cell><cell>76</cell><cell>74</cell><cell>86</cell><cell>78</cell><cell>77</cell><cell>77</cell><cell>72 63</cell><cell>56</cell><cell>68</cell><cell>86</cell><cell>71</cell><cell>71 73.4</cell></row><row><cell>SIN</cell><cell>27.2</cell><cell>69</cell><cell>70</cell><cell>70</cell><cell>77</cell><cell>84</cell><cell>76</cell><cell>82</cell><cell>74</cell><cell>75 69</cell><cell>65</cell><cell>69</cell><cell>80</cell><cell>64</cell><cell>77 73.3</cell></row><row><cell>AugMix</cell><cell>22.4</cell><cell>65</cell><cell>66</cell><cell>67</cell><cell>70</cell><cell>80</cell><cell>66</cell><cell>66</cell><cell>75</cell><cell>72 67</cell><cell>58</cell><cell>58</cell><cell>79</cell><cell>69</cell><cell>69 68.4</cell></row><row><cell>APR-S</cell><cell>24.5</cell><cell>61</cell><cell>64</cell><cell>60</cell><cell>73</cell><cell>87</cell><cell>72</cell><cell>81</cell><cell>72</cell><cell>67 62</cell><cell>56</cell><cell>70</cell><cell>83</cell><cell>79</cell><cell>71 70.5</cell></row><row><cell>APR-P</cell><cell>24.4</cell><cell>64</cell><cell>68</cell><cell>68</cell><cell>70</cell><cell>89</cell><cell>69</cell><cell>81</cell><cell>69</cell><cell>69 55</cell><cell>57</cell><cell>58</cell><cell>85</cell><cell>66</cell><cell>72 69.3</cell></row><row><cell>APR-SP</cell><cell>24.4</cell><cell>55</cell><cell>61</cell><cell>54</cell><cell>68</cell><cell>84</cell><cell>68</cell><cell>80</cell><cell>62</cell><cell>62 49</cell><cell>53</cell><cell>57</cell><cell>83</cell><cell>70</cell><cell>69 65.0</cell></row><row><cell>DeepAugment [19]</cell><cell>26.3</cell><cell>49</cell><cell>49</cell><cell>48</cell><cell>62</cell><cell>74</cell><cell>68</cell><cell>79</cell><cell>68</cell><cell>64 64</cell><cell>57</cell><cell>63</cell><cell>78</cell><cell>50</cell><cell>73 63.1</cell></row><row><cell cols="2">DeepAugment+APR-SP 26.4</cell><cell>44</cell><cell>45</cell><cell>41</cell><cell>57</cell><cell>70</cell><cell>60</cell><cell>79</cell><cell>56</cell><cell>56 50</cell><cell>54</cell><cell>54</cell><cell>78</cell><cell>47</cell><cell>71 57.5</cell></row><row><cell>(a) Original</cell><cell>(b) Fog</cell><cell cols="2">(c) Standard</cell><cell></cell><cell cols="2">(d) APR-SP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>CIFAR-10 Clean Error. All values are percentages and the best results are indicated in bold.Table 6. OOD performance of different methods on the larger and more difficult datasets, where ImageNet-1K is the in-distribution dataset and ImageNet-O is the OOD dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="10">Standard Cutout Mixup CutMix AutoAugment Adv Training AugMix APR-P APR-S APR-SP</cell></row><row><cell></cell><cell>AllConvNet</cell><cell>6.1</cell><cell>6.1</cell><cell>6.3</cell><cell>6.4</cell><cell>6.6</cell><cell>18.9</cell><cell>6.5</cell><cell>5.5</cell><cell>6.5</cell><cell>5.7</cell></row><row><cell>CIFAR-10-C</cell><cell>DenseNet WideResNet</cell><cell>5.8 5.2</cell><cell>4.8 4.4</cell><cell>5.5 4.9</cell><cell>5.3 4.6</cell><cell>4.8 4.8</cell><cell>17.9 17.1</cell><cell>4.9 4.9</cell><cell>5.0 4.8</cell><cell>5.1 5.0</cell><cell>4.8 4.3</cell></row><row><cell></cell><cell>ResNeXt</cell><cell>4.3</cell><cell>4.4</cell><cell>4.2</cell><cell>3.9</cell><cell>3.8</cell><cell>15.4</cell><cell>4.2</cell><cell>4.5</cell><cell>4.5</cell><cell>3.9</cell></row><row><cell cols="2">Mean</cell><cell>5.4</cell><cell>4.9</cell><cell>5.2</cell><cell>5.0</cell><cell>5.0</cell><cell>17.3</cell><cell>5.1</cell><cell>5.0</cell><cell>5.2</cell><cell>4.7</cell></row><row><cell cols="4">Method AUROC OSCR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Standard</cell><cell>40.9</cell><cell>36.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">APR-SP</cell><cell>62.3</cell><cell>53.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adversarial reciprocal points learning for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00953</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning open set network with discriminative reciprocal points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing network agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Akshay Raj Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9157" to="9168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Two-dimensional phase unwrapping: theory, algorithms, and software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghiglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pritt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low frequency adversarial perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16241</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07174</idno>
		<title level="m">Natural adversarial examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How to represent part-whole hierarchies in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12627</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10951" to="10960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarial examples are not bugs, they are features. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finding the secret of image saliency in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatialphase shallow learning: Rethinking face forgery detection in frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The importance of phase in signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">S</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adversarial training can hurt generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06032</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards spike-based machine intelligence with neuromorphic computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyadarshini</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7784</biblScope>
			<biblScope unit="page" from="607" to="617" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weight normalization: a simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archana</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grad-Cam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Eyal Ronen, and Orr Dunkelman. A simple explanation for the existence of adversarial examples with small hamming distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Safran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10861</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On the effectiveness of low frequency perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><forename type="middle">Weiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Brubaker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00073</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Conference on Neural Information Processing Systems (NeurIPS) 2020. Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High-frequency component helps explain the generalization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast is better than free: Revisiting adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scaling sgd batch size to 32k for imagenet training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference 2016. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
