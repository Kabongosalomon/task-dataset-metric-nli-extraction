<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recipe for a General, Powerful, Scalable Graph Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Ramp??ek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Wolf</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Valence Discovery</orgName>
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recipe for a General, Powerful, Scalable Graph Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being local, global or relative. The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the first architecture with a complexity linear in the number of nodes and edges O(N + E) by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework GRAPHGPS 1 that supports multiple types of encodings and that provides efficiency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical benefits gained by the modularity and the combination of different strategies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Transformers (GTs) alleviate fundamental limitations pertaining to the sparse message passing mechanism, e.g., over-smoothing <ref type="bibr" target="#b46">[47]</ref>, over-squashing <ref type="bibr" target="#b0">[1]</ref>, and expressiveness bounds <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b44">45]</ref>, by allowing nodes to attend to all other nodes in a graph (global attention). This benefits several real-world applications, such as modeling chemical interactions beyond the covalent bonds <ref type="bibr" target="#b62">[63]</ref>, or graph-based robotic control <ref type="bibr" target="#b36">[37]</ref>. Global attention, however, requires nodes to be better identifiable within the graph and its substructures <ref type="bibr" target="#b13">[14]</ref>. This has led to a flurry of recently proposed fullyconnected graph transformer models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31]</ref> as well as various positional encoding schemes leveraging spectral features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref> and graph features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>. Furthermore, standard global attention incurs quadratic computational costs O(N 2 ) for a graph with N nodes and E edges, that limits GTs to small graphs with up to a few hundred nodes.</p><p>Whereas various GT models focus on particular node identifiability aspects, a principled framework for designing GTs is still missing. In this work, we address this gap and propose a recipe for building general, powerful, and scalable (GPS) graph Transformers. The recipe defines (i) embedding modules responsible for aggregating positional encodings (PE) and structural encodings (SE) with the node, edge, and graph level input features; (ii) processing modules that employ a combination of local message passing and global attention layers (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The embedding modules organize multiple proposed PE and SE schemes into local and global levels serving as additional node features whereas positional and structural relative features contribute to edge features. The processing modules define a computational graph that allows to balance between message-passing graph neural networks (MPNNs) and Transformer-like global attention, including attention mechanisms linear in the number of nodes O(N ).</p><p>To the best of our knowledge, application of efficient attention models has not yet been thoroughly studied in the graph domain, e.g., only one work <ref type="bibr" target="#b10">[11]</ref> explores the adaptation of Performer-style <ref type="bibr" target="#b11">[12]</ref> attention approximation on small graphs. Particular challenges emerge with explicit edge features that are incorporated as attention bias in fully-connected graph transformers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b62">63]</ref>. Linear transformers do not materialize the attention matrix directly, hence incorporating edge features becomes a nontrivial task. In this work, we hypothesize that explicit edge features are not necessary for the global graph attention and adopt Performer <ref type="bibr" target="#b11">[12]</ref> and BigBird <ref type="bibr" target="#b65">[66]</ref> as exemplary linear attention mechanisms.</p><p>Our contributions are as follows. (i) Provide a general, powerful, scalable (GPS) GT blueprint that incorporates positional and structural encodings with local message passing and global attention, visualized in <ref type="figure" target="#fig_0">Figure 1.</ref> (ii) Provide a better definition of PEs and SEs and organize them into local, global, and relative categories. (iii) Show that GPS with linear global attention, e.g., provided by Performer <ref type="bibr" target="#b11">[12]</ref> or BigBird <ref type="bibr" target="#b65">[66]</ref>, scales to graphs with several thousand nodes and demonstrates competitive results even without explicit edge features within the attention module, whereas existing fully-connected GTs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b62">63]</ref> are limited to graphs of up to few hundred nodes. (iv) Conduct an extensive ablation study that evaluates contribution of PEs, local MPNN, and global attention components in perspective of several benchmarking datasets. (v) Finally, following the success of GraphGym <ref type="bibr" target="#b64">[65]</ref> we implement the blueprint within a modular and performant GRAPHGPS package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph Transformers (GT). Considering the great successes of Transformers in natural language processing (NLP) <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b31">32]</ref> and recently also in computer vision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>, it is natural to study their applicability in the graph domain as well. Particularly, they are expected to help alleviate the problems of over-smoothing and over-squashing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">54]</ref> in MPNNs, which are analogous to the vanishing gradients and lack of long-term dependencies in NLP. Fully-connected Graph Transformer <ref type="bibr" target="#b13">[14]</ref> was first introduced together with rudimentary utilisation of eigenvectors of the graph Laplacian as the node positional encoding (PE), to provide the otherwise graph-unaware Transformer a sense of nodes' location in the input graph. Building on top of this work, SAN <ref type="bibr" target="#b35">[36]</ref> implemented an invariant aggregation of Laplacian's eigenvectors for the PE, alongside conditional attention for real and virtual edges of a graph, which jointly yielded significant improvements. Concurrently, Graphormer <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b50">51]</ref> proposed using pair-wise graph distances (or 3D distances) to define relative positional encodings, with outstanding success on large molecular benchmarks. Further, GraphiT <ref type="bibr" target="#b43">[44]</ref> used relative PE derived from diffusion kernels to modulate the attention between nodes. Finally, GraphTrans <ref type="bibr" target="#b30">[31]</ref> proposed the first hybrid architecture, first using a stack of MPNN layers, before fully-connecting the graph. Since, the field has continued to propose alternative GTs: SAT <ref type="bibr" target="#b8">[9]</ref>, EGT <ref type="bibr" target="#b28">[29]</ref>, GRPE <ref type="bibr" target="#b47">[48]</ref>.</p><p>Positional and structural encodings. There have been many recent works on PE and SE, notably on Laplacian PE <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b38">39]</ref>, shortest-path-distance <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b62">63]</ref>, node degree centrality <ref type="bibr" target="#b62">[63]</ref>, kernel distance <ref type="bibr" target="#b43">[44]</ref>, random-walk SE <ref type="bibr" target="#b15">[16]</ref>, structure-aware <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>, and more. Some works also propose dedicated networks to learn the PE/SE from an initial encoding <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9]</ref>. To better understand the different PE/SE and the contribution of each work, we categorize them in <ref type="table" target="#tab_6">Table 1</ref> and examine their effect in Section 3.2. In most cases, PE/SE are used as soft bias, meaning they are simply provided as input features. But in other cases, they can be used to direct the messages <ref type="bibr" target="#b2">[3]</ref> or create bridges between distant nodes <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54]</ref>. Linear Transformers. The quadratic complexity of attention in the original Transformer architecture <ref type="bibr" target="#b54">[55]</ref> motivated the search for more efficient attention mechanisms that would scale linearly with the sequence length. Most of such linear transformers are developed for language modeling tasks, e.g., Linformer <ref type="bibr" target="#b57">[58]</ref>, Reformer <ref type="bibr" target="#b33">[34]</ref>, Longformer <ref type="bibr" target="#b3">[4]</ref>, Performer <ref type="bibr" target="#b11">[12]</ref>, BigBird <ref type="bibr" target="#b65">[66]</ref>, and have a dedicated Long Range Arena benchmark <ref type="bibr" target="#b51">[52]</ref> to study the limits of models against extremely long input sequences. Pyraformer <ref type="bibr" target="#b39">[40]</ref> is an example of a linear transformer for time series data, whereas S4 <ref type="bibr" target="#b22">[23]</ref> is a more general signal processing approach that employs the state space model theory without the attention mechanism. In the graph domain, linear transformers are not well studied. Choromanski et al. <ref type="bibr" target="#b10">[11]</ref> are the first to adapt Performer-style attention kernelization to small graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this work we provide a general, powerful, scalable (GPS) architecture for graph Transformers, following our 3-part recipe presented in <ref type="figure" target="#fig_0">Figure 1</ref>. We begin by categorization of existing positional (PE) and structural encodings (SE), a necessary ingredient for graph Transformers. Next, we analyse how these encodings also increase expressive power of MPNNs. The increased expressivity thus provides double benefit to our hybrid MPNN+Transformer architecture, which we introduce in Section 3.3. Last but not least, we provide an extensible implementation of GPS in GRAPHGPS package, built on top of PyG <ref type="bibr" target="#b19">[20]</ref> and GraphGym <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modular positional and structural encodings</head><p>One of our contribution is to provide a modular framework for PE/SE. It was shown in previous works that they are one of the most important factors in driving the performance of graph Transformers. Thus, a better understanding and organization of the PE and SE will aid in building of a more modular architecture and in guiding of the future research.</p><p>We propose to organize the PE and SE into 3 categories: local, global and relative in order to facilitate the integration within the pipeline and facilitate new research directions. They are presented visually in <ref type="figure" target="#fig_0">Figure 1</ref>, with more details in <ref type="table" target="#tab_6">Table 1</ref>. Although PE and SE can appear similar to some <ref type="table" target="#tab_6">Table 1</ref>: The proposed categorization of positional encodings (PE) and structural encodings (SE). Some encodings are assigned to multiple categories in order to show their multiple expected roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding type Description Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local PE node features</head><p>Allow a node to know its position and role within a local cluster of nodes. Within a cluster, the closer two nodes are to each other, the closer their local PE will be, such as the position of a word in a sentence (not in the text).</p><p>? Sum each column of non-diagonal elements of the m-steps random walk matrix. ? Distance between a node and the centroid of a cluster containing the node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global PE node features</head><p>Allow a node to know its global position within the graph. Within a graph, the closer two nodes are, the closer their global PE will be, such as the position of a word in a text.</p><p>? Eigenvectors of the Adjacency, Laplacian <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36]</ref> or distance matrices. ? SignNet <ref type="bibr" target="#b38">[39]</ref> (includes aspects of relative PE and local SE). ? Distance from the graph's centroid. ? Unique identifier for each connected component of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative PE edge features</head><p>Allow two nodes to understand their distances or directional relationships. Edge embedding that is correlated to the distance given by any global or local PE, such as the distance between two words.</p><p>? Pair-wise node distances <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b43">44]</ref> based on shortest-paths, heat kernels, random-walks, Green's function, graph geodesic, or any local/global PE. ? Gradient of eigenvectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> or any local/global PE. ? PEG layer <ref type="bibr" target="#b56">[57]</ref> with specific node-wise distances. ? Boolean indicating if two nodes are in the same cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local SE node features</head><p>Allow a node to understand what substructures it is a part of. Given an SE of radius m, the more similar the m-hop subgraphs around two nodes are, the closer their local SE will be.</p><p>? Degree of a node <ref type="bibr" target="#b62">[63]</ref>.</p><p>? Diagonal of the m-steps random-walk matrix <ref type="bibr" target="#b15">[16]</ref>.</p><p>? Time-derivative of the heat-kernel diagonal (gives the degree at t = 0). ? Enumerate or count predefined structures such as triangles, rings, etc. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b67">68]</ref>. ? Ricci curvature <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global SE graph features</head><p>Provide the network with information about the global structure of the graph. The more similar two graphs are, the closer their global SE will be.</p><p>? Eigenvalues of the Adjacency or Laplacian matrices <ref type="bibr" target="#b35">[36]</ref>. ? Graph properties: diameter, girth, number of connected components, # of nodes, # of edges, nodes-to-edges ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative SE edge features</head><p>Allow two nodes to understand how much their structures differ. Edge embedding that is correlated to the difference between any local SE.</p><p>? Pair-wise distance, encoding, or gradient of any local SE.</p><p>? Boolean indicating if two nodes are in the same substructure <ref type="bibr" target="#b4">[5]</ref> (similar to the gradient of sub-structure enumeration). extent, they are different yet complementary. PE gives a notion of distance, while SE gives a notion of structural similarity. One can always infer certain notions of distance from large structures, or certain notions of structure from short distances, but this is not a trivial task, and the objective of providing PE and SE remains distinct, as discussed in the following subsections.</p><p>Despite presenting a variety of possible functions, we focus our empirical evaluations on the global PE, relative PE and local SE since they are known to yield significant improvements. We leave the empirical evaluation of other encodings for future work.</p><p>Positional encodings (PE) are meant to provide an idea of the position in space of a given node within the graph. Hence, when two nodes are close to each other within a graph or subgraph, their PE should also be close. A common approach is to compute the pair-wise distance between each pairs of nodes or their eigenvectors as proposed in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57]</ref>, but this is not compatible with linear Transformers as it requires to materialize the full attention matrix <ref type="bibr" target="#b11">[12]</ref>. Instead, we want the PE to either be features of the nodes or real edges of the graph, thus a better fitting solution is to use the eigenvectors of the graph Laplacian or their gradient <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref>. See <ref type="table" target="#tab_6">Table 1</ref> for more PE examples.</p><p>Structural encodings (SE) are meant to provide an embedding of the structure of graphs or subgraphs to help increase the expressivity and the generalizability of graph neural networks (GNN). Hence, when two nodes share similar subgraphs, or when two graphs are similar, their SE should also be close. Simple approaches are to identify pre-defined patterns in the graphs as one-hot encodings, but they require expert knowledge of graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>. Instead, using the diagonal of the m-steps random-walk matrix encodes richer information into each node <ref type="bibr" target="#b15">[16]</ref>, such as for odd m it can indicate if a node is a part of an m-long cycle. Structural encodings can also be used to define the global graph structure, for instance using the eigenvalues of the Laplacian, or as relative edge features to identify if nodes are contained within the same clusters, with more examples in <ref type="table" target="#tab_6">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why do we need PE and SE in MPNN?</head><p>As reviewed earlier, several recent GNNs make use of positional encodings (PE) and structural encodings (SE) as soft biases to improve the model expressivity (summarized in <ref type="table" target="#tab_6">Table 1</ref>), which also leads to better generalization. In this section, we present an examination of PE and SE by showing how message-passing networks, despite operating on the graph structure, remain blind to the information encapsulated by the PE and SE.</p><p>1-Weisfeiler-Leman test (1-WL). It is well known that standard MPNNs are as expressive as the 1-WL test, meaning that they fail to distinguish non-isomorphic graphs under a 1-hop aggregation. We argue that the selected local, global and relative PE/SE allow MPNNs to become more expressive than the 1-WL test, thus making them fundamentally more expressive at distinguishing between nodes and graphs. To this end, we study the following two types of graphs ( <ref type="figure" target="#fig_4">Figure 2</ref> and Appendix C.1).    Circular Skip Link (CSL) graph. In a CSL graph-pair <ref type="bibr" target="#b45">[46]</ref>, we want to be able to distinguish the two non-isomorphic graphs. Since the 1-WL algorithm produces the same color for every node in both graphs, also every MPNN will fail to distinguish them. However, using a global PE (e.g., Laplacian PE <ref type="bibr" target="#b14">[15]</ref>) assigns each node a unique initial color and makes the CSL graph-pair distinguishable. This demonstrates that an MPNN cannot learn such a PE from the graph structure alone. Next, using a local SE (e.g., diagonals of m-steps random walk) can successfully capture the difference in the skip links of the two graphs <ref type="bibr" target="#b41">[42]</ref>, resulting in their different node coloring <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_0">v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p S Q f l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m r D m Z 1 w m q U H J l o v C V B A T k / n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c m G 4 K 2 + v E 7 a V 1 X v p n r d v K 7 U a 3 k c R T i D c 7 g E D 2 6 h D v f Q g B Y w Q H i G V 3 h</formula><formula xml:id="formula_1">v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p S Q f l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m r D m Z 1 w m q U H J l o v C V B A T k / n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c m G 4 K 2 + v E 7 a V 1 X v p n r d v K 7 U a 3 k c R T i D c 7 g E D 2 6 h D v f Q g B Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Y W n H z m F P 7 A + f w B w 0 m M 5 A = = &lt; / l a t e x i t &gt; a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / z 6 p g 4 U K R g I P 4 D X O / + 3 T 2 p o k O g M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p y Q b l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m r D m Z 1 w m q U H J l o v C V B A T k / n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c m G 4 K 2 + v E 7 a V 1 X v p n r d v K 7 U a 3 k c R T i D c 7 g E D 2 6 h D v f Q g B Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Y W n H z m F P 7 A + f w B x l G M 5 g = = &lt; / l a t e x i t &gt; c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / z 6 p g 4 U K R g I P 4 D X O / + 3 T 2 p o k O g M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p y Q b l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m r D m Z 1 w m q U H J l o v C V B A T k / n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c m G 4 K 2 + v E 7 a V 1 X v p n r d v K 7 U a 3 k c R T i D c 7 g E D 2 6 h D v f Q g B Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Y W n H z m F P 7 A + f w B x l G M 5 g = = &lt; / l a t e x i t &gt; c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W b j O q W G U u 6 Y M o g q M d B I n 2 3 m R k D g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q z K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y</formula><formula xml:id="formula_2">v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q z K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y</formula><p>Decalin molecule. In the bicyclic Decalin graph, Figures 2b and C.1b, the node a is isomorphic to node b, and so is the node c to node d. A 1-WL coloring of the nodes, and analogously MPNN, would generate one color for the nodes a, b and another color for c, d. The same applies to the aforementioned local SE <ref type="bibr" target="#b15">[16]</ref>. In case of link prediction, this causes potential links (a, d) and (b, d) to be indistinguishable <ref type="bibr" target="#b66">[67]</ref>. Using a distance-based relative PE on the edges or an eigenvectorbased global PE, however, would allow to differentiate the two links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GPS layer: an MPNN+Transformer hybrid</head><p>In this section we introduce the GPS layer, which is a hybrid MPNN+Transformer layer. First we argue how it alleviates the limitation of a closely related work. Next, we list the layer update equations which can be instantiated with a variety of MPNN and Transformer layers. Finally, we present its characteristics in terms of modularity, scalability and expressivity.</p><p>Preventing early smoothing. Why not use an architecture like GraphTrans <ref type="bibr" target="#b30">[31]</ref> comprising of a few layers of MPNNs before the Transformer? Since MPNNs are limited by problems of over-smoothing, over-squashing, and low expressivity against the WL test <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">54]</ref>, these layers could irreparably fail to keep some information in the early stage. Although they could make use of PE/SE or more expressive MPNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>, they are still likely to lose information. An analogous 2-stage strategy was successful in computer vision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> thanks to the high expressivity of convolutional layers on grids, but we do not expect it to achieve the same success on graphs due to the limitations of message-passing. </p><formula xml:id="formula_3">X +1 , E +1 = GPS X , E , A<label>(1)</label></formula><p>computed as X +1</p><formula xml:id="formula_4">M , E +1 = MPNN e X , E , A ,<label>(2)</label></formula><formula xml:id="formula_5">X +1 T = GlobalAttn X ,<label>(3)</label></formula><formula xml:id="formula_6">X +1 = MLP X +1 M + X +1 T ,<label>(4)</label></formula><p>where A ? R N ?N is the adjacency matrix of a graph with N nodes and E edges; X ? R N ?d , E ? R E?d are the d -dimensional node and edge features, respectively; MPNN e and GlobalAttn are instances of an MPNN with edge features and of a global attention mechanism at the -th layer with their corresponding learnable parameters, respectively; MLP is a 2-layer MLP block.</p><p>Modularity is achieved by allowing drop-in replacement for a number of module choices, including the initial PE/SE types, the networks that processes those PE/SE, the MPNN and global attention layers that constitute a GPS layer, and the final task-specific prediction head. Further, as research advances in different directions, GRAPHGPS allows to easily implement new PE/SE and other layers.</p><p>Scalability is achieved by allowing for a computational complexity linear in both the number of nodes and edges O(N + E); excluding the potential precomputation step required for various PE, such as Laplacian eigen-decomposition. By restricting the PE/SE to real nodes and edges, and by excluding the edge features from the global attention layer, we can avoid materializing the full quadratic attention matrix. Therefore we can utilize a linear Transformer with O(N ) complexity, while the complexity of an MPNN is O(E). For sparse graphs such as molecular graphs, regular graphs, and knowledge graphs, the edges are practically proportional to the nodes E = ?(N ), meaning the entire complexity can be considered linear in the number of nodes O(N ). Empirically, even on small molecular graphs, our architecture reduces computation time compared to other GT models, e.g., a model of~6M parameters requires 196s per epoch on the ogbg-molpcba <ref type="bibr" target="#b26">[27]</ref> dataset, compared to 883s for SAN <ref type="bibr" target="#b35">[36]</ref> on the same GPU type.</p><p>Expressivity in terms of sub-structure identification and the Weisfeiler-Leman (WL) test is achieved via providing a rich set of PE/SE, as proposed in various works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> and detailed in Section 3.1. Further, the Transformer allows to resolve the expressivity bottlenecks caused by over-smoothing <ref type="bibr" target="#b35">[36]</ref> and over-squashing <ref type="bibr" target="#b0">[1]</ref> by allowing information to spread across the graph via full-connectivity. Finally, in Section 3.4, we demonstrate that, given the right components, the proposed architecture does not lose edge information and is a universal function approximator on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical expressivity</head><p>In this section, we first discuss how the MPNN layer allows to propagate edge and neighbor information on the nodes. Then, we show that the proposed model is a universal function approximator on graphs, similarly to the SAN architecture <ref type="bibr" target="#b35">[36]</ref>.</p><p>Preserving edge information in the Transformer layer. Most GTs do not fully utilize edge features of the input graph. The Graph Transformer <ref type="bibr" target="#b13">[14]</ref>, SAN <ref type="bibr" target="#b35">[36]</ref> and Graphormer <ref type="bibr" target="#b62">[63]</ref> only use edge features to condition the attention between a pair of nodes, that is, they influence the attention gating mechanism but are not explicitly involved in updating of the node representations. GraphiT <ref type="bibr" target="#b43">[44]</ref> does not consider edge features at all. Recent 2-step methods GraphTrans <ref type="bibr" target="#b30">[31]</ref> and SAT <ref type="bibr" target="#b8">[9]</ref> can use edge features in their first MPNN step, however this step is applied only once and typically includes several k rounds of message passing. Therefore this latter approach may suffer from initial oversmoothing, as k-hop neighborhoods together with the respective edge features need to be represented in a fixed-sized node representation.</p><p>On the other hand, in GPS, interleaving one round of local neighborhood aggregation via an MPNN layer with global self-attention mechanism reduces the initial representation bottleneck and enables iterative local and global interactions. In the attention, the key-query-value mechanism only explicitly depends on the node features, but assuming efficient representation encoding by the MPNN, the node features can implicitly encode edge information, thus edges can play a role in either the key, query, or values. In Appendix C.2, we give a more formal argument on how, following an MPNN layer, node features can encode edge features alongside information related to node-connectivity.</p><p>Universal function approximator on graphs. Kreuzer et al. <ref type="bibr" target="#b35">[36]</ref>[Sec. 3.5] demonstrated the universality of graph Transformers. It was shown that, given the full set of Laplacian eigenvectors, the model was a universal function approximator on graphs and could provide an approximate solution to the isomorphism problem, making it more powerful than any Weisfeiler-Leman (WL) isomorphism test given enough parameters. Here, we argue that the same holds for our architecture since we can also use the full set of eigenvectors, and since all edge information can be propagated to the nodes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform ablation studies on 4 datasets to evaluate the contribution of the message-passing module, the global attention module, and the positional or structural encodings. Then, we evaluate GPS on a diverse set of 11 benchmarking datasets, and show state-of-the-art (SOTA) results in many cases.</p><p>We test on datasets from different sources to ensure diversity, providing their detailed description in Appendix A.1. From the Benchmarking GNNs <ref type="bibr" target="#b14">[15]</ref>, we test on the ZINC, PATTERN, CLUSTER, MNIST, CIFAR10. From the open graph benchmark (OGB) <ref type="bibr" target="#b26">[27]</ref>, we test on all graph-level datasets: ogbg-molhiv, ogbg-molpcba, ogbg-code2, and ogbg-ppa, and from their large-scale challenge we test on the OGB-LSC PCQM4Mv2 <ref type="bibr" target="#b27">[28]</ref>. Finally, we also select MalNet-Tiny <ref type="bibr" target="#b20">[21]</ref> with 5000 graphs, each of up to 5000 nodes, since the number of nodes provide a scaling challenge for Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation studies</head><p>In this section, we evaluate multiple options for the three main components of our architecture in order to gauge their contribution to predictive performance and to better guide dataset-specific hyper-parameter optimization. First, we quantify benefits of the considered global-attention modules in 4 tasks. Then, we note that the MPNN layer is an essential part for high-performing models, and identify the layer type most likely to help. Finally, we observe when different global PE or local SE provide significant boost in the performance. All ablation results are averaged over multiple random seeds and summarized in <ref type="table" target="#tab_1">Table 2</ref>, with additional information available in Appendix B.</p><p>Global-Attention module. Here we consider global attention implemented as O(N 2 ) key-queryvalue Transformer attention or linear-time attention mechanisms of Performer or BigBird. We notice in <ref type="table" target="#tab_1">Table 2a</ref> that using a Transformer is always beneficial, except for the ZINC dataset where no changes are observed. This motivates our architecture and the hypothesis that long-range dependencies are generally important. We further observe that Performer falls behind Transformer in terms of the predictive performance, although it provides a gain over the baseline and the ability to scale to very large graphs. Finally, BigBird in our setting offers no significant gain, while also being slower than Performer (see Appendix B).</p><p>Having no gain on the ZINC dataset is expected since the task is a combination of the computed octanol-water partition coefficient (cLogP) <ref type="bibr" target="#b59">[60]</ref> and the synthetic accessibility score (SA-score) <ref type="bibr" target="#b18">[19]</ref>, both of which only count occurrences of local sub-structures. Hence, there is no need for a global connectivity, but a strong need for structural encodings.</p><p>Message-passing module. Next, we evaluate the effect of various message-passing architectures, <ref type="table" target="#tab_1">Table 2a</ref>. It is apparent that they are fundamental to the success of our method: removing the layer leads to a significant drop in performance across all datasets. Indeed, without an MPNN, the edge  <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref> 0.282 ? 0.015 97.340 ? 0.143 67.312 ? 0.311 85.568 ? 0.088 73.840 ? 0.326 GatedGCN-LSPE <ref type="bibr" target="#b15">[16]</ref> 0.090 ? 0.001 ----PNA <ref type="bibr" target="#b12">[13]</ref> 0.188 ? 0.004 97.94 ? 0.12 70.35 ? 0.63 --DGN <ref type="bibr" target="#b2">[3]</ref> 0.168 ? 0.003 -72.838 ? 0.417 86.680 ? 0.034 -GSN <ref type="bibr" target="#b5">[6]</ref> 0.101 ? 0.010 ----CIN <ref type="bibr" target="#b4">[5]</ref> 0.079 ? 0.006 ----CRaWl <ref type="bibr" target="#b52">[53]</ref> 0.085 ? 0.004 97.944 ? 0.050 69.013 ? 0.259 --GIN-AK+ <ref type="bibr" target="#b67">[68]</ref> 0.080 ? 0.001 -72.19 ? 0.13 86.850 ? 0.057 -SAN <ref type="bibr" target="#b35">[36]</ref> 0.139 ? 0.006 --86.581 ? 0.037 76.691 ? 0.65 Graphormer <ref type="bibr" target="#b62">[63]</ref> 0.122 ? 0.006 ----K-Subgraph SAT <ref type="bibr" target="#b8">[9]</ref> 0.094 ? 0.008 --86.848 ? 0.037 77.856 ? 0.104 EGT <ref type="bibr" target="#b28">[29]</ref> 0.108 ? 0.009 98. features are not taken into consideration at all. Additionally, without reinforcing of the local graph structure, the network can overfit to the PE/SE. This reiterates findings of Kreuzer et al. <ref type="bibr" target="#b35">[36]</ref>, where considerably larger weights were assigned to the local attention.</p><p>We also find that although a vanilla PNA <ref type="bibr" target="#b12">[13]</ref> generally outperforms GINE <ref type="bibr" target="#b25">[26]</ref> and GatedGCN <ref type="bibr" target="#b6">[7]</ref>, adding the PE and SE results in major performance boost especially for the GatedGCN. This is consistent with results of Dwivedi et al. <ref type="bibr" target="#b15">[16]</ref> and shows the importance of these encodings for gating.</p><p>Perhaps the necessity of a local message-passing module is due to the limited amount of graph data, and scaling to colossal datasets <ref type="bibr" target="#b48">[49]</ref> that we encounter in language and vision could change that. Indeed, the Graphormer architecture <ref type="bibr" target="#b62">[63]</ref> was able to perform very well on the full PCQM4Mv2 dataset without a local module. However, even large Transformer-based language models <ref type="bibr" target="#b7">[8]</ref> and vision models <ref type="bibr" target="#b24">[25]</ref> can benefit from an added local aggregation and outperform pure Transformers.</p><p>Positional/Structural Encodings. Finally, we evaluate the effects of various PE/SE schemes, <ref type="table" target="#tab_1">Table 2b</ref>. We find them generally beneficial to downstream tasks, in concordance to the vast literature on the subject (see <ref type="table" target="#tab_6">Table 1</ref>). The benefits of the different encodings are very dataset dependant, with the random-walk structural encoding (RWSE) being more beneficial for molecular data and the Laplacian eigenvectors encodings (LapPE) being more beneficial in image superpixels. However, using SignNet with DeepSets encoding <ref type="bibr" target="#b38">[39]</ref> as an improved way of processing the LapPE seems to be consistently successful across tasks. We hypothesize that SignNet can learn structural representation using the eigenvectors, for example, to generate local heat-kernels that approximate random walks <ref type="bibr" target="#b1">[2]</ref>. Last but not least we evaluate PEG-layer design <ref type="bibr" target="#b56">[57]</ref> with Laplacian eigenmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmarking GPS</head><p>We compare GPS against a set of popular message-passing neural networks (GCN, GIN, GatedGCN, PNA, etc.), graph transformers (SAN, Graphormer, etc.), and other recent graph neural networks with SOTA results (CIN, CRaWL, GIN-AK+, ExpC). To ensure diverse benchmarking tasks, we use datasets from Benchmarking-GNNs <ref type="bibr" target="#b14">[15]</ref>, OGB <ref type="bibr" target="#b26">[27]</ref> and its large-scale challenge <ref type="bibr" target="#b27">[28]</ref>, and Long-Range Graph Benchmark <ref type="bibr" target="#b16">[17]</ref>, with more details given in Appendix A.1. We report the mean and standard deviation over 10 random seeds if not explicitly stated otherwise.</p><p>Benchmarking GNNs <ref type="bibr" target="#b14">[15]</ref>. We first benchmark our method on 5 tasks from Benchmarking GNNs <ref type="bibr" target="#b14">[15]</ref>, namely ZINC, MNIST, CIFAR10, PATTERN, and CLUSTER, shown in <ref type="table" target="#tab_2">Table 3</ref>. We observe that our GPS gives SOTA results on ZINC and the second best in 3 more datasets, showcasing the ability to perform very well on a variety of synthetic tasks designed to test the model expressivity.</p><p>Open Graph Benchmark <ref type="bibr" target="#b26">[27]</ref>. Next, we benchmark on all 4 graph-level tasks from OGB, namely molhiv, molpcba, ppa, and code2, <ref type="table" target="#tab_4">Table 4</ref>. On the molhiv dataset, we observed our model to suffer  OGB-LSC PCQM4Mv2 <ref type="bibr" target="#b27">[28]</ref>. The large-scale PCQM4Mv2 dataset has been a popular benchmark for recent GTs, particularly due to Graphormer <ref type="bibr" target="#b62">[63]</ref> winning the initial challenge. We report the results in <ref type="table" target="#tab_5">Table 5</ref>, observing significant improvements over message-passing networks at comparable parameter budget. GPS also outperforms GRPE <ref type="bibr" target="#b47">[48]</ref>, EGT <ref type="bibr" target="#b28">[29]</ref>, and Graphormer <ref type="bibr" target="#b62">[63]</ref> with less than half their parameters, and with significantly less overfitting on the training set. Contrarily to Graphormer, we do not need to precompute spatial distances from approximate 3D molecular conformers <ref type="bibr" target="#b63">[64]</ref>, the RWSEs we utilize are graph-based only.</p><p>MalNet-Tiny. The MalNet-Tiny <ref type="bibr" target="#b20">[21]</ref> dataset consists of function call graphs with up to 5,000 nodes. These graphs are considerably larger than previously considered inductive graph-learning benchmarks, which enables us to showcase scalability of GPS to much larger graphs than prior methods. Our GPS reaches 92.72% ? 0.7pp test accuracy when using Performer global attention. Interestingly, using Transformer global attention leads to further improved GPS performance, 93.36% ? 0.6pp (based on 10 runs), albeit at the cost of doubled run-time. In both cases, we used comparable architecture to Freitas et al. <ref type="bibr" target="#b20">[21]</ref>, with 5 layers and 64 dimensional hidden node representation, and outperform their best GIN model with 90% accuracy. See Appendix B for GPS ablation study on MalNet-Tiny.</p><p>Long-Range Graph Benchmark <ref type="bibr" target="#b16">[17]</ref>. Finally, we evaluate the GPS method on a recent Long-Range Graph Benchmark (LRGB) suite of 5 datasets that are intended to test a method's ability to capture long-range dependencies in the input graphs. We abide to the~500k model parameter budget and <ref type="table" target="#tab_6">Table 6</ref>: Test performance on long-range graph benchmarks (LRGB) <ref type="bibr" target="#b16">[17]</ref>. Shown is the mean ? s.d. of 4 runs. The first, second, and third best are highlighted. * SAN on COCO-SP exceeded 60h time limit on an NVidia A100 GPU system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PascalVOC-SP COCO-SP Peptides-func Peptides-struct PCQM-Contact</p><formula xml:id="formula_7">F1 score ? F1 score ? AP ? MAE ? MRR ? GCN</formula><p>0.1268 ? 0.0060 0.0841 ? 0.0010 0.5930 ? 0.0023 0.3496 ? 0.0013 0.3234 ? 0.0006 GINE 0.1265 ? 0.0076 0.1339 ? 0.0044 0.5498 ? 0.0079 0.3547 ? 0.0045 0.3180 ? 0.0027 GatedGCN 0.2873 ? 0.0219 0.2641 ? 0.0045 0.5864 ? 0.0077 0.3420 ? 0.0013 0.3218 ? 0.0011 GatedGCN+RWSE 0.2860 ? 0.0085 0.2574 ? 0.0034 0.6069 ? 0.0035 0.3357 ? 0.0006 0.3242 ? 0.0008 Transformer+LapPE 0.2694 ? 0.0098 0.2618 ? 0.0031 0.6326 ? 0.0126 0.2529 ? 0.0016 0.3174 ? 0.0020 SAN+LapPE 0.3230 ? 0.0039 0.2592 ? 0.0158* 0.6384 ? 0.0121 0.2683 ? 0.0043 0.3350 ? 0.0003 SAN+RWSE 0.3216 ? 0.0027 0.2434 ? 0.0156* 0.6439 ? 0.0075 0.2545 ? 0.0012 0.3341 ? 0.0006</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPS (ours)</head><p>0.3748 ? 0.0109 0.3412 ? 0.0044 0.6535 ? 0.0041 0.2500 ? 0.0005 0.3337 ? 0.0006 closely follow the experimental setup and hyperparameter choices of the graph Transformer baselines tested in LRGB <ref type="bibr" target="#b16">[17]</ref>. We keep the same node/edge encoders and model depth (number of layers), deviating only in two aspects: i) we slightly decrease the size of hidden node representations to fit within the parameter budget, ii) we employ cosine learning rate schedule as in our other experiments (Section A.3). For each dataset we utilize LapPE positional encodings and GPS with GatedGCN <ref type="bibr" target="#b6">[7]</ref> and Transformer <ref type="bibr" target="#b54">[55]</ref> components.</p><p>GPS improves over all evaluated baselines in 4 out of 5 LRGB datasets <ref type="table" target="#tab_6">(Table 6</ref>). Additionally, we conducted GPS ablation studies on PascalVOC-SP and Peptides-func datasets in the same fashion as for 4 previous datasets in <ref type="table" target="#tab_1">Table 2</ref>, presented in Tables B.5 and B.6, respectively. For both datasets the global attention, in form of Transformer or Performer, is shown to be a critical component of the GPS in outperforming MPNNs. In the case of PascalVOC-SP, interestingly, the Laplacian PEs are not beneficial, as without them the GPS scores even higher F1-score 0.3846, and PEG <ref type="bibr" target="#b56">[57]</ref> relative distance embeddings enable the highest score of 0.3956.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our work is setting the foundation for a unified architecture of graph neural networks, with modular and scalable graph Transformers and a broader understanding of the role of graphs with positional and structural encodings. In our ablation studies, we demonstrated the importance of each module: the Transformer, flexible message-passing, and rich positional and structural encodings all contributed to the success of GPS on a wide variety of benchmarks. Indeed, considering 5 Benchmarking-GNN tasks <ref type="bibr" target="#b14">[15]</ref>, 5 OGB(-LSC) tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, 5 LRGB tasks <ref type="bibr" target="#b16">[17]</ref> and MalNet-Tiny, we outperformed every graph Transformer on 11 out of 16 tasks while also achieving state-of-the-art on 8 of them. We further showed that the model can scale to very large graphs of several thousand nodes, far beyond any previous graph Transformer. By open-sourcing the GRAPHGPS package, we hope to accelerate the research in efficient and expressive graph Transformers, and move the field closer to a unified hybrid Transformer architecture for graphs.</p><p>Limitations. We find that graph transformers are sensitive to hyperparameters and there is no onesize-fits-all solution for all datasets. We also identify a lack of challenging graph datasets necessitating long-range dependencies where linear attention architectures could exhibit all scalability benefits.</p><p>Societal Impact. As a general graph representation learning method, we do not foresee immediate negative societal outcomes. However, its particular application, e.g., in drug discovery or computational biology, will have to be thoroughly examined for trustworthiness or malicious usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head><p>A.1 Datasets description  MNIST and CIFAR10 <ref type="bibr" target="#b14">[15]</ref> (CC BY-SA 3.0 and MIT License) are derived from like-named image classification datasets by constructing an 8 nearest-neighbor graph of SLIC superpixels for each image. The 10-class classification tasks and standard dataset splits follow the original image classification datasets, i.e., for MNIST 55K/5K/10K and for CIFAR10 45K/5K/10K train/validation/test graphs.</p><p>PATTERN and CLUSTER <ref type="bibr" target="#b14">[15]</ref> (MIT License) are synthetic datasets sampled from Stochastic Block Model. Unlike other datasets, the prediction task here is an inductive node-level classification.</p><p>In PATTERN the task is to recognize which nodes in a graph belong to one of 100 possible sub-graph patterns that were randomly generated with different SBM parameters than the rest of the graph. In CLUSTER, every graph is composed of 6 SBM-generated clusters, each drawn from the same distribution, with only a single node per cluster containing a unique cluster ID. The task is to infer which cluster ID each node belongs to.</p><p>ogbg-molhiv and ogbg-molpcba <ref type="bibr" target="#b26">[27]</ref> (MIT License) are molecular property prediction datasets adopted by OGB from MoleculeNet. These datasets use a common node (atom) and edge (bond) featurization that represent chemophysical properties. The prediction task of ogbg-molhiv is binary classification of molecule's fitness to inhibit HIV replication. The ogbg-molpcba, derived from PubChem BioAssay, targets to predict results of 128 bioassays in multi-task binary classification setting.</p><p>ogbg-ppa <ref type="bibr" target="#b26">[27]</ref> (CC-0 license) consists of protein-protein association (PPA) networks derived from 1581 species categorized to 37 taxonomic groups. Nodes represent proteins and edges encode the normalized level of 7 different associations between two proteins. The task is to classify which of the 37 groups does a PPA network originate from.</p><p>ogbg-code2 <ref type="bibr" target="#b26">[27]</ref> (MIT License) is comprised of abstract syntax trees (ASTs) derived from source code of functions written in Python. The task is to predict the first 5 subtokens of the original function's name.</p><p>A small number of these ASTs are much larger than the average size in the dataset. Therefore we truncated ASTs with over 1000 nodes and kept the first 1000 nodes according to their depth in the AST. This impacted 2521 (0.5%) graphs in the dataset.</p><p>OGB-LSC PCQM4Mv2 <ref type="bibr" target="#b27">[28]</ref> (CC BY 4.0 license) is a large-scale molecular dataset that shares the same featurization as ogbg-mol* datasets. The task is to regress the HOMO-LUMO gap, a quantum physical property originally calculated using Density Functional Theory. True labels for original "test-dev" and "test-challange" dataset splits are kept private by the OGB-LSC challenge organizers. Therefore for the purpose of this paper we used the original validation set as the test set, while we left out random 150K molecules for our validation set.</p><p>PCQM4Mv2-Subset (under the original PCQM4Mv2 CC BY 4.0 license) is a subset of PCQM4Mv2 <ref type="bibr" target="#b27">[28]</ref> that we created for the purpose of our ablation study. We sub-sampled the above-mentioned version of PCQM4Mv2 as follows; training set: 10%; validation set: 33%; test set: unchanged. This resulted in retaining 446,405 molecular graphs in total.</p><p>MalNet-Tiny <ref type="bibr" target="#b20">[21]</ref> (CC-BY license) is a subset of MalNet that is comprised of function call graphs (FCGs) derived from Android APKs. This subset contains 5,000 graphs of up to 5,000 nodes, each coming from benign software or 4 types of malware. The FCGs are stripped of any original node or edge features, the task is to predict the type of the software based on the structure alone. The benchmarking version of this dataset typically uses Local Degree Profile as the set of node features. PCQM-Contact <ref type="bibr" target="#b16">[17]</ref> (CC BY 4.0) is derived from PCQM4Mv2 and respective 3D molecular structures. The task is a binary link prediction, identifying pairs of nodes that are considered to be in 3D contact (&lt;3.5?) yet distant in the 2D graph (&gt;5 hops). The default evaluation ranking metric used is the Mean Reciprocal Rank (MRR).</p><p>Peptides-func and Peptides-struct <ref type="bibr" target="#b16">[17]</ref> (CC BY-NC 4.0) are both composed of atomic graphs of peptides retrieved from SATPdb. In Peptides-func the prediction is multi-label graph classification into 10 nonexclusive peptide functional classes. While for Peptides-struct the task is graph regression of 11 3D structural properties of the peptides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset splits and random seeds</head><p>All evaluated benchmarks define a standard train/validation/test dataset split. We follow these and report mean performance and standard deviation from multiple execution runs with different random seeds.</p><p>All main benchmarking results are based on 10 executed runs, except PCQM4Mv2 (for which we show the result of a single random seed run) and LRGB (for which we use 4 seed). The OGB-LSC <ref type="bibr" target="#b27">[28]</ref> leaderboard for PCQM4Mv2 does not keep track of variance w.r.t. random seeds. This is likely due to the size of the dataset, in our evaluation we had run 3 random seeds and the standard deviation for GPS-small was 0.00034 which is below the presentation precision.</p><p>For ablation studies we used a reduce number of 4 random seeds due to computational constraints, while for PCQM4Mv2-Subset and MalNet-Tiny we used 3 random seeds. All experiments in the ablation studies were run from scratch, results from the main text (with 10 repeats) were not reused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameters</head><p>In our hyperparameter search, we experimented with a variety of positional and structural encodings, MPNN types, global attention mechanisms and their hyperparameters. Considering the large number of hyperparameters and datasets, we did not perform an exhaustive search or a grid search beyond the ablation studies presented in the main text, Section 4.1. We have extrapolated from those results and established the PE/SE type and layer types for the remaining datasets. For each dataset we then adjusted the number of layers, dimensionality d , and other remaining hyperparameters based on hyperparameters reported in the related literature, or eventually based on validation performance using "line search" along one of the hyperparameters at a time. Namely, we followed several hyperparameter choices of SAN <ref type="bibr" target="#b35">[36]</ref>, SAT <ref type="bibr" target="#b8">[9]</ref>, Graphormer <ref type="bibr" target="#b62">[63]</ref>, and Freitas et al. <ref type="bibr" target="#b20">[21]</ref>.</p><p>For benchmarking datasets from Dwivedi et al. <ref type="bibr" target="#b14">[15]</ref> we followed the most commonly used parameter budgets: up to 500k parameters for ZINC, PATTERN, and CLUSTER; and~100k parameters for MNIST and CIFAR10.</p><p>The final hyperparameters are presented in <ref type="table" target="#tab_1">Tables A.2</ref> In all our experiments we used AdamW <ref type="bibr" target="#b40">[41]</ref> optimizer, with the default settings of ? 1 = 0.9, ? 2 = 0.999, and = 10 ?8 , together with linear "warm-up" increase of the learning rate at the beginning of the training followed by its cosine decay. The length of the warm-up period, base learning rate, and the total number of epoch were adjusted per dataset and are listed together with other hyperparameters <ref type="table" target="#tab_1">(Tables A.2</ref>    <ref type="bibr" target="#b27">[28]</ref> and MalNet-Tiny <ref type="bibr" target="#b20">[21]</ref>. GPS-medium architecture follows several hyperparameter choices of Graphormer <ref type="bibr" target="#b62">[63]</ref>. Listed run-times were measured on a single NVidia A100 GPU system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Computing environment and used resources</head><p>Our implementation is based on PyG and its GraphGym module <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b64">65]</ref> that are provided under MIT License. All experiments were run in a shared computing cluster environment with varying CPU and GPU architectures. These involved a mix of NVidia V100 (32GB), RTX8000 (48GB), and A100 (40GB) GPUs. The resource budget for each experiment was 1 GPU, between 4 and 6 CPUs, and up to 32GB system RAM. The only exception are ogbg-ppa and PCQM4Mv2 that due to their size required up to 48GB system RAM.</p><p>To measure the run-time we used Python time.perf_counter() function. Due to the variation in computing infrastructure and load on shared resources the execution time occasionally notably varied. Therefore for our ablation studies we used only compute nodes with NVidia A100 GPUs, which considerably improved the run-time consistency. We list the wall-clock run-time that is approximately a median of the observed durations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed ablation studies</head><p>Here we present the detailed ablation studies on impact of various MPNN, self attention, and positional / structural encoding types on GPS performance and run-time. In each case, we varied a single part of the model at a time, keeping the rest of the GPS hyperparameters unchanged from the best selected architecture for a given dataset. Results on ZINC are shown in   In this section, we review the 1-Weisfeiler-Leman test <ref type="bibr" target="#b58">[59]</ref>, their equivalence with MPNNs and the limitations brought by this equivalent expressive power which eventually brings us to a statement that indicates the theoretical need of equipping MPNNs or GTs with either or a combination of local, relative or global PE/SE. <ref type="figure" target="#fig_0">(1-WL)</ref>. The 1-WL test is a node-coloring algorithm, in the hierarchy of Weisfeiler-Leman (WL) heuristics for graph isomorphism, <ref type="bibr" target="#b58">[59]</ref>, which iteratively updates the color of a node based on its 1-hop local neighborhood until an iteration when the node colors do not change successively. The final histogram of the node colors determine whether the algorithm outputs the two graphs to be 'non-isomorphic' (when the histograms of 2 graphs are distinct) or 'possibly isomorphic' (when the histograms of 2 graphs are same). Although, it is not a sufficient test for the graph isomorphism problem, the heuristic is simple to apply and has been popularly used in the literature recently to quantify the expressive power of MPNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-Weisfeiler-Leman test</head><p>Expressive power of MPNNs. Based on the equivalence of the aggregate and update functions of MPNNs with the hash function of the 1-WL test, it was shown that MPNNs are at most powerful as 1-WL <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b44">45]</ref>, which is now popularly understood in the literature. Graph Isomorphism Network <ref type="bibr" target="#b60">[61]</ref> was developed by aligning the injectivity of the aggregate and update functions of GIN with the injectivity of the 1-WL's hash function, which makes it a 1-WL powerful MPNN. In direct consequence, the power of the GIN is quantified as 1-WL expressive, i.e., if 1-WL outputs two graphs to be 'non-isomorphic' then the GIN would output different feature vectors for the two graphs and conversely, if 1-WL outputs two graphs to be 'possibly isomorphic', the feature embeddings of the two graphs would be the same. We refer the readers to <ref type="bibr" target="#b60">[61]</ref> for the details on this theoretical result.</p><p>Since the expressive power of MPNNs are at most 1-WL, it leads to a serious limitation in distinguishing a wide-variety of non-isomorphic graphs <ref type="bibr" target="#b49">[50]</ref>. Note that numerous follow up works have proposed GNNs that are strictly powerful than 1-WL, often moving away from the message passing framework <ref type="bibr" target="#b21">[22]</ref> on which MPNNs are based <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43]</ref>. As higher-order GNNs are not within the scope of this section, we limit our discussion only to MPNNs, such as GINs, which makes them 1-WL powerful. There are numerous examples on which MPNNs fail as a result <ref type="bibr" target="#b49">[50]</ref>. Among such cases, we consider two examples tasks: the task to differentiate between two non-isomorphic Circular Skip Link (CSL) graphs, <ref type="figure">Figure C</ref>.1a, and the task to differentiate between two potential links, <ref type="figure">Figure  C</ref>.1b. The nodes in these examples do not have discriminating node features.</p><p>The CSL graph, <ref type="figure">Figure C.</ref>1a. In the CSL graph-pair <ref type="bibr" target="#b45">[46]</ref>, the two graphs G skip (11, 2) and G skip <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b2">3)</ref> differ in the length of skip-link of a node and are hence non-isomorphic. Since the 1-WL algorithm produces the same color for all the nodes in both graphs, MPNNs will generate similar node colors. See the colors generated by 1-WL and MPNN in the second row of <ref type="figure">Figure C</ref>.1a. However, the use of a global PE (eg. Laplacian PE <ref type="bibr" target="#b14">[15]</ref>) assigns each node a unique color, as depicted in the third row. Consequently, the feature embeddings of the two graphs which are the hash function outputs of the collection of node colors are different, thus making the task to distinguish the graphs successful. Similarly, the use of a local SE (e.g. diagonals of m-steps random walk) allows the coloring of the nodes of the 2 graphs to be different <ref type="bibr" target="#b15">[16]</ref> since it captures the difference of the skip links of the two graphs successfully <ref type="bibr" target="#b41">[42]</ref>. See the fourth row where the local SE based colors are depicted on the nodes. Therefore, either of the specific local SE or global PE can help distinguish the two graphs which cannot be learnt by 1-WL or MPNNs.</p><p>The Decalin molecular graph, <ref type="figure">Figure C.</ref>1b. In the Decalin graph, the node a is isomorphic to node b, and so is the node c to node d. A 1-WL coloring of the nodes, and equivalently MPNN, would generate one color for the node a, b and another color for c, d, see the second row in <ref type="figure">Figure C</ref>.1b. If that task is to identify a potential link between the node-sets (a, d) and (b, d), the combination of the node colors of the node-sets will produce the same embedding for the two links, thus making the 1-WL or MPNNs based coloring unsuitable to certain tasks <ref type="bibr" target="#b66">[67]</ref>. A similar observation also follows for the node coloring based on the aforementioned local SE <ref type="bibr" target="#b15">[16]</ref>, which is illustrated in the fourth row in <ref type="figure">Figure C</ref>.1b. However, using a distance-based relative PE on the edges or an eigenvector-based  <ref type="bibr" target="#b45">[46]</ref> where the nodes have skip links of 2 and 3 respectively. (b) A Decalin molecular graph which has two rings of all Carbon atoms, thus with no distinguishing node features. Second Row: The nodes colored with the feature generated by 1-WL <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b44">45]</ref>. Third Row: The nodes colored with the feature generated by global PE <ref type="bibr" target="#b14">[15]</ref>. Fourth Row: The nodes colored with the feature generated by local SE <ref type="bibr" target="#b15">[16]</ref>. Note: The colors depicted on nodes in the graphs represent a unique feature vector generated, for a given graph, from the corresponding PE/SE. Figure best visualized in color.</p><p>global PE would successfully differentiate the embeddings of the two links. Therefore, the relative PE or the global PE which can help to distinguish between the two links cannot be learnt by 1-WL or MPNNs.</p><p>We can then conclude the following statement based on the above discussion which provides a theoretical basis for the need of PE and SE, as the PE and SE can be directly supplying essential information for the task:</p><p>Proposition 1. Assuming no modification applied to MPNNs for a learning task, there exists Positional Encodings (PE) and Structural Encoding (SE) which MPNNs are not guaranteed to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Preserving edge information in the self-attention layer</head><p>In this section, we argue that an MPNN layer is able to propagate the information from edges to nodes such that, when computing the attention between nodes, the global Attention (Transformer) layer can infer whether two nodes are connected and what are the edge features between them.</p><p>Suppose an MPNN with the sum aggregator, with the update function as given below:</p><formula xml:id="formula_8">h l+1 u = v?Nu f (h l u , h l v , e uv ),<label>(5)</label></formula><p>where f is a learned function, e.g., an MLP; u is the index of a central node whose neighborhood is being aggregated; v is the index of a neighbor of u; h l u the node features at layer l for node u, and e uv the edge features between nodes u and v.</p><p>We know from the Lemma 5 of Xu et al. <ref type="bibr" target="#b60">[61]</ref> that the sum over a countable multiset is universal, meaning it can map a unique multiset to any possible function. Let's assume that h u is unique and countable for every node u, which can be accomplised using all the Laplacian eigenvectors as PE. Then, there exist a function f such that an encoding ? uv that respects the following characteristics is propagated to the nodes: (i) unique for the triplet {h u , h v , e uv }, (ii) invariant to the permutation of u and v, (iii) contains the information of e ij , (iv) all information of ? uv is preserved after the .</p><p>Hence, an Attention layer that follows the message-passing is able to infer whether two nodes are connected since both nodes will contain the unique identifier ? uv , and will also be able to infer the edge features from it.</p><p>An example of such function ? uv is the tensor product ? of a one-hot encoding unique for each edge o uv and the edge features e uv . For example, if e uv = [e 1 , e 2 , e 3 ] and the edge is represented with o uv = [0, 1, 0, 0], then ? uv = o uv ? e uv = [0, 0, 0, e 1 , e 2 , e 3 , 0, 0, 0, 0, 0, 0] satisfies all the above conditions. Although this function requires an exponential increase in the hidden dimension, this is also the case for the Lemma 5 in Xu et al. <ref type="bibr" target="#b60">[61]</ref>. Local MPNN encodes real edge features into the node-level hidden representations, while global attention mechanism can implicitly make use of this information together with PE/SE to infer relation between two nodes without explicit edge features. After each functional block (an MPNN layer, a global attention layer, an MLP) we apply residual connections followed by batch normalization (BN) <ref type="bibr" target="#b29">[30]</ref>.</p><p>In the 2-layer MLP block we use ReLU activations and its inner hidden dimension is twice the layerinput feature dimensionality d . Note, similarly to Transformer, the input and output dimensionality of the GPS-layer as a whole is the same.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Modular GPS graph Transformer, with examples of PE and SE. Task specific layers for node/graph/edge-level predictions, such as pooling or output MLP, are omitted for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>14</label><figDesc>" 8 k J M b S 8 q 0 C Y T K U M N 2 U N n w Z w F z e o = " &gt; A A A C E H i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W s I G V G i 7 o s u t B l B f u A z j B k 0 r Q N T T J D k h G G Y T 7 B j b / i x o U i b l 2 6 8 2 9 M H w u t H g i c n H M v 9 9 4 T x o w q 7 T h f V m F h c W l 5 p b h a W l v f 2 N y y t 3 d a K k o k J k 0 c s U h 2 Q q Q I o 4 I 0 N d W M d G J J E A 8 Z a Y e j q 7 H f v i d S 0 U j c 6 T Q m P k c D Q f s U I 2 2 k w D 7 0 O N J D j F h 2 n Q f Z 5 C N 5 5 m k q U q h G N M 7 z i u s e n x 4 F d t m p O h P A v 8 S d k T K Y o R H Y n 1 4 v w g k n Q m O G l O q 6 T q z 9 D E l N M S N 5 y U s U i R E e o Q H p G i o Q J 8 r P J g f l 8 M A o P d i P p H l C w 4 n 6 s y N D X K m U h 6 Z y v L G a 9 8 b i f 1 4 3 0 f 0 L P 6 M i T j Q R e D q o n z C o I z h O B / a o J F i z 1 B C E J T W 7 Q j x E E m F t M i y Z E N z 5 k / + S 1 k n V P a v W b m v l + u U s j i L Y A / u g A l x w D u r g B j R A E 2 D w A J 7 A C 3 i 1 H q 1 n 6 8 1 6 n 5 Y W r F n P L v g F 6 + M b o J i c 7 A = = &lt; / l a t e x i t &gt; G skip (11, 3) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H u T k H I z k H l k A g i S s R M G a H I I x 0 mk = " &gt; A A A C E H i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W s I G W m F H V Z d K H L C v Y Bn a F k 0 r Q N T T J D k h H K M J / g x l 9 x 4 0 I R t y 7 d + T d m p l 1 o 9 U D g 5 J x 7 u f e e I G J U a c f 5 s g p L y y u r a 8 X 1 0 s b m 1 v a O v b v X V m E s M W n h k I W y G y B F G B W k p a l m p B t J g n j A S C e Y X G V + 5 5 5 I R U N x p 6 c R 8 T k a C T q k G G k j 9 e 1 j j y M 9 x o g l 1 2 k / y T + S J 5 6 m Y g r V h E Z p W n H d 0 9 p J 3 y 4 7 V S c H / E v c O S m D O Z p 9 + 9 M b h D j m R G j M k F I 9 1 4 m 0 n y C p K W Y k L X m x I h H C E z Q i P U M F 4 k T 5 S X 5 Q C o + M M o D D U J o n N M z V n x 0 J 4 k p N e W A q s 4 3 V o p e J / 3 m 9 W A 8 v / I S K K N Z E 4 N m g Y c y g D m G W D h x Q S b B m U 0 M Q l t T s C v E Y S Y S 1 y b B k Q n A X T / 5 L 2 r W q e 1 a t 3 9 b L j c t 5 H E V w A A 5 B B b j g H D T A D W i C F s D g A T y B F / B q P V r P 1 p v 1 P i s t W P O e f f A L 1 s c 3 n x O c 6 w = = &lt; / l a t e x i t &gt; G skip (11, 2) (a) Circular Skip Link graphs &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 6 G m 6 + m a A v y x Q 3 K 5 I N Y x b Z r 1 f a Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s p u 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 E P O q L F S M x y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V V C M o y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N c O a n 3 G Z p A Y l W y 4 a p o K Y m M y / J i F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 W 9 m + p 1 8 7 p S r + V x F O E M z u E S P L i F O t x D A 1 r A A O E Z X u H N e X R e n H f n Y 9 l a c P K Z U / g D 5 / M H x 9 W M 5 w = = &lt; / l a t e x i t &gt; d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 6 G m 6 + m a A v y x Q 3 K 5 I N Y x b Z r 1 f a Q = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s p u 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 E P O q L F S M x y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V V C M o y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N c O a n 3 G Z p A Y l W y 4 a p o K Y m M y / J i F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 W 9 m + p 1 8 7 p S r + V x F O E M z u E S P L i F O t x D A 1 r A A O E Z X u H N e X R e n H f n Y 9 l a c P K Z U / g D 5 / M H x 9 W M 5 w = = &lt; / l a t e x i t &gt; d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R F h I s e + g e X 5 f E O T 0 X B V a F n n j M l E = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>z H p 0 X 5 9 3 5 W L Y W n H z m F P 7 A + f w B w 0 m M 5 A = = &lt; / l a t e x i t &gt; a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R F h I s e + g e X 5 f E O T 0 X B V a F n n j M l E = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N W H N z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 r e T f W 6 e V 2 p 1 / I 4 i n A G 5 3 A J H t x C H e 6 h A S 1 g g P A M r / D m P D o v z r v z s W w t O P n M K f y B 8 / k D x M 2 M 5 Q = = &lt; / l a t e x i t &gt; b &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W b j O q W G U u 6 Y M o g q M d B I n 2 3 m R k D g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E t M e C F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N W H N z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 r e T f W 6 e V 2 p 1 / I 4 i n A G 5 3 A J H t x C H e 6 h A S 1 g g P A M r / D m P D o v z r v z s W w t O P n M K f y B 8 / k D x M 2 M 5 Q = = &lt; / l a t e x i t &gt; b (b) Decalin molecular graph Example graphs with anonymous nodes without distinguishing features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ZINC [ 15 ]</head><label>15</label><figDesc>(MIT License) consists of 12K molecular graphs from the ZINC database of commercially available chemical compounds. These molecular graphs are between 9 and 37 nodes large. Each node represents a heavy atom (28 possible atom types) and each edge represents a bond (3 possible types). The task is to regress constrained solubility (logP) of the molecule. The dataset comes with a predefined 10K/1K/1K train/validation/test split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, A.3, A.4, A.5, together with the number of parameters and median wall-clock run-time for node encoding precomputation, one full epoch (including validation and test split evaluation), and the total time spent in the main loop. See Section A.4 for more details on the run-time measurements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>, A.3, A.4, A.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure C. 1 :</head><label>1</label><figDesc>First Row: Example graphs with anonymous nodes, i.e., nodes do not have any distinguishing node features. (a) A pair of Circular Skip Link (CSL) graphs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure D. 1 :</head><label>1</label><figDesc>Modular GPS layer that combines local MPNN and global attention blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Update function. At each layer, the features are updated by aggregating the output of an MPNN layer with that of a global attention layer, as shown in Figures 1 and D.1, and described by the equations below. Note that the edge features are only passed to the MPNN layer, and that residual connections with batch normalization<ref type="bibr" target="#b29">[30]</ref> are omitted for clarity. Both the MPNN and GlobalAttn layers are modular, i.e., MPNN can be any function that acts on a local neighborhood and GlobalAttn can be any fully-connected layer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of the ablation studies. Details of the architectural choices, parameters, standard deviation, and computation times are presented in Appendix B.(a) Ablation of the Transformer and MPNN layers. We observe a major drop when using only a Transformer without an MPNN. Further, most datasets benefit from using a Transformer, without any negative impact. Ablation of the PE and SE types. RWSE provides consistent gains at relatively low computational cost, while SignNet DeepSets is the single best performing encoding, albeit at increased computational cost.</figDesc><table><row><cell cols="2">Ablation</cell><cell>ZINC</cell><cell cols="2">PCQM4Mv2 CIFAR10 subset</cell><cell>MalNet -Tiny</cell><cell cols="2">(b) Ablation</cell><cell>ZINC</cell><cell cols="2">PCQM4Mv2 CIFAR10 subset</cell><cell>MalNet -Tiny</cell></row><row><cell></cell><cell></cell><cell cols="3">MAE ? MAE ? Acc. ?</cell><cell>Acc. ?</cell><cell></cell><cell></cell><cell cols="3">MAE ? MAE ? Acc. ?</cell><cell>Acc. ?</cell></row><row><cell>Global MPNN Attention</cell><cell cols="2">none Transformer 0.070 0.070 Performer 0.071 BigBird 0.071 GatedGCN 0.086 PNA 0.070 none 0.113 GINE 0.070</cell><cell>0.1213 0.1159 0.1142 0.1237 0.1159 0.1409 0.3294 0.1284</cell><cell>69.95 72.31 70.67 70.48 72.31 73.42 68.86 71.11</cell><cell>92.23 93.50 92.64 92.34 92.64 91.67 73.90 92.27</cell><cell>PE / SE</cell><cell cols="2">none RWSE LapPE SignNet MLP SignNet DeepSets 0.079 0.113 0.070 0.116 0.090 PEG LapEig 0.161</cell><cell>0.1355 0.1159 0.1201 0.1158 0.1144 0.1209</cell><cell>71.49 71.96 72.31 71.74 72.37 72.10</cell><cell>92.64 92.77 92.74 92.57 93.13 92.27</cell></row></table><note>*Encodings are color-coded by their positional or structural type.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test performance in five benchmarks from<ref type="bibr" target="#b14">[15]</ref>. Shown is the mean ? s.d. of 10 runs with different random seeds. Highlighted are the top first, second, and third results.</figDesc><table><row><cell>Model</cell><cell>ZINC</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell>PATTERN</cell><cell>CLUSTER</cell></row><row><cell></cell><cell>MAE ?</cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell></row><row><cell>GCN [33]</cell><cell cols="5">0.367 ? 0.011 90.705 ? 0.218 55.710 ? 0.381 71.892 ? 0.334 68.498 ? 0.976</cell></row><row><cell>GIN [61]</cell><cell cols="5">0.526 ? 0.051 96.485 ? 0.252 55.255 ? 1.527 85.387 ? 0.136 64.716 ? 1.553</cell></row><row><cell>GAT [56]</cell><cell cols="5">0.384 ? 0.007 95.535 ? 0.205 64.223 ? 0.455 78.271 ? 0.186 70.587 ? 0.447</cell></row><row><cell>GatedGCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test performance in graph-level OGB benchmarks<ref type="bibr" target="#b26">[27]</ref>. Shown is the mean ? s.d. of 10 runs. Models that were first pre-trained on another dataset or use an ensemble are not included here.</figDesc><table><row><cell>Model</cell><cell>ogbg-molhiv</cell><cell>ogbg-molpcba</cell><cell>ogbg-ppa</cell><cell>ogbg-code2</cell></row><row><cell></cell><cell>AUROC ?</cell><cell>Avg. Precision ?</cell><cell>Accuracy ?</cell><cell>F1 score ?</cell></row><row><cell>GCN+virtual node</cell><cell>0.7599 ? 0.0119</cell><cell>0.2424 ? 0.0034</cell><cell cols="2">0.6857 ? 0.0061 0.1595 ? 0.0018</cell></row><row><cell>GIN+virtual node</cell><cell>0.7707 ? 0.0149</cell><cell>0.2703 ? 0.0023</cell><cell cols="2">0.7037 ? 0.0107 0.1581 ? 0.0026</cell></row><row><cell>GatedGCN-LSPE</cell><cell>-</cell><cell>0.267 ? 0.002</cell><cell>-</cell><cell>-</cell></row><row><cell>PNA</cell><cell>0.7905 ? 0.0132</cell><cell>0.2838 ? 0.0035</cell><cell>-</cell><cell>0.1570 ? 0.0032</cell></row><row><cell>DeeperGCN</cell><cell>0.7858 ? 0.0117</cell><cell>0.2781 ? 0.0038</cell><cell>0.7712 ? 0.0071</cell><cell>-</cell></row><row><cell>DGN</cell><cell>0.7970 ? 0.0097</cell><cell>0.2885 ? 0.0030</cell><cell>-</cell><cell>-</cell></row><row><cell>GSN (directional)</cell><cell>0.8039 ? 0.0090</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GSN (GIN+VN base)</cell><cell>0.7799 ? 0.0100</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CIN</cell><cell>0.8094 ? 0.0057</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GIN-AK+</cell><cell>0.7961 ? 0.0119</cell><cell>0.2930 ? 0.0044</cell><cell>-</cell><cell>-</cell></row><row><cell>CRaWl</cell><cell>-</cell><cell>0.2986 ? 0.0025</cell><cell>-</cell><cell>-</cell></row><row><cell>ExpC [62]</cell><cell>0.7799 ? 0.0082</cell><cell>0.2342 ? 0.0029</cell><cell>0.7976 ? 0.0072</cell><cell>-</cell></row><row><cell>SAN</cell><cell>0.7785 ? 0.2470</cell><cell>0.2765 ? 0.0042</cell><cell>-</cell><cell>-</cell></row><row><cell>GraphTrans (GCN-Virtual)</cell><cell>-</cell><cell>0.2761 ? 0.0029</cell><cell>-</cell><cell>0.1830 ? 0.0024</cell></row><row><cell>K-Subtree SAT</cell><cell>-</cell><cell>-</cell><cell cols="2">0.7522 ? 0.0056 0.1937 ? 0.0028</cell></row><row><cell>GPS (ours)</cell><cell>0.7880 ? 0.0101</cell><cell>0.2907 ? 0.0028</cell><cell cols="2">0.8015 ? 0.0033 0.1894 ? 0.0024</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>GCN</cell><cell>0.1398</cell><cell>0.1379</cell><cell>n/a</cell><cell>2.0M</cell></row><row><cell>GCN-virtual</cell><cell>0.1152</cell><cell>0.1153</cell><cell>n/a</cell><cell>4.9M</cell></row><row><cell>GIN</cell><cell>0.1218</cell><cell>0.1195</cell><cell>n/a</cell><cell>3.8M</cell></row><row><cell>GIN-virtual</cell><cell>0.1084</cell><cell>0.1083</cell><cell>n/a</cell><cell>6.7M</cell></row><row><cell>GRPE [48]</cell><cell>0.0898</cell><cell>0.0890</cell><cell>n/a</cell><cell>46.2M</cell></row><row><cell>EGT [29]</cell><cell>0.0872</cell><cell>0.0869</cell><cell>n/a</cell><cell>89.3M</cell></row><row><cell>Graphormer [51]</cell><cell>n/a</cell><cell>0.0864</cell><cell>0.0348</cell><cell>48.3M</cell></row><row><cell>GPS-small</cell><cell>n/a</cell><cell>0.0938</cell><cell>0.0653</cell><cell>6.2M</cell></row><row><cell>GPS-medium</cell><cell>n/a</cell><cell>0.0858</cell><cell>0.0726</cell><cell>19.4M</cell></row></table><note>Evaluation on PCQM4Mv2 [28] dataset. For GPS evaluation, we treated the validation set of the dataset as a test set, since the test-dev set labels are private. For more details refer to Appendix A.Model PCQM4Mv2 Test-dev MAE ? Validation MAE ? Training MAE # Param.from overfitting, but to still outperform SAN, while other graph Transformers do not report results. For the molpcba, ppa, and code2, GPS always ranks among the top 3 models, highlighting again the versatility and expressiveness of the GPS approach. Further, GPS outperforms every other GT on all 4 benchmarks, except SAT on code2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A .</head><label>A</label><figDesc>1: Overview of the graph learning dataset<ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17]</ref> used in this study.</figDesc><table><row><cell>Dataset</cell><cell># Graphs</cell><cell cols="3">Avg. # Avg. # Directed nodes edges</cell><cell>Prediction level</cell><cell>Prediction task</cell><cell>Metric</cell></row><row><cell>ZINC</cell><cell>12,000</cell><cell>23.2</cell><cell>24.9</cell><cell>No</cell><cell>graph</cell><cell>regression</cell><cell>Mean Abs. Error</cell></row><row><cell>MNIST</cell><cell>70,000</cell><cell>70.6</cell><cell>564.5</cell><cell>Yes</cell><cell>graph</cell><cell>10-class classif.</cell><cell>Accuracy</cell></row><row><cell>CIFAR10</cell><cell>60,000</cell><cell>117.6</cell><cell>941.1</cell><cell>Yes</cell><cell>graph</cell><cell>10-class classif.</cell><cell>Accuracy</cell></row><row><cell>PATTERN</cell><cell>14,000</cell><cell cols="2">118.9 3,039.3</cell><cell>No</cell><cell>inductive node</cell><cell>binary classif.</cell><cell>Accuracy</cell></row><row><cell>CLUSTER</cell><cell>12,000</cell><cell cols="2">117.2 2,150.9</cell><cell>No</cell><cell>inductive node</cell><cell>6-class classif.</cell><cell>Accuracy</cell></row><row><cell>ogbg-molhiv</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell>No</cell><cell>graph</cell><cell>binary classif.</cell><cell>AUROC</cell></row><row><cell>ogbg-molpcba</cell><cell>437,929</cell><cell>26.0</cell><cell>28.1</cell><cell>No</cell><cell>graph</cell><cell>128-task classif.</cell><cell>Avg. Precision</cell></row><row><cell>ogbg-ppa</cell><cell>158,100</cell><cell cols="2">243.4 2,266.1</cell><cell>No</cell><cell>graph</cell><cell>37-task classif.</cell><cell>Accuracy</cell></row><row><cell>ogbg-code2</cell><cell>452,741</cell><cell>125.2</cell><cell>124.2</cell><cell>Yes</cell><cell>graph</cell><cell>5 token sequence</cell><cell>F1 score</cell></row><row><cell>PCQM4Mv2</cell><cell>3,746,620</cell><cell>14.1</cell><cell>14.6</cell><cell>No</cell><cell>graph</cell><cell>regression</cell><cell>Mean Abs. Error</cell></row><row><cell>MalNet-Tiny</cell><cell cols="3">5,000 1,410.3 2,859.9</cell><cell>Yes</cell><cell>graph</cell><cell>5-class classif.</cell><cell>Accuracy</cell></row><row><cell>PascalVOC-SP</cell><cell>11,355</cell><cell cols="2">479.4 2,710.5</cell><cell>No</cell><cell cols="2">inductive node 21-class classif.</cell><cell>F1 score</cell></row><row><cell>COCO-SP</cell><cell>123,286</cell><cell cols="2">476.9 2,693.7</cell><cell>No</cell><cell cols="2">inductive node 81-class classif.</cell><cell>F1 score</cell></row><row><cell>PCQM-Contact</cell><cell>529,434</cell><cell>30.1</cell><cell>61.0</cell><cell>No</cell><cell>inductive link</cell><cell>link ranking</cell><cell>MRR</cell></row><row><cell>Peptides-func</cell><cell>15,535</cell><cell>150.9</cell><cell>307.3</cell><cell>No</cell><cell>graph</cell><cell>10-task classif.</cell><cell>Avg. Precision</cell></row><row><cell>Peptides-struct</cell><cell>15,535</cell><cell>150.9</cell><cell>307.3</cell><cell>No</cell><cell>graph</cell><cell cols="2">11-task regression Mean Abs. Error</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A .</head><label>A</label><figDesc>2: GPS hyperparameters for five datasets from Dwivedi et al.<ref type="bibr" target="#b14">[15]</ref>.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">ZINC</cell><cell cols="2">MNIST</cell><cell cols="2">CIFAR10</cell><cell>PATTERN</cell><cell>CLUSTER</cell></row><row><cell># GPS Layers</cell><cell cols="2">10</cell><cell>3</cell><cell></cell><cell>3</cell><cell>6</cell><cell>16</cell></row><row><cell>Hidden dim</cell><cell cols="2">64</cell><cell>52</cell><cell></cell><cell>52</cell><cell>64</cell><cell>48</cell></row><row><cell>GPS-MPNN</cell><cell cols="2">GINE</cell><cell cols="2">GatedGCN</cell><cell cols="2">GatedGCN</cell><cell>GatedGCN</cell><cell>GatedGCN</cell></row><row><cell>GPS-GlobAttn</cell><cell cols="6">Transformer Transformer Transformer Transformer Transformer</cell></row><row><cell># Heads</cell><cell>4</cell><cell></cell><cell>4</cell><cell></cell><cell>4</cell><cell>4</cell><cell>8</cell></row><row><cell>Dropout</cell><cell>0</cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0.1</cell></row><row><cell>Attention dropout</cell><cell cols="2">0.5</cell><cell>0.5</cell><cell></cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Graph pooling</cell><cell cols="2">sum</cell><cell cols="2">mean</cell><cell>mean</cell><cell>-</cell><cell>-</cell></row><row><cell>Positional Encoding</cell><cell cols="2">RWSE-20</cell><cell cols="2">LapPE-8</cell><cell cols="2">LapPE-8</cell><cell>LapPE-16</cell><cell>LapPE-10</cell></row><row><cell>PE dim</cell><cell cols="2">28</cell><cell>8</cell><cell></cell><cell>8</cell><cell>16</cell><cell>16</cell></row><row><cell>PE encoder</cell><cell cols="2">linear</cell><cell cols="2">DeepSet</cell><cell cols="2">DeepSet</cell><cell>DeepSet</cell><cell>DeepSet</cell></row><row><cell>Batch size</cell><cell cols="2">32</cell><cell>16</cell><cell></cell><cell>16</cell><cell>32</cell><cell>16</cell></row><row><cell>Learning Rate</cell><cell cols="2">0.001</cell><cell cols="2">0.001</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0005</cell></row><row><cell># Epochs</cell><cell cols="2">2000</cell><cell cols="2">100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell># Warmup epochs</cell><cell cols="2">50</cell><cell>5</cell><cell></cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Weight decay</cell><cell cols="2">1e-5</cell><cell cols="2">1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell></row><row><cell># Parameters</cell><cell cols="2">423,717</cell><cell cols="2">115,394</cell><cell cols="2">112,726</cell><cell>337,201</cell><cell>502,054</cell></row><row><cell>PE precompute</cell><cell cols="2">23s</cell><cell>96s</cell><cell></cell><cell cols="2">2.55min</cell><cell>28s</cell><cell>67s</cell></row><row><cell>Time (epoch/total)</cell><cell cols="2">21s / 11.67h</cell><cell cols="2">76s / 2.13h</cell><cell cols="2">64s / 1.78h</cell><cell>32s / 0.89h</cell><cell>86s / 2.40h</cell></row><row><cell cols="7">Table A.3: GPS hyperparameters for graph-level prediction datasets from OGB [27].</cell></row><row><cell>Hyperparameter</cell><cell cols="5">ogbg-molhiv ogbg-molpcba</cell><cell>ogbg-ppa</cell><cell>ogbg-code2</cell></row><row><cell># GPS Layers</cell><cell></cell><cell>10</cell><cell></cell><cell>5</cell><cell></cell><cell>3</cell><cell>4</cell></row><row><cell>Hidden dim</cell><cell></cell><cell>64</cell><cell></cell><cell>384</cell><cell></cell><cell>256</cell><cell>256</cell></row><row><cell>GPS-MPNN</cell><cell cols="3">GatedGCN</cell><cell cols="2">GatedGCN</cell><cell>GatedGCN</cell><cell>GatedGCN</cell></row><row><cell>GPS-GlobAttn</cell><cell cols="3">Transformer</cell><cell cols="2">Transformer</cell><cell>Performer</cell><cell>Performer</cell></row><row><cell># Heads</cell><cell></cell><cell>4</cell><cell></cell><cell>4</cell><cell></cell><cell>8</cell><cell>4</cell></row><row><cell>Dropout</cell><cell></cell><cell>0.05</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.1</cell><cell>0.2</cell></row><row><cell>Attention dropout</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Graph pooling</cell><cell></cell><cell>mean</cell><cell></cell><cell cols="2">mean</cell><cell>mean</cell><cell>mean</cell></row><row><cell cols="2">Positional Encoding</cell><cell cols="2">RWSE-16</cell><cell cols="2">RWSE-16</cell><cell>None</cell><cell>None</cell></row><row><cell>PE dim</cell><cell></cell><cell>16</cell><cell></cell><cell>20</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>PE encoder</cell><cell></cell><cell>linear</cell><cell></cell><cell cols="2">linear</cell><cell>-</cell><cell>-</cell></row><row><cell>Batch size</cell><cell></cell><cell>32</cell><cell></cell><cell>512</cell><cell></cell><cell>32</cell><cell>32</cell></row><row><cell>Learning Rate</cell><cell></cell><cell>0.0001</cell><cell></cell><cell cols="2">0.0005</cell><cell>0.0003</cell><cell>0.0001</cell></row><row><cell># Epochs</cell><cell></cell><cell>100</cell><cell></cell><cell>100</cell><cell></cell><cell>200</cell><cell>30</cell></row><row><cell># Warmup epochs</cell><cell></cell><cell>5</cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell><cell>2</cell></row><row><cell>Weight decay</cell><cell></cell><cell>1e-5</cell><cell></cell><cell>1e-5</cell><cell></cell><cell>1e-5</cell><cell>1e-5</cell></row><row><cell># Parameters</cell><cell></cell><cell>558,625</cell><cell></cell><cell cols="2">9,744,496</cell><cell>3,434,533</cell><cell>12,454,066</cell></row><row><cell>PE precompute</cell><cell></cell><cell>58s</cell><cell></cell><cell cols="2">8.33min</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Time (epoch/total)</cell><cell cols="2">96s / 2.64h</cell><cell cols="2">196s / 5.44h</cell><cell>276s / 15.33h 1919s / 16h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A</head><label>A</label><figDesc>.4: GPS hyperparameters for large-scale graph-level prediction dataset OGB-LSC PCQM4Mv2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A .</head><label>A</label><figDesc>5: GPS hyperparameters for 5 datasets from Long Range Graph Benchmark (LRGB)<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table><row><cell>Hyperparameter</cell><cell>PascalVOC-SP</cell><cell>COCO-SP</cell><cell cols="3">PCQM-Contact Peptides-func Peptides-struct</cell></row><row><cell># GPS Layers</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>Hidden dim</cell><cell>96</cell><cell>96</cell><cell>96</cell><cell>96</cell><cell>96</cell></row><row><cell>GPS-MPNN</cell><cell>GatedGCN</cell><cell>GatedGCN</cell><cell>GatedGCN</cell><cell>GatedGCN</cell><cell>GatedGCN</cell></row><row><cell>GPS-SelfAttn</cell><cell>Transformer</cell><cell>Transformer</cell><cell>Transformer</cell><cell>Transformer</cell><cell>Transformer</cell></row><row><cell># Heads</cell><cell>8</cell><cell>8</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>Dropout</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Attention dropout</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Graph pooling</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>mean</cell><cell>mean</cell></row><row><cell>Positional Encoding</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell></row><row><cell>PE dim</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell></row><row><cell>PE encoder</cell><cell>DeepSet</cell><cell>DeepSet</cell><cell>DeepSet</cell><cell>DeepSet</cell><cell>DeepSet</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>32</cell><cell>256</cell><cell>128</cell><cell>128</cell></row><row><cell>Learning Rate</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0003</cell><cell>0.0003</cell><cell>0.0003</cell></row><row><cell># Epochs</cell><cell>300</cell><cell>300</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell># Warmup epochs</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>5</cell><cell>5</cell></row><row><cell>Weight decay</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell># Parameters</cell><cell>510,453</cell><cell>516,273</cell><cell>512,704</cell><cell>504,362</cell><cell>504,459</cell></row><row><cell>PE precompute</cell><cell>8.7min</cell><cell>1h 34min</cell><cell>5.23min</cell><cell>73s</cell><cell>73s</cell></row><row><cell>Time (epoch/total)</cell><cell>17.5s / 1.46h</cell><cell>213s / 17.8h</cell><cell>154s / 8.54h</cell><cell>6.36s / 0.35h</cell><cell>6.15s / 0.34h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table B.1, on PCQM4Mv2-Subset in Table B.2, on MalNet-Tiny in Table B.3, on CIFAR10 in Table B.4, on PascalVOC-SP inTable B.5, and on Peptides-func inTable B.6. The first data row of each table reproduces results of the best selected architecture with hyperparameters detailed in Appendix A; any deviations compared to the main benchmarking results of Section 4.2 are well within the reported standard deviation. While for benchmarking results we used 10 different random seeds, here we reduced the count due to computational cost to 4 for ZINC and CIFAR10, and 3 for PCQM4Mv2-Subset and MalNet-Tiny.</figDesc><table><row><cell>All time measurements reported in this section are obtained on a system with identical hardware</cell></row><row><cell>configuration: 1x NVidia A100 (40GB) GPU and allocation of 4 AMD Milan 7413 (2.65GHz) CPU</cell></row><row><cell>cores.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table B .</head><label>B</label><figDesc>1: GPS ablation study on ZINC dataset.</figDesc><table><row><cell>GPS-MPNN GPS-GlobAttn</cell><cell>PE / SE type</cell><cell>Test MAE ?</cell><cell># Param. Epoch / Total</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Table B.3: Ablation study on MalNet-Tiny. *Configuration required decreased batch size. Why do we need PE and SE?</figDesc><table><row><cell>C Theoretical results</cell></row><row><cell>C.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Output: Node representations X L ? R N ?D and edge representations E L ? R E?D , that can downstream be composed with appropriate prediction head for graph, node, or edge -level prediction.</p><p>where denotes an operator for combining the input node or edge features with their respective positional and/or structural encoding, in practice this is a concatenation operator which can be changed to sum or other operators; NodeEncoder and EdgeEncoder are dataset-specific initial node and edge feature encoders potentially with learnable parameters; MPNN e and GlobalAttn have their corresponding learnable parameters at each layer ;X +1 M andX +1 T denote the intermediate node representations given by the local message passing module and the global attention module respectively; and MLP is a multi layer perceptron module with its own learnable parameters that combines the intermediate X +1</p><p>M and X +1 T . Note that a relative F PE or F SE produces PE or SE for each edge which are thence handled accordingly in lines 2 and 3 in Algorithm 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Heat kernel estimates for random walks with degenerate weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Dominique</forename><surname>Deuschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Slowik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Probability</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weisfeiler and Lehman go cellular: CW networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2625" to="2640" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stefanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual Gated Graph ConvNets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Chelombiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Justus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frithjof</forename><surname>Gressmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Koliousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Luschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groupbert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05822</idno>
		<title level="m">Enhanced transformer architecture with efficient grouped structures</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structure-aware transformer for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Sehanobish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Parker-Holder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Weingarten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07999</idno>
		<title level="m">From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph neural networks with learnable structural and positional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long range graph benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Parviz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Track on Datasets and Benchmarks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2286" to="2296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansgar</forename><surname>Schuffenhauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A large-scale database for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duen Horng</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Conference on Neural Information Processing Systems: Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263</idno>
		<title level="m">Convolutional neural networks meet vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Open Graph Benchmark: Datasets for Machine Learning on Graphs. 34th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">OGB-LSC: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Conference on Neural Information Processing Systems: Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Global selfattention as a replacement for graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Md Shamim Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmashankar</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representing long-range context for graph neural networks with global attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ammus: A survey of transformer-based pretrained models in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajit</forename><surname>Katikapalli Subramanyam Kalyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivanesan</forename><surname>Rajasekharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sangeetha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05542</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral modification of graphs for improved spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Koutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huong</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">My body is a cage: the role of morphology in graph-based incompatible control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Vitaly Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Igl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendelin</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Boehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01856</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4465" to="4478" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Sign and basis invariant networks for spectral graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13013</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schahram</forename><surname>Dustdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11136</idno>
		<title level="m">Provably powerful graph networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<title level="m">GraphiT: Encoding graph structure in transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonggi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
		<idno>arXiv:22201.12787</idno>
		<title level="m">GRPE: Relative positional encoding for graph transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04810</idno>
		<title level="m">Benchmarking graphormer on large-scale molecular modeling datasets</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Toenshoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrikus</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08786</idno>
		<title level="m">Graph learning with 1d convolutions on random walks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Paul</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14522</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Equivariant and stable positional encoding for more powerful graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haorui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTI, Series</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Prediction of physicochemical parameters by atomic contributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon M</forename><surname>Wildman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crippen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="868" to="873" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Breaking the expression bottleneck of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08279</idno>
		<title level="m">First place solution of KDD Cup 2021 &amp; OGB large-scale challenge graph prediction track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Big Bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Labeling trick: A theory of using graph neural networks for multi-node representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">From stars to subgraphs: Uplifting any GNN with local structure awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
