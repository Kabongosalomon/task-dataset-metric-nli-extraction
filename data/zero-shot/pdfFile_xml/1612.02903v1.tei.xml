<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Expression Recognition using Convolutional Neural Networks: State of the Art</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pramerdorfer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">TU Wien Vienna</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kampel</surname></persName>
							<email>kampel@caa.tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">TU Wien Vienna</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Expression Recognition using Convolutional Neural Networks: State of the Art</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to recognize facial expressions automatically enables novel applications in human-computer interaction and other areas. Consequently, there has been active research in this field, with several recent works utilizing Convolutional Neural Networks (CNNs) for feature extraction and inference. These works differ significantly in terms of CNN architectures and other factors. Based on the reported results alone, the performance impact of these factors is unclear. In this paper, we review the state of the art in image-based facial expression recognition using CNNs and highlight algorithmic differences and their performance impact. On this basis, we identify existing bottlenecks and consequently directions for advancing this research field. Furthermore, we demonstrate that overcoming one of these bottlenecks -the comparatively basic architectures of the CNNs utilized in this field -leads to a substantial performance increase. By forming an ensemble of modern deep CNNs, we obtain a FER2013 test accuracy of 75.2%, outperforming previous works without requiring auxiliary training data or face registration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Being able to recognize facial expressions is key to nonverbal communication between humans, and the production, perception, and interpretation of facial expressions have been widely studied <ref type="bibr" target="#b0">[1]</ref>. Due to the important role of facial expressions in human interaction, the ability to perform Facial Expression Recognition (FER) automatically via computer vision enables a range of novel applications in fields such as human-computer interaction and data analytics <ref type="bibr" target="#b1">[2]</ref>.</p><p>Consequently, FER has been widely studied and significant progress has been made in this field. In fact, recognizing basic expressions under controlled conditions (e.g. frontal faces and posed expressions) can now be considered a solved problem <ref type="bibr" target="#b0">[1]</ref>. The term basic expression refers to a set of expressions that convey universal emotions, usually anger, disgust, fear, happiness, sadness, and surprise. Recognizing such expressions under naturalistic conditions is, however, more challenging. This is due to variations in head pose and illumination, occlusions, and the fact that unposed expressions are often subtle, as <ref type="figure">Fig. 1</ref> illustrates. Reliable FER under naturalistic conditions is mandatory in the aforementioned applications, yet still an unsolved problem <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Convolutional Neural Networks (CNNs) have the potential to overcome these challenges. CNNs have enabled significant performance improvements in related tasks (e.g. <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>), and several recent works on FER successfully utilize CNNs for feature extraction and inference (e.g. <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>). These works <ref type="figure">Fig. 1</ref>. Example images from the FER2013 dataset <ref type="bibr" target="#b2">[3]</ref>, illustrating variabilities in illumination, age, pose, expression intensity, and occlusions that occur under realistic conditions. Images in the same column depict identical expressions, namely anger, disgust, fear, happiness, sadness, surprise, as well as neutral.</p><p>differ significantly in terms of CNN architecture, preprocessing, as well as training and test protocols, factors that all affect performance. It is therefore not possible to assess the impact of the CNN architecture and other factors based on the reported results alone. Being able to do so is, however, required in order to be able to identify existing bottlenecks in CNN-based FER, and consequently for improving FER performance.</p><p>The aim of this paper is to shed light on this matter by reviewing existing CNN-based FER methods and highlighting their differences (Section II), as well as comparing the utilized CNN architectures empirically under consistent settings (Section III). On this basis, we identify existing bottlenecks and directions for improving FER performance. Finally, we confirm empirically that overcoming one such bottleneck improves performance substantially, demonstrating that modern deep CNNs achieve competitive results without auxiliary data or face registration (Section IV). An ensemble of such CNNs obtains a FER2013 <ref type="bibr" target="#b2">[3]</ref> test accuracy of 75.2%, outperforming existing CNN-based FER methods.</p><p>In this paper, we consider the task of predicting basic expressions from single images using CNNs. For more general surveys, we refer to <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. We note that it is straight-forward to adapt image-based methods to support image sequences by integrating per-frame results using graphical models. The conclusions drawn in this paper are thus relevant for sequencebased FER as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STATE OF THE ART IN CNN-BASED FER</head><p>We review six state-of-the-art methods for CNN-based FER, highlight methodological differences, and discuss the reported arXiv:1612.02903v1 [cs.CV] 9 Dec 2016 =? <ref type="figure">Fig. 2</ref>. Illustration of a standard preprocessing pipeline, which involves face detection (green square), facial landmark detection (red crosses), registration to reference landmarks (blue circles), and illumination correction.</p><p>performances. Most of these methods were evaluated on several databases in the original papers, the most common dataset being FER2013 <ref type="bibr" target="#b2">[3]</ref>. For consistency, we study the methods as they were used for this dataset, and summarize and discuss the reported performances on this dataset.</p><p>FER2013 is a large, publicly available FER dataset consisting of 35,887 face crops. The dataset is challenging as the depicted faces vary significantly in terms of person age, face pose, and other factors ( <ref type="figure">Fig. 1)</ref>, reflecting realistic conditions. The dataset is split into training, validation, and test sets with 28,709, 3,589, and 3,589 samples, respectively. Basic expression labels are provided for all samples. All images are grayscale and have a resolution of 48 by 48 pixels. The human accuracy on this dataset is around 65.5% <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Image-based FER under naturalistic conditions has been an active research field for years, and several public challenges have been held to promote progress in this field. One such challenge was FER2013 <ref type="bibr" target="#b2">[3]</ref>, which was won by one of the first CNN-based FER methods <ref type="bibr" target="#b6">[7]</ref>. The method uses an ensemble of CNNs trained to minimize the squared hinge loss.</p><p>In a more recent work, Yu and Zhang <ref type="bibr" target="#b7">[8]</ref> also utilize an ensemble of CNNs, and employ data augmentation at both training and test time in order to improve performance. Instead of performing ensemble voting via uniform averaging as in <ref type="bibr" target="#b6">[7]</ref>, ensemble predictions are integrated via weighted averaging with learned weights. The method ranked second in the recent EmotiW2015 challenge <ref type="bibr" target="#b9">[10]</ref>.</p><p>The winner <ref type="bibr" target="#b10">[11]</ref> of this challenge employs a large committee of CNNs. Certain properties of the individual networks (e.g. input preprocessing and receptive field size) vary in order to obtain more diverse models. The ensemble predictions are integrated in a hierarchical fashion, with network weights assigned according to validation set performance.</p><p>Mollahosseini et al. <ref type="bibr" target="#b11">[12]</ref> trained a single CNN based on the Inception architecture <ref type="bibr" target="#b12">[13]</ref> on data compiled from multiple posed and naturalistic datasets in an effort to obtain a model that generalizes well across datasets.</p><p>In <ref type="bibr" target="#b13">[14]</ref> Zhang et al. present a method for inferring social relation traits from images using a Siamese network. In order to increase the amount of available training data, they Method FD LM Registration Illumination <ref type="bibr" target="#b6">[7]</ref> no no no normalize <ref type="bibr" target="#b10">[11]</ref> several <ref type="bibr" target="#b14">[15]</ref> rigid (LM) several <ref type="bibr" target="#b7">[8]</ref> several no no histeq, lpf <ref type="bibr" target="#b11">[12]</ref> no <ref type="bibr" target="#b14">[15]</ref> affine (LM) no <ref type="bibr" target="#b13">[14]</ref> no <ref type="bibr" target="#b14">[15]</ref> indirect no <ref type="bibr" target="#b8">[9]</ref> no <ref type="bibr" target="#b14">[15]</ref> rigid (LM) several utilize multiple datasets with heterogeneous labels. The authors present a patch-based registration and feature extraction technique, and perform feature integration via early fusion. A recent work by Kim et al. <ref type="bibr" target="#b8">[9]</ref> shows that it is beneficial to use both unregistered and registered versions of a given face image during both training and testing. In order to prevent registration errors from affecting FER performance, registration is performed selectively based on the results of facial landmark detection. The authors show that registration can also be performed by deep networks, and that utilizing pose information captured by such networks leads to a small increase in FER performance (about 0.4%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Methodological Differences</head><p>In order to highlight the methodological differences between these works, we break down each method into the three components (i) preprocessing, (ii) CNN architecture, and (iii) CNN training and inference.</p><p>1) Preprocessing: Preprocessing entails operations that are applied once to each image. This typically includes face detection, face registration to compensate for pose variations, and means for correcting for illumination variations. <ref type="figure">Fig. 2</ref> illustrates these steps. <ref type="table" target="#tab_0">Table I</ref> summarizes the preprocessing steps of every method. Only <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b7">[8]</ref> perform face detection; all other methods rely on face crops provided by the datasets.</p><p>Face registration is common, with rigid or affine transformations based on extracted facial landmark locations being the most popular approach. This form of registration has the potential to improve FER performance <ref type="bibr" target="#b8">[9]</ref> provided that landmarks can be detected reliably. However, this is not always the case in practice due to challenging face poses and/or partial occlusions <ref type="bibr" target="#b8">[9]</ref>. There are different approaches to account for this problem; <ref type="bibr" target="#b10">[11]</ref> perform landmark detection on multiple versions of a given face image and utilize the detections with the highest detector confidence. <ref type="bibr" target="#b8">[9]</ref> perform alignment only if this confidence exceeds a threshold.</p><p>The majority of existing methods uses some form of illumination correction. In <ref type="bibr" target="#b6">[7]</ref> every image is normalized to have a mean of 0 and a norm of 100. <ref type="bibr" target="#b7">[8]</ref> employ histogram equalization and linear plane fitting. In <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b8">[9]</ref> the methods vary between individual CNNs in the ensembles.</p><p>2) CNN Architecture: <ref type="table" target="#tab_0">Table II</ref> compares the utilized CNN architectures and their depths (number of layers with weights)  <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>. As the corresponding papers lack details on architecture selection, the reason for this discrepancy is unknown.</p><p>The small size of available FER datasets such as FER2013 is not the limiting factor. First, deeper networks do not necessarily have more parameters, as shown in <ref type="table" target="#tab_0">Table II</ref>. Second, deeper networks impose a stronger prior on the structure of the learned decision function, and this prior effectively combats overfitting <ref type="bibr" target="#b16">[17]</ref>. Third, modern deep CNNs achieve impressive results on datasets with a similar size, such as CIFAR10 <ref type="bibr" target="#b4">[5]</ref>. A possible explanation is that CNNs do not have to be as deep for FER; <ref type="bibr" target="#b17">[18]</ref> shows that a CNN with depth 5 is already able to learn discriminative high-level features. We postpone further discussions on this matter to Section III.</p><p>3) CNN Training and Inference: <ref type="table" target="#tab_0">Table III</ref> highlights the differences in terms of CNN training and inference. Of the six works compared, four use only the FER2013 training set for CNN training. <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b13">[14]</ref> instead train on an union of seven and three datasets, respectively, in order to compensate for the fact that available FER image datasets are comparatively small. <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b8">[9]</ref> use additional features. In <ref type="bibr" target="#b13">[14]</ref> a vector of HoG features is computed from face patches and processed by the first fully connected layer of the CNN (early fusion). In <ref type="bibr" target="#b8">[9]</ref> the these features encode face pose information, and classifiers are trained to perform FER on this basis. Integration is performed via ensemble voting (late fusion).</p><p>All works except <ref type="bibr" target="#b13">[14]</ref> employ data augmentation during training in order to increase the amount of available data. Most works use standard augmentation methods that are not specific to FER, including horizontal mirroring and random cropping. The exception is <ref type="bibr" target="#b8">[9]</ref>, which additionally augments the training set by a registered version of every image.</p><p>In <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> augmentation is also performed at test time. In the former work, multiple perturbed versions of each test image are generated by applying affine transformations randomly, and the CNN output probabilities are averaged. The latter work follows the same approach but uses ten-crops  <ref type="bibr" target="#b6">[7]</ref> no no S,M -average <ref type="bibr" target="#b10">[11]</ref> no no T,M -hierarchy <ref type="bibr" target="#b7">[8]</ref> no no A,M A weighted <ref type="bibr" target="#b11">[12]</ref> yes no ten-crop -- <ref type="bibr" target="#b13">[14]</ref> yes yes --- <ref type="bibr" target="#b8">[9]</ref> no yes T,M,reg ten-crop,reg average (center and corner crops and mirrored versions) of a given image before and after face registration. Most works use an ensemble of CNNs, whose predictions are integrated via different forms of averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reported Results on FER2013</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3 compares the reported FER2013 test accuracies.</head><p>Based on the methodological differences and these results, we draw the following remarks.</p><p>The three best-performing works use comparatively shallow CNNs (depths of 5 and 6). The work utilizing the deepest and most modern (in terms of layer types and arrangement) CNN <ref type="bibr" target="#b11">[12]</ref> performs worst on this dataset. However, a direct comparison of the results is not possible because the CNN was trained on a superset of FER2013; in case of <ref type="bibr" target="#b11">[12]</ref>, this presumably has a negative impact on FER2013 test performance. On the other hand, Zhang et al. <ref type="bibr" target="#b13">[14]</ref> demonstrate that utilizing additional training data in a way that accounts for dataset bias improves the performance on FER2013.</p><p>The three best-performing methods use face registration, suggesting that registration is beneficial even under challenging conditions (according to <ref type="bibr" target="#b13">[14]</ref>, facial landmark extraction is inaccurate for about 15% of images in the FER dataset).</p><p>Data augmentation and ensemble voting are important in order to improve generalization performance. The bestperforming work that was trained on FER2013 alone uses the most comprehensive form of data augmentation during both training and testing. Ensemble voting reportedly improves the test accuracy by 2-3% <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EMPIRICAL COMPARISON</head><p>Im summary, <ref type="figure">Fig. 3</ref> shows that the best-performing FER methods utilize CNNs that are shallow and basic (in terms of layer types and arrangement) compared to the state of the art in related fields <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, contradicting the general trend towards deeper and deeper networks.</p><p>This, however, does not mean that such CNNs are inapplicable for FER because the CNN architecture is one of many factors that influence FER performance. In order to obtain more information on this matter, we perform an empirical comparison of the utilized CNN architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments</head><p>We train and test all CNN architectures utilized in the compared works using the same protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) CNN Architectures:</head><p>We test all architectures as they are described in the corresponding papers, apart from the following differences. We add batch normalization <ref type="bibr" target="#b18">[19]</ref> layers after every convolutional and fully connected (fc) layer for robustness to suboptimal network initialization. Furthermore, we add a dropout layer <ref type="bibr" target="#b19">[20]</ref> after the first fc layer.</p><p>[11] uses an ensemble of CNNs with different receptive fields and numbers of neurons in the first fc layer. We use the configuration with 3 ? 3 receptive fields and 2,048 neurons.</p><p>2) Dataset and Preprocessing: In order to enable comparison with reported results, we perform all experiments on the FER2013 dataset <ref type="bibr" target="#b2">[3]</ref>, adhering to the official training, validation, and test sets. We use the face crops as provided by the dataset and employ histogram equalization for illumination correction. This follows subtraction and division by the mean and standard deviation over all training pixels, respectively.</p><p>We do not perform landmark-based registration for two reasons. First, this prevents registration errors from affecting the results. Second, this forces the CNNs to learn to compensate for pose variations, potentially leading to more general models.</p><p>3) CNN Training and Inference: We train every architecture for up to 300 epochs, optimizing the cross-entropy loss using stochastic gradient descent with a momentum of 0.9. The initial learning rate, batch size, and weight decay are fixed at 0.1, 128, and 0.0001, respectively. The learning rate is halved if the validation accuracy does not improve for 10 epochs.</p><p>For training data augmentation we use horizontal mirroring and random crops of size 48 by 48 pixels after zero-padding (as the input images are already of this size). We ensure that all CNNs see the same training samples (after augmentation) in the same order, thereby enabling a fair comparison.</p><p>As the individual architectures were designed for training on larger datasets and/or other forms of data augmentation, training using the same regularization parameters would be unfair. In order to account for this, we perform a grid-search to find out an optimal dropout rate for every architecture.</p><p>Finally, the best model obtained for each architecture in terms of validation accuracy is tested on the test set using standard ten-crop oversampling <ref type="bibr" target="#b4">[5]</ref>. 4) Feature Comparison: Furthermore, we empirically compare the quality of the features learned by the models with the highest validation accuracy. This is accomplished by replacing the fc backends of the trained CNNs with a two-layer MLP with 1,024 hidden units, which is then trained using the above protocol. All other parameters are fixed, effectively causing the MLP to learn to perform FER using the features extracted by the pretrained frontend of the network.</p><p>Doing so allows a more direct comparison of the results because the backends of the resulting models are no longer different. With different backends, a more powerful backend could mask limitations in the learned representations. Fixing the backend enables us to study the impact of the network depth on the capabilities of the learned representations. <ref type="figure">Fig. 4</ref> summarizes the results. All results, except those for <ref type="bibr" target="#b11">[12]</ref>, are lower than the reported ones <ref type="figure">(Fig. 3)</ref>. The main reason for this is the lack of ensemble voting, as most results are comparable to reported results of single networks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and Discussion</head><p>In some cases, the differences cannot be explained by the lack of ensemble voting; in case of <ref type="bibr" target="#b13">[14]</ref>, the reason for the lower performance is the lack of auxiliary training data. The reported accuracy in <ref type="bibr" target="#b11">[12]</ref> is lower than the measured accuracy. This is also explained by the additional training data, which in this case has a negative effect on FER2013 performance. This shows that auxiliary training data has the potential to significantly improve FER performance, provided that care is taken in order to address dataset bias.</p><p>In case of <ref type="bibr" target="#b7">[8]</ref>, the measured accuracy is about 3% lower than the reported one using a single CNN. We tested both stochastic pooling (as used in <ref type="bibr" target="#b7">[8]</ref>) and max pooling, but in both cases were unable to reach the reported accuracy.</p><p>Overally, shallower CNN architectures again perform better than deeper ones (cf <ref type="table" target="#tab_0">. Table II</ref>). This also applies to the learned features. This, however, does not confirm that modern deep networks are not suitable for FER; there is only one architecture in the comparison that qualifies as such <ref type="bibr" target="#b11">[12]</ref>, and some architectural choices of this network are questionable (initial convolution with a 7 ? 7 receptive field, which appears too large given the input resolution, and a wide backend in the form of a three-layer MLP with 4,096 units in the first layer).</p><p>On the contrary, we postulate that modern deep networks can outperform the shallow architectures of current works, based on findings in related research fields <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In Section IV we perform experiments to confirm this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Current Bottlenecks</head><p>In this section, we highlight major bottlenecks in CNNbased FER based on the reported and measured results. We postulate that overcoming these bottlenecks will lead to substantial improvements in FER performance. CNN Inference Feature Comparison <ref type="figure">Fig. 4</ref>. Ten-crop test results of the different network architectures (green), and when using pretrained frontends as feature extractors (orange).</p><p>The CNN architectures used are basic and shallow compared to state-of-the-art architectures in related fields <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Most works use general data augmentation techniques such as random crops and mirroring, which are not optimized for the task at hand. Kim et al. <ref type="bibr" target="#b8">[9]</ref> show that face-aware data augmentation via face registration improves performance. This approach can be extended, for instance by data augmentation via frontalized samples <ref type="bibr" target="#b20">[21]</ref>, or by synthesizing faces in various poses via 3D face pose estimation <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Such data augmentation techniques, utilized for both training and inference, have the potential to effectively compensate for the limited size of current FER datasets.</p><p>For the same reason, training on a combination of multiple datasets can lead to significant improvements, provided that dataset bias is accounted for. <ref type="bibr" target="#b13">[14]</ref> show that this is beneficial even with datasets with heterogeneous labels.</p><p>Lastly, we put forward that the biggest bottleneck that currently hinders FER performance is the fact that there is no publicly available dataset that is large by current deep learning standards. The introduction of datasets with hundreds of thousands or millions of images has enabled significant performance gains in related research fields such as face recognition <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In contrast, FER2013, one of the largest FER image datasets available, has only 35,887 images. Compiling a large FER dataset is a laborious task due to the challenging annotation process; assigning correct expression labels in presence of subtle expressions, partial occlusions, and pose variations is a challenging task for humans <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEEP CNNS FOR FER</head><p>In this section, we confirm experimentally that overcoming one of these bottlenecks, the comparatively shallow and basic CNN architectures of current FER methods, leads to a substantial improvement in accuracy on FER2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments</head><p>In order to enable a fair comparison with the results in Section III, we use the exact same dataset, preprocessing, as well as training and testing protocols. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) CNN Architectures:</head><p>We consider three CNNs whose architectures are summarized in <ref type="table" target="#tab_0">Table IV</ref>. All are inspired by current state-of-the-art architectures in related fields:</p><p>VGG. An architecture similar to VGG-B <ref type="bibr" target="#b23">[24]</ref> but with one CCP block less. We also use dropout after each such block (this improved the validation accuracy by around 1%). For consistency with Section III, the backend consists of a single hidden layer with 1024 units.</p><p>Inception. An architecture similar to GoogLeNet <ref type="bibr" target="#b12">[13]</ref>, but with a more consistent structure and without initial strided convolutions or pooling (the input images are already small enough). The net uses a consistent distribution of feature map sizes in a given Inception layer that is based on the number of 3 ? 3 features maps n; the 1 ? 1, 3 ? 3 reduce, 5 ? 5 reduce, 5 ? 5, and pool projection layers have 3 ?4n, 1 ?2n, 1 ?8n, 1 ?4n, and 1 ?4n feature maps, respectively. n is initialized to 32 and increased by 32 after every Inception layer.</p><p>ResNet. Our architecture is identical to the 34-layer ResNet from <ref type="bibr" target="#b4">[5]</ref>, but without the initial CP block. Our network is also more narrow, having 256 feature maps in the final residual group to reduce the number of parameters. We use dropout after the final pooling layer.</p><p>VGG and Inception have less parameters than any of the architectures used in the pertinent literature, despite being significantly deeper (cf. Tables IV. and II). Even the very deep ResNet has fewer parameters than most of these architectures.</p><p>We did not specifically search for architectures that perform well on FER2013. Our goal is to confirm that modern deep architectures generally perform well, not to obtain the absolute best accuracies on this dataset.</p><p>2) CNN Ensembles: In order to demonstrate the potential of an ensemble of such deep CNNs, we perform an exhaustive search to identify optimal ensembles of up to 8 models in terms of FER2013 validation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and Discussion</head><p>The test accuracies of the individual models with the best validation accuracies are given in <ref type="table" target="#tab_0">Table IV</ref>. The best modern deep model outperforms the best shallow model by almost 2% under identical conditions (cf. <ref type="figure">Fig. 4</ref>). All considered architectures outperform the best shallow model, including Inception, which has only half as many parameters. These results confirm that utilizing modern deep architectures has the potential to substantially improve FER performance.</p><p>Our individual CNNs already perform competitively to previous works that utilize ensemble voting <ref type="figure">(Fig. 3)</ref>. By forming an ensemble of 8 such CNNs, we achieve a FER2013 test accuracy of 75.2%, performing comparably to the current best method we are aware of <ref type="bibr" target="#b13">[14]</ref>.</p><p>Our ensemble of deep models obtains state-of-the-art performance without utilizing additional training data or features, comprehensive data augmentation, or requiring face registration. By not requiring face registration, our FER method is conceptually simpler than previous methods and not affected by registration errors. We expect that utilizing auxiliary training data and comprehensive, FER-specific data augmentation would improve the performance further. This paper has been studying the FER performance by means of the FER2013 dataset, which is the most common image dataset in CNN-based FER and one of the largest publicly available datasets in this field. (There are several video datasets that contain a much larger number of frames, but these frames are naturally highly correlated and the number of subjects in such datasets is small.) Still, results obtained on this and other FER datasets are only indicative of real-world FER performance due to dataset bias. This limitation applies not only to this study but to FER research in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper we have reviewed the state of the art in CNNbased FER, highlighted key differences between the individual works, and compared and discussed their performance with a focus on the underling CNN architectures. On this basis, we have identified existing bottlenecks and consequently means for advancing the state of the art in this challenging research field. Furthermore, we have shown that overcoming one such bottleneck by employing modern deep CNNs leads to a significant improvement in FER2013 performance. Finally, we have demonstrated that an ensemble of such CNNs outperforms state of the art methods without the use of additional training data or requiring face registration.</p><p>We expect that overcoming the remaining bottlenecks identified in this paper will result in further substantial performance improvements. For the future, we plan to investigate ways for overcoming these bottlenecks, with a focus on FERspecific data augmentation. Furthermore, we will study the bias that affects FER2013 and other datasets, and investigate the possibility of creating a new, more comprehensive, and publicly available FER dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PREPROCESSING</head><label>I</label><figDesc>OPERATIONS. FD: FACE DETECTION, LM: FACIAL LANDMARK EXTRACTION., HISTEQ: HISTOGRAM EQUALIZATION, LPF: LINEAR PLANE FITTING.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CNN</head><label>II</label><figDesc>ARCHITECTURES. C, P, N, I, AND F STANDS FOR CONVOLUTIONAL, POOLING, RESPONSE-NORMALIZATION, INCEPTION, AND FULLY CONNECTED LAYERS, RESPECTIVELY. The counts were calculated assuming single-channel input images of size 48 by 48 pixels (the size of images in the FER2013 dataset). In some works, the CNNs operate on images of different sizes. The table highlights that the individual architectures vary significantly in terms of layer composition, depth, and number of parameters. Most architectures are shallow compared to architectures in related fields</figDesc><table><row><cell>Method</cell><cell cols="3">Architecture Depth Parameters</cell></row><row><cell>[7]</cell><cell>CPCPFF</cell><cell>4</cell><cell>12.0 m</cell></row><row><cell>[11]</cell><cell>CPCPCPFF</cell><cell>5</cell><cell>4.8 m</cell></row><row><cell>[8]</cell><cell>PCCPCCPCFFF</cell><cell>8</cell><cell>6.2 m</cell></row><row><cell>[12]</cell><cell>CPCPIIPIPFFF</cell><cell>11</cell><cell>7.3 m</cell></row><row><cell>[14]</cell><cell>CPNCPNCPCFF</cell><cell>6</cell><cell>21.3 m</cell></row><row><cell>[9]</cell><cell>CPCPCPFF</cell><cell>5</cell><cell>2.4 m</cell></row><row><cell cols="2">and parameter counts.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III DIFFERENCES</head><label>III</label><figDesc>IN TERMS OF CNN TRAINING AND INFERENCE. AD: ADDITIONAL TRAINING DATA, AF: ADDITIONAL FEATURES, +: DATA AUGMENTATION, S,A: SIMILARITY/AFFINE TRANSFORM, T: TRANSLATION, M: HORIZONTAL MIRRORING, REG: FACE REGISTRATION.</figDesc><table><row><cell>Method AD</cell><cell>AF</cell><cell>+ Train</cell><cell>+ Test</cell><cell>Ensemble</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV TESTED</head><label>IV</label><figDesc>DEEP ARCHITECTURES AND THEIR TEN-CROP TEST ACCURACY ON FER2013. 3R MEANS GROUP OF THREE RESIDUAL BLOCKS.</figDesc><table><row><cell>Name</cell><cell cols="4">Architecture Depth Parameters Accuracy</cell></row><row><cell>VGG</cell><cell>CCPCCPCCPCCPFF</cell><cell>10</cell><cell>1.8 m</cell><cell>72.7%</cell></row><row><cell>Inception</cell><cell>CIPIIPIIPIIPF</cell><cell>16</cell><cell>1.2 m</cell><cell>71.6%</cell></row><row><cell>ResNet</cell><cell>3R4R6R3RPF</cell><cell>33</cell><cell>5.3 m</cell><cell>72.4%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial affect: A survey of registration, representation, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances, Challenges, and Opportunities in Automatic Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V B</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="63" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Athanasakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romaszko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1512</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Learning using Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction (MMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusing Aligned and Non-Aligned Face Information for Automatic Affect Recognition in the Wild: A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video and Image Based Emotion Recognition Challenges in the Wild: EmotiW</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="423" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep convolutional neural networks for robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going Deeper in Facial Expression Recognition using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1511</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Social Relation Traits from Face Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3631" to="3639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Shet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">6082</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1502</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective Face Frontalization in Unconstrained Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dense 3D Face Alignment from 2D Videos in Real-Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1409</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
