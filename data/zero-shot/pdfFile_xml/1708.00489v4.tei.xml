<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
							<email>ozan.sener@intel.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question.</p><p>The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy <ref type="bibr" target="#b4">(Dasgupta, 2004)</ref>, there exist many heuristics <ref type="bibr" target="#b37">(Settles, 2010)</ref> which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query Published as a conference paper at ICLR 2018 labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes.</p><p>In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We discuss the related work in the following categories separately. Briefly, our work is different from existing approaches in that i) it defines the active learning problem as core-set selection, ii) we consider both fully supervised and weakly supervised cases, and iii) we rigorously address the core-set selection problem directly for CNNs with no extra assumption.</p><p>Active Learning Active learning has been widely studied and most of the early work can be found in the classical survey of <ref type="bibr" target="#b37">Settles (2010)</ref>. It covers acquisition functions such as information theoretical methods <ref type="bibr" target="#b30">(MacKay, 1992)</ref>, ensemble approaches <ref type="bibr" target="#b31">(McCallumzy &amp; Nigamy, 1998;</ref><ref type="bibr" target="#b9">Freund et al., 1997)</ref> and uncertainty based methods <ref type="bibr" target="#b40">(Tong &amp; Koller, 2001;</ref><ref type="bibr" target="#b24">Joshi et al., 2009;</ref><ref type="bibr" target="#b28">Li &amp; Guo, 2013)</ref>.</p><p>Bayesian active learning methods typically use a non-parametric model like Gaussian process to estimate the expected improvement by each query <ref type="bibr" target="#b26">(Kapoor et al., 2007)</ref> or the expected error after a set of queries <ref type="bibr" target="#b35">(Roy &amp; McCallum, 2001)</ref>. These approaches are not directly applicable to large CNNs since they do not scale to large-scale datasets. A recent approach by <ref type="bibr" target="#b10">Gal &amp; Ghahramani (2016)</ref> shows an equivalence between dropout and approximate Bayesian inference enabling the application of Bayesian methods to deep learning. Although Bayesian active learning has been shown to be effective for small datasets <ref type="bibr" target="#b11">(Gal et al., 2017)</ref>, our empirical analysis suggests that they do not scale to large-scale datasets because of batch sampling.</p><p>One important class is that of uncertainty based methods, which try to find hard examples using heuristics like highest entropy <ref type="bibr" target="#b24">(Joshi et al., 2009)</ref>, and geometric distance to decision boundaries <ref type="bibr" target="#b40">(Tong &amp; Koller, 2001;</ref><ref type="bibr" target="#b2">Brinker, 2003)</ref>. Our empirical analysis find them not to be effective for CNNs.</p><p>There are recent optimization based approaches which can trade-off uncertainty and diversity to obtain a diverse set of hard examples in batch mode active learning setting. Both <ref type="bibr" target="#b8">Elhamifar et al. (2013)</ref> and <ref type="bibr" target="#b48">Yang et al. (2015)</ref> design a discrete optimization problem for this purpose and use its convex surrogate. Similarly, <ref type="bibr" target="#b17">Guo (2010)</ref> cast a similar problem as matrix partitioning. However, the optimization algorithms proposed in these papers use n 2 variables where n is the number of data points. Hence, they do not scale to large datasets. There are also many pool based active learning algorithms designed for the specific class of machine learning algorithms like k-nearest neighbors and naive Bayes <ref type="bibr" target="#b45">(Wei et al., 2015</ref><ref type="bibr">), logistic regression Hoi et al. (2006</ref>; <ref type="bibr" target="#b18">Guo &amp; Schuurmans (2008)</ref>, and linear regression with Gaussian noise <ref type="bibr" target="#b49">(Yu et al., 2006)</ref>. Even in the algorithm agnostic case, one can design a set-cover algorithm to cover the hypothesis space using sub-modularity <ref type="bibr" target="#b16">(Guillory &amp; Bilmes, 2010;</ref><ref type="bibr" target="#b13">Golovin &amp; Krause, 2011)</ref>. On the other hand, <ref type="bibr" target="#b5">Demir et al. (2011)</ref> uses a heuristic to first filter the pool based on uncertainty and then choose point to label using diversity. Our algorithm can be considered to be in this class; however, we do not use any uncertainty information. Our algorithm is also the first one which is applied to the CNNs. Most similar to ours are <ref type="bibr" target="#b25">(Joshiy et al., 2010)</ref> and <ref type="bibr" target="#b43">(Wang &amp; Ye, 2015)</ref>. <ref type="bibr" target="#b25">Joshiy et al. (2010)</ref> uses a similar optimization problem. However, they offer no theoretical justification or analysis. <ref type="bibr" target="#b43">Wang &amp; Ye (2015)</ref> proposes to use empirical risk minimization like us; however, they try to minimize the difference between two distributions (maximum mean discrepancy between iid. samples from the dataset and the actively selected samples) instead of core-set loss. Moreover, both algorithms are also not experimented with CNNs. In our experimental study, we compare with <ref type="bibr" target="#b43">(Wang &amp; Ye, 2015)</ref>.</p><p>Recently, a discrete optimization based method <ref type="bibr" target="#b1">(Berlind &amp; Urner, 2015)</ref> which is similar to ours has been presented for k-NN type algorithms in the domain shift setting. Although our theoretical analysis borrows some techniques from them, their results are only valid for k-NNs.</p><p>Active learning algorithms for CNNs are also recently presented in <ref type="bibr" target="#b42">(Wang et al., 2016;</ref><ref type="bibr" target="#b39">Stark et al., 2015)</ref>. <ref type="bibr" target="#b42">Wang et al. (2016)</ref> propose an heuristic based algorithm which directly assigns labels to the data points with high confidence and queries labels for the ones with low confidence. Moreover, <ref type="bibr" target="#b39">Stark et al. (2015)</ref> specifically targets recognizing CAPTCHA images. Although their results are promising for CAPTCHA recognition, their method is not effective for image classification. We discuss limitations of both approaches in Section 5.</p><p>On the theoretical side, it is shown that greedy active learning is not possible in algorithm and data agnostic case <ref type="bibr" target="#b4">(Dasgupta, 2005)</ref>. However, there are data dependent results showing that it is indeed possible to obtain a query strategy which has better sample complexity than querying all points. These results either use assumptions about data-dependent realizability of the hypothesis space like <ref type="bibr" target="#b14">(Gonen et al., 2013)</ref> or a data dependent measure of the concept space called disagreement coefficient <ref type="bibr" target="#b19">(Hanneke, 2007)</ref>. It is also possible to perform active learning in a batch setting using the greedy algorithm via importance sampling <ref type="bibr" target="#b12">(Ganti &amp; Gray, 2012)</ref>. Although the aforementioned algorithms enjoy theoretical guarantees, they do not apply to large-scale problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core-Set Selection</head><p>The closest literature to our work is the problem of core-set selection since we define active learning as a core-set selection problem. This problem considers a fully labeled dataset and tries to choose a subset of it such that the model trained on the selected subset will perform as closely as possible to the model trained on the entire dataset. For specific learning algorithms, there are methods like core-sets for SVM <ref type="bibr" target="#b41">(Tsang et al., 2005)</ref> and core-sets for k-Means and k-Medians <ref type="bibr" target="#b20">(Har-Peled &amp; Kushal, 2005)</ref>. However, we are not aware of such a method for CNNs.</p><p>The most similar algorithm to ours is the unsupervised subset selection algorithm in <ref type="bibr" target="#b44">(Wei et al., 2013)</ref>. It uses a facility location problem to find a diverse cover for the dataset. Our algorithm differs in that it uses a slightly different formulation of facility location problem. Instead of the min-sum, we use the minimax (Wolf, 2011) form. More importantly, we apply this algorithm for the first time to the problem of active learning and provide theoretical guarantees for CNNs.</p><p>Weakly-Supervised Deep Learning Our paper is also related to semi-supervised deep learning since we experiment the active learning both in the fully-supervised and weakly-supervised scheme. One of the early weakly-supervised convolutional neural network algorithms was Ladder networks <ref type="bibr" target="#b34">(Rasmus et al., 2015)</ref>. Recently, we have seen adversarial methods which can learn a data distribution as a result of a two-player non-cooperative game <ref type="bibr" target="#b36">(Salimans et al., 2016;</ref><ref type="bibr" target="#b15">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b33">Radford et al., 2015)</ref>. These methods are further extended to feature learning <ref type="bibr" target="#b7">(Dumoulin et al., 2016;</ref><ref type="bibr" target="#b6">Donahue et al., 2016)</ref>. We use Ladder networks in our experiments; however, our method is agnostic to the weakly-supervised learning algorithm choice and can utilize any model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>In this section, we formally define the problem of active learning in the batch setting and set up the notation for the rest of the paper. We are interested in a C class classification problem defined over a compact space X and a label space Y = {1, . . . , C}. We also consider a loss function l(?, ?; w) : X ? Y ? R parametrized over the hypothesis class (w), e.g. parameters of the deep learning algorithm. We further assume class-specific regression functions ? c (x) = p(y = c|x) to be ? ? -Lipschitz continuous for all c.</p><p>We consider a large collection of data points which are sampled i.i.d. over the space</p><formula xml:id="formula_0">Z = X ? Y as {x i , y i } i?[n] ? p Z where [n] = {1, .</formula><p>. . , n}. We further consider an initial pool of data-points chosen uniformly at random as s 0 = {s 0 (j) ? [n]} j? <ref type="bibr">[m]</ref> .</p><p>An active learning algorithm only has access to {x i } i?[n] and {y s(j) } j? <ref type="bibr">[m]</ref> . In other words, it can only see the labels of the points in the initial sub-sampled pool. It is also given a budget b of queries to ask an oracle, and a learning algorithm A s which outputs a set of parameters w given a labelled set s. The active learning with a pool problem can simply be defined as min</p><formula xml:id="formula_1">s 1 :|s 1 |?b E x,y?p Z [l(x, y; A s 0 ?s 1 )]</formula><p>(1)</p><p>In other words, an active learning algorithm can choose b extra points and get them labelled by an oracle to minimize the future expected loss. There are a few differences between our formulation and the classical definition of active learning. Classical methods consider the case in which the budget is 1 (b = 1) but a single point has negligible effect in a deep learning regime hence we consider the batch case. It is also very common to consider multiple rounds of this game. We also follow the multiple round formulation with a myopic approach by solving the single round of labelling as; min</p><formula xml:id="formula_2">s k+1 :|s k+1 |?b E x,y?p Z [l(x, y; A s 0 ?...s k+1 )]<label>(2)</label></formula><p>We only discuss the first iteration where k = 0 for brevity although we apply it over multiple rounds.</p><p>At each iteration, an active learning algorithm has two stages: 1. identifying a set of data-points and presenting them to an oracle to be labelled, and 2. training a classifier using both the new and the previously labeled data-points. The second stage (training the classifier) can be done in a fully or weakly-supervised manner. Fully-supervised is the case where training the classifier is done using only the labeled data-points. Weakly-supervised is the case where training also utilizes the points which are not labelled yet. Although the existing literature only focuses on the active learning for fully-supervised models, we consider both cases and experiment on both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ACTIVE LEARNING AS A SET COVER</head><p>In the classical active learning setting, the algorithm acquires labels one by one by querying an oracle (i.e. b = 1). Unfortunately, this is not feasible when training CNNs since i) a single point will not have a statistically significant impact on the model due to the local optimization algorithms. ii) it is infeasible to train as many models as number of points since many practical problem of interest is very large-scale. Hence, we focus on the batch active learning problem in which the active learning algorithm choose a moderately large set of points to be labelled by an oracle at each iteration.</p><p>In order to design an active learning strategy which is effective in batch setting, we consider the following upper bound of the active learning loss we formally defined in <ref type="formula">(1)</ref>:</p><formula xml:id="formula_3">Ex,y?p Z [l(x, y; As)] ? Ex,y?p Z [l(x, y; As)] ? 1 n i?[n]</formula><p>l(xi, yi; As) </p><p>The quantity we are interested in is the population risk of the model learned using a small labelled subset (s). The population risk is controlled by the training error of the model on the labelled subset, the generalization error over the full dataset ([n]) and a term we define as the core-set loss. Core-set loss is simply the difference between average empirical loss over the set of points which have labels for and the average empirical loss over the entire dataset including unlabelled points. Empirically, it is widely observed that the CNNs are highly expressive leading to very low training error and they typically generalize well for various visual problems. Moreover, generalization error of CNNs is also theoretically studied and shown to be bounded by <ref type="bibr" target="#b47">Xu &amp; Mannor (2012)</ref>. Hence, the critical part for active learning is the core-set loss. Following this observation, we re-define the active learning problem as:</p><formula xml:id="formula_5">min s 1 :|s 1 |?b 1 n i?[n]</formula><p>l(x i , y i ; A s 0 ?s 1 ) ? 1 |s 0 + s 1 | j?s 0 ?s 1 l(x j , y j ; A s 0 ?s 1 ) (4) <ref type="figure">Figure 1</ref>: Visualization of the Theorem 1. Consider the set of selected points s and the points in the remainder of the dataset [n] \ s, our results shows that if s is the ? s cover of the dataset,</p><formula xml:id="formula_6">1 n i?[n] l(x i , y i , A s ) ? 1 |s| j?s l(x j , y j ; A s ) ? O (? s ) + O 1 n</formula><p>Informally, given the initial labelled set (s 0 ) and the budget (b), we are trying to find a set of points to query labels (s 1 ) such that when we learn a model, the performance of the model on the labelled subset and that on the whole dataset will be as close as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CORE-SETS FOR CNNS</head><p>The optimization objective we define in <ref type="formula">(4)</ref> is not directly computable since we do not have access to all the labels (i.e.</p><p>[n] \ (s 0 ? s 1 ) is unlabelled). Hence, in this section we give an upper bound for this objective function which we can optimize.</p><p>We start with presenting this bound for any loss function which is Lipschitz for a fixed true label y and parameters w, and then show that loss functions of CNNs with ReLu non-linearities satisfy this property. We also rely on the zero training error assumption. Although the zero training error is not an entirely realistic assumption, our experiments suggest that the resulting upper bound is very effective. We state the following theorem; Theorem 1. Given n i.i.d. samples drawn from p Z as {x i , y i } i? <ref type="bibr">[n]</ref> , and set of points s. If loss function l(?, y, w) is ? l -Lipschitz continuous for all y, w and bounded by L, regression function is ? ? -Lipschitz, s is ? s cover of {x i , y i } i? <ref type="bibr">[n]</ref> , and l(x s(j) , y s(j) ; A S ) = 0 ?j ? [m]; with probability at least 1 ? ?,</p><formula xml:id="formula_7">1 n i?[n] l(x i , y i ; A s ) ? 1 |s| j?s l(x j , y j ; A s ) ? ?(? l + ? ? LC) + L 2 log(1/?) 2n .</formula><p>Since we assume a zero training error for core-set, the core-set loss is equal to the average error over entire dataset as 1</p><formula xml:id="formula_8">n i?[n] l(x i , y i ; A s ) ? 1 |s| j?s l(x j , y j ; A s ) = 1 n i?[n] l(x i , y i ; A s )</formula><p>. We state the theorem in this form to be consistent with (3). We visualize this theorem in <ref type="figure">Figure 1</ref> and defer its proof to the appendix. In this theorem, "a set s is a ? cover of a set s " means a set of balls with radius ? centered at each member of s can cover the entire s . Informally, this theorem suggests that we can bound the core-set loss with covering radius and a term which goes to zero with rate depends solely on n. This is an interesting result since this bound does not depend on the number of labelled points. In other words, a provided label does not help the core-set loss unless it decreases the covering radius.</p><p>In order to show that this bound applies to CNNs, we prove the Lipschitz-continuity of the loss function of a CNN with respect to input image for a fixed true label with the following lemma where max-pool and restricted linear units are the non-linearities and the loss is defined as the l 2 distance between the desired class probabilities and the soft-max outputs. CNNs are typically used with cross-entropy loss for classification problems in the literature. Indeed, we also perform our experiments using the cross-entropy loss although we use l 2 loss in our theoretical study. Although our theoretical study does not extend to cross-entropy loss, our experiments suggest that the resulting algorithm is very effective for cross-entropy loss. Lemma 1. Loss function defined as the 2-norm between the class probabilities and the softmax output of a convolutional neural network with n c convolutional (with max-pool and ReLU) and n f c fully connected layers defined over C classes is</p><formula xml:id="formula_9">? C?1</formula><p>C ? nc+n f c -Lipschitz function of input for fixed class probabilities and network parameters.</p><p>Here, ? is the maximum sum of input weights per neuron (see appendix for formal definition). Although it is in general unbounded, it can be made arbitrarily small without changing the loss function behavior (i.e. keeping the label of any data point s unchanged). We defer the proof to the appendix and conclude that CNNs enjoy the bound we presented in Theorem 1.</p><p>In order to computationally perform active learning, we use this upper bound. In other words, the practical problem of interest becomes min s 1 :|s 1 ?b| ? s 0 ?s 1 . This problem is equivalent to the k-Center problem (also called min-max facility location problem) (Wolf, 2011). In the next section, we explain how we solve the k-Center problem in practice using a greedy approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SOLVING THE K-CENTER PROBLEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 k-Center-Greedy</head><p>Input: data x i , existing pool s 0 and a budget b Initialize s = s 0 repeat u = arg max i?[n]\s min j?s ?(</p><formula xml:id="formula_10">x i , x j ) s = s ? {u} until |s| = b + |s 0 | return s \ s 0</formula><p>We have so far provided an upper bound for the loss function of the core-set selection problem and showed that minimizing it is equivalent to the k-Center problem (minimax facility location (Wolf, 2011)) which can intuitively be defined as follows; choose b center points such that the largest distance between a data point and its nearest center is minimized. Formally, we are trying to solve: min</p><formula xml:id="formula_11">s 1 :|s 1 |?b max i min j?s 1 ?s 0 ?(x i , x j )<label>(5)</label></formula><p>Unfortunately this problem is NP-Hard (Cook et al., 1998). However, it is possible to obtain a 2 ? OP T solution efficiently using a greedy approach shown in Algorithm 1. If OP T = min s 1 max i min j?s 1 ?s 0 ?(x i , x j ), the greedy algorithm shown in Algorithm 1 is proven to have a solution (s 1 ) such that; max i min j?s 1 ?s 0 ?(x i , x j ) ? 2 ? OP T .</p><p>Although the greedy algorithm gives a good initialization, in practice we can improve the 2 ? OP T solution by iteratively querying upper bounds on the optimal value. In other words, we can design an algorithm which decides if OP T ? ?. In order to do so, we define a mixed integer program (MIP) parametrized by ? such that its feasibility indicates min s 1 max i min j?s 1 ?s 0 ?(x i , x j ) ? ?. A straight-forward algorithm would be to use this MIP as a sub-routine and performing a binary search between the result of the greedy algorithm and its half since the optimal solution is guaranteed to be included in that range. While constructing this MIP, we also try to handle one of the weaknesses of k-Center algorithm, namely robustness. To make the k-Center problem robust, we assume an upper limit on the number of outliers ? such that our algorithm can choose not to cover at most ? unsupervised data points. This mixed integer program can be written as:</p><formula xml:id="formula_12">F easible(b, s 0 , ?, ?) : j u j , = |s 0 | + b, i,j ? i,j ? ? j ? i,j = 1 ?i, ? i,j ? u j ?i, j u i = 1 ?i ? s 0 , u i ? {0, 1} ?i ? i,j = ? i,j ?i, j | ?(x i , x j ) &gt; ?.<label>(6)</label></formula><p>In this formulation, u i is 1 if the i th data point is chosen as center, ? i,j is 1 if the i th point is covered by the j th , point and ? i,j is 1 if the i th point is an outlier and covered by the j th point without the ? constraint, and 0 otherwise. And, variables are binary as u i , ? i,j , ? i,j ? {0, 1}. We further visualize these variables in a diagram in <ref type="figure">Figure 2</ref>, and give the details of the method in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Robust k-Center</head><p>Input: data x i , existing pool s 0 , budget b and outlier bound ?</p><formula xml:id="formula_13">Initialize s g = k-Center-Greedy(x i , s 0 , b) ? 2?OP T = max j min i?sg ?(x i , x j ) lb = ? 2?OP T 2 , ub = ? 2?OP T repeat if F easible(b, s 0 , lb+ub 2 , ?) then ub = max i,j|?(xi,xj )? lb+ub 2 ?(x i , x j ) else lb = min i,j|?(xi,xj )? lb+ub 2 ?(x i , x j ) end if until ub = lb return {i s.t. u i = 1}</formula><p>Figure 2: Visualizations of the variables. In this solution, the 4 th node is chosen as a center and nodes 0, 1, 3 are in a ? ball around it. The 2 nd node is marked as an outlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">IMPLEMENTATION DETAILS</head><p>One of the critical design choices is the distance metric ?(?, ?). We use the l 2 distance between activations of the final fully-connected layer as the distance. For weakly-supervised learning, we used Ladder networks <ref type="bibr" target="#b34">(Rasmus et al., 2015)</ref> and for all experiments we used VGG-16 <ref type="bibr" target="#b38">(Simonyan &amp; Zisserman, 2014)</ref> as the CNN architecture. We initialized all convolutional filters according to <ref type="bibr" target="#b21">He et al. (2016)</ref>. We optimized all models using RMSProp with a learning rate of 1e?3 using Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. We train CNNs from scratch after each iteration.</p><p>We used the Gurobi (Inc., 2016) framework for checking feasibility of the MIP defined in (6). As an upper bound on outliers, we used ? = 1e?4 ? n where n is the number of unlabelled points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>We tested our algorithm on the problem of classification using three different datasets. We performed experiments on CIFAR <ref type="bibr" target="#b27">(Krizhevsky &amp; Hinton, 2009</ref>) dataset for image classification and on SVHN <ref type="bibr" target="#b32">(Netzer et al., 2011)</ref> dataset for digit classification. CIFAR <ref type="bibr" target="#b27">(Krizhevsky &amp; Hinton, 2009</ref>) dataset has two tasks; one coarse-grained over 10 classes and one fine-grained over 100 classes. We performed experiments on both.</p><p>We compare our method with the following baselines: i)Random: Choosing the points to be labelled uniformly at random from the unlabelled pool. ii)Best Empirical Uncertainty: Following the empirical setup in <ref type="bibr" target="#b11">(Gal et al., 2017)</ref>, we perform active learning using max-entropy, BALD and Variation Ratios treating soft-max outputs as probabilities. We only report the best performing one for each dataset since they perform similar to each other. iii) Deep Bayesian Active Learning (DBAL) <ref type="bibr" target="#b11">(Gal et al., 2017)</ref>: We perform Monte Carlo dropout to obtain improved uncertainty measures and report only the best performing acquisition function among max-entropy, BALD and Variation Ratios for each dataset. iv) Best Oracle Uncertainty: We also report a best performing oracle algorithm which uses the label information for entire dataset. We replace the uncertainty with l(x i , y i , A s 0 ) for all unlabelled examples. We sample the queries from the normalized form of this function by setting the probability of choosing the i th point to be queried as p i = l(xi,yi,A s 0 ) j l(xj ,yj ,A s 0 ) . v)k-Median:</p><p>Choosing the points to be labelled as the cluster centers of k-Median (k is equal to the budget) algorithm. vi)Batch Mode Discriminative-Representative Active Learning(BMDR) <ref type="bibr" target="#b43">(Wang &amp; Ye, 2015)</ref>: ERM based approach which uses uncertainty and minimizes MMD between iid. samples from the dataset and the actively chosen points. vii)CEAL <ref type="bibr" target="#b42">(Wang et al., 2016)</ref>: CEAL <ref type="bibr" target="#b42">(Wang et al., 2016</ref>) is a weakly-supervised active learning method proposed specifically for CNNs. we include it in the weakly-supervised analysis.    We conducted experiments on active learning for fully-supervised models as well as active learning for weakly-supervised models. In our experiments, we start with small set of images sampled uniformly at random from the dataset as an initial pool. The weakly-supervised model has access to labeled examples as well as unlabelled examples. The fully-supervised model only has access to the labeled data points. We run all experiments with five random initializations of the initial pool of labeled points and use the average classification accuracy as a metric. We plot the accuracy vs the number of labeled points. We also plot error bars as standard deviations. We run the query algorithm iteratively; in other words, we solve the discrete optimization problem min s k+1 :|s k+1 |?b E x,y?p Z [l(x, y; A s 0 ?...,s k+1 )] for each point on the accuracy vs number of labelled examples graph. We present the results in <ref type="figure" target="#fig_0">Figures 3 and 4</ref>.</p><p>Figures 3 and 4 suggests that our algorithm outperforms all other baselines in all experiments; for the case of weakly-supervised models, by a large margin. We believe the effectiveness of our approach in the weakly-supervised case is due to the better feature learning. Weakly-supervised models provide better feature spaces resulting in accurate geometries. Since our method is geometric, it performs significantly better with better feature spaces. We also observed that our algorithm is less effective in CIFAR-100 when compared with CIFAR-10 and SVHN. This can easily be explained using our theoretical analysis. Our bound over the core-set loss scales with the number of classes, hence it is better to have fewer classes.</p><p>One interesting observation is the fact that a state-of-the-art batch mode active learning baseline (BMDR <ref type="bibr" target="#b43">(Wang &amp; Ye, 2015)</ref>) does not necessarily perform better than greedy ones. We believe this is due to the fact that it still uses an uncertainty information and soft-max probabilities are not a good proxy for uncertainty. Our method does not use any uncertainty. And, incorporating uncertainty to our method in a principled way is an open problem and a fruitful future research direction. On the other hand, a pure clustering based batch active learning baseline (k-Medoids) is also not effective. We believe this is rather intuitive since cluster sentences are likely the points which are well covered with initial iid. samples. Hence, this clustering based method fails to sample the tails of the data distribution.</p><p>Our results suggest that both oracle uncertainty information and Bayesian estimation of uncertainty is helpful since they improve over empirical uncertainty baseline; however, they are still not effective in the batch setting since random sampling outperforms them. We believe this is due to the correlation in the queried labels as a consequence of active learning in batch setting. We further investigate this with a qualitative analysis via tSNE <ref type="bibr" target="#b29">(Maaten &amp; Hinton, 2008)</ref> embeddings. We compute embeddings for all points using the features which are learned using the labelled examples and visualize the points (a) Uncertainty Oracle (b) Our Method <ref type="figure">Figure 5</ref>: tSNE embeddings of the CIFAR dataset and behavior of uncertainty oracle as well as our method. For both methods, the initial labeled pool of 1000 images are shown in blue, 1000 images chosen to be labeled in green and remaining ones in red. Our algorithm results in queries evenly covering the space. On the other hand, samples chosen by uncertainty oracle fails to cover the large portion of the space.  <ref type="figure">Figure 6</ref>: We compare our method with k-Center-Greedy. Our algorithm results in a small but important accuracy improvement.</p><p>sampled by our method as well as the oracle uncertainty. This visualization suggests that due to the correlation among samples, uncertainty based methods fail to cover the large portion of the space confirming our hypothesis.</p><p>Optimality of the k-Center Solution: Our proposed method uses the greedy 2-OPT solution for the k-Center problem as an initialization and checks the feasibility of a mixed integer program (MIP). We use LP-relaxation of the defined MIP and use branch-and-bound to obtain integer solutions. The utility obtained by solving this expensive MIP should be investigated. We compare the average run-time of MIP 1 with the run-time of 2-OPT solution in <ref type="table" target="#tab_3">Table 1</ref>. We also compare the accuracy obtained with optimal k-Center solution and the 2-OPT solution in <ref type="figure">Figure 6</ref> on CIFAR-100 dataset.</p><p>As shown in the Table 1; although the run-time of MIP is not polynomial in worst-case, in practice it converges in a tractable amount of time for a dataset of 50k images. Hence, our algorithm can easily be applied in practice. <ref type="figure">Figure 6</ref> suggests a small but significant drop in the accuracy when the 2-OPT solution is used. Hence, we conclude that unless the scale of the dataset is too restrictive, using our proposed optimal solver is desired. Even with the accuracy drop, our active learning strategy using 2-OPT solution still outperforms the other baselines. Hence, we can conclude that our algorithm can scale to any dataset size with small accuracy drop even if solving MIP is not feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We study the active learning problem for CNNs. Our empirical analysis showed that classical uncertainty based methods have limited applicability to the CNNs due to the correlations caused by batch sampling. We re-formulate the active learning problem as core-set selection and study the core-set problem for CNNs. We further validated our algorithm using an extensive empirical study. Empirical results on three datasets showed state-of-the-art performance by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOF FOR LEMMA 1</head><p>Proof. We will start with showing that softmax function defined over C class is , i = 1, 2, ..., C For brevity, we will denote f i (x) as f i . The Jacobian matrix will be,</p><formula xml:id="formula_14">J = ? ? ? f 1 (1 ? f 1 ) ?f 1 f 2 ... ?f 1 f C ?f 2 f 1 f 2 (1 ? f 2 ) ... ?f 2 f C ... ... ... ... ?f C f 1 ?f C f 2 ... ?f C (1 ? f C ) ? ? ?</formula><p>Now, Frobenius norm of above matrix will be,</p><formula xml:id="formula_15">J F = C i=1 C j=1,i =j f 2 i f 2 j + C i=1 f 2 i (1 ? f i ) 2</formula><p>It is straightforward to show that f i = 1 C is the optimal solution for J * F = max x J F Hence, putting f i = 1 C in the above equation , we get J * F = ? C?1 C . Now, consider two inputs x andx, such that their representation at layer d is x d andx d . Let's consider any convolution or fully-connected layer as x d j = i w d i,j x d?1 i . If we assume, i |w i,j | ? ? ?i, j, d, for any convolutional or fully connected layer, we can state:</p><formula xml:id="formula_16">x d ?x d 2 ? ? x d?1 ?x d?1</formula><p>On the other hand, using |a ? b| ? | max(0, a) ? max(0, a)| and the fact that max pool layer can be written as a convolutional layer such that only one weight is 1 and others are 0, we can state for ReLU and max-pool layers,</p><formula xml:id="formula_17">x d ?x d 2 ? x d?1 ?x d?1 2</formula><p>Combining with the Lipschitz constant of soft-max layer,</p><formula xml:id="formula_18">CN N (x; w) ? CN N (x; w) 2 ? ? C ? 1 C ? nc+n f c x ?x 2</formula><p>Using the reverse triangle inequality as |l(x, y; w)?l(x, y; w)| = | CN N (x; w)?y 2 ? CN N (x; w)?y 2 | ? CN N (x; w)?CN N (x; w) 2 , we can conclude that the loss function is</p><formula xml:id="formula_19">? C?1</formula><p>C ? nc+n f c -Lipschitz for any fixed y and w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF FOR THEOREM 1</head><p>Before starting our proof, we state the Claim 1 from <ref type="bibr" target="#b1">Berlind &amp; Urner (2015)</ref>. Fix some p, p ? [0, 1] and y ? {0, 1}. Then, p y?p (y = y ) ? p y?p (y = y ) + |p ? p | Proof. We will start our proof with bounding E yi??(xi) [l(x i , y i ; A s )]. We have a condition which states that there exists and x j in ? ball around x i such that x j has 0 loss.</p><formula xml:id="formula_20">E yi??(xi) [l(x i , y i ; A s )] = k?[C] p yi?? k (xi) (y i = k)l(x i , k; A s ) (d) ? k?[C] p yi?? k (xj ) (y i = k)l(x i , k; A s ) + k?[C] |? k (x i ) ? ? k (x j )|l(x i , k; A s ) (e) ? k?[C]</formula><p>p yi?? k (xj ) (y i = k)l(x i , k; A s ) + ?? ? LC With abuse of notation, we represent {y i = k} ? ? k (x i ) with y i ? ? k (x i ). We use Claim 1 in (d), and Lipschitz property of regression function and bound of loss in (d). Then, we can further bound the remaining term as; ? ?? l where last step is coming from the fact that the trained classifier assumed to have 0 loss over training points. If we combine them,</p><formula xml:id="formula_21">E yi??(xi) [l(x i , y i , A s )] ? ?(? l + ? ? LC)</formula><p>We further use the Hoeffding's Bound and conclude that with probability at least 1 ? ?, 1 n i? <ref type="bibr">[n]</ref> l(x i , y i ; A s ) ? 1 |s| j?s l(x j , y j ; A s ) ? ?(? l + ? ? LC) + L 2 log(1/?) 2n</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Results on Active Learning for Weakly-Supervised Model (error bars are std-dev)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Results on Active Learning for Fully-Supervised Model (error bars are std-dev)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>continuous. It is easy to show that for any differentiable function f :R n ? R m , f (x) ? f (y) 2 ? J * F x ? y 2 ?x, y ? R n where J * F = maxx J F and J is the Jacobian matrix of f . Softmax function is defined as f (x) i = exp(x i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>p</head><label></label><figDesc>yi?? k (xj ) (y i = k)l(x i , k; A s ) = k?[C] p yi?? k (xj ) (y i = k)[l(x i , k; A s ) ? l(x j , k; A s )] + k?[C] p yi?? k (xj ) (y i = k)l(x j , k; A s )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Average run-time of our algorithm for b = 5k and |s 0 | = 10k in seconds.</figDesc><table><row><cell cols="4">Distance Greedy</cell><cell>MIP</cell><cell>MIP</cell></row><row><cell cols="7">Matrix (2-OPT) (iteration) (total) Total</cell></row><row><cell></cell><cell cols="2">104.2</cell><cell>2</cell><cell>7.5</cell><cell cols="2">244.03 360.23</cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell>0.64</cell><cell>0.63</cell><cell>0.65</cell></row><row><cell></cell><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.55</cell><cell></cell><cell>0.54</cell><cell></cell><cell></cell></row><row><cell>Classification Accuracy</cell><cell>0.40 0.45 0.50</cell><cell>0.45</cell><cell>0.43</cell><cell>0.52</cell><cell></cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.30</cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.29</cell><cell></cell><cell></cell><cell></cell><cell>Greedy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Our Method</cell></row><row><cell></cell><cell>0.25</cell><cell>10k</cell><cell>20k</cell><cell>30k</cell><cell>40k</cell><cell>50k</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Number of Labelled Images</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">On Intel Core i7-5930K@3.50GHz and 64GB memory</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active nearest neighbors in changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berlind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incorporating diversity in active learning with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>William R Pulleyblank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schrijver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">605</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of a greedy active learning strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2636-analysis-of-a-greedy-active-learning-strategy.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch-mode active-learning methods for the interactive classification of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beg?m</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Persello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1014" to="1031" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
	</analytic>
	<monogr>
		<title level="j">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convex optimization framework for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Shankar</forename><surname>Sasrty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selective sampling using the query by committee algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02910</idno>
		<title level="m">Deep bayesian active learning with image data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Upal: Unbiased pool based active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="422" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive submodularity: Theory and applications in active learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="427" to="486" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient active learning of halfspaces: an aggressive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2583" to="2615" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Interactive submodular set cover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1002.3345</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active instance sampling via matrix partition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative batch mode active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A bound on the label complexity of agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Smaller coresets for k-median and k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sariel</forename><surname>Har</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Peled</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Kushal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Computational geometry</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch mode active learning and its application to medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gurobi Optimization Inc. Gurobi optimizer reference manual</title>
		<ptr target="http://www.gurobi.com" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-class batch-mode active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Joshiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROBOT.2010.5509293</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="1873" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active learning with gaussian processes for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Information-based objective functions for active data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="590" to="604" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Employing em and pool-based active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kachites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mccallumzy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Toward optimal active learning through monte carlo estimation of error reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">11</biblScope>
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Captcha recognition with active deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Haz?rbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR Workshop on New Challenges in Neural Computation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Core vector machines: Fast svm training on very large data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ivor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pak-Ming</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="363" to="392" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cost-effective active learning for deep image classification. Transactions on Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Querying discriminative and representative samples for batch mode active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using document summarization techniques for speech data subset selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Submodularity in data subset selection and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rishabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robustness and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="423" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-class active learning by uncertainty sampling with diversity maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Active learning via transductive experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
