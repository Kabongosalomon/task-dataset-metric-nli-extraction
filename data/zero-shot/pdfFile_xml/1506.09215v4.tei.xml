<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning from Narrated Instruction Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Agrawal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Learning from Narrated Instruction Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks 1 that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner, the main steps to achieve the task and locate the steps in the input videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Millions of people watch narrated instruction videos <ref type="bibr" target="#b0">2</ref> to learn new tasks such as assembling IKEA furniture or changing a flat car tire. Many of such tasks have large amounts of videos available on-line. For example, querying for "how to change a tire" results in more than 300,000 hits on YouTube. Most of these videos, however, are made with the intention to teach other people to perform the task and do not provide direct supervisory signal for automatic learning algorithms. Developing unsupervised methods that could learn tasks from myriads of instruction videos on the Internet is therefore a key challenge. Such automatic cogni- * WILLOW project-team, D?partement d'Informatique de l'Ecole Normale Sup?rieure, ENS/INRIA/CNRS UMR 8548, Paris, France.</p><p>? SIERRA project-team, D?partement d'Informatique de l'Ecole Normale Sup?rieure, ENS/INRIA/CNRS UMR 8548, Paris, France.</p><p>? IIIT Hyderabad 1 How to : change a car tire, perform CardioPulmonary resuscitation (CPR), jump a car, repot a plant and make coffee <ref type="bibr" target="#b0">2</ref> Some instruction videos on YouTube have tens of millions of views, e.g. www.youtube.com/watch?v=J4-GRH2nDvw. tive ability would enable constructing virtual assistants and smart robots that learn new skills from the Internet to, for example, help people achieve new tasks in unfamiliar situations.</p><p>In this work, we consider instruction videos and develop a method that learns a sequence of steps, as well as their textual and visual representations, required to achieve a certain task. For example, given a set of narrated instruction videos demonstrating how to change a car tire, our method automatically discovers consecutive steps for this task such as loosen the nuts of the wheel, jack up the car, remove the spare tire and so on as illustrated in <ref type="figure">Figure 1</ref>. In addition, the method learns the visual and linguistic variability of these steps from natural videos.</p><p>Discovering key steps from instruction videos is a highly challenging task. First, linguistic expressions for the same step can have high variability across videos, for example: "...Loosen up the wheel nut just a little before you start jacking the car..." and "...Start to loosen the lug nuts just enough to make them easy to turn by hand...". Second, the visual appearance of each step varies greatly between videos as the people and objects are different, the action is captured from a different viewpoint, and the way people perform actions also vary. Finally, there is also a variability of the overall structure of the sequence of steps achieving the task. For example, some videos may omit some steps or change slightly their order.</p><p>To address these challenges, in this paper we develop an unsupervised learning approach that takes advantage of the complementarity of the visual signal in the video and the corresponding natural language narration to resolve their ambiguities. We assume that the same ordered sequence of steps (also called script in the NLP literature <ref type="bibr" target="#b24">[26]</ref>) is common to all input videos of the same task, but the actual sequence and the individual steps are unknown and are learnt directly from data. This is in contrast to other existing methods for modeling instruction videos <ref type="bibr" target="#b17">[19]</ref> that assume a script (recipe) is known and fixed in advance. We address the problem by first performing temporal clustering of text followed by clustering in video, where the two clustering tasks are linked by joint constraints. The complementary nature of the two clustering problems helps to resolve ambiguities in the two individual modalities. For example, two video segments with very different appearance but depict-Figure 1: Given a set of narrated instruction videos demonstrating a particular task, we wish to automatically discover the main steps to achieve the task and associate each step with its corresponding narration and appearance in each video. Here frames from two videos demonstrating changing the car tire are shown, together with excerpts of the corresponding narrations. Note the large variations in both the narration and appearance of the different steps highlighted by the same colors in both videos (here only three steps are shown).</p><p>ing the same step can be grouped together because they are narrated in a similar language. Conversely, two video segments described with very different expressions, for example, "jack up the car" and "raise the vehicle" can be identified as belonging to the same instruction step because they have similar visual appearance. The output of our method is the script listing the discovered steps of the task as well as the temporal location of each step in the input videos. We validate our method on a new dataset of instruction videos composed of five different tasks with a total of 150 videos and about 800,000 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This work relates to unsupervised and weaklysupervised learning methods in computer vision and natural language processing. Particularly related to ours is the work on learning script-like knowledge from natural language descriptions <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b24">26]</ref>. These methods aim to discover typical events (steps) and their order for particular scenarios (tasks) <ref type="bibr" target="#b1">3</ref> such as "cooking scrambled egg", "taking a bus" or "making coffee". While <ref type="bibr" target="#b4">[6]</ref> uses large-scale news copora, <ref type="bibr" target="#b24">[26]</ref> argues that many events are implicit and are not described in such general-purpose text data. Instead, <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b24">26]</ref> use event sequence descriptions collected for particular scenarios. Differently to this work, we learn sequences of events from narrated instruction videos on the Internet. Such data contains detailed event descriptions but is not structured and contains more noise compared to the input of <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b24">26]</ref>.</p><p>Interpretation of narrated instruction videos has been re-cently addressed in <ref type="bibr" target="#b17">[19]</ref>. While this work analyses cooking videos at a great scale, it relies on readily-available recipes which may not be available for more general scenarios. Differently from <ref type="bibr" target="#b17">[19]</ref>, we here aim to learn the steps of instruction videos using a discriminative clustering approach. A similar task to ours is addressed in <ref type="bibr" target="#b19">[21]</ref> using latent variable structured perceptron algorithm to align nouns in instruction sentences with objects touched by hands in instruction videos. However, similarly to <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b19">[21]</ref> uses laboratory experimental protocols as textual input, whereas here we consider a weaker signal in the form of the real transcribed narration of the video. In computer vision, unsupervised action recognition has been explored in simple videos <ref type="bibr" target="#b21">[23]</ref>. More recently, weakly supervised learning of actions in video using video scripts or event order has been addressed in <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b14">16]</ref>. Particularly related to ours is the work <ref type="bibr" target="#b2">[4]</ref> which explores the known order of events to localize and learn actions in training data. While <ref type="bibr" target="#b2">[4]</ref> uses manually annotated sequences of events, we here discover the sequences of main events by clustering transcribed narrations of the videos. Related is also the work of <ref type="bibr" target="#b3">[5]</ref> that aligns natural text descriptions to video but in contrast to our approach does not discover automatically the common sequence of main steps. Methods in <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b23">25]</ref> learn in an unsupervised manner the temporal structure of actions from video but do not discover textual expressions for actions as we do in this work. The recent concurrent work <ref type="bibr" target="#b25">[27]</ref> is addressing, independently of our work, a similar problem but with a different approach based on a probabilistic generative model and considering a different set of tasks mainly focussed on cooking activities.</p><p>Our work is also related to video summarization and in particular to the recent work on category-specific video summarization <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b27">29]</ref>. While summarization is a subjective task, we here aim to extract the key steps required to achieve a concrete task that consistently appear in the same sequence in the input set of videos. In addition, unlike video summarization <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b27">29]</ref> we jointly exploit visual and linguistic modalities in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">New dataset of instruction videos</head><p>We have collected a dataset of narrated instruction videos for five tasks: Making a coffee, Changing car tire, Performing cardiopulmonary resuscitation (CPR), Jumping a car and Repotting a plant. The videos were obtained by searching YouTube with relevant keywords. The five tasks were chosen so that they have a large number of available videos with English transcripts while trying to cover a wide range of activities that include complex interactions of people with objects and other people. For each task, we took the top 30 videos with English ASR returned by YouTube. We also quickly verified that each video contains a person actually performing the task (as opposed to just talking about it). The result is a total of 150 videos, 30 videos for each task. The average length of our videos is about 4,000 frames (or 2 minutes) and the entire dataset contains about 800,000 frames.</p><p>The selected videos have English transcripts obtained from YouTube's automatic speech recognition (ASR) system. To remove the dependence of results on errors of the particular ASR method, we have manually corrected misspellings and punctuations in the output transcriptions. We believe this step will soon become obsolete given rapid improvements of ASR methods. As we do not modify the content of the spoken language in videos, the transcribed verbal instructions still represent an extremely challenging example of natural language with large variability in the used expressions and terminology. Each word of the transcript is associated with a time interval in the video (usually less than 5 seconds) obtained from the closed caption timings.</p><p>For the purpose of evaluation, we have manually annotated the temporal location in each video of the main steps necessary to achieve the given task. For all tasks, we have defined the ordered sequence of ground truth steps before running our algorithm. The choice of steps was made by an agreement of 2-3 annotators who have watched the input videos and verified the steps on instruction video websites such as http://www.howdini.com. While some steps can be occasionally left out in some videos or the ordering slightly modified, overall we have observed a good consistency in the given sequence of instructions among the input videos. We measured that only 6% of the step annotations did not fit the global order, while a step was missing from the video 27% of the time. <ref type="bibr" target="#b2">4</ref> We hypothesize that this could be attributed to the fact that all videos are made with the same goal of giving other humans clear, concise and comprehensible verbal and visual instructions on how to achieve the given task. Given the list of steps for each task, we have manually annotated each time interval in each input video to one of the ground truth steps (or no step). The actions of the individual steps are typically separated by hundreds of frames where the narrator transitions between the steps or explains verbally what is going to happen. Furthermore, some steps could be missing in some videos, or could be present but not described in the narration. Finally, the temporal alignment between the narration and the actual actions in video is only coarse as the action is often described before it is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Modelling narrated instruction videos</head><p>We are given a set of N instruction videos all depicting the same task (such as "changing a tire"). The n-th input video is composed of a video stream of T n segments of frames (x n t ) Tn t=1 and an audio stream containing a detailed verbal description of the depicted task. We suppose that the audio description was transcribed to raw text and then processed to a sequence of S n text tokens (d n s ) Sn s=1 . Given this data, we want to automatically recover the sequence of K main steps that compose the given task and locate each step within each input video and text transcription.</p><p>We formulate the problem as two clustering tasks, one in text and one in video, applied one after each other and linked by joint constraints linking the two modalities. This two-stage approach is based on the intuition that the variation in natural language describing each task is easier to capture than the visual variability of the input videos. In the first stage, we cluster the text transcripts into a sequence of K main steps to complete the given task. Empirically, we have found (see results in Sec. 5.1) that it is possible to discover the sequence of the K main steps for each task with high precision. However, the text itself gives only a poor localization of each step in each video. Therefore, in the second stage we accurately localize each step in each video by clustering the input videos using the sequence of K steps extracted from text as constraints on the video clustering. To achieve this, we use two types of constraints between video and text. First, we assume that both the video and the text narration follow the same sequence of steps. This results in a global ordering constraint on the recovered clustering. Second, we assume that people perform the action approximately at the same time that they talk about it. This constraint temporally links the recovered clusters in text and video. The important outcome of the video clustering stage is that the K extracted steps get propagated by visual similarity to videos where the text descriptions are missing or ambiguous.</p><p>We first describe the text clustering in Sec. 4.1 and then introduce the video clustering with constraints in Sec. 4.2. Here, an illustration of four sequences from four different videos is shown. Middle: Multiple sequence alignment is used to align all sequences together. Note that different direct object relations are aligned together as long as they have the same sense, e.g. "loosen nut" and "undo bolt". Right: The main instruction steps are extracted as the K = 3 most common steps in all the sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Clustering transcribed verbal instructions</head><p>The goal here is to cluster the transcribed verbal descriptions of each video into a sequence of main steps necessary to achieve the task. This stage is important as the resulting clusters will be used as constraints for jointly learning and localizing the main steps in video. We assume that the important steps are common to many of the transcripts and that the sequence of steps is (roughly) preserved in all transcripts. Hence, following <ref type="bibr" target="#b24">[26]</ref>, we formulate the problem of clustering the input transcripts as a multiple sequence alignment problem. However, in contrast to <ref type="bibr" target="#b24">[26]</ref> who cluster manually provided descriptions of each step, we wish to cluster transcribed verbal instructions. Hence our main challenge is to deal with the variability in spoken natural language. To overcome this challenge, we take advantage of the fact that completing a certain task usually involves interactions with objects or people and hence we can extract a more structured representation from the input text stream.</p><p>More specifically, we represent the textual data as a sequence of direct object relations. A direct object relation d is a pair composed of a verb and its direct object complement, such as "remove tire". Such a direct object relation can be extracted from the dependency parser of the input transcribed narration <ref type="bibr" target="#b6">[8]</ref>. We denote the set of all different direct object relations extracted from all narrations as D, with cardinality D. For the n-th video, we thus represent the text signal as a sequence of direct object relation tokens: d n = (d n 1 , . . . , d n Sn ), where the length S n of the sequence varies from one video clip to another. This step is key to the success of our method as it allows us to convert the problem of clustering raw transcribed text into an easier problem of clustering sequences of direct object relations. The goal is now to extract from the narrations the most common sequence of K main steps to achieve the given task. To achieve this, we first find a globally consistent alignment of the direct object relations that compose all text sequences by solving a multiple sequence alignment problem. Second, we pick from this alignment the K most globally consistent clusters across videos.</p><p>Multiple sequence alignment model. We formulate the first stage of finding the common alignment between the input sequences of direct object relations as a multiple sequence alignment problem with the sum-of-pairs score <ref type="bibr" target="#b29">[31]</ref>. In details, a global alignment can be defined by re-mapping each input sequence d n of tokens to a global common template of L slots, for L large enough. We let (?(d n )) 1?l?L represent the (increasing) re-mapping for sequence d n at the new locations indexed by l: ?(d n ) l represents the direct object relation put at location l, with ?(d n ) l = ? if a slot is left empty (denoting the insertion of a gap in the original sequence of tokens). See the middle of <ref type="figure" target="#fig_0">Figure 2</ref> for an example of re-mapping. The goal is then to find a global alignment that minimizes the following sum-of-pairs cost function:</p><formula xml:id="formula_0">(n,m) L l=1 c(?(d n ) l , ?(d m ) l ),<label>(1)</label></formula><p>where c(d 1 , d 2 ) denotes the cost of aligning the direct object relations d 1 and d 2 at the same common slot l in the global template. The above cost thus denotes the sum of all pairwise alignments of the individual sequences (the outer sum), where the quality of each alignment is measured by summing the cost c of matches of individual direct object relations mapped into the common template sequence. We use a negative cost when d 1 and d 2 are similar according to the distance in the WordNet tree <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b18">20]</ref> of their verb and direct object constituents, and positive if they are dissimilar (details are given in Sec. 5). As the verbal narrations can talk about many other things than the main steps of a task, we set c(d, d ) = 0 if either d or d is ?. An illustration of clustering the transcribed verbal instructions into a sequence of K steps is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Optimization using Frank-Wolfe. Optimizing the cost (1) is NP-hard <ref type="bibr" target="#b29">[31]</ref> because of the combinatorial nature of the problem. The standard solution from computational biology is to apply a heuristic algorithm that proceeds by incremental pairwise alignment using dynamic programming <ref type="bibr" target="#b15">[17]</ref>. In contrast, we show in Appendix B.1 that the multiple sequence alignment problem given by (1) can be reformulated as an integer quadratic program with combinatorial constraints, for which the Frank-Wolfe optimization algorithm has been used recently with increasing success <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15]</ref>. Interestingly, we have observed empirically (see Appendix B.2) that the Frank-Wolfe algorithm was giving better solutions (in terms of objective (1)) than the state-of-the-art heuristic procedures for this task <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b15">17]</ref>. Our Frank-Wolfe based solvers also offer us greater flexibility in defining the alignment cost and scale better with the length of input sequences and the vocabulary of direct object relations.</p><p>Extracting the main steps. After a global alignment is obtained, we sort the global template l by the number of direct object relations aligned to each slot. Given K as input, the top K slots give the main instruction steps for the task, unless there are multiple steps with the same support, which go beyond K. In this case, we pick the next smaller number below K which excludes these ties, allowing the choice of an adaptive number of main instruction steps when there is not enough saliency for the last steps. This strategy essentially selects k ? K salient steps, while refusing to make a choice among steps with equal support that would increase the total number of steps beyond K. As we will see in our results in Sec. 5.1, our algorithm sometimes returns a much smaller number than K for the main instruction steps, giving more robustness to the exact choice of parameter K.</p><p>Encoding of the output. We post-process the output of multiple sequence alignment into an assignment matrix R n ? {0, 1} Sn?K for each input video n, where (R n ) sk = 1 means that the direct object token d n s has been assigned to step k. If a direct object has not been assigned to any step, the corresponding row of the matrix R n will be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discriminative clustering of videos under text constraints</head><p>Given the output of the text clustering that identified the important K steps forming a task, we now want to find their temporal location in the video signal. We formalize this problem as looking for an assignment matrix Z n ? {0, 1} Tn?K for each input video n, where (Z n ) tk = 1 indicates the visual presence of step k at time interval t in video n, and T n is the length of video n. Similarly to R n , we allow the possibility that a whole row of Z n is zero, indicating that no step is visually present for the corresponding time interval.</p><p>We propose to tackle this problem using a discriminative clustering approach with global ordering constraints, as was successfully used in the past for the temporal localization of actions in videos <ref type="bibr" target="#b2">[4]</ref>, but with additional weak temporal constraints. In contrast to <ref type="bibr" target="#b2">[4]</ref> where the order of actions was manually given for each video, our multiple sequence alignment approach automatically discovers the main steps. More importantly, we also use the text caption timing to provide a fine-grained weak temporal supervision for the visual appearance of steps, which is described next.</p><p>Temporal weak supervision from text. From the output of the multiple sequence alignment (encoded in the matrix R n ? {0, 1} Sn?K ), each direct object token d n s has been assigned to one of the possible K steps, or to no step at all. We use the tokens that have been assigned to a step as a constraint on the visual appearance of the same step in the video (using the assumption that people do what they say approximately when they say it). We encode the closed caption timing alignment by a binary matrix A n ? {0, 1} Sn?Tn for each video, where (A n ) st is 1 if the s-th direct object is mentioned in a closed caption that overlaps with the time interval t in video. Note that this alignment is only approximate as people usually do not perform the action exactly at the same time that they talk about it, but instead with a varying delay. Second, the alignment is noisy as people typically perform the action only once, but often talk about it multiple times (e.g. in a summary at the beginning of the video). We address these issues by the following two weak supervision constraints. First, we consider a larger set of possible time intervals [t ? ? b , t + ? a ] in the matrix A rather than the exact time interval t given by the timing of the closed caption. ? b and ? a are global parameters fixed either qualitatively, or by cross-validation if labeled data is provided. Second, we put as a constraint that the action happens at least once in the set of all possible video time intervals where the action is mentioned in the transcript (rather than every time it is mentioned). These constraints can be encoded as the following linear inequality constraint on Z n : A n Z n ? R n (see Appendix C.2 for the detailed derivation).</p><p>Ordering constraint. In addition, we also enforce that the temporal order of the steps appearing visually is consistent with the discovered script from the text, encoding our assumption that there is a common ordered script for the task across videos. We encode these sequence constraints on Z n in a similar manner to <ref type="bibr" target="#b3">[5]</ref>, which was shown to work better than the encoding used in <ref type="bibr" target="#b2">[4]</ref>. In particular, we only predict the most salient time interval in the video that describes a given step. This means that a particular step is assigned to exactly one time interval in each video. We denote by Z n this sequence ordering constraint set.</p><p>Discriminative clustering. The main motivation behind discriminative clustering is to find a clustering of the data that can be easily recovered by a linear classifier through the minimization of an appropriate cost function over the assignment matrix Z n . The approach introduced in [2] allows to easily add prior information on the expected clustering. Such priors have been recently introduced in the context of aligning video and text <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b3">5]</ref> in the form of ordering constraints over the latent label variables. Here we use a similar approach to cluster the N input video streams (x t ) into a sequence of K steps, as follows. We represent each time interval by a d-dimensional feature vector. The feature vectors for the n-th video are stacked in a T n ? d design matrix denoted by X n . We denote by X the T ? d matrix obtained by the concatenation of all X n matrices (and similarly, by Z, R and A the appropriate concatenation of the Z n , R n and A n matrices over n). In order to obtain the temporal localization into K steps, we learn a linear classifier represented by a d ? K matrix denoted by W . This model is shared among all videos.  <ref type="table">Table 1</ref>: Automatically recovered sequences of steps for the five tasks. Each recovered step is represented by one of the aligned direct object relations (shown in bold). Note that most of the recovered steps correspond well to the ground truth steps (shown in italic). The results are shown for the maximum number of discovered steps K set to 10. Note how our method automatically selects less than 10 steps in some cases. These are the automatically chosen k ? K steps that are the most salient in the aligned narrations as described in Sec. 4.1. For CPR, our method recovers fine-grained steps e.g. tilt head, lift chin, which are not included in the main ground truth steps, but nevertheless could be helpful in some situations, as well as repetitions that were not annotated but were indeed present.</p><p>The target assignment? is found by minimizing the clustering cost function h under both the consistent script ordering constraints Z and our weak supervision constraints:</p><formula xml:id="formula_1">minimize Z h(Z) s.t. Z ? Z ordered script , AZ ? R weak textual constraints .</formula><p>(</p><p>The clustering cost h(Z) is given as in DIFFRAC <ref type="bibr" target="#b0">[2]</ref> as:</p><formula xml:id="formula_3">h(Z) = min W ?R K?d 1 2T Z ? XW 2 F Discriminative loss on data + ? 2 W 2 F Regularizer .<label>(3)</label></formula><p>The first term in <ref type="formula" target="#formula_3">(3)</ref> is the discriminative loss on the data that measures how easy the input data X is separable by the linear classifier W when the target classes are given by the assignments Z. For the squared loss considered in eq. (3), the optimal weights W * minimizing (3) can be found in closed form, which significantly simplifies the computation. However, to solve (2), we need to optimize over assignment matrices Z that encode sequences of events and incorporate constraints given by clusters obtained from transcribed textual narrations (Sec. 4.1). This is again done by using the Frank-Wolfe algorithm, which allows the use of efficient dynamic programs to handle the combinatorial constraints on Z. More details are given in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental evaluation</head><p>In this section, we first describe the details of the text and video features. Then we present the results divided into two experiments: (i) in Sec. 5.1, we evaluate the quality of steps extracted from video narrations, and (ii) in Sec. 5.2, we evaluate the temporal localization of the recovered steps in video using constraints derived from text. All the data and code are available at our project webpage [1].</p><p>Video and text features. We represent the transcribed narrations as sequences of direct object relations. For this purpose, we run a dependency parser <ref type="bibr" target="#b6">[8]</ref> on each transcript. We lemmatize all direct object relations and keep the ones for which the direct object corresponds to nouns. To represent a video, we use motion descriptors in order to capture actions (loosening, jacking-up, giving compressions) and frame appearance descriptors to capture the depicted objects (tire, jack, car). We split each video into 10-frame time intervals and represent each interval by its motion and appearance descriptors aggregated over a longer block of 30 frames. The motion representation is a histogram of local optical flow (HOF) descriptors aggregated into a single bagof-visual-word vector of 2,000 dimensions <ref type="bibr" target="#b28">[30]</ref>. The visual vocabulary is generated by k-means on a separate large set of training descriptors. To capture the depicted objects in the video, we apply the VGG-verydeep-16 CNN <ref type="bibr" target="#b26">[28]</ref> over each frame in a sliding window manner over multiple scales. This can be done efficiently in a fully convolutional manner. The resulting 512-dimensional feature maps of conv5 responses are then aggregated into a single bag-ofvisual-word vector of 1,000 dimensions, which aims to capture the presence/absence of different objects within each video block. A similar representation (aggregated into compact VLAD descriptor) was shown to work well recently for a variety of recognition tasks <ref type="bibr" target="#b5">[7]</ref>. The bag-of-visualword vectors representing the motion and the appearance are normalized using the Hellinger normalization and then concatenated into a single 3,000 dimensional vector representing each time interval.</p><p>WordNet distance. For the multiple sequence alignment presented in Sec. 4.1, we set c(d 1 , d 2 ) = ?1 if d 1 and d 2 have both their verbs and direct objects that match exactly in the Wordnet tree (distance equal to 0). Otherwise we set c(d 1 , d 2 ) to be 100. This is to ensure a high precision for the resulting alignment.</p><p>(a) Change tire <ref type="bibr" target="#b9">(11)</ref> (b) Perform CPR <ref type="bibr" target="#b5">(7)</ref> (c) Repot plant (7) (d) Make coffee (10) (e) Jump car (12) <ref type="figure">Figure 3</ref>: Results for temporally localizing recovered steps in the input videos. We give in bold the number of ground truth steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results of step discovery from text narrations</head><p>Results of discovering the main steps for each task from text narrations are presented in <ref type="table">Table 1</ref>. We report results of the multiple sequence alignment described in Sec. 4.1 when the maximum number of recoverable steps is K = 10. Additional results for different choices of K are given in the Appendix E.1. With increasing K, we tend to recover more complete sequences at the cost of occasional repetitions, e.g. position jack and jack car that refer to the same step. To quantify the performance, we measure precision as the proportion of correctly recovered steps appearing in the correct order. We also measure recall as the proportion of the recovered ground truth steps. The values of precision and recall are given at the bottom of Table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results of localizing instruction steps in video</head><p>In the previous section, we have evaluated the quality of the sequences of steps recovered from the transcribed narrations. In this section, we evaluate how well we localize the individual instruction steps in the video by running our two-stage approach from Sec. 4.</p><p>Evaluation metric. To evaluate the temporal localization, we need to have a one-to-one mapping between the discovered steps in the videos and the ground truth steps. Following <ref type="bibr" target="#b16">[18]</ref>, we look for a one-to-one global matching (shared across all videos of a given task) that maximizes the evaluation score for a given method (using the Hungarian algorithm). Note that this mapping is used only for evaluation, the algorithm does not have access to the ground truth annotations for learning.</p><p>The goal is to evaluate whether each ground truth step has been correctly localized in all instruction videos. We thus use the F1 score that combines precision and recall into a single score as our evaluation measure. For a given video and a given recovered step, our video clustering method predicts exactly one video time interval t. This detection is considered correct if the time interval falls inside any of the corresponding ground truth intervals, and incorrect otherwise (resulting in a false positive for this video). We compute the recall across all steps and videos, defined as the ratio of the number of correct predictions over the total number of possible ground truth steps across videos. A recall of 1 indicates that every ground truth step has been correctly detected across all videos. The recall decreases towards 0 when we miss some ground truth steps (missed detections). This happens either because this step was not recovered globally, or because it was detected in the video at an incorrect location. This is because the algorithm predicts exactly one occurrence of each step in each video. Similarly, precision measures the proportion of correct predictions among all N ?K pred possible predictions, where N is the number of videos and K pred is the number of main steps used by the method. The F1 score is the harmonic mean of precision and recall, giving a score that ranges between 0 and 1, with the perfect score of 1 when all the steps are predicted at their correct locations in all videos.</p><p>Hyperparameters. We set the values of parameters ? b and ? a to 0 and 10 seconds. The setting is the same for all five tasks. This models the fact that typically each step is first described verbally and then performed on the camera. We set ? = 1/(N K pred ) for all methods that use (3).</p><p>Baselines. We compare results to four baselines. To demonstrate the difficulty of our dataset, we first evaluate a "Uniform" baseline, which simply distributes instructions steps uniformly over the entire instruction video. The second baseline "Video only" <ref type="bibr" target="#b2">[4]</ref> does not use the narration and performs only discriminative clustering on visual features with a global order constraint. <ref type="bibr" target="#b3">5</ref> The third baseline "Video + BOW dobj" basically adds text-based features to the "Video only" baseline (by concatenating the text and video features in the discriminative clustering approach). Here the goal is to evaluate the benefits of our two-stage clustering approach, in contrast to this single-stage clustering baseline. The text features are bag-of-words histograms over a fixed vocabulary of direct object relations. <ref type="bibr" target="#b4">6</ref> The fourth baseline is our own implementation of the alignment method of <ref type="bibr" target="#b17">[19]</ref> (without the supervised vision refinement procedure that requires a set of pre-trained visual classifiers that are not available a-priori in our case). We use <ref type="bibr" target="#b17">[19]</ref> to re-align the speech transcripts to the sequence of steps discovered by our method of Sec. 4.1 (as a proxy for the recipe assumed to be known in <ref type="bibr" target="#b17">[19]</ref>). <ref type="bibr" target="#b5">7</ref> To assess the difficulty of the task and dataset, we also compare results with a "Supervised" approach. The classifiers W for the visual steps are trained by running the discriminative clustering of Sec. 4.2 with only ground truth annotations as constraints on the training set. At test time, these classifiers are used to make predictions under the global ordering constraint on unseen videos. We report results using 5-fold cross validation for the supervised approach, with the variation across folds giving the error bars. For the unsupervised discriminative clustering methods, the error bars represent the variation of performance obtained from different rounded solutions collected during the Frank-Wolfe optimization.</p><p>Results. Results for localizing the discovered instruction steps are shown in <ref type="figure">Figure 3</ref>. In order to perform a fair comparison to the baseline methods that require a known number of steps K, we report results for a range of K values. Note that in our case the actual number of automatically recovered steps can be (and often is) smaller than K. For Change tire and Perform CPR, our method consistently outperforms all baselines for all values of K demonstrating the benefits of our approach. For Repot, our method is comparable to text-based baselines, underlying the importance of the text signal for this problem. For Jump car, our method delivers the best result (for K = 15) but struggles for lower values of K, which we found was due to visually similar repeating steps (e.g. start car A and start car B) which are mixed-up for lower values of K. For the Make coffee task, the video only baseline is comparable to our method, which by inspecting the output could be attributed to large variability of narrations for this task. Qualitative results of the recovered steps are illustrated in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>We have described a method to automatically discover the main steps of a task from a set of narrated instruction videos in an unsupervised manner. The proposed approach has been tested on a new annotated dataset of challenging real-world instruction videos containing complex personobject interactions in a variety of indoor and outdoor scenes. Our work opens up the possibility for large scale learning from instruction videos on the Internet. Our model currently assumes the existence of a common script with a fixed ordering of the main steps. While this assumption is often true, e.g. one cannot remove the wheel before jacking up the car, or make coffee before filling the water, some tasks can be performed while swapping (or even leaving out) some of the steps. Recovering more complex temporal structures is an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outline of Supplementary Material</head><p>This supplementary material provides additional details for our method and presents a more complete set of results. Section A gives detailed statistics and an illustration of the newly collected dataset of instruction videos. Section B gives details about our new formulation of the multiple sequence alignment problem (Section 4.1 of the main paper) as a quadratic program and presents empirical results showing that our Frank-Wolfe optimization approach obtains solutions with lower objective values than the stateof-the-art heuristic algorithms for multiple sequence alignment. Section C provides the details for the discriminative clustering of videos with text constraints that was briefly described in Section 4.2 of the main paper. Section D gives additional details about the experimental protocol used in Section 5.2 in the main paper. Finally, in Section E, we give a more complete set of qualitative results for both the clustering of transcribed verbal instructions (see E.1) and localizing instruction steps in video (see E.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. New challenging dataset of instruction videos A.1. Dataset statistics</head><p>In this section, we introduce three different scores which aim to illustrate different properties of our dataset. The scores characterize (i) the step ordering consistency, (ii) the missing steps and (iii) the possible step repetitions.</p><p>Let N be the number of videos for a given task and K the number of steps defined in the ground truth. We assume that the ground truth steps are given in an ordered fashion, meaning the global order is defined as the sequence {1, . . . , K}. For the nth video, we denote by gn the total number of annotated steps, by un the number of unique annotated steps and finally by ln the length of the longest common subsequence between the annotated sequence of steps and the ground truth sequence {1, . . . , K}.</p><p>Order consistency error. The order error score O is defined as the proportion of non repeated annotated steps that are not consistent with the global ordering. In other words, it is defined as the number of steps that do not fit the global ordering defined in the ground truth divided by the total number of unique annotated steps. More formally, O is defined as follows:</p><formula xml:id="formula_4">O := 1 ? N n=1 ln N n=1 un .<label>(4)</label></formula><p>Missing steps. We define the missing steps score M as the proportion of steps that are visually missing in the videos when compared to the ground truth. Formally,</p><formula xml:id="formula_5">M := 1 ? N n=1 un KN .<label>(5)</label></formula><p>Repeated steps. The repetition score R is defined as the proportion of steps that are repeated:</p><formula xml:id="formula_6">R := 1 ? N n=1 un N n=1 gn .<label>(6)</label></formula><p>Results. In <ref type="table" target="#tab_2">Table 2</ref>, we give the previously defined statistics for the five tasks of the instruction videos dataset. Interestingly, we observed that globally the order is consistent for the five tasks with a total order error of only 6%. Steps are missing in 27% of the cases. This illustrates the difficulty of defining the right granularity of the ground truth for this task. Indeed, some steps might be optional and thus not visually demonstrated in all videos. Finally the global repetition score is 14%. Looking more closely, we observe that the Performing CPR task is the main contributor to this score. This is obviously a good example where one needs to repeat several times the same steps (here alternating between compressions and giving breath). Even if our model is not explicitly handling this case, we observed that our multiple sequence alignment technique for clustering the text inputs discovered these repetitions (see <ref type="table">Table 4</ref>). Finally, these statistics show that the problem introduced in this paper is very challenging and that designing models which are able to capture more complex structure in the organization of the steps is a promising direction for future work. <ref type="figure" target="#fig_1">Figure 6</ref> illustrates all five tasks in our newly collected dataset. For each task, we show a subset of 3 events that compose the task. Each event is represented by several sample frames and extracted verbal narrations. Note the large variability of verbal expressions and the terminology in the transcribed narrations as well as the large variability of visual appearance due to viewpoint, used objects, and actions performed in different manner. At the same time, note the the consistency of the actions between the different videos and the underlying script of each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Complete illustration of the dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Clustering transcribed verbal instructions</head><p>In this section, we review in details the way we model the text clustering. In particular, we give details on how we can reformulate multiple sequence alignment as a quadratic program. Recall that we are given N narrated instruction videos. For the n-th video, the text signal is represented as a sequence of direct object relation tokens : d n = (d n 1 , . . . , d n Sn ), where the length Sn of the sequences varies from one video clip to another. The number of possible direct object relations in our dictionary is denoted D. The multiple sequence alignment (MSA) problem was formulated as mapping each input sequence d n of tokens to a global common template of L slots, while minimizing the sum-of-pairs score given in (1). For each input sequence d n , we used the notation (?(d n )) 1?l?L to denote the re-mapped sequence of tokens into L slots: ?(d n ) l represents the direct object relation put at location l, with ?(d n ) l = ? denoting that a gap was inserted in the original sequence and the slot l is left empty. We also have defined a cost c(d1, d2) of aligning two direct object relations together, with the possibility that d1 or d2 is ?, in which case we defined the cost to be 0 by default. In the following, we summarize the cost of aligning non-empty direct object relations by the matrix Co ? R D?D . (Co)ij is equal to the cost of aligning the i-th and the j-th direct object relation from the dictionary together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Reformulating multiple sequence alignment as a quadratic program</head><p>We now present our formalization of the search problem as a quadratic program. To the best of our knowledge this is a new  formulation of the multiple sequence alignment (MSA) problem, which in our setting (results shown later) consistently obtains better values of the multiple sequence alignment objective than the current state-of-the-art MSA heuristic algorithms. We encode the identity of a direct object relation with a Ddimensional indicator vector. The text sequence n can then be represented by an indicator matrix Yn ? {0, 1} Sn?D . The j-th row of Yn indicates which direct object relations is evoked at the jth position. Similarly, the token re-mapping (?(d n )) 1?l?L can be represented as a L?D indicator matrix; where each row l encodes which token is appearing in slot l (and a whole row of zero is used to indicates an empty ? slot). This re-mapping can be constructed from two pieces of information: first, which token index s of the original sequence is re-mapped to which global template slot l; we represent this by the decision matrix Un ? {0, 1} Sn?L , which satisfies very specific constraints (see below). The second piece of information is the composition of the input sequence encoded by Yn. We thus have ?(d n ) = U T n Yn (as a L ? D indicator matrix). Given this encoding, the cost matrix Co, and the fact that the alignment of empty slots has zero cost, we can then rewrite the MSA problem that minimizes the sum-of-pairs objective (1) as follows:</p><p>minimize Un,n?{1,...,N } (n,m)</p><formula xml:id="formula_7">Tr(U T n YnCoY T m Um)</formula><p>subject to Un ? Un, n = 1, . . . , N.</p><p>In the above equation, the trace (Tr) is computing the cost of aligning sequence m with sequence n (the inner sum in (1)). Moreover, Un is a constraint set that encodes the fact that Un has to be a valid (increasing) re-mapping. <ref type="bibr" target="#b6">8</ref> As before, we can eliminate the video index n by simply stacking the assignment matrices Un in one matrix U of size S ? L. Similarly, we denote Y the S ? D matrix which is obtained by the concatenation of all the Yn matrices. We can then rewrite the equation <ref type="formula" target="#formula_8">(7)</ref> as a quadratic program over the (integer) variable U :</p><formula xml:id="formula_9">minimize U Tr(U T BU ), subject to U ? U .<label>(8)</label></formula><p>In this equation, the S ? S matrix B is deduced from the input sequences and the cost between different direct object relations by computing B := Y CoY T . It represents the pairwise cost at the token level, i.e. the cost of aligning token s in one sequence to token s in another sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Comparison of methods</head><p>The problem (8) is NP-hard <ref type="bibr" target="#b29">[31]</ref> in general, as is typical for integer quadratic programs. However, much work has been done in computational biology to develop efficient heuristics to solve <ref type="bibr" target="#b6">8</ref> More formally Un := {U ? {0, 1} Sn?L s.t. U 1 L = 1 Sn and ?l, (U sl = 1 ? ((?s &gt; s, l ? l), U s l = 0)}. the MSA problem, as it is an important problem in their field. We briefly describe below some of the existing heuristics to solve it, and then present our Frank-Wolfe optimization approach, which gave surprisingly good empirical results for our problem. <ref type="bibr" target="#b7">9</ref> Standard methods. Here, we compare to a standard state-ofthe-art method for multiple sequence alignment <ref type="bibr" target="#b15">[17]</ref>. Similarly to <ref type="bibr" target="#b10">[12]</ref>, they first align two sequences and merge them in a common template. Then they align a new sequence to the template and then update the template. They continue like this until no sequence is left. Differently from <ref type="bibr" target="#b10">[12]</ref>, they use a better representation of the template by using partial order graph instead of simple linear representations. This gives more accuracy for the final alignment. For the experiments, we use the author's implementation. <ref type="bibr" target="#b8">10</ref> Our solution using Frank-Wolfe optimization. We first note that problem (8) has a very similar structure to an optimization problem that we solve using Frank-Wolfe optimization for the discriminative clustering of videos; see Equations <ref type="bibr" target="#b10">(12)</ref> and <ref type="formula" target="#formula_0">(13)</ref> below. For this, we first perform a continuous relaxation of the set of constraints U by replacing it with its convex hull?. The Frank-Wolfe optimization algorithm <ref type="bibr" target="#b11">[13]</ref> can solve quadratic program over constraint sets for which we have access to an efficient linear minimization oracle. In the case of U, the linear oracle can be solved exactly with a dynamic program very similar to the one described in Section C.2. We note here that even with the continuous relaxation over?, the resulting problem is still non-convex because B is not positive semidefinite -this is because of the cost function appearing in the MSA problem. However, the standard convergence proof for Frank-Wolfe can easily be extended to show that it converges at a rate of O(1/ ? k) to a stationary point on non-convex objectives <ref type="bibr">[33]</ref>. Once the algorithm has converged to a (local) stationary point, we need to round the fractional solution to obtain a valid encoding U . We follow here a similar rounding strategy that was originally proposed by <ref type="bibr">[32]</ref> and then re-used in <ref type="bibr" target="#b12">[14]</ref>: we pick the last visited corner (which is necessarily integer) which was given as a solution to the linear minimization oracle (this is called Frank-Wolfe rounding). <ref type="table">Table 3</ref>, we give the value of the objective (8) for the rounded solutions obtained by the two different optimization approaches (lower is better), for the MSA problem on our five tasks. Interestingly, we observe that the Frank-Wolfe algorithm  <ref type="table">Table 3</ref>: Comparison of different optimization approaches for solving problem <ref type="bibr" target="#b6">(8)</ref>. (Objective value, lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. In</head><p>consistently outperforms the state-of-the-art method of <ref type="bibr" target="#b15">[17]</ref> in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discriminative clustering of videos under text constraints</head><p>We give more details here on the discriminative clustering framework from <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b3">5]</ref> (and our modifications to include the text constraints) that we use to localize the main actions in the video signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Explicit form of h(Z)</head><p>We recall that h(Z) is the cost of clustering all the video streams {x n }, n = 1, . . . , N , into a sequence of K steps. The design matrix X ? R T ?d contains the feature describing the time intervals in our videos. The indicator latent variable Z ? Z := {0, 1} T ?K encodes the visual presence of a step k at a time interval t. Recall also that X and Z contains the information about all videos n ? {1, . . . , N }. Finally, W ? R d?K represents a linear classifier for our K steps, that is shared among all videos. We now derive the explicit form of h(Z) as in the DIFFRAC approach <ref type="bibr" target="#b0">[2]</ref>, though yielding a somewhat simpler expression (as in <ref type="bibr" target="#b3">[5]</ref>) due to our use of a (weakly regularized) bias feature in X instead of a separate (unregularized) bias b. Consider the following joint cost function f on Z and W defined as</p><formula xml:id="formula_10">f (Z, W ) = 1 2T Z ? XW 2 F + ? 2 W 2 F .<label>(9)</label></formula><p>The cost function f simply represents the ridge regression objective with output labels Z and input design matrix X. We note that f has the nice property of being jointly convex in both Z and W , implying that its unrestricted minimization with respect to W yields a convex function in Z. This minimization defines our clustering cost h(Z); rewriting the definition of h with the joint cost f from (9), we have:</p><formula xml:id="formula_11">h(Z) = min W ?R d?K f (Z, W ).<label>(10)</label></formula><p>As f is strongly convex in W (for any Z), we can obtain its unique minimizer W * (Z) as a function of Z by zeroing its gradient and solving for W . For the case of the square loss in equation <ref type="formula" target="#formula_10">(9)</ref>, the optimal classifier W * (Z) can be computed in closed form:</p><formula xml:id="formula_12">W * (Z) = (X T X + T ?I d ) ?1 X T Z,<label>(11)</label></formula><p>where I d is the d-dimensional identity matrix. We obtain the explicit form for h(Z) by substituting the expression (11) for W * (Z) in equation <ref type="bibr" target="#b7">(9)</ref> and properly simplifying the expression:</p><formula xml:id="formula_13">h(Z) = f (Z, W * ) = 1 2T Tr(ZZ T B),<label>(12)</label></formula><p>where B := IT ? X(X T X + T ?I d ) ?1 X T is a strictly positive definite matrix (and so h is actually strongly convex). The clustering cost is a quadratic function in Z, encoding how the clustering decisions in one interval t interact with the clustering decisions in another interval t . In the next section, we explain how we can optimize the clustering cost h(Z) subject to the constraints from Section 4.2 using the Frank-Wolfe algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Frank Wolfe algorithm for minimizing h(Z)</head><p>The localization of steps in the video stream is done by solving the following optimization problem (repeated from (2) here for convenience):</p><formula xml:id="formula_14">minimize Z h(Z) s.t. Z ? Z ordered script , AZ ? R weak textual constraints .<label>(13)</label></formula><p>where Z is the latent assignment matrix of video time intervals to K clusters and R is the matrix of assignments of direct object relations in text to K clusters. Note that R is obtained from the text clustering using multiple sequence alignment as described in Section 4.1 and B.1, and is fixed before optimizing over Z. R is a S ? K matrix obtaining by picking the K main columns of the U matrix defined in Section B.1. This selection step was described in the "extracting the main steps" paragraph in Section 4.1. The constraint set encodes several concepts. First, it imposes the temporal consistency between the text stream and the video stream. We recall that this constraint was written as AZ ? R, <ref type="bibr" target="#b9">11</ref> where A encodes the temporal alignment constraints between video and text (type I). Second, it includes the event ordering constraints within each video input (type II). Finally, it encodes the fact that each event is assigned to exactly one time interval within each video (type III). The last two constraints are encoded in the set of constraints Z. To summarize, letZ denote the resulting (discrete) feasible space for Z i.e.Z := {Z ? Z | AZ ? R}.</p><p>We are then left with a problem in Z which is still hard to solve because the setZ is not convex. To approximately optimize h overZ, we follow the strategy of <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b3">5]</ref>. First, we optimize h over the relaxed conv(Z) by using the Frank-Wolfe algorithm to get a fractional solution Z * ? conv(Z). We then find a feasible candidate? ?Z by using a rounding procedure. We now give the details of these steps.</p><p>First we note that the linear oracle of the Frank-Wolfe algorithm can be solved separately for each video n. Indeed, because we solve a linear program, there is no quadratic term that brings dependence between different videos in the objective, and moreover all the constraints are blockwise in n. Thus, in the following, <ref type="bibr" target="#b9">11</ref> When R sk = 0, then this constraint does not do anything. When R sk = 1 (i.e. the text token s was assigned to the main action k), then the constraint enforces that t?As? Z tk ? 1, where As? represents which video frames are temporally close to the caption time of the text token s. It thus then enforces that at least one temporally close video frame is assigned to the main action k. <ref type="figure">Figure 5</ref>: Illustration of the dynamic programming solution to the linear program <ref type="bibr" target="#b12">(14)</ref>. The drawing shows a possible cost matrixC and an optimal path in red. The gray entries in the matrixC correspond to the values from the matrix C. The white entries have minimal cost and are thus always preferred over any gray entry. Note that we displayC in a transpose manner to better fit on the page.</p><p>we will give details for one video only by adding an index n toZ, to Z and to T .</p><p>The linear oracle of the Frank-Wolfe algorithm can be solved via an efficient dynamic program. Let us suppose that the linear oracle corresponds to the following problem:</p><formula xml:id="formula_15">min Zn?Zn Tr(C n Zn),<label>(14)</label></formula><p>where Cn ? R Tn?K is a cost matrix that arises by computing the gradient of h with respect to Zn at the current iterate. The goal of the dynamic program is to find which entries of Zn are equal to 1, recalling that (Zn) tk = 1 means that the step k was assigned to time interval t. From the constraint of type III (unique prediction per step), we know that each column k of Zn has exactly one 1 (to be found). From the ordering constraint (type II), we know that if (Zn) tk = 1, then the only possible locations for a 1 in the (k + 1)-th column is for t &gt; t (i.e. the pattern of 1's is going downward when traveling from left to right in Zn). Note that there can be "jumps" in between the time assignment for two subsequent steps k and k + 1. In order to encode this possibility using a continuous path search in a matrix, we insert dummy columns into the cost matrix C. We first subtract the minimum value from C and then insert columns filled with zeros in between every pair of columns of C. In the end, we pad C with an additional row filled with zeros at the bottom. The resulting cost matrixC is of size (Tn + 1) ? (2K + 1) and is illustrated (as its transpose) along with the corresponding update rules in <ref type="figure">Figure 5</ref>.</p><p>The problem that we are interested in is subject to the additional linear constraints given by the clustering of text transcripts (constraints of type I). These constraint can be added by constraining the path in the dynamic programming algorithm. This can be done for instance by setting an infinite alignment cost outside of the constrained region.</p><p>At the end of the Frank-Wolfe optimization algorithm, we obtain a continuous solution Z * n for each n. By stacking them all together again, we obtain a continuous solution Z * . From the def-inition of h, we can also look at the corresponding model W * (Z * ) defined by equation <ref type="formula" target="#formula_0">(11)</ref> which again is shared among all videos. All Z * n have to be rounded in order to obtain a feasible point for the initial, non relaxed problem. Several rounding options were suggested in <ref type="bibr" target="#b3">[5]</ref>; it turns out that the one which uses W * gives better results in our case. More precisely, in order to get a good feasible binary matrix?n ?Zn, we solve the following problem: min Zn?Zn Zn ? XnW * 2 F . By expanding the norm, we notice that this corresponds to a simple linear program overZn as in equation <ref type="formula" target="#formula_0">(14)</ref> that can be solved using again the same dynamic program detailed above. Finally, we stack these rounded matrice? Zn to obtain our predicted assignment matrix? ?Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental protocol</head><p>In this section, we give more details about the setting for our experiments on the time localization of events with results given in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Supervised experiments.</head><p>Here, we describe in more details how we obtained the scores for the supervised approach depicted in yellow in <ref type="figure">Figure 3</ref>. We first divided the N input videos in 5 different folds. One fold is kept for the test set while the 4 other are used as train/validation dataset. With the 4 remaining folds, we perform a 4-fold cross validation in order to choose the hyperparameter ?. Once the hyper parameter is fixed, we retrain a model on the 4 folds and evaluate it on the test set. By iterating over the five possible test folds, we report variation in performance with error bars in <ref type="figure">Figure 3</ref>.</p><p>Training phase. The goal of this phase is to learn classifiers W for the visual steps. To that end, we minimize the cost defined in (2) under the ground truth annotations constraints. This is very close to our setting, and in practice we can use exactly the same framework as in problem (13) by simply replacing the constraints coming from the text by the constraints coming from the ground truth annotations.</p><p>Testing phase. At test time, we simply use the classifiers W to perform least-square prediction of Ztest under ordering constraints. Performance are evaluated with the F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Error bars for Frank-Wolfe methods.</head><p>We explain here how we obtained the error bars of <ref type="figure">Figure 3</ref> in the main paper for the unsupervised approaches. Let us first recall that the Frank-Wolfe algorithm is used to solve a continuous relaxation of problem <ref type="bibr" target="#b11">(13)</ref>. To obtain back an integer solution, we round the continuous solution using the rounding method described at the end of Section C.2. This rounding procedure is performed at each iteration of the optimization method. When the stopping criterion of the Frank-Wolfe scheme is reached (fixed number of iterations or target sub-optimality in practice), we have as many rounded solutions as number of iterations. Our output integer solution is then the integer point that achieves the lowest objective. Note that we are only guaranteed to diminish objective in the continuous domain and not for the integer points, therefore there are no guarantees that this solution is the last rounded point. In order to illustrate the variation of the performance with respect to the optimization scheme, we defined our error bars as being the interval with bounds determined by the minimal performance and the maximal performance obtained after visiting the best rounded point (the output solution). This notably explains why the error bars of <ref type="figure">Figure 3</ref> are not necessarily symmetric. Overall, the observed variation is not very important, thus highlighting the stability of the procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative results</head><p>In Section E.1, we give detailed results of script discovery for the five different tasks. In Section E.2, we present detailed results for the action localization experiment. <ref type="table">Table 4</ref> shows the automatically recovered sequences of steps for the five tasks considered in this work. The results are shown for setting the maximum number of discovered steps, K = {7, 10, 12, 15}. Note how our method automatically selects less than K steps in some cases. These are the automatically chosen k ? K steps that are the most salient in the aligned narrations as described in Section 4.1. This is notably the case for the Repotting a plant task. Even for K ? 12, the algorithm recovers only 6 steps that match very well the seven ground truth steps for this task. This saliency based task selection is important because it allows for a better precision at high K without lowering much the recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Script discovery</head><p>Please note also how the steps and their ordering recovered by our method correspond well to the ground truth steps for each task. For CPR, our method recovers fine-grained steps e.g. tilt head, lift chin, which are not included in the main ground truth steps, but nevertheless could be helpful in some situations. For Changing tire, we also recover more detailed actions such as remove jack or put jack. In some cases, our method recovers repeated steps. For example, for CPR our method learns that one has to alternate between giving breath and performing compressions even if this alternation was not annotated in the the ground truth. Or for Jumping Cars our method learns that cables need to be connected twice (to both cars).</p><p>These results demonstrate that our method is able to automatically discover meaningful scripts describing very different tasks. The results also show that the constraint of a single script providing an ordering of events is a reasonable prior for a variety of different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Action localization</head><p>Examples of the recovered instruction steps for all five tasks are shown in <ref type="figure">Figure 7</ref>-11. Each row shows one recovered step. For each step, we first show the clustered direct object relations, followed by representative example frames localizing the step in the videos. Correct localizations are shown in green. Some steps are incorrectly localized in some videos (red), but often look visually very similar. Note how our method correctly recovers the main steps of the task and localizes them in the input videos. Those results have been obtained by imposing K ? 10 in our method. The video on the project website illustrates action localization for the five tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary References</head><p>[32] V. Chari, S. Lacoste-Julien, I. Laptev, and J. Sivic. On pairwise costs for network flow multi-object tracking. In CVPR, 2015.</p><p>[33] S. Lacoste-Julien. Convergence rate of Frank-Wolfe for nonconvex objectives. arXiv preprint, 2016. GT (11)</p><formula xml:id="formula_16">K ? 7 K ? 10 K ? 12 K ? 15</formula><p>put brake on get tools out get tire get tire get tire start loose loosen nut loosen nut loosen nut loosen nut lift car put jack put jack put jack put jack raise vehicle raise vehicle jack car jack car jack car jack car jack car unscrew wheel remove nut remove nut remove nut remove nut remove wheel take wheel take wheel take wheel put wheel take tire take tire take tire take tire screw wheel put nut put nut put nut lower car lower jack lower jack lower jack lower jack remove jack tight wheel tighten nut tighten nut tighten nut tighten nut put things back take tire take tire Precision 0.85 0.9 0.83 0.71 Recall 0.54 0.9 0.9 0.9 (a) Changing a tire GT (10) K ? 7 K ? 10 K ? 12 K ? 15 grind coffee put filter add coffee put coffee put coffee put coffee even surface fill chamber fill chamber fill chamber make noise fill water fill water fill water fill water fill water screw top put filter put filter put filter fill basket see steam see steam see steam put stove take minutes take minutes take minutes take minutes make coffee make coffee make coffee make coffee see coffee see coffee see coffee see coffee see coffee withdraw stove turn heat pour coffee make cup make cup make cup make cup pour coffee  <ref type="table">Table 4</ref>: Automatically recovered sequences of steps for the five tasks considered in this work. Each recovered step is represented by one of the aligned direct object relations (shown in bold). Note that most of the recovered steps correspond well to the ground truth steps (showed in italic). The results are shown for setting the maximum number of discovered steps, K = {7, 10, 12, 15}. Note how our method automatically selects less than K steps in some cases. These are the automatically chosen k ? K steps that are the most salient in the aligned narrations as described in Sec. 4.1. <ref type="figure">Figure 7</ref>: Examples of the recovered instruction steps for the task "Changing the car tire".    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Clustering transcribed verbal instructions. Left: The input raw text for each video is converted into a sequence of direct object relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of our newly collected dataset of instructions videos. Examples of transcribed narrations together with still frames from the corresponding videos are shown for the 5 tasks of the dataset: Repotting a plant, Performing CPR, Jumping cars, Changing a car tire and Making coffee. The dataset contains challenging real-world videos performed by many different people, captured in uncontrolled settings in a variety of outdoor and indoor environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results for the task "Jumping cars".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results for the task "Repot a plant".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results for the task "Making coffee".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative results for the task "Performing CPR".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>K ? 10 GT (7) K ? 10 GT<ref type="bibr" target="#b5">(7)</ref> K ? 10 GT (10) K ? 10 GT(12) K ? 10 get tools out get tire open airway open airway take plant remove plant add coffee put coffee connect red A take wheel do compressions do compression water plant water plant put stove take minutes remove cable B disconnect cable</figDesc><table><row><cell cols="2">Changing a tire</cell><cell cols="2">Performing CPR</cell><cell cols="2">Repot a plant</cell><cell>Make coffee</cell><cell>Jump car</cell></row><row><cell>GT (11)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>connect cable</cell></row><row><cell>start loose</cell><cell cols="2">loosen nut check pulse</cell><cell cols="2">put hand put soil</cell><cell>use soil</cell><cell>fill chamber</cell><cell>charge battery</cell></row><row><cell></cell><cell>put jack</cell><cell></cell><cell cols="2">tilt head loosen roots</cell><cell cols="2">loosen soil fill water</cell><cell>fill water connect red B</cell><cell>connect end</cell></row><row><cell>jack car</cell><cell>jack car</cell><cell></cell><cell cols="2">lift chin place plant</cell><cell cols="2">place plant screw filter</cell><cell>put filter start car A</cell><cell>start car</cell></row><row><cell cols="3">unscrew wheel remove nut give breath</cell><cell cols="2">give breath add top</cell><cell>add soil</cell><cell>see steam remove cable A</cell><cell>remove cable</cell></row><row><cell>remove wheel put wheel</cell><cell>take tire</cell><cell></cell><cell>open airway</cell><cell></cell><cell></cell><cell>make coffee</cell></row><row><cell>screw wheel</cell><cell>put nut</cell><cell></cell><cell>start compression</cell><cell></cell><cell cols="2">see coffee</cell><cell>see coffee</cell></row><row><cell>lower car</cell><cell>lower jack</cell><cell></cell><cell>do compression</cell><cell></cell><cell cols="2">pour coffee</cell><cell>make cup</cell></row><row><cell>tight wheel</cell><cell>tighten nut</cell><cell></cell><cell>give breath</cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.9 Precision</cell><cell></cell><cell cols="2">0.4 Precision</cell><cell cols="2">1 Precision</cell><cell>0.67 Precision</cell><cell>0.83</cell></row><row><cell>Recall</cell><cell>0.9 Recall</cell><cell></cell><cell>0.57 Recall</cell><cell></cell><cell>0.86 Recall</cell><cell>0.6 Recall</cell><cell>0.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TaskChanging tire Performing CPR Repoting plant Making coffee Jumping cars Average</figDesc><table><row><cell>Order error</cell><cell>0.7%</cell><cell>11%</cell><cell>6%</cell><cell>3%</cell><cell>8%</cell><cell>6%</cell></row><row><cell>Missing steps</cell><cell>16%</cell><cell>32%</cell><cell>30%</cell><cell>28%</cell><cell>27%</cell><cell>27%</cell></row><row><cell>Repetition score</cell><cell>4%</cell><cell>50%</cell><cell>7%</cell><cell>11%</cell><cell>0.4%</cell><cell>14%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the instruction video dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>connect cable conn. cable conn. cable conn. cable conn. clamp charge battery charge batt. charge batt. charge batt.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Precision</cell><cell>0.8</cell><cell>0.67</cell><cell>0.67</cell><cell>0.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Recall</cell><cell>0.4</cell><cell>0.6</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Making coffee</cell><cell></cell></row><row><cell>GT (7)</cell><cell>K ? 7</cell><cell>K ? 10</cell><cell>K ? 12</cell><cell>K ? 15</cell><cell cols="2">GT (7)</cell><cell>K ? 7</cell><cell>K ? 10</cell><cell>K ? 12</cell><cell>K ? 15</cell></row><row><cell>cover hole take plant put soil</cell><cell cols="3">take plant take plant take plant use soil use soil use soil</cell><cell>take piece keep soil stop soil take plant use soil</cell><cell cols="2">open airway check response call 911 check breathing check pulse</cell><cell cols="4">open airway open airway open airway open airway put hand put hand put hand tilt head tilt head tilt head tilt head</cell></row><row><cell cols="4">loosen root loosen soil loosen soil loosen soil</cell><cell>loosen soil</cell><cell></cell><cell></cell><cell>lift chin</cell><cell>lift chin</cell><cell>lift chin</cell><cell>lift chin</cell></row><row><cell cols="4">place plant place plant place plant place plant</cell><cell>place plant</cell><cell cols="2">give breath</cell><cell cols="4">give breath give breath give breath give breath</cell></row><row><cell>add top</cell><cell>add soil</cell><cell>add soil</cell><cell>add soil</cell><cell>add soil</cell><cell cols="2">give compression</cell><cell>do compr.</cell><cell>do compr.</cell><cell>do compr.</cell><cell>do compr.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>fill pot</cell><cell></cell><cell></cell><cell cols="4">open airway open airway open airway open airway</cell></row><row><cell cols="5">get soil give drink water plant water plant water plant water plant water plant give watering</cell><cell></cell><cell></cell><cell></cell><cell cols="3">start compr. start compr. start compr. continue cpr do compr. do compr. do compr. put hand give breath give breath give breath</cell></row><row><cell>Precision</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.54</cell><cell cols="2">Precision</cell><cell>0. 5</cell><cell>0.4</cell><cell>0.4</cell><cell>0.33</cell></row><row><cell>Recall</cell><cell>0.86</cell><cell>0.86</cell><cell>0.86</cell><cell>1</cell><cell cols="2">Recall</cell><cell>0.43</cell><cell>0.57</cell><cell>0.57</cell><cell>0.57</cell></row><row><cell></cell><cell cols="2">(c) Repot a plant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(d) Performing CPR</cell></row><row><cell></cell><cell></cell><cell cols="2">GT (12)</cell><cell>K ? 7</cell><cell>K ? 10</cell><cell>K ? 12</cell><cell>K ? 15</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">get cars</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">open hood</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">have terminal</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>attach cab.</cell><cell>attach cab.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">connect red A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">connect red B</cell><cell cols="2">connect end conn. end</cell><cell>conn. end</cell><cell>conn. end</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">connect black A</cell><cell></cell><cell></cell><cell>conn. cab.</cell><cell>conn. cab.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">connect ground</cell><cell></cell><cell></cell><cell>have cab.</cell><cell>have cab.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">start car A</cell><cell>start car</cell><cell>start car</cell><cell>start car</cell><cell>start car</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">start car B</cell><cell></cell><cell cols="2">start vehicle</cell><cell>start veh.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>start engine</cell><cell>start eng.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">remove ground</cell><cell cols="3">remove cable rem. cable rem. cable</cell><cell>rem. cable</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">remove black A disconnect cable disc. cable disc. cable</cell><cell>disc. cable</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">remove red B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">remove red A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Precision</cell><cell>0.83</cell><cell>0.83</cell><cell>0.72</cell><cell>0.69</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell>0.42</cell><cell>0.42</cell><cell>0.67</cell><cell>0.67</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(e) Jumping cars</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We here assign the same meaning to terms "event" and "step" as well as to terms "script" and "task".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We describe these measurements in more details in the supplementary material given in Appendix A.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use here the improved model from<ref type="bibr" target="#b3">[5]</ref> which does not require a "background class" and yields a stronger baseline equivalent to our model (2) without the weak textual constraints.<ref type="bibr" target="#b4">6</ref> Alternative features of bag-of-words histograms treating separately nouns and verbs also give similar results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that our method finds at the same time the sequence of steps (a recipe in<ref type="bibr" target="#b17">[19]</ref>) and the alignment of the transcripts.Figure 4: Examples of three recovered instruction steps for each of the five tasks in our dataset. For each step, we first show clustered direct object relations, followed by representative example frames localizing the step in the videos. Correct localizations are shown in green. Some steps are incorrectly localized in some videos (red), but often look visually very similar. See Appendix E.2 for additional results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We stress here that we do not claim that our formulation of the multiple sequence alignment (MSA) problem as a quadratic program outperforms the state-of-the-art computational biology heuristics for their MSA problems arising in biology. We report our observations on application of multiple sequence alignment to our application, which might have a structure for which these heuristics are not as appropriate.<ref type="bibr" target="#b8">10</ref> Code available at http://sourceforge.net/projects/ poamsa/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments This research was supported in part by a</head><p>Google Research Award, and the ERC grants VideoWorld (no. 267907), Activia (no. 307574) and LEAP (no. 336845).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DIFFRAC: A discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding actors and actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly-supervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wordnet: An electronic lexical database. Cambridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<idno>1998. 4</idno>
		<imprint>
			<publisher>MIT Press</publisher>
			<pubPlace>MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian model for unsupervised induction of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clustal: A package for performing multiple sequence alignment on a microcomputer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gene</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting Frank-Wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with Frank-Wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the global linear convergence of Frank-Wolfe optimization variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple sequence alignment using partial order graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Clustering of time series data, a survey. Pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What&apos;s cookin&apos;? Interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Wordnet: A lexical database for english. Communications of the ACM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised alignment of natural language instructions with corresponding video segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Naim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning script knowledge with Web experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing of video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ranking domain-specific highlights by analyzing edited videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the complexity of multiple sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
