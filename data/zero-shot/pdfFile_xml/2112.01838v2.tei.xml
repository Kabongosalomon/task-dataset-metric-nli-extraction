<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent developments in transformer models for visual data have led to significant improvements in recognition and detection tasks. In particular, using learnable queries in place of region proposals has given rise to a new class of one-stage detection models, spearheaded by the Detection Transformer (DETR). Variations on this one-stage approach have since dominated human-object interaction (HOI) detection. However, the success of such one-stage HOI detectors can largely be attributed to the representation power of transformers. We discovered that when equipped with the same transformer, their two-stage counterparts can be more performant and memory-efficient, while taking a fraction of the time to train. In this work, we propose the Unary-Pairwise Transformer, a two-stage detector that exploits unary and pairwise representations for HOIs. We observe that the unary and pairwise parts of our transformer network specialise, with the former preferentially increasing the scores of positive examples and the latter decreasing the scores of negative examples. We evaluate our method on the HICO-DET and V-COCO datasets, and significantly outperform state-of-the-art approaches. At inference time, our model with ResNet50 approaches realtime performance on a single GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-object interaction (HOI) detectors localise interactive human-object pairs in an image and classify the actions. They can be categorised as one-or two-stage, mirroring the grouping of object detectors. Exemplified by Faster R-CNN <ref type="bibr" target="#b23">[24]</ref>, two-stage object detectors typically include a region proposal network, which explicitly encodes potential regions of interest in the form of bounding boxes. These bounding boxes can then be classified and further refined via regression in a downstream network. In contrast, one-stage detectors, such as RetinaNet <ref type="bibr" target="#b17">[18]</ref>, retain the ab-(a) Image with human and object detections.  stract feature representations of objects throughout the network, and decode them into bounding boxes and classification scores at the end of the pipeline.</p><p>In addition to the same categorisation convention, HOI detectors need to localise two bounding boxes per instance instead of one. Early works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23</ref>] employ a pretrained object detector to obtain a set of human and object boxes, which are paired up exhaustively and processed by a downstream network for interaction classification. This methodology coincides with that of two-stage detectors and quickly became the mainstream approach due to the accessibility of high-quality pre-trained object detectors. The first instance of one-stage HOI detectors was introduced by <ref type="bibr">Figure 2</ref>. Mean average precision as a function of the number of epochs (left) and training time (right) to convergence. The backbone networks for all methods have been initialised with the same weights and trained on 8 GeForce GTX TITAN X GPUs. <ref type="table">Table 1</ref>. The performance discrepancy between existing stateof-the-art one-stage and two-stage HOI detectors is largely attributable to the choice of backbone network. We report the mean average precision (?100) on the HICO-DET <ref type="bibr" target="#b1">[2]</ref> test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Type Detector Backbone mAP SCG <ref type="bibr" target="#b27">[28]</ref> two-stage Faster R-CNN R-50-FPN 24.88 SCG <ref type="bibr" target="#b27">[28]</ref> two-stage DETR R-50 28.79 SCG <ref type="bibr" target="#b27">[28]</ref> two-stage DETR R-101 29.26</p><p>QPIC <ref type="bibr" target="#b24">[25]</ref> one-stage DETR R-50 29.07 QPIC <ref type="bibr" target="#b24">[25]</ref>  Liao et al. <ref type="bibr" target="#b16">[17]</ref>. They characterised human-object pairs as interaction points, represented as the midpoint of the human and object box centres. Recently, due to the great success in using learnable queries in transformer decoders for localisation <ref type="bibr" target="#b0">[1]</ref>, the development of one-stage HOI detectors has been greatly advanced. However, HOI detectors that adapt the DETR model rely heavily on the transformer, which is notoriously difficult to train <ref type="bibr" target="#b19">[20]</ref>, to produce discriminative features. In particular, when initialised with DETR's pretrained weights, the decoder attends to regions of high objectness by default. The heavy-weight decoder stack then has to be adapted to attend to regions of high interactiveness. Consequently, training such one-stage detectors often consumes large amounts of memory and time as shown in <ref type="figure">Fig. 2</ref>. In contrast, two-stage HOI detectors do not repurpose the backbone network, but maintain it as an object detector. Since the first half of the pipeline already functions as intended at the beginning of training, the second half can be trained quickly for the specific task of HOI detection. Furthermore, since the object detector can be decoupled from the downstream interaction head during training, its weights can be frozen, and a lighter-weight network can be used for interaction detection, saving a substantial amount of memory and computational resources. Despite these advantages, the performance of two-stage detectors has lagged behind their one-stage counterparts. However, most of these two-stage models used Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> rather than more recent object detectors. We found that simply replacing Faster R-CNN with the DETR model in an existing two-stage detector (SCG) <ref type="bibr" target="#b27">[28]</ref> resulted in a significant improvement, putting it on par with a stateof-the-art one-stage detector (QPIC), as shown in Tab. 1. We attribute this performance gain to the representation power of transformers and bipartite matching loss <ref type="bibr" target="#b0">[1]</ref>. The latter is particularly important because it resolves the misalignment between the training procedure and evaluation protocol. The evaluation protocol dictates that, amongst all detections associated with the same ground truth, the highest scoring one is the true positive while the others are false positives. Without bipartite matching, all such detections will be labelled as positives. The detector then has to resort to heuristics such as non-maximum suppression to mitigate the issue, resulting in procedural misalignment.</p><p>We propose a two-stage model that refines the output features from DETR with additional transformer layers for HOI classification. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, we encode the instance information in two ways: a unary representation where individual human and object instances are encoded separately, and a pairwise representation where humanobject pairs are encoded jointly. These representations provide orthogonal information, and we observe different behaviours in their associated layers. The unary encoder layer preferentially increases the predicted interaction scores for positive examples, while the pairwise encoder layer suppresses the negative examples. As a result, this complementary behaviour widens the gap between scores of positive and negative examples, particularly benefiting ranking metrics such as mean average precision (mAP).</p><p>Our primary contribution is a novel and efficient twostage HOI detector with unary and pairwise encodings. Our secondary contribution is demonstrating how pairwise box positional encodings-critical for HOI detection-can be incorporated into a transformer architecture, enabling it to jointly reason about unary appearance and pairwise spatial information. We further provide a detailed analysis on the behaviour of the two encoder layers, showing that they have complementary properties. Our proposed model not only outperforms state-of-the-art methods, but also consumes much less time and memory to train. The latter allows us to employ more memory-intensive backbone networks, further improving the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Transformer networks <ref type="bibr" target="#b26">[27]</ref>, initially developed for machine translation, have recently become ubiquitous in computer vision due to their representation power, flexibility, and global receptive field via the attention mechanism. The image transformer ViT <ref type="bibr" target="#b3">[4]</ref> represented an image as a set of spatial patches, each of which was encoded as a token through simple linear transformations. This approach for tokenising images rapidly gained traction and inspired  <ref type="figure">Figure 3</ref>. Flowchart for our unary-pairwise transformer. An input image is processed by a backbone CNN to produce image features, which are partitioned into patches of equal size and augmented with sinusoidal positional encodings. These tokens are fed into the DETR <ref type="bibr" target="#b0">[1]</ref> transformer encoder-decoder stack, generating new features for a fixed number of learnable object queries. These are decoded by an MLP as object classification scores and bounding boxes, and are also passed to the interaction head as unary tokens. The interaction head also receives pairwise positional encodings computed from the predicted bounding box coordinates. A modified transformer encoder layer then refines the unary tokens using the pairwise positional encodings. The output tokens are paired up and fused with the same positional encodings to produce pairwise tokens, which are processed by a standard transformer encoder layer before an MLP decodes the final features as action classification scores. many subsequent works <ref type="bibr" target="#b20">[21]</ref>. Another key innovation of transformers is the use of learnable queries in the decoder, which are initialised randomly and updated through alternating self-attention and cross-attention with encoder tokens. Carion et al. <ref type="bibr" target="#b0">[1]</ref> use these as object queries in place of conventional region proposals for their object detector. Together with a bipartite matching loss, this design gave rise to a new class of one-stage detection models that formulate the detection task as a set prediction problem. It has since inspired numerous works in HOI detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>To adapt the DETR model to HOI detection, Tamura et al. <ref type="bibr" target="#b24">[25]</ref> and Zou et al. <ref type="bibr" target="#b28">[29]</ref> add additional heads to the transformer in order to localise both the human and object, as well as predict the action. As for bipartite matching, additional cost terms are added for action prediction. On the other hand, Kim et al. <ref type="bibr" target="#b12">[13]</ref> and Chen et al. <ref type="bibr" target="#b2">[3]</ref> propose an interaction decoder to be used alongside the DETR instance decoder. It is specifically responsible for predicting the action while also matching the interactive human-object pairs. These aforementioned one-stage detectors have achieved tremendous success in pushing the state-of-the-art performance. However, they all require significant resources to train the models. In contrast, this work focuses on exploiting novel ideas to produce equally discriminative features while preserving the memory efficiency and low training time of two-stage detectors.</p><p>Two-stage HOI detectors have also undergone significant development recently. Li et al. <ref type="bibr" target="#b14">[15]</ref> studied the integration and decomposition of HOIs in an analogy to the superposition of waves in harmonic analysis. Hou et al. explored fewshot learning by fabricating object representations in feature space <ref type="bibr" target="#b11">[12]</ref> and learning to transfer object affordance <ref type="bibr" target="#b10">[11]</ref>. Finally, Zhang et al. <ref type="bibr" target="#b27">[28]</ref> proposed to fuse features of different modalities within a graphical model to produce more discriminative features. We make use of this modality fusion in our transformer model and show that it leads to significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unary-pairwise transformers</head><p>To leverage the success of transformer-based detectors, we use DETR <ref type="bibr" target="#b0">[1]</ref> as our backbone object detector and focus on designing an effective and efficient interaction head for HOI detection, as shown in <ref type="figure">Fig. 3</ref>. The interaction head consists of two types of transformer encoder layers, with the first layer modified to accommodate additional pairwise input. The first layer operates on unary tokens, i.e., individual human and object instances, while the second layer operates on pairwise tokens, i.e., human-object pairs. Based on our analysis and experimental observations in Sec. 4.3 and Sec. 4.4, self-attention in the unary layer preferentially increases the interaction scores for positive HOI pairs, whereas self-attention in the pairwise layer decreases the scores for negative pairs. As such, we refer to these layers as cooperative and competitive layers respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cooperative layer</head><p>A standard transformer encoder layer takes as input a set of tokens and performs self-attention. Positional encodings are usually indispensable to compensate for the lack of order in the token set. Typically, sinusoidal functions of the position <ref type="bibr" target="#b26">[27]</ref> or learnable embeddings <ref type="bibr" target="#b0">[1]</ref> are used for this purpose. It is possible to extend sinusoidal encodings to bounding box coordinates, however, our unary tokens already contain positional information, since they were decoded into bounding boxes. Instead, we take this as an opportunity to inject pairwise spatial information into the transformer, something that has been shown to be helpful for the task of HOI detection <ref type="bibr" target="#b27">[28]</ref>. Specifically, we com- Concat. pute the unary and pairwise spatial features used by Zhang et al. <ref type="bibr" target="#b27">[28]</ref> from the bounding boxes, including the unary box centre, width and height, and pairwise intersection-overunion, relative area, and direction, and pass this through an MLP to obtain the pairwise positional encodings. We defer the full details to Appendix A. We also found that the usual additive approach did not perform as well for our positional encodings. So we slightly modified the attention operation in the transformer encoder layer to allow directly injecting the pairwise positional encodings into the computation of values and attention weights. More formally, given the detections returned by DETR, we first apply non-maximum suppression and thresholding. This leaves a smaller set</p><formula xml:id="formula_0">Linear Softmax Duplicate !?!? 2$ ? !?!?1 !?!? 3$ ? !?!? $ ? FFN LayerNorm Concat.</formula><formula xml:id="formula_1">{d i } n i=1 , where a detec- tion d i = (b i , s i , c i , x i ) consists of the box coordinates b i ? R 4 , the confidence score s i ? [0, 1]</formula><p>, the object class c i ? K for a set of object categories K, and the object query or feature x i ? R m . We compute the pairwise box positional encodings {y i,j ? R m } n i,j=1 as outlined above. We denote the collection of unary tokens by X ? R n?m and the pairwise positional encodings by Y ? R n?n?m . The complete structure of the modified transformer encoder layer is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. For brevity of exposition, let us assume that the number of heads h is 1, and defin?</p><formula xml:id="formula_2">X ? R n?n?m ,? i X ? R n?m ,<label>(1)</label></formula><formula xml:id="formula_3">X ? R n?n?2m ,? i,j x i ? x j ? R 2m ,<label>(2)</label></formula><p>where ? denotes vector concatenation. That is, the tensor? X and? are the results of duplication and pairwise concatenation. The equivalent values and attention weights can then be computed as</p><formula xml:id="formula_4">V =? ? Y,<label>(3)</label></formula><formula xml:id="formula_5">W = softmax((? ? Y )w + b),<label>(4)</label></formula><p>where ? denotes elementwise product and w ? R 3m and b ? R are the parameters of the linear layer. The output of the attention layer is then computed as W ? V . Additional details can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Competitive layer</head><p>To compute the set of pairwise tokens, we form all pairs of distinct unary tokens and remove those where the first token is not human, as object-object pairs are beyond the scope of HOI detection. We denote the resulting set as</p><formula xml:id="formula_6">{p k = (x i , x j , y i,j ) | i = j, c i = "human"}.</formula><p>We then compute the pairwise tokens from the unary tokens and positional encodings via multi-branch fusion (MBF) <ref type="bibr" target="#b27">[28]</ref> as</p><formula xml:id="formula_7">z k = MBF(x i ? x j , y i,j ).<label>(5)</label></formula><p>Specifically, the MBF module fuses two modalities in multiple homogeneous branches and return a unified feature representation. For completeness, full details are provided in Appendix C. Last, the set of pairwise tokens are fed into an additional transformer encoder layer, allowing the network to compare the HOI candidates, before an MLP predicts each HOI pair's action classification logits s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and inference</head><p>To make full use of the pre-trained object detector, we incorporate the object confidence scores into the final scores of each human-object pair. Denoting the action logits of the k th pair p k as s k , the final scores are computed as</p><formula xml:id="formula_8">s k = (s i ) ? ? (s j ) ? ? ?( s k ),<label>(6)</label></formula><p>where ? &gt; 1 is a constant used during inference to suppress overconfident objects <ref type="bibr" target="#b27">[28]</ref> and ? is the sigmoid function. We use focal loss 1 <ref type="bibr" target="#b17">[18]</ref> for action classification to counter the imbalance between positive and negative examples. Following previous practice <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>, we only compute the loss on valid action classes for each object type, specified by the dataset. During inference, scores for invalid combinations of actions and objects (e.g., eating a car) are zeroed out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first demonstrate that the proposed unary-pairwise transformer achieves state-of-the-art performance on both the HICO-DET <ref type="bibr" target="#b1">[2]</ref> and V-COCO <ref type="bibr" target="#b6">[7]</ref> datasets, outperforming the next best method by a significant margin. We then provide a thorough analysis on the effects of the cooperative and competitive layers. In particular, we show that the cooperative layer increases the scores of positive examples while the competitive layer suppresses </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We fine-tune the DETR model on the HICO-DET and V-COCO datasets prior to training and then freeze its weights. For HICO-DET, we use the publicly accessible DETR models pre-trained on MS COCO <ref type="bibr" target="#b18">[19]</ref>. However, for V-COCO, as its test set is contained in the COCO val2017 subset, we first pre-train DETR models from scratch on MS COCO, excluding those images in the V-COCO test set. For the interaction head, we filter out detections with scores lower than 0.2, and sample at least 3 and up to 15 humans and objects each, prioritising high scoring ones. For the hidden dimension of the transformer, we use m = 256, the same as DETR. Additionally, we set ? to 1 during training and 2.8 during inference <ref type="bibr" target="#b27">[28]</ref>. For the hyperparameters used in the focal loss, we use the same values as SCG <ref type="bibr" target="#b27">[28]</ref>.</p><p>We apply a few data augmentation techniques used in other detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>. Inputs images are scaled such that the shortest side is at least 480 and at most 800 pixels. The longest side is limited at 1333 pixels. Additionally, each image is cropped with a probability of 0.5 to a random rectangle with each side being at least 384 pixels and at most 600 pixels before being scaled. We also apply colour jittering, where the brightness, contrast and saturation values are adjusted by a random factor between 0.6 to 1.4. We use AdamW <ref type="bibr" target="#b21">[22]</ref> as the optimiser with an initial learning rate of 10 ?4 . All models are trained for 20 epochs with a learning rate reduction at the 10 th epoch by a factor of 10. Training is conducted on 8 GeForce GTX TITAN X devices, with a batch size of 2 per GPU-an effective batch size of 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-art methods</head><p>The performance of our model is compared to existing methods on the HICO-DET <ref type="bibr" target="#b1">[2]</ref> and V-COCO <ref type="bibr" target="#b6">[7]</ref> datasets in Tab. 2. There are two different settings for evaluation on HICO-DET. Default Setting: A detected human-object <ref type="table">Table 3</ref>. Comparing the effect of the cooperative (coop.) and competitive (comp.) layers on the interaction scores. We report the change in the interaction scores as the layer in the ? Architecture column is added to the reference network, for positives, easy negatives and hard negatives, with the number of examples in parentheses. As indicated by the bold numbers, the cooperative layer significantly increases the scores of positive examples while the competitive layer suppresses hard negative examples. Together, these layers widen the gap between scores of positive and negative examples, improving the detection mAP.  pair is considered matched with a ground truth pair, if the minimum intersection over union (IoU) between the human boxes and object boxes exceeds 0.5. Amongst all matched pairs, the one with the highest score is considered the true positive while others are false positives. Pairs without a matched ground truth are also considered false positives. Known Objects Setting: Besides the aforementioned criteria, this setting assumes the set of object types in ground truth pairs are known. Therefore, detected pairs with an object type outside the set are removed automatically, thus reducing the difficulty of the problem. For V-COCO, the average precision (AP) is computed under two scenarios, differentiated by the superscripts S1 and S2. This is to account for missing objects due to occlusion. For scenario 1, empty object boxes should be predicted in case of occlusion for a detected pair to be considered a match with the corresponding ground truth, while for scenario 2, object boxes are always assumed to be matched in such cases. We report our model's performance for three different backbone networks. Notably, our model with the lightestweight backbone already outperforms the next best method by a significant margin in almost every category. This gap is further widened with more powerful backbone networks. In particular, since the backbone CNN and object detection transformer are detached from the computational graph, our model has a small memory footprint. This allows us to use a higher-resolution feature map by removing the stride in the 5 th convolutional block (C5) of ResNet <ref type="bibr" target="#b8">[9]</ref>, which has been shown to improve detection performance on small objects <ref type="bibr" target="#b0">[1]</ref>. We denote this as dilated C5 (DC5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Macroscopic effects of the interaction head</head><p>In this section, we compare the effects of the unary (cooperative) and pairwise (competitive) layers on the HICO-DET test set, with ResNet50 <ref type="bibr" target="#b8">[9]</ref> as the CNN backbone. Since the parameters in the object detector are kept frozen for our model, the set of detections processed by the downstream network remains the same, regardless of any architectural changes in the interaction head. This allows us to compare how different variants of our model perform on the same human-object pairs. To this end, we collected the predicted interaction scores for all human-object pairs over the test set and compare how adding certain layers influence them. In Tab. 3, we show some statistics on the change of scores upon an architectural modification. In particular, note that the vast majority of collected pairs are easy negatives with scores close to zero. For analysis, we divide the negative examples into easy and hard, where we define an easy negative as one with a score lower than 0.05 as predicted by the "Ours w/o both layers" model, which accounts for 90% of the negative examples. In addition, we also show the distribution of the change in score with respect to the reference score as scatter plots in <ref type="figure" target="#fig_4">Fig. 5</ref>. The points are naturally bounded by the half-spaces 0 ? x + y ? 1.</p><p>Notably, adding the cooperative layer results in a significant average increase (+0.15) in the scores of positive examples, with little effect on the negative examples. This can be seen in <ref type="figure" target="#fig_4">Fig. 5a</ref> as well, where the score changes for almost all positive examples are larger than zero. In contrast, adding the competitive layer leads to a significant average decrease (?0.11) in the scores of hard negative examples, albeit with a small decrease in the score of positive examples as well. This minor decrease is compensated by the cooperative layer as shown in the last row of Tab. 3. Furthermore, looking at <ref type="figure" target="#fig_4">Fig. 5b</ref>, we can see a dense mass near the line y = ?x, which indicates that many negative examples have had their scores suppressed to zero.</p><p>Ablation study: In Tab. 4, we ablate the effect of different design decisions on performance. Adding the cooperative and competitive layers individually improves the performance by around 1.5 mAP, while adding both layers jointly improves by over 2 mAP. We also demonstrate the significance of the pairwise position encodings by removing them from the modified encoder and the multi-branch fusion module. This results in a 1.3 mAP decrease. Finally, we observe a slight improvement (0.3 mAP) when adding an additional cooperative or competitive layer, but no further improvements with more layers. As the competitive layer is more costly, we use two cooperative layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Microscopic effects of the interaction head</head><p>In this section, we focus on a specific image and visualise the effect of attention in our cooperative and competitive layers. In <ref type="figure" target="#fig_5">Fig. 6</ref>, we display a detection-annotated image and its associated attention map from the unary (cooperative) layer. The human-object pairs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b4">5)</ref> and <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6)</ref> are engaged in the interaction riding a horse. Excluding attention weights along the diagonal, we see that the corresponding human and horse instances attend to each other.  We hypothesise that attention between pairs of unary tokens (e.g., 1 and 4) helps increase the interaction scores for the corresponding pairs. To validate this hypothesis, we manually set the attention logits between the three positive pairs to minus infinity, thus zeroing out the corresponding attention weights. The effect of this was an average decrease of 0.06 (8%) in the interaction scores for the three pairs, supporting the hypothesis.</p><p>In <ref type="figure" target="#fig_6">Fig. 7</ref>, we visualise the attention map of the pairwise (competitive) layer. Notably, all human-object pairs attend to the interactive pairs (1, 4), <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b4">5)</ref> and <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6)</ref> in decreasing order, except for the interactive pairs themselves. We hypothesise that attention is acting here to have the dominant pairs suppress the other pairs. To investigate, we manually set the weights such that the three interactive pairs all attend to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref> as well, with a weight of 1. This resulted in a decrease of their interaction scores by 0.08 (11%). We then instead zeroed out the attention weights between the rest of the pairs and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref>, which resulted in a small increase in the scores of negative pairs. These results together suggest that attention in the competitive layer is acting as a soft version of non-maximum suppression, where pairs less likely to foster interactions attend to, and are suppressed by, the most dominant pairs. See Appendix E for more examples.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative results and limitations</head><p>In <ref type="figure" target="#fig_8">Fig. 8</ref>, we present several qualitative examples of successful HOI detections, where our model accurately localises the human and object instances and assigns high scores to the interactive pairs. For example, in <ref type="figure" target="#fig_8">Fig. 8b</ref>, our model correctly identifies the subject of an interaction (the lady in red) despite her proximity to a non-interactive human (the lady in black). We also observe in <ref type="figure" target="#fig_8">Fig. 8a</ref> that our model becomes less confident when there is overlap and occlusion. This stems from the use of object detection scores in our model. Confusion in the object detector often translates to confusion in action classification. We also show five representative failure cases for our model, illustrating its limitations. In <ref type="figure" target="#fig_9">Fig. 9a</ref>, due to the indefinite position of drivers in the training set (and real life), the model struggled to identify the driver. For <ref type="figure" target="#fig_9">Fig. 9d</ref>, the model failed to recognise the interaction due to a lack of training data (1 training example), even though the action is well-defined. Overall, ambiguity in the actions and insufficient data are the biggest challenges for our model. Another limitation, specific to our model, is that the computation and memory requirements of our pairwise layer scale quadratically with the number of unary tokens. For scenes involving many interactive humans and objects, this becomes quite costly. Moreover, since the datasets we used are limited, we may expect poorer performance on data in the wild, where image resolution, lighting condition, etc. may be less controlled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a two-stage detector of human-object interactions using a novel transformer architecture that exploits both unary and pairwise representations of the human and object instances. Our model not only outperforms the current state-of-the-art-a one-stage detector-but also consumes much less time and memory to train. Through extensive analysis, we demonstrate that attention between unary tokens acts to increase the scores of positive examples, while attention between pairwise tokens acts like non-maximum suppression, reducing the scores of negative examples. We show that these two effects are complementary, and together boost performance significantly.</p><p>Potential negative societal impact: Transformer models are large and computationally-expensive, and so have a significant negative environmental impact. To mitigate this, we use pre-trained models and a two-stage architecture, since fine-tuning an existing model requires less resources, as does training a single stage with the other stage fixed. There is also the potential for HOI detection models to be misused, such as for unauthorised surveillance, which disproportionately affects minority and marginalised communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pairwise positional encodings</head><p>We describe the details of the pairwise positional encodings as introduced in Sec. 3.1 of the main paper. Formally, denote a bounding box as b = [x, y, w, h] T ? [0, 1] 4 , where x and y represent the centre coordinates of the bounding box while w and h represent the width and height. Note that these values have been normalised by the image dimensions. For a pair of bounding boxes b 1 and b 2 , we start by encoding the unary terms besides the box representation itself, including box areas and aspect ratios as below</p><formula xml:id="formula_9">u = b 1 ? b 2 ? w 1 h 1 , w 2 h 2 , w 1 h 1 , w 2 h 2 T ,<label>(7)</label></formula><p>where ? denotes vector concatenation. We then proceed to encode the pairwise terms as follows</p><formula xml:id="formula_10">p = w 1 h 1 w 2 h 2 , IoU(b 1 , b 2 ) T ? f (d x ) ? f (d y ), (8) d x = x 1 ? x 2 w 1 , d y = y 1 ? y 2 h 1 ,<label>(9)</label></formula><formula xml:id="formula_11">f (d) = [ReLU(d), ReLU(?d)] T .<label>(10)</label></formula><p>This includes additional features such as the ratio of box areas, intersection over union (IoU) and directional encodings that characterise the distance between box centres. Note that the directional encodings are normalised by the dimension of the first (human) bounding box instead of that of the image. In addition, function f (?) ensures the componentwise positivity of the feature vector. Finally, denote a multi-layer perceptron as MLP, the complete pairwise positional encoding is computed as below</p><formula xml:id="formula_12">y = MLP(u ? p ? log(u ? p + )),<label>(11)</label></formula><p>where is a small constant added to the vector to avoid taking the logarithm of zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Numerical stability in the loss function</head><p>For the sake of numerical stability, loss function for logits is often preferred to that for normalised scores. In our case, due to the fact that the final interaction score is the product of multiple factors, we cannot directly use the loss function for logits. Therefore, we first need to recover the scale prior to normalisation. Denote the normalised object confidence score and action logit as? 1 ? [0, 1] and? 2 ? R respectively, the final score is computed as? =? 1 ? ?(? 2 ), where ? denotes the sigmoid function. We can then retrieve the corresponding logit? as below</p><formula xml:id="formula_13">y = ? ?1 (?),<label>(12)</label></formula><formula xml:id="formula_14">= log ? 1 1 + exp(?? 2 ) ?? 1 + ,<label>(13)</label></formula><p>where is a small constant added to the term to avoid taking the logarithm of zero. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-branch fusion</head><p>The multi-branch fusion (MBF) module <ref type="bibr" target="#b27">[28]</ref> employs multiple homogeneous branches, wherein each branch maps the two input features into a subspace of reduced dimension and performs fusion (elementwise product by default). The resultant feature is then mapped back to the original size. Afterwards, elementwise sum is used to aggregate results across all branches. The reduced representation size in a branch is intentionally configured in a way that renders the total number of parameters independent of the number of branches. For brevity of exposition, let us assume the number of branches is 1, for two input vectors x, y ? R n , the output vector z ? R n is computed as</p><formula xml:id="formula_15">z = W T 3 ? (W T 1 x + b 1 ) ? (W T 2 y + b 2 ) + b 3 ,<label>(14)</label></formula><p>where W 1 , W 2 , W 3 ? R n?n and b 1 , b 2 , b 3 ? R n are parameters of linear layers, ? refers to the rectified linear unit (ReLU), and ? denotes elementwise product. The implementation for this module in PyTorch is shown in Listing 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Modified transformer encoder layer</head><p>In this section, we compare the performance and interpretability of the modified transformer encoder layer to alternative formulations. To this end, we test multiple variants of the cooperative layer. As shown in Tab. 5, using a vanilla transformer encoder results in a small decrease (0.2 mAP) in performance. This gap persists after applying additive positional encodings learned from the unary box terms shown in Eq. <ref type="bibr" target="#b6">(7)</ref>. We then demonstrate the importance of the pairwise terms in Eq. (8) by removing them from the positional encodings, which resulted in a 0.4 mAP decrease. Together, these results indicate that the pairwise terms provide useful information for the cooperative layer and a consistent mAP performance boost.</p><p>In addition, we show the attention maps in different variants of the cooperative layer in <ref type="figure" target="#fig_1">Fig. 10</ref>. Notably, our modified encoder <ref type="figure" target="#fig_1">(Fig. 10b</ref>) accurately infers the correspondence between instances, where the interactive humans and objects attend to each other. This suggests that the pairwise positional encoding instills an inductive bias in the modi-  lar, there is no strong mutual attention between interactive instances, but more attention between non-interactive ones. The complete implementation of the modified encoder in PyTorch is shown in Listing 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional qualitative results</head><p>We show more visualisations of attention maps in <ref type="figure" target="#fig_1">Fig. 11</ref>. We intentionally avoided images with very few human and object instances and instead selected those with more complicated scenes for the purpose of demonstration. As shown by the attention maps, our model behaves consistently across different interaction types. More qualitative results for detected HOIs from HICO-DET <ref type="bibr" target="#b1">[2]</ref> and V-COCO <ref type="bibr" target="#b6">[7]</ref> can be found in <ref type="figure" target="#fig_1">Fig. 12</ref> and <ref type="figure" target="#fig_1">Fig. 13</ref> respectively.</p><p>To better understand the limitations of our model, we also show some false positives on HICO-DET in <ref type="figure" target="#fig_1">Fig. 14.</ref> In particular, the model sometimes struggles to identify the correct human instance for the interaction as shown in <ref type="figure" target="#fig_1">Figs. 14a, 14b and 14d</ref>. This is largely due to the plausible spatial relationship and the saliency of the human instance, both of which the model relies on heavily. Another false positive of similar cause can be seen in <ref type="figure" target="#fig_1">Fig. 14c</ref>, where both human instances are plausible candidates for the interaction boarding airplane based on their spatial locations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Asset attrition</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Unary and pairwise tokens with predicted scores (riding a motorcycle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Our Unary-Pairwise Transformer encodes human and object instances individually and in pairs, allowing it to reason about the data in complementary ways. In this example, our network correctly identifies the interactive pairs for the action riding a motorcycle, while suppressing the visually-similar non-interactive pairs and those with different associated actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the modified transformer encoder layer (left) and its attention module (right). FFN stands for feedforward network<ref type="bibr" target="#b26">[27]</ref>. "Pairwise concat." refers to the operation of pairing up all tokens and concatenating the features. "Duplicate" refers to the operation of repeating the features along a new dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Change in the interaction score (delta) with respect to the reference score. (a) The distribution of score deltas when adding the cooperative layer (first row of Tab. 3). (b) Adding the competitive layer to the model (second row). (c) Adding both layers (last row). For visualisation purposes, only 20% of the negatives are sampled and displayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Detected human and object instances (left) and the unary attention map for these instances (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Pairwise attention map for the human and object instances inFig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) standing on a snowboard (b) holding an umbrella (c) carrying a suitcase (d) sitting at a dining table (e) sitting on a bench (f) flying an airplane (g) holding a surfboard (h) wielding a baseball bat (i) riding a bike (j) holding a wineglass</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of detected HOIs. Interactive human-object pairs are connected by red lines, with the interaction scores overlaid above the human box. Pairs with scores lower than 0.2 are filtered out. (a) driving a truck (b) buying bananas (c) repairing a laptop (d) washing a bicycle (e) cutting a tie</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Failure cases often occur when there is ambiguity in the interaction (a), (b), (c) or a lack of training data (c), (d), (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Sample image (left) and the attention map (right) between pairs of unary tokens. (b) Sample image (left) and the attention map (right) between pairs of unary tokens. (c) Attention map between pairwise tokens. (d) Attention map between pairwise tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Our model exhibits the same behaviour on sample images with numerous human and object instances. Specifically, the attention map for unary tokens shows high symmetry, where potentially interactive instances attend to each other. And the pairwise attention map indicates that non-interactive pairs attend to the most dominant pairs to be suppressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .Figure 13 .Figure 14 .</head><label>121314</label><figDesc>Annotations from the HICO-DET<ref type="bibr" target="#b1">[2]</ref> dataset have no license specified. Images from this dataset are licensed under Creative Commons from Flickr. Annotations from the V-COCO<ref type="bibr" target="#b6">[7]</ref> dataset are licensed under the MIT License. V-COCO makes use of annotations and images from the MS COCO<ref type="bibr" target="#b18">[19]</ref> dataset. The MS-COCO annotations are licensed under a Creative Commons Attribution 4.0 License, and the images are sourced from from Flickr and have a variety of Creative Commons licenses, listed in the MS-COCO annotation files: Attribution-NonCommercial-ShareAlike License, Attribution-NonCommercial License, Attribution-NonCommercial-NoDerivs License, Attribution License, Attribution-ShareAlike License, Attribution-NoDerivs License, and No known copyright restrictions.(a) holding a fork (b) jumping skis (c) holding a teddy bear (d) petting a zebra (e) sitting on a chair (f) riding an elephant (g) wearing a tie (h) swinging a tennis racket (i) holding a toothbrush (j) standing on a surfboard Additional qualitative results for detected human-object pairs on HICO-DET [2] test set. (a) carrying a backpack (b) cutting with a knife (c) drinking from a bottle (d) eating with a fork (e) holding a surfboard (f) eating a hotdog (g) holding a knife (h) jumping a skateboard (i) kicking a sports ball (j) eating a pizza (k) holding a tennis racket (l) reading a book (m) laying on a bed (n) sitting on a couch (o) riding a horse (p) talking on a phone (q) working on a computer (r) riding a boat (s) throwing a frisbee (t) riding a motorcycle Additional qualitative results for detected human-object pairs on V-COCO [7] test set. (a) blocking a sports ball (b) flying an airplane (c) boarding an airplane (d) brushing with a toothbrush (e) washing a car False positives on HICO-DET [2] test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of HOI detection performance (mAP?100) on the HICO-DET<ref type="bibr" target="#b1">[2]</ref> and V-COCO<ref type="bibr" target="#b6">[7]</ref> test sets. The highest result in each section is highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HICO-DET</cell><cell></cell><cell></cell><cell cols="2">V-COCO</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Default Setting</cell><cell cols="3">Known Objects Setting</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell><cell>AP S1 role</cell><cell>AP S2 role</cell></row><row><cell>HO-RCNN [2]</cell><cell>CaffeNet</cell><cell>7.81</cell><cell>5.37</cell><cell>8.54</cell><cell>10.41</cell><cell>8.94</cell><cell>10.85</cell><cell>-</cell><cell>-</cell></row><row><cell>InteractNet [6]</cell><cell>ResNet-50-FPN</cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>40.0</cell><cell>-</cell></row><row><cell>GPNN [23]</cell><cell>ResNet-101</cell><cell>13.11</cell><cell>9.34</cell><cell>14.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.0</cell><cell>-</cell></row><row><cell>TIN [16]</cell><cell>ResNet-50</cell><cell>17.03</cell><cell>13.42</cell><cell>18.11</cell><cell>19.17</cell><cell>15.51</cell><cell>20.26</cell><cell>47.8</cell><cell>54.2</cell></row><row><cell>Gupta et al. [8]</cell><cell>ResNet-152</cell><cell>17.18</cell><cell>12.17</cell><cell>18.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VSGNet [26]</cell><cell>ResNet-152</cell><cell>19.80</cell><cell>16.05</cell><cell>20.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.8</cell><cell>57.0</cell></row><row><cell>DJ-RN [14]</cell><cell>ResNet-50</cell><cell>21.34</cell><cell>18.53</cell><cell>22.18</cell><cell>23.69</cell><cell>20.64</cell><cell>24.60</cell><cell>-</cell><cell>-</cell></row><row><cell>PPDM [17]</cell><cell>Hourglass-104</cell><cell>21.94</cell><cell>13.97</cell><cell>24.32</cell><cell>24.81</cell><cell>17.09</cell><cell>27.12</cell><cell>-</cell><cell>-</cell></row><row><cell>VCL [10]</cell><cell>ResNet-50</cell><cell>23.63</cell><cell>17.21</cell><cell>25.55</cell><cell>25.98</cell><cell>19.12</cell><cell>28.03</cell><cell>48.3</cell><cell>-</cell></row><row><cell>ATL [11]</cell><cell>ResNet-50</cell><cell>23.81</cell><cell>17.43</cell><cell>27.42</cell><cell>27.38</cell><cell>22.09</cell><cell>28.96</cell><cell>-</cell><cell>-</cell></row><row><cell>DRG [5]</cell><cell>ResNet-50-FPN</cell><cell>24.53</cell><cell>19.47</cell><cell>26.04</cell><cell>27.98</cell><cell>23.11</cell><cell>29.43</cell><cell>51.0</cell><cell>-</cell></row><row><cell>IDN [15]</cell><cell>ResNet-50</cell><cell>24.58</cell><cell>20.33</cell><cell>25.86</cell><cell>27.89</cell><cell>23.64</cell><cell>29.16</cell><cell>53.3</cell><cell>60.3</cell></row><row><cell>HOTR [13]</cell><cell>ResNet-50</cell><cell>25.10</cell><cell>17.34</cell><cell>27.42</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.2</cell><cell>64.4</cell></row><row><cell>FCL [12]</cell><cell>ResNet-50</cell><cell>25.27</cell><cell>20.57</cell><cell>26.67</cell><cell>27.71</cell><cell>22.34</cell><cell>28.93</cell><cell>52.4</cell><cell>-</cell></row><row><cell>HOI-Trans [29]</cell><cell>ResNet-101</cell><cell>26.61</cell><cell>19.15</cell><cell>28.84</cell><cell>29.13</cell><cell>20.98</cell><cell>31.57</cell><cell>52.9</cell><cell>-</cell></row><row><cell>AS-Net [3]</cell><cell>ResNet-50</cell><cell>28.87</cell><cell>24.25</cell><cell>30.25</cell><cell>31.74</cell><cell>27.07</cell><cell>33.14</cell><cell>53.9</cell><cell>-</cell></row><row><cell>SCG [28]</cell><cell>ResNet-50-FPN</cell><cell>29.26</cell><cell>24.61</cell><cell>30.65</cell><cell>32.87</cell><cell>27.89</cell><cell>34.35</cell><cell>54.2</cell><cell>60.9</cell></row><row><cell>QPIC [25]</cell><cell>ResNet-101</cell><cell>29.90</cell><cell>23.92</cell><cell>31.69</cell><cell>32.38</cell><cell>26.06</cell><cell>34.27</cell><cell>58.8</cell><cell>61.0</cell></row><row><cell>Ours (UPT)</cell><cell>ResNet-50</cell><cell>31.66</cell><cell>25.94</cell><cell>33.36</cell><cell>35.05</cell><cell>29.27</cell><cell>36.77</cell><cell>59.0</cell><cell>64.5</cell></row><row><cell>Ours (UPT)</cell><cell>ResNet-101</cell><cell>32.31</cell><cell>28.55</cell><cell>33.44</cell><cell>35.65</cell><cell>31.60</cell><cell>36.86</cell><cell>60.7</cell><cell>66.2</cell></row><row><cell>Ours (UPT)</cell><cell>ResNet-101-DC5</cell><cell>32.62</cell><cell>28.62</cell><cell>33.81</cell><cell>36.08</cell><cell>31.41</cell><cell>37.47</cell><cell>61.3</cell><cell>67.1</cell></row></table><note>those of the negative examples. We then visualise the at- tention weights for specific images, and show how these behaviours are achieved by the attention mechanism. At inference time, our method with ResNet50 [9] runs at 24 FPS on a single GeForce RTX 3090 device.Datasets: HICO-DET [2] is a large-scale HOI detection dataset with 37 633 training images, 9 546 test images, 80 object types, 117 actions, and 600 interaction types. The dataset has 117 871 human-object pairs with annotated bounding boxes in the training set and 33 405 in the test set. V-COCO [7] is much smaller in scale, with 2 533 train- ing images, 2 867 validation images, 4 946 test images, and only 24 different actions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Effect of the cooperative and competitive layers on the HICO-DET test set under the default settings.</figDesc><table><row><cell>Model</cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell></row><row><cell>Ours w/o both layers</cell><cell>29.22</cell><cell>23.09</cell><cell>31.05</cell></row><row><cell>Ours w/o comp. layer</cell><cell>30.78</cell><cell>24.92</cell><cell>32.53</cell></row><row><cell>Ours w/o coop. layer</cell><cell>30.68</cell><cell>24.69</cell><cell>32.47</cell></row><row><cell>Ours w/o pairwise pos. enc.</cell><cell>29.98</cell><cell>23.72</cell><cell>31.64</cell></row><row><cell>Ours (1? coop., 1? comp.)</cell><cell>31.33</cell><cell>26.02</cell><cell>32.91</cell></row><row><cell>Ours (1? coop., 2? comp.)</cell><cell>31.62</cell><cell>26.18</cell><cell>33.24</cell></row><row><cell>Ours (2? coop., 1? comp.)</cell><cell>31.66</cell><cell>25.94</cell><cell>33.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison amongst different variants of the cooperative layer on HICO-DET<ref type="bibr" target="#b1">[2]</ref> test set under default setting. All variants below use ResNet50<ref type="bibr" target="#b8">[9]</ref> as the backbone CNN and employ one layer. The acronym M.E. stands for modified encoder.</figDesc><table><row><cell>Variant</cell><cell>Full</cell><cell>Rare</cell><cell>Non-rare</cell></row><row><cell>Vanilla</cell><cell>31.15 ? .03</cell><cell>25.70</cell><cell>32.77</cell></row><row><cell>Vanilla w/ add. pos. enc.</cell><cell>31.14</cell><cell>25.59</cell><cell>32.80</cell></row><row><cell>M.E. w/o pairwise terms</cell><cell>30.93</cell><cell>24.53</cell><cell>32.84</cell></row><row><cell>M.E.</cell><cell>31.33 ? .04</cell><cell>26.02</cell><cell>32.91</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Final scores in Eq. (6) are normalised to the interval [0, 1]. In training, we instead recover the scale prior to normalisation and use the corresponding loss with logits for numerical stability. See more details in Appendix B.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We are grateful for support from Continental AG (D.C.). We would also like to thank Jia-Bin Huang and Yuliang Zou for their help with the reproduction of some experiment results.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>import torch import torch.nn as nn import torch.nn.functional as F class MultiBranchFusion(nn.Module):</p><p>""" Parameters: -----------appearance_size:   fied encoder that allows it to identify interactive and noninteraction pairs, and preferentially share information between the interactive ones. Furthermore, we show that, without the pairwise terms, as shown in <ref type="figure">Fig. 10c</ref>, the atten-tion map becomes uniform along one dimension. Similarly, the vanilla encoder does not make use of the pairwise spatial information either. This results in the attention maps being much less interpretable as shown in <ref type="figure">Fig. 10d</ref>. In particu- .sum(dim=0) for w, m in zip(weights, messages)], dim=-1) )) aggregated_messages = self.dropout(aggregated_messages) x = self.norm(x + aggregated_messages) return x, weights</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reformulating hoi detection as adaptive set prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DRG: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Affordance transfer learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting human-object interaction via fabricated compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hotr: End-to-end human-object interaction detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun-Sol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hoi analysis: Integrating and decomposing human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PPDM: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog., 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">QPIC: Query-based pairwise human-object interaction detection with image-wide contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoaki</forename><surname>Yoshinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VS-GNet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatially conditioned graphs for detecting human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
