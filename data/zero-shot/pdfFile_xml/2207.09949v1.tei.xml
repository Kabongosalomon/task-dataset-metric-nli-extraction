<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">EIT Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhou.wang@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inst. for Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Absolute 3D Human Pose Estimation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While monocular 3D pose estimation seems to have achieved very accurate results on the public datasets, their generalization ability is largely overlooked. In this work, we perform a systematic evaluation of the existing methods and find that they get notably larger errors when tested on different cameras, human poses and appearance. To address the problem, we introduce VirtualPose, a two-stage learning framework to exploit the hidden "free lunch" specific to this task, i.e. generating infinite number of poses and cameras for training models at no cost. To that end, the first stage transforms images to abstract geometry representations (AGR), and then the second maps them to 3D poses. It addresses the generalization issue from two aspects: (1) the first stage can be trained on diverse 2D datasets to reduce the risk of over-fitting to limited appearance; (2) the second stage can be trained on diverse AGR synthesized from a large number of virtual cameras and poses. It outperforms the SOTA methods without using any paired images and 3D poses from the benchmarks, which paves the way for practical applications. Code is available at https://github.com/wkom/VirtualPose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monocular 3D pose estimation has attracted much attention since it can benefit many applications. Most works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> focus on a simpler sub-task of relative 3D pose estimation where only relative joint positions are estimated. Absolute 3D pose estimation needs to estimate the depth of a person's root joint in the camera coordinate system. This is more challenging because it is ill-posed and multiple entangled latent factors jointly determine the depth. As  <ref type="figure">Fig. 1</ref>, the relevant factors include at least the height of the person in neutral standing pose, its relative posture, its projection size in 2D, camera focal length, and camera view point. Some factors such as focal length may be assumed known in some cases but most others need to be implicitly estimated from images along with depth.</p><p>Many works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref> propose to brute-forcely learn the mapping from images to depth. Although they have got good results on the public datasets, they have poor generalization ability. We are aware that other tasks also face the issue but the situation is very different for pose estimation. First, the pose datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref> have extremely limited variations in terms of cameras, human poses and appearance. Second, many data augmentation techniques are not applicable, e.g., we can not change the view point neither the human poses in an image. So, addressing the generalization issue is non-trivial compared to other tasks. This was not identified as a serious issue previously because the current training and testing data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> are similar. In fact, even for the cross-dataset experiment (train on MuCo-3DHP <ref type="bibr" target="#b21">[22]</ref> and test on MuPoTS-3D <ref type="bibr" target="#b21">[22]</ref>), the camera views are also similar.</p><p>In this paper, we address the challenges by introducing an intermediate representation, termed as Abstract Geometry Representation (AGR), into the 3D pose estimation network. It is a bundle of multiple geometry representations that satisfy three requirements: (1) they are helpful for recovering absolute 3D poses, <ref type="bibr" target="#b1">(2)</ref> can be synthesized from 3D poses, and (3) can be robustly estimated from images even when the model is trained on wild images rather than mocap datasets. As shown in <ref type="figure">Fig. 2</ref>, AGR splits a 3D pose estimator into two successive modules. The first module maps a raw image to AGR which, in current implementation, consists of human detection and joint localization results. Since it only handles 2D tasks which are barely affected by 3D projection geometry, we can train the model on the diverse 2D datasets such as COCO <ref type="bibr" target="#b15">[16]</ref>, which covers a large number of camera views, human poses and backgrounds, and apply extensive data augmentation. As a result, the module is robust to different factors and achieves desirable results on wild data.</p><p>The second module learns to regress 3D pose from AGR. Different from the previous works, we propose a novel training strategy which synthesizes a large number of paired &lt;AGR, Pose&gt; data from diverse camera views, poses and person positions to learn a generalizable model. It is worth noting that AGR suffers less from domain gaps than raw images. In some cases when we want to deploy a model for a fixed environment, e.g. installing a camera at home for elderly care, we can even generate virtual training data specifically for the environment, which as shown in our experiment, gets more accurate results. Combining the two modules, we get an accurate yet generalizable 3D pose estimator. It not only outperforms the state-of-the-art methods on the benchmark datasets but also achieves good results on our own videos collected in retail stores with cluttered background and severe occlusion.</p><p>We implement the above idea following the architecture of VoxelPose <ref type="bibr" target="#b32">[33]</ref>, as shown in <ref type="figure">Fig. 2</ref>  2D pose heatmaps as AGR using a CNN network. Then they are integrated into the 3D space to estimate the 3D positions of all persons in the image. Finally, for each person, we construct a feature volume around its position to estimate a fine-grained 3D pose.</p><p>In summary, our contributions are three-fold. First, we present the first systematic study on models' generalization ability which is largely overlooked in the previous works. We argue that it is important to purposely prevent models from over-fitting when designing and evaluating new models. Second, we present one possible way for learning generalizable models purely from virtual 3D poses and cameras which is made possible by AGR. Note that the decoupling strategy has been used in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> for different motivations. Our contribution lies in leveraging it in our training strategy for absolute 3D pose estimation with concrete designs. Finally, the method outperforms the existing methods without using paired images and 3D poses for training, which notably improves its applicability in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Datasets and Generalization Collecting motion capture datasets is cumbersome because it either uses multiview systems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref> or wearable IMUs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45]</ref>. As a result, they usually have limited camera views, poses and actors. Some works propose to generate more data by changing the image backgrounds <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> of the existing datasets, or using game engines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>. However, the domain gap between synthetic and real images poses challenges for neural networks. We find that the models trained on these datasets lack generalization capability. Some works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48]</ref> address the generalization problem in relative pose estimation but they cannot be extended to the absolute task. There are some works in other areas that also decouple a network into different stages <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>. While we also follow this general idea, we have specific designs to apply it to the 3D absolute pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimizing Projection Error</head><p>The methods of this class <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> first estimate relative 3D human poses with sizes from an image in an end-to-end manner. Then based on the estimated 2D projection size and the focal length, they can coarsely compute the depth of each pose by minimizing an error between the 2D pose and the projection of the estimated 3D pose. The results of these methods heavily rely on the accuracy of relative 3D pose estimation. Some later works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref> show that they have poor generalization results because they need to be trained on the (small) motion capture datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Ground Plane Geometry Some works such as <ref type="bibr" target="#b20">[21]</ref> propose to analytically compute the real 3D size and depth from ground plane geometry. They assume that the camera is calibrated, people are standing on the ground and their feet are visible in the image. Then the absolute human height and depth can be calculated by projection geometry. However, these assumptions are not always true in practice which limits their applicability.</p><p>End-to-end Learning Some works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref> directly estimate depth from images. We first explain why this is possible. Consider a simple case where a person with height h real stands on the ground plane and a camera with focal length f is placed with pitch angle ?, as shown in <ref type="figure">Fig. 1(a)</ref>. Then the height of the person in the image h img is approximately h img ? f ?h real ?cos? d . In general, people may be in various poses, e.g. sitting on the ground as shown in <ref type="figure">Fig. 1(b)</ref>. We define the height of the person perpendicular to the ground as h pose , and a pose-dependent correction factor as ? pose = hpose h real , then we have h img ? f ??pose?h real ?cos? d , from which the depth d of the person can be computed.</p><p>These methods usually assume f is known and estimate normalized depth</p><formula xml:id="formula_0">d norm = d f .</formula><p>It is challenging to estimate h real from a single image. But since there are only few actors in the benchmark datasets, they are implicitly learned by the network. The rest factors of ? pose , ? and h img can be estimated from images when the training data is large and diverse. However, since most benchmark datasets are small, e.g. having limited camera poses, human poses and backgrounds, the depth estimation models may easily over-fit to these datasets but can not generalize well on unseen images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalization Study</head><p>In this section, we systematically evaluate the robustness of the existing methods to the variations of the key factors. In Section 3.1, we first introduce three representative baselines for estimating depth which are most frequently used by the existing 3D pose estimators <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. Then we evaluate their robustness to the three factors including camera pose, image background and human pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baselines and Datasets</head><p>Top-down Box Size based method (TBS) Given an image as input, it first localizes all people by bounding boxes and then estimates depth for each people. They assume that the 3D sizes of all people are constant and known, and geometrically compute the depth based on the 2D size of the bounding box. However, as discussed in <ref type="figure">Fig. 1(b)</ref>, the 2D size is also dependent on the human pose. So they learn a correction factor from images. The factor is used to refine the estimated depth. Multiple methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> follow this strategy. We choose RootNet <ref type="bibr" target="#b22">[23]</ref> in our experiments.</p><p>Top-down Image Feature based method (TIF) It first detects all people in the input image. But instead of using the size of the detected bounding box to analytically compute depth, they use the estimated 2D heatmaps as attention masks to pool features from the image and directly predict the human depth. We choose HDNet <ref type="bibr" target="#b14">[15]</ref> in our experiments.</p><p>Bottom-up Depth Regression based method (BDR) This method directly estimates a depth map from each image using a deep network. The depth of each joint can then be obtained from the depth map directly. Many works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref> follow this pipeline. We present a simple 3D pose estimator which has two branches for estimating 2D pose heatmaps and depth maps, respectively. The 3D pose can then be analytically computed from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In order to mitigate the influence of detection results, we use the GT human bounding box in TBS and TIF methods. The backbone in the BDR method is ResNet-50 <ref type="bibr" target="#b8">[9]</ref>. In order to mitigate the influence of 2D pose estimation, we use the GT 2D root position to obtain root depth from the depth maps in BDR method. We conduct experiments on the CMU Panoptic <ref type="bibr" target="#b11">[12]</ref>, MuCo-3DHP and MuPoTS-3D <ref type="bibr" target="#b21">[22]</ref> datasets.</p><p>CMU Panoptic <ref type="bibr" target="#b11">[12]</ref> is a large-scale multi-person pose dataset captured in a studio with multiple cameras. We select data of four activities (Haggling, Mafia, Ultimatum, Pizza) as our training and test set. The training and testing splits are different in different ablative experiments and will be made clear as needed.  MuCo-3DHP <ref type="bibr" target="#b21">[22]</ref> is created by compositing randomly sampled 3D poses from single-person 3D human pose dataset MPI-INF-3DHP <ref type="bibr" target="#b19">[20]</ref> to synthesize multiperson scenes. The images are synthesized by resizing and compositing the corresponding segmented images of each person according to their 3D positions. We use the data provided by <ref type="bibr" target="#b22">[23]</ref> which contains images with green screen background and images with augmented background using COCO <ref type="bibr" target="#b15">[16]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MuPoTS-3D</head><p>[22] is a multi-person test set comprising 20 general real world scenes with GT 3D pose obtained with a multi-view marker-less motion capture system. It contains 5 indoor and 15 outdoor settings, varying human poses and camera poses, which is a commonly used test set to validate the models' generalization ability. Metrics We focus on evaluating the quality of the estimated depth of root joint in this section. We use the metric of MRPE z (in mm) proposed in <ref type="bibr" target="#b22">[23]</ref>, which is the mean of the root position errors of all people in the z direction (i.e. the depth direction in the camera coordinate system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Camera Views We conduct the cross-view experiment on the CMU Panoptic <ref type="bibr" target="#b11">[12]</ref> dataset using cameras of 14, 16 and 19 as shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. We train the three baseline models on one of the cameras and test them on the rest. The results are shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. All methods get several times larger depth estimation errors when they are trained and tested on different cameras. The results are reasonable because (1) camera angle affects image appearance and image features;</p><p>(2) camera angle also affects the 2D size of the people in the image. The results suggest that the models learned on few camera views are barely generalizable. It is helpful to train the models on as many views as possible.</p><p>Image Background For each method, we train two models on the MuCo-3DHP <ref type="bibr" target="#b21">[22]</ref> dataset, with the first on images with green screen background and the second with both green screen and augmented background. We select 70% of the dataset for training, and the rest for testing. Tab. 1 shows the results. When tested on the green screen subset of MuCo-3DHP, BDR (green) achieves a smaller error than BDR (augmented). It means that BDR probably learns some scene priors to reduce the ambiguity in estimation but it may also risk over-fitting. For example, BDR (green) gets a significantly larger error on the augmented subset of MuCo-3DHP. The results not only validate the limitations of those methods on small training datasets, but also suggest that we actually need a better evaluation protocol, e.g. training and testing on datasets with large differences, to convincingly evaluate the future works.</p><p>Human Pose We train two models either using poses only from the CMU Panoptic dataset, or from the combined CMU Panoptic and MuCo-3DHP datasets. We test the models on the MuPoTS-3D dataset. To keep other factors such as human appearance the same, we modify BDR to take pose heatmaps as input and output the root depth map D (this is actually part of our method as will be described). The results are shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. The model trained on CMU Panoptic poses achieves a larger error meaning that the model trained on limited poses may have poor generalization results. While the conclusion is not surprising, it actually points out an overlooked problem that relying on limited mocap datasets to train 3D pose estimators will probably fail in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VirtualPose</head><p>In this section, we present our VirtualPose. To address the challenges, we introduce an intermediate representation, termed as Abstract Geometry Representation (AGR, Section 4.1), into the 3D pose estimation network so that we can simultaneously leverage the diverse 2D datasets and large amounts of synthetic data. As shown in <ref type="figure">Fig. 2</ref>, AGR splits a 3D pose estimator into two successive modules. The first module maps a raw image to AGR which consists of human detection and joint localization results. Then the second module maps AGR to the corresponding 3D pose which is trained on synthesized &lt;AGR, Pose&gt; data. In particular, we present Root Estimation Network (REN, Section 4.2) to estimate 3D positions of the persons. Then for each person, Pose Estimation Network (PEN, Section 4.3) is proposed to estimate the 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Abstract Geometry Representation</head><p>AGR is a general concept representing a bundle of geometry representations that satisfy three requirements: (1) they are helpful for recovering absolute 3D poses, and (2) can be robustly estimated from images even when the model is only trained on the wild 2D datasets rather than the 3D motion capture datasets, and (3) can be synthesized or rendered from 3D poses. In current implementation, we only use 2D pose heatmaps and human detection bounding boxes in AGR for simplicity. But more cues such as ordinal depth, occlusion relationship or segmentation mask can also be leveraged. We adopt a simple architecture for estimating AGR from images following CenterNet <ref type="bibr" target="#b48">[49]</ref>. As shown in <ref type="figure">Fig. 2(a)</ref>, given an image I as input, the 2D backbone outputs the corresponding 2D pose heatmaps H 2D ? [0, 1] W ?H?N and box detection map B ? R W ?H?4 where the four channels encode the distances from the human root joint to the four edges of the human bounding box, respectively. Here, N indicates the number of human joints, and W, H are the width and height of the output maps. The 2D human pose estimation and detection network is trained by:</p><formula xml:id="formula_1">L 2D = L 2D heat + ? bbox L bbox (1) L 2D heat = ||H 2D ? H 2D || 2 2 (2) L bbox = p?P ||B p ? B p || 1 ,<label>(3)</label></formula><p>where H 2D is the GT 2D pose heatmaps and ? bbox is a hyper-parameter. The box supervision is only enforced at GT root joint locations P where B p is the estimated box embedding for the p th person and B p ? R 4 is the GT box embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Root Estimation Network</head><p>We first estimate a coarse depth map for the root joint as shown in <ref type="figure">Fig. 2</ref>, which will be used to reduce the ambiguity when projecting 2D heatmaps to the 3D space. To that end, the previously estimated 2D pose heatmaps H 2D are fed to the 2D CNN-based Depth Estimator (DE) to estimate a root depth map D. It is trained by minimizing:</p><formula xml:id="formula_2">L depth = p?P |Dp ? Dp|,<label>(4)</label></formula><p>where D p and D p are the estimated and GT depth values of the root joint for the p th person. Then we estimate the 3D positions of the root joints by constructing a 3D representation. We discretize the 3D space in which people can freely move into X ? Y ? Z voxels {G x,y,z }, and construct a 3D feature volume V in REN ? [0, 1] X?Y ?Z?N by inversely projecting 2D pose heatmaps of N body joints to the 3D space using camera parameters. Different from the previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref>, for a heatmap vector at a pixel location within a bounding box, we only project it to a few voxels whose depths are similar to the estimated depth D p of the root joint in the bounding box. For example, if a voxel G x,y,z whose projected 2D pixel location (u, v) is within the p th human bounding box, then the feature of G x,y,z can be calculated as</p><formula xml:id="formula_3">V in REN (x, y, z) = H 2D (u, v) exp(? (z ? Dp) 2 2? 2 ),<label>(5)</label></formula><p>where ? is empirically set to be 200. This is particularly important for monocular 3D pose estimation which helps reduce the ambiguity. The feature volume V in REN coarsely encodes the likelihood of each person's position in each voxel. We feed V in REN to a 3D network to estimate the corresponding 3D heatmaps H REN ? [0, 1] X?Y ?Z indicating the confidence of each voxel containing a root joint as shown in <ref type="figure">Fig. 2(b)</ref>. We select the voxels with large confidence values in the estimated H REN with Non-Maximum Suppression (NMS). We train the 3D network by minimizing:</p><formula xml:id="formula_4">L REN = ||H REN ? H REN || 2 2 ,<label>(6)</label></formula><p>where H REN ? [0, 1] X?Y ?Z is the GT 3D heatmap for root joint. We set X, Y and Z to be 80, 80 and 24, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pose Estimation Network</head><p>After estimating the root joint position for each person, we will estimate a complete 3D pose for it. As shown in <ref type="figure">Fig. 2(c)</ref>, we first construct a finer-grained feature volume centered at the estimated root joint position by inversely projecting the estimated 2D pose heatmaps H 2D . The spatial size of the feature volume V in P EN ? [0, 1] X ? ?Y ? ?Z ? ?N is set to be 2m ? 2m ? 2m which is sufficiently large to cover people in arbitrary poses. We set X ? = Y ? = Z ? = 64, so approximately each voxel has a size of 30mm.</p><p>We feed V in P EN to a 3D Pose Estimation Network (PEN) to estimate 3D heatmaps H P EN ? [0, 1] X ? ?Y ? ?Z ? ?N for N joints, including the root joint. For each joint k, the 3D location J k can be obtained using the integration trick proposed in <ref type="bibr" target="#b31">[32]</ref> to reduce quantization error:</p><formula xml:id="formula_5">J k = X ? x=1 Y ? y=1 Z ? z=1 (x, y, z) ? H P EN,k (x, y, z).<label>(7)</label></formula><p>We train PEN using the L 1 loss as in <ref type="bibr" target="#b32">[33]</ref>:</p><formula xml:id="formula_6">L P EN = 1 N N k=1 ||J k ? J k || 1 .<label>(8)</label></formula><p>PEN has the same network structure as the 3D network in REN, and the weights are shared for different people. We find that by estimating the absolute 3D locations of all joints, it further improves the absolute position of the root joint compared to the REN output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>General Details The 2D backbone for estimating boxes and 2D pose heatmaps is ResNet-152 <ref type="bibr" target="#b8">[9]</ref>. The backbone for estimating depth map from heatmaps is ResNet-18 <ref type="bibr" target="#b8">[9]</ref>. The input image is resized to 512 ? 960 and the size of the resulting heatmap is 128 ? 240. We adopt Adam <ref type="bibr" target="#b12">[13]</ref> as optimizer, the learning rate is 1 ? 10 ?4 and the batch size is 24. The 2D backbone network is trained for 40 epochs. The depth estimator, REN and PEN are trained end-to-end with synthetic heatmaps and their GT 3D poses for 20 epochs.</p><p>Training Data For the experiments on CMU Panoptic <ref type="bibr" target="#b11">[12]</ref>, the backbone is trained on the combined COCO <ref type="bibr" target="#b15">[16]</ref> and CMU Panoptic datasets. When synthesizing AGR training data, the absolute 3D poses and camera views are the same as the original training data in Panoptic dataset. For the MuPoTS-3D <ref type="bibr" target="#b21">[22]</ref> experiment, the backbone is trained on the combined COCO and MuCo-3DHP <ref type="bibr" target="#b21">[22]</ref> datasets. When synthesizing AGR training data, the relative 3D poses and camera views are from MuCo-3DHP, and we randomly place the poses in 3D space to enhance the diversity of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to the State-of-the-arts</head><p>CMU Panoptic <ref type="bibr" target="#b11">[12]</ref> Following <ref type="bibr" target="#b41">[42]</ref>, we use the video sequences of camera 16 and 30 as training and test data which consist of four activities, i.e. Haggling, Mafia, Ultimatum, Pizza. We show the results in <ref type="table" target="#tab_3">Table 2</ref>. We achieve better results than previous methods except for HMOR <ref type="bibr" target="#b36">[37]</ref> which uses the GT depth when estimating the absolute 3D pose. Besides, our method achieves the best performance in the Pizza sequence which is not included in the training data. This partially validates the generalization ability of our method. MuPoTS-3D <ref type="bibr" target="#b21">[22]</ref> Our method is trained on the synthetic data where the 3D poses are from the MuCo-3DHP dataset. The rest methods are trained end-to-end on paired images and 3D poses from the MuCo-3DHP dataset. Following the standard practice on this dataset, the metric of percentage of correct keypoints (3DPCK) is used to measure the estimation results. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. PCK abs measures the accuracy of absolute pose and PCK root measures the accuracy of the root joint. We can see that our method achieves significantly better depth and pose estimation results than the state-of-the-arts. It validates that our absolute 3D depth estimation method has strong generalization capability. We hope to emphasize the importance of the results because it means the method has the potential to be applied in the wild environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>In this section, we will evaluate the impact of our proposed modules and the training strategies to the estimation results of the root joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root Estimation Network</head><p>We first introduce two baselines. Baseline (a) analytically computes the 3D root position based on the 2D position and the estimated depth map. The baseline (b) uses the 3D root estimator to estimate the 3D root position on top of (a). The results on the CMU Panoptic and MuPoTS-3D datasets are shown in <ref type="table" target="#tab_5">Table 4</ref>. By comparing the results of (a) and (b), we can see that the root depth error has a significant reduction which validates the effectiveness of the 3D root estimator.</p><p>Pose Estimation Network By comparing the results of (b) and (c) in <ref type="table" target="#tab_5">Table  4</ref>, we can see that depth estimation can be notably improved by PEN, which leverages the rest of the body joints to refine the root joint. Another reason for the improvement is that the quantization error is reduced by computing continuous root locations via the integral trick.</p><p>Depth Based Feature Projection As stated in Section 4.2, we construct the 3D feature volume by projecting 2D pose heatmaps based on the estimated depths and bounding boxes. We compare it to a baseline which naively projects the heatmaps to all voxels as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref> and find that MRPE and MRPE z of our method are significantly better than the baseline (204.0mm vs. 253.2mm, and 159.8mm vs. 210.5mm) on the MuPoTS-3D dataset.  <ref type="table" target="#tab_6">Table  5</ref>. We can see that by training a universal model, our method achieves smaller depth estimation errors than the model trained on a single camera. Second, if we know the camera view point in the testing environment, we can generate training data specifically for it. If we train a model for camera 19, then the testing error on camera 19 will be decreased significantly to 62.3mm. This is helpful when the camera is fixed, e.g. deployed at home for child or elderly care. In this case, we can get very accurate estimation results. We have similar observations for the pose factor. In particular, if we have the pose information for the people who are going to appear in the camera, then we can generate AGR using those poses. In that case, the testing accuracy will also be significantly improved. For example, if we train the model using the poses from the MuPoTS-3D dataset, then the PCK abs will be improved from 44.0% to 50.4%.  <ref type="figure" target="#fig_3">Fig. 4(a)</ref> shows some estimation results for images from the COCO dataset and MuPoTS-3D dataset. Since the camera parameters are not provided in the COCO dataset, we choose a general focal length (i.e. 1400) and assume the pitch angle of the camera is zero. We can see that our approach not only obtains accurate 3D pose for each person in the image but also estimates their absolute depth correctly in the 3D space. In particular, the model is robust to pose and background variations. For instance, in the baseball example, we get correct depth estimate for the person in sitting posture. <ref type="figure" target="#fig_3">Fig. 4(b)</ref> and (c) show some typical failure cases. In <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, the man in the red circle is occluded and truncated, while in <ref type="figure" target="#fig_3">Fig. 4(c)</ref>, the little girl is much shorter than the people in the training dataset. In the future, to address the second issue, we will study the possibility of adding another parallel branch in the 2D network to estimate a correction factor to refine the estimated depth of each person. (c) Typical failure cases due to the person is shorter than those in the training dataset. As a result, the estimated depth is larger than GT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present a systematic study of the generalization problem of 3D pose estimation. We hope the study can inspire future works to consider the generalization aspect when designing and evaluating new models. Then we present VirtualPose, an approach for absolute 3D human pose estimation, which does not require paired images and 3D poses for training. In particular, part of the network is trained on abundant 2D datasets such as COCO and the rest are trained on synthetic datasets. The decoupling strategy helps the approach avoid over-fitting to small training data. As a result, it can be flexibly adapted to a new environment with minimal human effort. Our method achieves significantly better results than the existing methods on the benchmark datasets especially when the training and test datasets have different distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Future Work</head><p>First, in the current implementation, AGR only consists of 2D joint heatmaps and person-level bounding boxes which are powerful to recover the absolute positions of the root joints. But we can actually explore finer-grained AGRs such as segmentation maps and occlusion relationship maps which can benefit more to relative pose estimation. Second, we only used the 3D poses from the Panoptic and MuCo-3DHP dataset for generating AGR training data. But we can actually generate more poses by manipulating joint angles which may further improve the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Corresponding author. arXiv:2207.09949v1 [cs.CV] 20 Jul 2022 shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Projection geometry of a pinhole camera model. (a) shows the case where a person is in a standing pose. (b) shows the case with a different pose. Our 3D human pose estimation pipeline. (a) Given an image as input, it first estimates the AGR (i.e. bounding boxes and heatmaps) of all people. (b) Then, in Root Estimation Network (REN), it estimates the depth map of root joints from the AGR. The AGR and the depth map are integrated via depth based feature projection to estimate 3D root positions. (c) Finally, in Pose Estimation Network (PEN) for each root joint, we construct a feature volume around it to estimate a fine-grained 3D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Cross-view results (MRPE z ) of the three models on the CMU Panoptic dataset. The x-and y-axes represent the camera index in training and test, respectively. (b) Depth estimation results (MRPE z ) when the training poses are from the CMU Panoptic dataset, or from the mixture of "Panoptic+MuCo" datasets. The MuPoTS-3D dataset is the test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Some pose estimation results on the COCO (top) and MuPoTS-3D (bottom) datasets. (b) Typical failure cases due to occlusion and truncation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Given an image as input, we first estimate bounding boxes and</figDesc><table><row><cell></cell><cell></cell><cell cols="2">?$%&amp; ?</cell><cell cols="2">+ . ?'()* . /01! ,</cell><cell></cell><cell></cell><cell cols="2">? $%&amp; ? =</cell><cell>+ . ? 234( . /01! , + . 6234( . ?'()* . /01! ,</cell></row><row><cell>!</cell><cell></cell><cell>?</cell><cell>, ?</cell><cell cols="2">+ . ?'()* . /01! ?$%&amp;</cell><cell>!</cell><cell></cell><cell>?</cell><cell>, ?</cell><cell>+ . 6234( . ?'()* . /01! ? $%&amp;</cell></row><row><cell>+</cell><cell>? $%&amp;</cell><cell>!</cell><cell></cell><cell>? '()*</cell><cell>"</cell><cell>+</cell><cell>?$%&amp;</cell><cell>!</cell><cell>?234( "</cell></row><row><cell>,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,</cell><cell></cell><cell></cell><cell>6 234( =</cell><cell>? 234( ?'()*</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Training data</cell><cell>MuCo (green)</cell><cell>Test data MuCo (augmented)</cell></row><row><cell>TBS</cell><cell>green augmented</cell><cell>76.0 73.3</cell><cell>281.6 82.5</cell></row><row><cell>TIF</cell><cell>green augmented</cell><cell>57.3 58.7</cell><cell>168.8 65.5</cell></row><row><cell>BDR</cell><cell>green augmented</cell><cell>126.3 141.7</cell><cell>267.8 148.9</cell></row></table><note>Depth estimation results (MRPE z ) when we train the models on im- ages with green-screen background (green) or with mixture of green-screen and augmented background (augmented), respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison to the state-of-the-art methods on the CMU Panoptic dataset. The metric is MPJPE (mm).<ref type="bibr" target="#b36">[37]</ref> ? uses the GT depth when estimating absolute 3D poses so it is not fairly comparable to other methods. Our method does not use paired images and poses data for training but it still achieves smaller errors than other methods.</figDesc><table><row><cell>Method</cell><cell cols="5">Haggling Mafia Ultimatum Pizza Mean ?</cell></row><row><cell>Li et al . (HMOR) [37]  ?</cell><cell>50.9</cell><cell>50.5</cell><cell>50.7</cell><cell>68.2</cell><cell>55.1</cell></row><row><cell>PoPa et al . [27]</cell><cell>217.9</cell><cell>187.3</cell><cell>193.6</cell><cell>221.3</cell><cell>203.4</cell></row><row><cell>Zanfir et al . [42]</cell><cell>140.0</cell><cell>165.9</cell><cell>150.7</cell><cell>156.0</cell><cell>153.4</cell></row><row><cell>Moon et al . (RootNet) [23]</cell><cell>89.6</cell><cell>91.3</cell><cell>79.6</cell><cell>90.1</cell><cell>87.6</cell></row><row><cell>Zanfir et al . [43]</cell><cell>72.4</cell><cell>78.8</cell><cell>66.8</cell><cell>94.3</cell><cell>78.1</cell></row><row><cell>Zhen et al . (SMAP) [47]</cell><cell>63.1</cell><cell>60.3</cell><cell>56.6</cell><cell>67.1</cell><cell>61.8</cell></row><row><cell>Ours</cell><cell>54.1</cell><cell>61.6</cell><cell>54.6</cell><cell>65.4</cell><cell>58.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison to the state-of-the-art methods on MuPoTS-3D. Matched people only computes accuracy for GT poses which are matched to predictions and All people computes accuracy for all GT poses in the dataset. The methods are not strictly comparable because they may have different backbones.</figDesc><table><row><cell>Method</cell><cell cols="2">Matched people PCK abs ? PCKroot ?</cell><cell>All people PCK abs ?</cell></row><row><cell>Moon et al . (RootNet) [23]</cell><cell>31.8</cell><cell>31.0</cell><cell>31.5</cell></row><row><cell>Lin et al . (HDNet) [15]</cell><cell>35.2</cell><cell>39.4</cell><cell>-</cell></row><row><cell>Zhen et al . (SMAP) [47]</cell><cell>38.7</cell><cell>45.5</cell><cell>35.4</cell></row><row><cell>Veges et al . [35]</cell><cell>39.6</cell><cell>-</cell><cell>37.3</cell></row><row><cell>Sarandi et al . [30]</cell><cell>40.5</cell><cell>-</cell><cell>38.4</cell></row><row><cell>Li et al . (HMOR) [37]</cell><cell>-</cell><cell>-</cell><cell>43.8</cell></row><row><cell>Guo et al . [8]</cell><cell>39.6</cell><cell>-</cell><cell>39.2</cell></row><row><cell>Ours</cell><cell>47.0</cell><cell>53.5</cell><cell>44.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on Root Estimation Network (REN) and Pose Estimation Network (PEN) in our method. We report the MRPE and MRPE z (mm) on the test set of CMU Panoptic and MuPoTS-3D dataset.Training Data Generation Strategies We compare several training data generation strategies for different scenarios. First, when we have little knowledge about the testing camera view point, we can only uniformly sample camera views for the virtual camera to synthesize training data. The results are shown in</figDesc><table><row><cell>Method</cell><cell>REN</cell><cell>PEN</cell><cell cols="4">CMU Panoptic MRPE ? MRPEz ? MRPE ? MRPEz ? MuPoTS-3D</cell></row><row><cell>(a) DE</cell><cell>2D</cell><cell>?</cell><cell>113.9</cell><cell>104.1</cell><cell>282.0</cell><cell>245.4</cell></row><row><cell cols="2">(b) REN 2D+3D</cell><cell>?</cell><cell>115.7</cell><cell>93.6</cell><cell>217.5</cell><cell>165.0</cell></row><row><cell cols="2">(c) Ours 2D+3D</cell><cell>?</cell><cell>97.0</cell><cell>86.0</cell><cell>204.0</cell><cell>159.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Impact of training data generation strategies. The metric is MRPE z (mm). We can see that by training a universal model, our method achieves smaller depth estimation errors than the model trained on a single camera.</figDesc><table><row><cell>Training Camera View</cell><cell cols="3">Test Camera view 14 16 19</cell></row><row><cell>14</cell><cell>-</cell><cell>170.3</cell><cell>294.6</cell></row><row><cell>16</cell><cell>454.6</cell><cell>-</cell><cell>198.6</cell></row><row><cell>19</cell><cell>621.3</cell><cell>385.1</cell><cell>-</cell></row><row><cell>Random</cell><cell cols="3">273.8 134.0 155.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement. This work was supported in part by MOST-2018AAA0102004</head><p>and NSFC-62061136001.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Long-term human motion prediction with scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Absposelifter: Absolute 3d human pose lifting network from a single noisy 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual networks based 3d multi-person pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Locally connected network for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-person 3d human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compressed volumetric heatmaps for multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular 3d multi-person pose estimation via predicting factorised correction factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hdnet: Human depth estimation for multi-person camera-space localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context modeling in 3d human pose estimation: A unified perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>3DV. pp. 120-130</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lcr-net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metrabs: Metric-scale truncationrobust heatmaps for absolute 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Absolute human pose estimation with depth prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?rincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN. pp</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-person absolute 3d human pose estimation with weak depth supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?rincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hmor: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Robust 3d human pose estimation from single images or video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d interpreter networks for viewer-centered wireframe modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Voxeltrack: Multi-person 3d human pose estimation and tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fusing wearable imus with multi-view images for human pose estimation: A geometric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adafuse: Adaptive multiview fusion for accurate human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Smap: Single-shot multi-person absolute 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Reconstructing nba players</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
