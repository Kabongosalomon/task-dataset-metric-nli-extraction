<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
							<email>takasu@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The Graduate University for Advanced Studies</orgName>
								<orgName type="institution" key="instit2">SOKENDAI</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embedding aims to predict the missing relations between entities in knowledge graphs. Tensordecomposition-based models, such as ComplEx, provide a good trade-off between efficiency and expressiveness, that is crucial because of the large size of real world knowledge graphs. The recent multi-partition embedding interaction (MEI) model subsumes these models by using the block term tensor format and provides a systematic solution for the trade-off. However, MEI has several drawbacks, some of which carried from its subsumed tensor-decomposition-based models. In this paper, we address these drawbacks and introduce the Multi-partition Embedding Interaction iMproved beyond block term format (MEIM) model, with independent core tensor for ensemble effects and soft orthogonality for max-rank mapping, in addition to multipartition embedding. MEIM improves expressiveness while still being highly efficient, helping it to outperform strong baselines and achieve state-of-the-art results on difficult link prediction benchmarks using fairly small embedding sizes. The source code is released at https://github.com/tranhungnghiep/MEIM-KGE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs are used to represent relational information between entities. There are large real world knowledge graphs such as <ref type="bibr">YAGO [Mahdisoltani et al., 2015]</ref> containing millions of entities. These knowledge graphs and their representations can be used in artificial intelligent applications such as semantic queries and question answering <ref type="bibr" target="#b7">[Tran and Takasu, 2019b]</ref>  <ref type="bibr" target="#b9">[Tran, 2020]</ref>.</p><p>Knowledge graph embedding aims to predict the missing relations between entities in knowledge graphs. They usually represents a triple (h, t, r) as embeddings and use a score function to compute its matching score S(h, t, r). The score function defines the interaction mechanism such as bilinear map between the embeddings and the interaction pattern specifying how embedding entries interact with each other.</p><p>On large real world knowledge graphs, a good tradeoff between efficiency and expressiveness is crucial. Tensordecomposition-based models usually provide good trade-off by designing special interaction mechanisms with sparse and</p><p>In the International Joint Conference on Artificial Intelligence (IJCAI), 2022. expressive interaction patterns. They have been subsumed by the recent multi-partition embedding interaction (MEI) model <ref type="bibr" target="#b8">[Tran and Takasu, 2020]</ref>, that divides the embedding vector into multiple partitions for sparsity, automatically learns the local interaction patterns on each partition for expressiveness, then combines the local scores to get the full interaction score. The trade-off between efficiency and expressiveness can be systematically controlled by changing the partition size and learning the interaction patterns through the core tensor of the block term tensor format.</p><p>However, by looking from two perspectives beyond the scope of block term format, that is, ensemble boosting effects and max-rank relational mapping, we identify two drawbacks of MEI and related tensor-decomposition-based models. First, using one core tensor for all partitions leads to similar local interactions that may harm the ensemble effects of the full interaction in MEI. Second, the relational mapping matrices in MEI are generated from relatively small relation embeddings, that may make the model prone to degenerate.</p><p>In this paper, we propose the novel Multi-partition Embedding Interaction iMproved beyond block term format (MEIM) model with two techniques, namely independent core tensor to improve ensemble effects and max-rank mapping by soft orthogonality to improve model expressiveness. This introduces new aspects to tensor-decomposition-based models exploiting ensemble boosting effects and max-rank relational mapping, in addition to multi-partition embedding by MEI. MEIM improves expressiveness while still being highly efficient, helping it to outperform strong baselines and achieve new state-of-the-art results on difficult link prediction benchmarks using fairly small embedding sizes.</p><p>In general, our contributions include the following.</p><p>? We propose MEIM, a novel tensor-decomposition-based model with independent core tensor and max-rank mapping by soft orthogonality to improve expressiveness.</p><p>? We extensively experiment to show that the proposed model is highly efficient and expressive, and achieves state-of-the-art results.</p><p>? We analyze the proposed model to clarify its characteristics and demonstrate the critical advantages of its soft orthogonality compared to previous models.  <ref type="figure">Figure 1</ref>: MEIM architecture: multi-partition embedding interaction improved with independent core tensors and max-rank mapping matrices.</p><p>The new aspects are noted by the grey boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations and Definitions</head><p>A knowledge graph is a collection of triples D, with each triple denoted as a tuple (h, t, r), where h and t are head and tail entities in the entity set E and r belongs to the relation set R. A knowledge graph can also be represented by a thirdorder binary data tensor G ? {0, 1} |E|?|E|?|R| , where each entry g htr = 1 ? (h, t, r) exists in D. The contextual link prediction task, also called link prediction, aims to predict the connection between two entities given a relation as the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tensor-Decomposition-based Knowledge Graph Embedding Models</head><p>There are several knowledge graph embedding models that adapt tensor formats to represent the knowledge graph data tensor and adapt tensor decomposition methods to solve the link prediction task. This approach has led to some of the best models in terms of efficiency and expressiveness. <ref type="bibr">RESCAL [Nickel et al., 2011</ref>] is an early model that adapts Tucker2 decomposition to compute the embedding interaction score by a bilinear map</p><formula xml:id="formula_0">S(h, t, r) = h M r t,<label>(1)</label></formula><p>where h, t ? R D , and M r ? R D?D are the embeddings of h, t, and r, respectively, with D being the embedding size. RESCAL is expressive, but the mapping matrix M r grows quadratically with embedding size, making the model expensive and prone to overfitting. The most simple model is DistMult <ref type="bibr">[Yang et al., 2015]</ref>, that uses a sparse diagonal mapping matrix M r = diag(r), where r ? R D is the relation embedding vector. It is an adaptation of CP decomposition <ref type="bibr" target="#b3">[Kolda and Bader, 2009]</ref>, with the score function written as a trilinear product</p><formula xml:id="formula_1">S(h, t, r) = h diag(r)t = i h i t i r i = h, t, r .</formula><p>(2) Recent models aim to be more expressive than DistMult but still efficient by designing new special interaction mechanisms between the embeddings. It has been shown that these models are equivalent to bilinear model with sparse blockdiagonal mapping matrix M r ,</p><formula xml:id="formula_2">M r = ? ? ? M r,1 0 0 0 . . . 0 0 0 M r,K ? ? ? ,<label>(3)</label></formula><p>where the matrix block M r,k have a special interaction pattern resulted from the specific interaction mechanism <ref type="bibr" target="#b8">[Tran and Takasu, 2020]</ref>. For example, ComplEx <ref type="bibr" target="#b9">[Trouillon et al., 2016]</ref> uses complex-valued trilinear product, resulting in the 2-dimensional rotation-scaling pattern on complex plane,</p><formula xml:id="formula_3">M r,k = real(r) k ?imaginary(r) k imaginary(r) k real(r) k .<label>(4)</label></formula><p>Sparsity and the interaction pattern are crucial concepts in previous tensor-decomposition-based models. They are generalized by the recent multi-partition embedding interaction (MEI) model <ref type="bibr" target="#b8">[Tran and Takasu, 2020]</ref>, that divides the embedding vector into multiple partitions, learns the local interaction patterns on each partition, then sums the local scores to get the full interaction score. The trade-off between efficiency and expressiveness can be systematically controlled by changing the partition size and learning the interaction patterns through the core tensor of the block term tensor format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-partition Embedding Interaction iMproved Beyond Block Term Format</head><p>In this section, we propose the MEIM model that introduces new aspects to tensor-decomposition-based models exploiting ensemble boosting effects and max-rank relational mapping, in addition to multi-partition embedding by MEI, as illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Score Function</head><p>Inspired by the generality of MEI, we adopt the multipartition embedding representation, with Tucker format <ref type="bibr" target="#b10">[Tucker, 1966]</ref> for local interaction and block term format <ref type="bibr" target="#b1">[De Lathauwer, 2008]</ref> for full interaction. Following convention of MEI, triple (h, t, r) provides the embedding vectors h, t ? R De , and r ? R Dr that are treated as multi-partition embedding matrices H, T ? R K?Ce , and R ? R K?Cr , respectively. The score function of MEIM is written as</p><formula xml:id="formula_4">S(h, t, r; ?) = K k=1 S k (h, t, r; ?) (5) = K k=1 W k?1 h k:?2 t k:?3 r k: (6) = K k=1 h k: (W k?3 r k: )t k: (7) = K k=1 h k: M W,r,k t k: ,<label>(8)</label></formula><p>where h k: , t k: , and r k: are the embedding partitions k 1 ; W k ? R Ce?Ce?Cr is the core tensor at partition k;? n denotes the n-mode tensor product with a vector; and M W,r,k ? R Ce?Ce is the bilinear mapping matrix.</p><p>Note that Eq. 5 shows the sum of local interaction scores S k (h, t, r; ?). Eq. 6 shows the block term format. Eq. 8 shows the bilinear format with block-diagonal mapping matrix, in which each block M W,r,k is generated by W k?3 r k: . Eq. 8 can be seen as a dynamic linear neural network <ref type="bibr" target="#b8">[Tran and Takasu, 2020]</ref>, where the hidden layer M W,r,k is generated by another hyper neural network <ref type="bibr" target="#b3">[Ha et al., 2016]</ref> with the core tensor W k as its weights and r k: as its input.</p><p>The main novelties of MEIM are in better parameterization of the core tensor and mapping matrix, by looking from two perspectives beyond the scope of block term format, that is, ensemble boosting effects and max-rank relational mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Core Tensor for Ensemble Boosting</head><p>Technically, the score function in Eq. 5 can be seen as an ensemble system of K local interactions by summing their scores. This works in a similar manner to gradient boosting <ref type="bibr" target="#b4">[Mason et al., 1999]</ref> because the full interaction score is computed at training time and gradients are back-propagated to optimize all local interactions together, enabling them to implicitly minimize the residual error of each other.</p><p>The MEI model briefly mentioned this perspective but did not actively exploit the ensemble boosting effects. MEI used only one core tensor for all partitions by enforcing the following shared core tensor constraint:</p><formula xml:id="formula_5">W 1 = W 2 = ? ? ? = W K = W,<label>(9)</label></formula><p>where W ? R C?C?C is the learned shared core tensor, although block term format may have different core tensors. MEI learns one interaction pattern for all partitions, similar to that of other tensor-decomposition-based models such as ComplEx. However, this may be harmful to the performance of an ensemble system because of the similar local interaction patterns instead of combining different patterns. To exploit the ensemble boosting effects, it is crucial to promote independence and difference between the local interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Independent Core Tensor Parameterization</head><p>There are several potential ways to resolving the problem of independence and difference between the local interactions. First, we may enforce the predicted scores to be different:</p><p>?k, l ? {1, . . . , K}, k = l : S k (?, ?, ?) = S l (?, ?, ?). (10) 1 Partitions are column vectors, transpose notation is omitted for simplicity. Illustration as row is just for easy visualization.</p><p>However, because this forces the predicted scores to be not the same, the local interactions cannot be correct at the same time. Thus, this is an overstrong constraint that may cause more harm than good. Another way is to explicitly enforce different parameters of the local interaction functions, such as the core tensors:</p><formula xml:id="formula_6">?k, l ? {1, . . . , K}, k = l : W k = W l .<label>(11)</label></formula><p>However, such explicit difference constraints do not guarantee different interactions because of symmetric swapping or scaling reparameterizations of the core tensors and the embeddings. For example, scaling the core tensor by x and the corresponding embedding by 1/x do not change the score. Therefore, in this paper, we do not use difference constraints, but instead independence constraints, denoted as:</p><formula xml:id="formula_7">W 1 ? W 2 ? ? ? ? ? W K .<label>(12)</label></formula><p>For simplicity, we let the model implicitly learn independent core tensors from data automatically by removing the constraint in Eq. 9. Using independent core tensors enables the model to learn independent and possibly different local interactions to improve the ensemble boosting effects. More advanced methods based on further studying the ensemble boosting effects are left for future work. Note that MEI is easy to implement in common deep learning frameworks by using only matrix product. However, independent core tensor makes the multi-partition tensor product in MEIM more difficult to implement efficiently. To resolve this problem, we stack together K independent core tensors as a fourth-order tensor:</p><formula xml:id="formula_8">W = [W 1 , W 2 , . . . , W K ] ,<label>(13)</label></formula><p>where W ? R K?C?C?C is used for the Einstein sum notation technique to create an efficient implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Max-Rank Relational Mapping</head><p>MEI is a contextual link prediction model with two components, the multi-partition relation-based contextual mapping and the simple dot product matching. The MEI model used a linear mapping where M W,r,k can be any C e ?C e square matrix. However, when some columns of M W,r,k are dependent, it becomes a singular matrix with rank smaller than C e . Such singular matrix would map the head embedding to a subspace with lower dimension, thus reduce the embedding space effective size. It may make the score function degenerate to always produces score 0 for some entities, thus, drastically reduces the expressiveness of the model. This may become a critical issue in MEI because M W,r,k is generated from a relatively small relation embedding partition r k: as shown in Eq. 8. To resolve this problem, the mapping matrices M W,r,k need to have max rank, that may not always be full rank C e , but should be as large as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-Rank Mapping Matrix by Soft Orthogonality</head><p>There are several different types of full rank matrices. A particularly interesting case is the orthogonal matrices, that act as the rotational transformations when the determinant is 1 and an additional reflection across the origin when the determinant is ?1. These matrices have linearly independent column vectors with unit Frobenius norms, written as the constraints:</p><formula xml:id="formula_9">?k ? {1, . . . , K} : M W,r,k M W,r,k = I,<label>(14)</label></formula><p>where I ? R Ce?Ce is the identity matrix. In addition, note that the mapping matrices are generated by the core tensors and the relation embedding vectors. To reduce the search space and make optimization easier, we also constrain each relation partition r k: to have unit norm:</p><formula xml:id="formula_10">?k ? {1, . . . , K} : r k: r k: = 1.<label>(15)</label></formula><p>Because of the way M W,r,k are generated, it may be difficult to enforce the mapping matrices to be strictly orthogonal. Even if it is possible, it may not always be preferable because the main objective is link prediction. Geometrically speaking, the orthogonal matrices form the Stiefel manifold <ref type="bibr">[Sok?? and Park, 2020]</ref>, we need to make sure that the mapping matrices stay as close to this manifold as possible without hurting the link prediction accuracy. Using Lagrangian relaxation, the hard constraints in Eq. 14 and Eq. 15 can be converted to the following soft orthogonality loss term:</p><formula xml:id="formula_11">L ortho = ? ortho K k=1 ||M W,r,k M W,r,k ? I|| 2 2 . + ? unitnorm K k=1 |r k: r k: ? 1| p ,<label>(16)</label></formula><p>where ? ortho , ? unitnorm , and p are hyperparameters. To penalize large differences from unit norm, p = 3 is used. When ? ortho is small, the model tends to learn linear mappings. When it is large, the model tends to learn strictly orthogonal mappings. The soft orthogonality loss enables the model to balance between orthogonality and link prediction. Minimizing this loss effectively influences the column vectors to be orthogonal and have unit norms, pushes the matrix M W,r,k close to the Stiefel manifold and maximizes its rank in balance with the link prediction objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Connections to Previous Models</head><p>Compared to the MEI model, MEIM inherits its key benefit, that is, the systematic trade-off between efficiency and expressiveness by controlling the partition size and learning the interaction patterns. About computational cost, the number of parameters in MEIM is similar to that of MEI, except that the independent core tensors use KC 2 e C r instead of C 2 e C r parameters, but this becomes less relevant when the number of entities and relations are large, and MEIM is as fast as MEI by using Einstein sum notation technique for implementation of independent core tensor product.</p><p>On the other hand, MEIM introduces new parameterization techniques to address some major drawbacks of MEI. With independent core tensor, MEIM can learn independent local interaction patterns to improve ensemble effects. With soft orthogonality, MEIM balances the orthogonality and link prediction objectives to learn max-rank mapping that improves expressiveness of the model.</p><p>Note that, coincidentally, some knowledge graph embedding models also try to use orthogonal mapping, not to maximize the mapping matrix rank but to model some relational patterns, such as symmetry, inversion, and composition <ref type="bibr">[Sun et al., 2019]</ref>. There are many different ways to design and use orthogonal mapping in knowledge graph embedding. For example, <ref type="bibr">RotatE [Sun et al., 2019]</ref> uses rotation in the complex plane, QuatE <ref type="bibr" target="#b10">[Zhang et al., 2019]</ref> uses rotation in the quaternion space, <ref type="bibr">GC-OTE [Tang et al., 2020]</ref> uses rotation by the Gram Schmidt process, and RotH <ref type="bibr">[Chami et al., 2020]</ref> uses rotation by the Givens matrix. However, there are some important differences to our method. First, all of them use rigid orthogonality where the mapping matrix is strictly orthogonal. This may be suboptimal because orthogonal mapping may not always be useful for link prediction, and even when it is useful, strictly orthogonal mapping may reduce the potential result. In contrast, we approach the problem from the perspective of maximizing the mapping matrix rank, thus, we use soft orthogonality as a means instead of rigid orthogonality. Soft orthogonality can avoid the above problems because we can choose how to balance the orthogonality and the link prediction objectives for each dataset, as we will show this is crucial for knowledge graph embedding in experiments. Second, MEIM has a totally different model architecture compared to previous orthogonal models, because the mapping matrix in MEIM is not learned directly but generated. Finally, MEIM is a semantic matching model, with the matching score computed by inner product, whereas other orthogonal models are translation-based model, in which the score is computed by distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Loss Function and Learning</head><p>The model is learned via the link prediction task that can be modeled as a multi-class classification problem. There are two directions, classifying correct tail entity t among all entities given a (h, r) pair, and classifying correct head entity h among all entities given a (t, r) pair. Following recent practice, we use the 1-vs-all and k-vs-all sampling methods with the softmax cross-entropy loss function <ref type="bibr" target="#b1">[Dettmers et al., 2018]</ref> [Ruffinelli et al., 2020].</p><p>First, using 1-vs-all and k-vs-all sampling, we define the groundtruth categorical distributions over all entities E given (h, r) and given (t, r), denotedp h?r andp? tr , respectively. Second, we compute the corresponding predicted distributions p h?r and p? tr using the softmax function on the matching scores. The link prediction loss is the cross-entropy summed over training data D: L link prediction = ? (h,t,r)?D ? ?Eph?r log p h?r + ? ?Ep?tr log p? tr . <ref type="formula" target="#formula_0">(17)</ref> The final loss function is the sum of link prediction loss and soft orthogonality loss:  <ref type="bibr">-237 14,541 237 272,115 17,535 20,466</ref> 18.71 YAGO3-10 123,182 37 1,079,040 5,000 5,000 8.76 We did not use some recent complemental techniques for training, regularization, and additional losses that can be used with existing methods including MEIM. Adding them to MEIM to further improve the result is left for future work.</p><formula xml:id="formula_12">L = L link prediction + L ortho ,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets. We use three standard benchmark datasets, as shown in <ref type="table" target="#tab_2">Table 1</ref>. <ref type="bibr">WN18RR [Dettmers et al., 2018]</ref> is a subset of WordNet containing lexical information. FB15K-237 <ref type="bibr" target="#b5">[Toutanova and Chen, 2015</ref>] is a subset of Freebase containing general facts. In addition, YAGO3-10 [Mahdisoltani et al., 2015] is a large and very competitive dataset containing general facts from Wikipedia. Evaluations. We evaluate on the link prediction task. For each true triple (h, t, r) in the test set, we replace h and t by every other entity to generate corrupted triples (h , t, r) and (h, t , r), respectively. The model tries to rank the true triple (h, t, r) before the corrupted triples based on the score S. We compute M RR (mean reciprocal rank) and H@k for k ? {1, 3, 10} (how many triples correctly ranked in the top k) <ref type="bibr" target="#b9">[Trouillon et al., 2016]</ref>. The higher M RR and H@k are, the better the model performs. Filtered metrics are used to avoid penalization when ranking other true triples before the current target triple <ref type="bibr" target="#b0">[Bordes et al., 2013]</ref>.</p><p>Note that MEIM can solve other tasks by converting them to contextual link prediction task. For example, by defining the alignment relation r a , we have the alignment triples of the form (e 1 , e 2 , r a ), that can be used directly in MEIM for solving entity alignment. These tasks are left for future work. Implementations. The MEIM model is implemented as a neural network using PyTorch. Following MEI, we use embedding size K = 3, C = 100 on WN18RR and FB15K-237, and K = 5, C = 100 on YAGO3-10. They are equivalent in terms of model size to ComplEx with embedding size 190 on WN18RR, 250 on FB15K-237, and 270 on YAGO3-10, that are relatively small compared to most related work. By preliminary experiments, we use k-vs-all sampling on WN18RR and 1-vs-all sampling on other datasets. We use batch size 1024 and learning rate 3e-3, with exponential decay 0.995 on YAGO3-10 and 0.99775 on other datasets. Dropout and batch normalization are used on the input and hidden layers, that is, on h and h M r , respectively. Other hyperparameters are tuned by grid search, with grids [0, 0.75] and step size 0.01 for drop rates, {1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 0} for ? ortho , {1, 1e-1, 1e-2, 1e-3, 5e-4, 1e-4, 0} for ? unitnorm . The following values were found. On WN18RR, input drop rate is 0.71, hidden drop rate is 0.67, ? ortho = 1e-1, ? unitnorm = 5e-4. On FB15K-237, input drop rate is 0.66, hidden drop rate is 0.67, ? ortho = 0, ? unitnorm = 0. On YAGO3-10, input drop rate is   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parameter Efficiency</head><p>We first compare the results and model sizes of MEIM to that of popular baselines as shown in <ref type="table" target="#tab_4">Table 2</ref>. We can see that MEIM strongly outperforms these baselines while using roughly similar number of parameters.</p><p>In addition, <ref type="table" target="#tab_5">Table 3</ref> shows the scalability of MEIM and other baselines at different model sizes. MEIM outperforms the baselines at every model size and achieves the best result overall. This demonstrates that MEIM achieves a better balance between efficiency and expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Results</head><p>The main results for link prediction are shown in <ref type="table" target="#tab_7">Table 4</ref>. In general, MEIM achieves good result on all three datasets, including the very large and difficult YAGO3-10 dataset. Considering the most important and robust metric MRR, MEIM outperforms all baselines by a large margin on all datasets.</p><p>The strongest baselines on FB15K-237 and YAGO3-10 include the MEI model. This demonstrates that MEI was a   strong model, but MEIM successfully addresses the drawbacks of MEI and its subsumed tensor-decomposition-based models to significantly improve the result on all datasets. Another strong baseline is the RotH model on WN18RR, but the results on FB15K-237 are quite weak. We notice that most baselines usually have good results on one dataset and poor results on other datasets. This sets MEIM apart as it can achieve good results on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analyses and Discussions</head><p>Now we look into the characteristics of the model to understand why it gives good result. <ref type="table" target="#tab_8">Table 5</ref> shows the main ablation results of MEIM, in which each feature is removed from the full model to see how much it contributes to the final results. For comparison, we also ablate the multi-partition embedding inherited from MEI.  In general, we can see that removing the features decreases the results, which confirms their effectiveness. In particular, the independent core tensor works well on all three datasets. It strongly contributes to the full model results on FB15K-237, and slightly but consistently contributes to the final results on WN18RR and YAGO3-10. The effects of soft orthogonality is more complicated. On WN18RR and YAGO3-10, soft orthogonality is crucial to achieve the best results. However, it is harmful on FB15K-237 and was not included in the full model. This mixed effect guarantees further detailed investigation as follows. <ref type="table" target="#tab_9">Table 6</ref> shows the results when ? ortho is changing from no orthogonality to strong orthogonality. We see that strong orthogonality works well on WN18RR, average orthogonality works well on YAGO3-10, but none orthogonality works on FB15K-237. These results empirically demonstrate the weakness of previous models compared to MEIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effects of Soft Orthogonality</head><p>For example, models with rigid orthogonality such as QuatE and RotH get strong results on WN18RR but weak on FB15K-237, whereas models without orthogonality such as RESCAL, TuckER, and MEI get strong results on FB15K-237 but weak on WN18RR. In contrast, MEIM with soft orthogonality can get strong results on both WN18RR and FB15K-237. More importantly, the large and difficult YAGO3-10 dataset requires average orthogonality. Thus, only MEIM can get the best results on this dataset. This shows a crucial advantage of MEIM, because its soft orthogonality can be tuned to work optimally on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Knowledge graph embedding methods can be categorized based on how they compute the matching score.</p><p>Tensor-decomposition-based models adapt tensor representation formats such as CP format, Tucker format, and block term format to represent the knowledge graph data tensor <ref type="bibr" target="#b3">[Kolda and Bader, 2009]</ref>. They make up most of recent state-of-the-art models such as ComplEx <ref type="bibr" target="#b9">[Trouillon et al., 2016]</ref>, <ref type="bibr">SimplE [Kazemi and Poole, 2018]</ref>, TuckER <ref type="bibr" target="#b0">[Bala?evi? et al., 2019]</ref>, and MEI <ref type="bibr" target="#b8">[Tran and Takasu, 2020]</ref>. They often achieve good trade-off between efficiency and expressiveness. Especially, the recent MEI model provides a systematic solution for this trade-off. However, they have some limitations affecting the ensemble effects and making them prone to degenerate, that we address in this paper.</p><p>Neural-network-based models use a neural network to compute the matching score, such as ConvE <ref type="bibr" target="#b1">[Dettmers et al., 2018]</ref> using convolutional neural networks, <ref type="bibr">CompGCN [Vashishth et al., 2020b]</ref> using graph convolutional networks. These models are generally more expensive but not always get better results than tensor-decomposition-based models.</p><p>Translation-based models use geometrical distance to compute the score, with relation embeddings act as the translation vectors, such as TransE <ref type="bibr" target="#b0">[Bordes et al., 2013]</ref>. These models are efficient and intuitive, but they have limitations in expressive power <ref type="bibr" target="#b3">[Kazemi and Poole, 2018]</ref>.</p><p>There are several ways to use orthogonality in knowledge graph embedding, such as RotatE <ref type="bibr">[Sun et al., 2019]</ref> using complex product, Quaternion <ref type="bibr" target="#b6">[Tran and Takasu, 2019a]</ref> and QuatE <ref type="bibr" target="#b10">[Zhang et al., 2019]</ref> using quaternion product, <ref type="bibr">GC-OTE [Tang et al., 2020]</ref> using the Gram Schmidt process, and RotH <ref type="bibr">[Chami et al., 2020]</ref> using the Givens matrix. These models usually compute the score using distance, differently from MEIM. Moreover, they use rigid orthogonality that is harmful on some datasets and potentially suboptimal on other datasets compared to soft orthogonality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduce new aspects to tensordecomposition-based models exploiting ensemble boosting effects and max-rank relational mapping, in addition to multipartition embedding by MEI. We propose the MEIM model with two new techniques, namely independent core tensor to improve ensemble effects and max-rank mapping by soft orthogonality to improve expressiveness. MEIM achieves stateof-the-art link prediction results, including the large and difficult YAGO3-10 dataset, using fairly small embedding sizes. Moreover, we analyze and demonstrate the limitations of previous rigid orthogonality models and show that MEIM with soft orthogonality works well on multiple datasets.</p><p>For future work, it is promising to continue research on combining deep learning techniques and tensor decomposition. It is also interesting to use MEIM to solve tasks such as entity alignment and recommendation by converting them to contextual link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extra Experiments</head><p>In this section, we present some extra experimental results that may be useful but did not fit in the conference page limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Detailed Link Prediction Results on WN18RR</head><p>In addition to the overall result, we look into the detailed relation result on WN18RR in <ref type="table" target="#tab_10">Table 7</ref>. In general, MEI and MEIM outperform RotatE on all relations in WN18RR. MEIM achieves best results for almost all relations, and comes closely second on a few others. This shows that MEIM works consistently well on different relation types. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Number of Partitions and Partition Size</head><p>To examine the trade off between the core tensor and the embedding vectors, we keep the total number of parameters roughly the same and grid search different configurations on the FB15K-237 validation set. <ref type="table" target="#tab_11">Table 8</ref> shows that the MEIM model with 3 partitions each of size 100 achieves the best results. This is in agreement with previous theoretical prediction in the MEI paper <ref type="bibr" target="#b8">[Tran and Takasu, 2020]</ref>. In practice, the best model should have the number of partitions K &gt; 1 for ensemble effects, and the partition size C large enough for expressiveness but not too large for efficiency. As a rule of thumb, we can start with 3 ? 100 and grid search further. A.3 Embedding sizes for comparable model sizes <ref type="table" target="#tab_12">Table 9</ref> shows the MEI embedding sizes used in this paper to make its model sizes comparable to that of MEIM. Please refer to the source code GitHub page for more information.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Parameter efficiency compared to popular baselines.</figDesc><table><row><cell>ComplEx-N3</cell><cell>MEI</cell><cell>MEIM</cell></row><row><cell>#params MRR H@10</cell><cell>MRR H@10</cell><cell>MRR H@10</cell></row><row><cell>1.2 M 0.404 0.439</cell><cell>0.448 0.500</cell><cell>0.460 0.525</cell></row><row><cell>3.8 M 0.453 0.507</cell><cell>0.476 0.544</cell><cell>0.488 0.557</cell></row><row><cell>6.5 M 0.464 0.528</cell><cell>0.481 0.547</cell><cell>0.494 0.569</cell></row><row><cell>15.3 M 0.473 0.550</cell><cell>0.481 0.544</cell><cell>0.499 0.574</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Model scalability on WN18RR validation set.</figDesc><table><row><cell>Baselines. The main baselines are MEI and tensor-</cell></row><row><cell>decomposition-based models such as ComplEx, RotatE,</cell></row><row><cell>QuatE that we aim to improve. MEI results are reproduced</cell></row><row><cell>in this paper using comparable model sizes and recent train-</cell></row><row><cell>ing techniques that lead to better performance than previously</cell></row><row><cell>reported. We also compare to translation-based models such</cell></row><row><cell>as TransE, neural-network-based models such as ConvE, and</cell></row><row><cell>recent models such as RotH. We tried to evaluate against pop-</cell></row><row><cell>ular and strong baselines with comparable settings and eval-</cell></row><row><cell>uation protocol for fair and informative comparisons. These</cell></row><row><cell>exclude incomparable results that use extra data, excessively</cell></row><row><cell>large model size, complemental techniques, or inappropriate</cell></row><row><cell>evaluation protocol [Sun et al., 2020]. Note that even at nor-</cell></row><row><cell>mal model size, MEIM can get state-of-the-art results.</cell></row></table><note>0.1, hidden drop rate is 0.15, ? ortho = 1e-3, ? unitnorm = 0. Hy- perparameters were tuned to maximize the validation MRR.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Link prediction results on WN18RR, FB15K-237, and YAGO3-10. ? are reported in [Ruffinelli et al., 2020], in [Chami et al., 2020], ? in [Rossi et al., 2021], YAGO3-10 in [Rossi et al., 2021], are reproduced here, other results are reported in their papers.</figDesc><table><row><cell></cell><cell>Ablation</cell><cell>MRR H@10</cell></row><row><cell></cell><cell>Full model</cell><cell>0.499 0.574</cell></row><row><cell>WN18RR</cell><cell cols="2">without Multi-partition Embedding 0.492 0.572 without Independent Core Tensor 0.498 0.573</cell></row><row><cell></cell><cell>without Soft Orthogonality</cell><cell>0.483 0.550</cell></row><row><cell></cell><cell>Full model</cell><cell>0.375 0.562</cell></row><row><cell>FB15K-237</cell><cell cols="2">without Multi-partition Embedding 0.372 0.554</cell></row><row><cell></cell><cell cols="2">without Independent Core Tensor 0.370 0.554</cell></row><row><cell></cell><cell>Full model</cell><cell>0.582 0.710</cell></row><row><cell>YAGO3-10</cell><cell cols="2">without Multi-partition Embedding 0.580 0.706 without Independent Core Tensor 0.581 0.708</cell></row><row><cell></cell><cell>without Soft Orthogonality</cell><cell>0.574 0.708</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Model ablation result on the validation sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The effects of soft orthogonality on the validation sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Test set MRR for each relation in WN18RR. RotatE results are reported in [Zhang et al., 2019].</figDesc><table><row><cell>Relation</cell><cell>RotatE</cell><cell>MEI</cell><cell>MEIM</cell></row><row><cell>similar to</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>hypernym</cell><cell>0.148</cell><cell>0.162</cell><cell>0.200</cell></row><row><cell>instance hypernym</cell><cell>0.318</cell><cell>0.405</cell><cell>0.411</cell></row><row><cell>member meronym</cell><cell>0.232</cell><cell>0.235</cell><cell>0.251</cell></row><row><cell>member of domain region</cell><cell>0.200</cell><cell>0.270</cell><cell>0.322</cell></row><row><cell>member of domain usage</cell><cell>0.318</cell><cell>0.295</cell><cell>0.352</cell></row><row><cell>synset domain topic of</cell><cell>0.341</cell><cell>0.359</cell><cell>0.418</cell></row><row><cell>also see</cell><cell>0.585</cell><cell>0.616</cell><cell>0.620</cell></row><row><cell>has part</cell><cell>0.184</cell><cell>0.203</cell><cell>0.211</cell></row><row><cell>verb group</cell><cell>0.943</cell><cell>0.974</cell><cell>0.969</cell></row><row><cell>derivationally related form</cell><cell>0.947</cell><cell>0.959</cell><cell>0.955</cell></row><row><cell>All relations</cell><cell>0.476</cell><cell>0.481</cell><cell>0.499</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>The trade-off (K ? C) between the number of partitions (K) and partition size (C) on FB15K-237 validation set.</figDesc><table><row><cell cols="4">Embedding size #params MRR H@10</cell></row><row><cell>500 ? 1</cell><cell>7.39 M</cell><cell>0.368</cell><cell>0.552</cell></row><row><cell>250 ? 2</cell><cell>7.39 M</cell><cell>0.367</cell><cell>0.549</cell></row><row><cell>50 ? 10</cell><cell>7.44 M</cell><cell>0.366</cell><cell>0.551</cell></row><row><cell>20 ? 25</cell><cell>7.70 M</cell><cell>0.369</cell><cell>0.556</cell></row><row><cell>9 ? 50</cell><cell>7.78 M</cell><cell>0.371</cell><cell>0.558</cell></row><row><cell>3 ? 100</cell><cell>7.43 M</cell><cell>0.375</cell><cell>0.562</cell></row><row><cell>1 ? 170</cell><cell>7.43 M</cell><cell>0.372</cell><cell>0.554</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Embedding sizes for comparable MEI model sizes.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Cross-ministerial Strategic Innovation Promotion Program (SIP) Second Phase, "Bigdata and AI-enabled Cyberspace Technologies" by the New Energy and Industrial Technology Development Organization (NEDO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bala?evi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher R?</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decompositions of a Higher-Order Tensor in Block Terms-Part II: Definitions and Uniqueness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lathauwer ; Lieven De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dettmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<editor>Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel</editor>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
	<note>Convolutional 2D Knowledge Graph Embeddings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalized Translation-based Embedding of Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="951" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Ba; Bader</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="455" to="500" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sok?? and Park, 2020] Piotr Aleksander Sok?? and Il Memming Park. Information Geometry of Orthogonal Initializations and Training</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2713" to="2722" />
		</imprint>
	</monogr>
	<note>Proceedings of the Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and Their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and Their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takasu ; Hung-Nghiep</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Science for Industry 4.0 Workshop at EDBT/ICDT</title>
		<meeting>the Data Science for Industry 4.0 Workshop at EDBT/ICDT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takasu ; Hung-Nghiep</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Theory and Practice of Digital Libraries</title>
		<meeting>the 23rd International Conference on Theory and Practice of Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Artificial Intelligence</title>
		<meeting>the European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
	<note>and Takasu, 2020</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-Relational Embedding for Knowledge Graph Representation and Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran ; Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Complex Embeddings for Simple Link Prediction</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based Multi-Relational Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Tucker ; Ledyard R Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vashishth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2735" to="2745" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
