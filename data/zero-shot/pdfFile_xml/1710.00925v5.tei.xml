<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Head Pose Estimation Without Keypoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
							<email>nataniel.ruiz@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
							<email>eunjichong@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
							<email>rehg@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Head Pose Estimation Without Keypoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The related problems of head pose estimation and facial expression tracking have played an important role over the past 25 years in driving vision technologies for nonrigid registration and 3D reconstruction and enabling new ways to manipulate multimedia content and interact with users. Historically, there have been several major approaches to face modeling, with two primary ones being discriminative/landmark-based approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> and parameterized appearance models, or PAMs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> (see <ref type="bibr" target="#b29">[30]</ref> for additional discussion). In recent years, methods which directly extract 2D facial keypoints using modern deep learning tools <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14]</ref> have become the dominant approach to facial expression analysis, due to their flexibility 1 https://github.com/natanielruiz/deep-head-pose and robustness to occlusions and extreme pose changes. A by-product of keypoint-based facial expression analysis is the ability to recover the 3D pose of the head, by establishing correspondence between the keypoints and a 3D head model and performing alignment. However, in some applications the head pose may be all that needs to be estimated. In that case, is the keypoint-based approach still the best way forward? This question has not been thoroughlyaddressed using modern deep learning tools, a gap in the literature that this paper attempts to fill.</p><p>We demonstrate that a direct, holistic approach to estimating 3D head pose from image intensities using convolutional neural networks delivers superior accuracy in comparison to keypoint-based methods. While keypoint detectors have recently improved dramatically due to deep learning, head pose recovery inherently is a two step process with numerous opportunities for error. First, if sufficient keypoints fail to be detected, then pose recovery is impossible. Second, the accuracy of the pose estimate depends upon the quality of the 3D head model. Generic head models can introduce errors for any given participant, and the process of deforming the head model to adapt to each participant requires significant amounts of data and can be computationally expensive.</p><p>While it is common for deep learning based methods using keypoints to jointly predict head pose along with facial landmarks, the goal in this case is to improve the accuracy of the facial landmark predictions, and the head pose branch is not sufficiently accurate on its own: for example <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> which are studied in Section 4.1 and 4.3. A conv-net architecture which directly predicts head pose has the potential to be much simpler, more accurate, and faster. While other works have addressed the direct regression of pose from images using conv-nets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref> they did not include a comprehensive set of benchmarks or leverage modern deep architectures.</p><p>In applications where accurate head pose estimation is required, a common solution is to utilize RGBD (depth) cameras. These can be very accurate, but suffer from a number of limitations: First, because they use active sensing, they can be difficult to use outdoors and in uncontrolled environments, as the active illumination can be swamped by sunlight or ambient light. Second, depth cameras draw more power than RGB, resulting is significant battery life issues in mobile applications, and they are much less prevalent in general. Third, the data rates for RGBD are higher than for RGB, increasing storage and data transfer times. As a consequence, for a wide range of applications in domains such as pedestrian tracking and safety monitoring in autonomous driving, computer graphics, driver alertness monitoring, and social scene understanding from video, there remains a need for an RGB-based 3D head pose estimation solution which is fast and reliable.</p><p>The key contributions of our work are the following:</p><p>? Proposing a method to predict head pose Euler angles directly from image intensities using a multi loss network which has a loss for each angle and each loss has two components: a pose bin classification and a regression component. We outperform published methods in single frame pose estimation in several datasets.</p><p>? Demonstrating the generalization capacity of our model by training it on a large synthetic dataset and obtaining good results on several testing datasets.</p><p>? Presenting ablation studies on the convolutional architecture of the network as well as on the multiple components of our loss function.</p><p>? Presenting a detailed study of the accuracy of pose from 2D landmark methods, and detail weaknesses of this approach which are solved by the appearance based approach that we take.</p><p>? Studying the effects of low resolution on pose estimation for different methods. We show that our method coupled with data augmentation is effective in tackling the interesting problem of head pose estimation on low resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Human head pose estimation is a widely studied task in computer vision with very diverse approaches throughout its history. In the classic literature we can discern Appearance Template Models which seek to compare test images with a set of pose exemplars <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Detector arrays were once a popular method when frontal face detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> had increased success, the idea was to train multiple face detectors for different head poses <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Recently, facial landmark detectors which have become very accurate <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14]</ref>, have been popular for the task of pose estimation.</p><p>Also recently, work has developed on estimating head pose using neural networks. <ref type="bibr" target="#b18">[19]</ref> presents an in-depth study of relatively shallow networks trained using a regression loss on the AFLW dataset. In KEPLER <ref type="bibr" target="#b13">[14]</ref> the authors present a modified GoogleNet architecture which predicts facial keypoints and pose jointly. They use the coarse pose supervision from the AFLW dataset in order to improve landmark detection. Two works dwell on building one network to fulfill various prediction tasks regarding facial analysis. Hyperface <ref type="bibr" target="#b19">[20]</ref> is a CNN that sets out to detect faces, determine gender, find landmarks and estimate head pose at once. It does this by using an R-CNN <ref type="bibr" target="#b6">[7]</ref> based approach and a modified AlexNet architecture which fuses intermediate convolutional layer outputs and adds separate fully-connected networks to predict each subtask. All-In-One Convolutional Neural Network <ref type="bibr" target="#b20">[21]</ref> for Face Analysis adds smile, age estimation and facial recognition to the former prediction tasks. We compare our results to all of these works.</p><p>Chang et al. <ref type="bibr" target="#b2">[3]</ref> also argue for landmark-free head pose estimation. They regress 3D head pose using a simple CNN and focus on facial alignment using the predicted head pose. They demonstrate the success of their approach by improving facial recognition accuracy using their facial alignment pipeline. They do not directly evaluate their head pose estimation results. This differs from our work since we directly evaluate and compare our head pose results extensively on annotated datasets.</p><p>Work from Gu et al. <ref type="bibr" target="#b4">[5]</ref> uses a VGG network to regress the head pose Euler angles. Instead of improving singleframe prediction by modifying the network structure it focuses on using a recurrent neural network to improve pose prediction by leveraging the time dimension which we do not use. They evaluate their work on a synthetic dataset as well as a real-world dataset. Another key difference with our work is that we set out to show generalization capacity of our network by training on a large dataset and testing the performance of that network on various external datasets without finetuning the network on those datasets. We believe this is a good way to measure how the model will generalize in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>In this section we describe the advantages of estimating head pose with deep networks directly from image intensities and argue that it should be preferred to landmark-topose methods. We explain how combined classification and regression can be used to improve performance when training on the larger synthetic 300W-LP <ref type="bibr" target="#b34">[35]</ref> dataset. We also talk about key insights regarding data augmentation, training and testing datasets and how to improve performance for low-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Advantages of Deep Learning for Head Pose Estimation</head><p>Even though it might seem evident to the reader that given careful training deep networks can accurately predict head pose this approach has not been studied extensively and is not commonly used for head pose estimation tasks. Instead if very accurate head pose is needed then depth cameras are installed and if no depth footage exists landmarks are detected and pose is retrieved. In this work we show that a network trained on a large synthetic dataset, which by definition has accurate pose annotations, can predict pose accurately in real cases. We test the networks on real datasets which have accurate pose annotations and show state-ofthe-art results on the AFLW, AFLW2000 <ref type="bibr" target="#b34">[35]</ref> and BIWI <ref type="bibr" target="#b5">[6]</ref> datasets. Additionally we are starting to close the gap with very accurate methods which use depth information on the BIWI dataset.</p><p>We believe that deep networks have large advantages compared to landmark-to-pose methods, for example:</p><p>? They are not dependent on: the head model chosen, the landmark detection method, the subset of points used for alignment of the head model or the optimization method used for aligning 2D to 3D points.</p><p>? They always output a pose prediction which is not the case for the latter method when the landmark detection method fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Multi-Loss Approach</head><p>All previous work which predicted head pose using convolutional networks regressed all three Euler angles directly using a mean squared error loss. We notice that this approach does not achieve the best results on our large-scale synthetic training data.</p><p>We propose to use three separate losses, one for each angle. Each loss is a combination of two components: a binned pose classification and a regression component. Any backbone network can be used and augmented with three fully-connected layers which predict the angles. These three fully-connected layers share the previous convolutional layers of the network.</p><p>The idea behind this approach is that by performing bin classification we use the very stable softmax layer and cross-entropy, thus the network learns to predict the neighbourhood of the pose in a robust fashion. By having three cross-entropy losses, one for each Euler angle, we have three signals which are backpropagated into the network which improves learning. In order to obtain a fine-grained predictions we compute the expectation of each output angle for the binned output. The detailed architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>We then add a regression loss to the network, namely a mean-squared error loss, in order to improve fine-grained predictions. We have three final losses, one for each angle, and each is a linear combination of both the respective classification and the regression losses. We vary the weight of the regression loss in Section 4.4 and we hold the weight of the classification loss constant at 1. The final loss for each Euler angle is the following:</p><formula xml:id="formula_0">L = H(y,?) + ? ? M SE(y,?)</formula><p>Where H and M SE respectively designate the crossentropy and mean squared error loss functions.</p><p>We experiment with different coefficients for the regression loss and present our results in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Datasets for Fine-Grained Pose Estimation</head><p>In order to truly make progress in the problem of predicting pose from image intensities we have to find real datasets which contain precise pose annotations, numerous identities, different lighting conditions, all of this across large poses. We identify two very different datasets which fill these requirements.</p><p>First is the challenging AFLW2000 dataset. This dataset contains the first 2000 identities of the in-the-wild AFLW dataset which have been re-annotated with 68 3D landmarks using a 3D model which is fit to each face. Consequently this dataset contains accurate fine-grained pose annotations and is a prime candidate to be used as a test set for our task. Second the BIWI dataset is gathered in a laboratory setting by recording RGB-D video of different subjects across different head poses using a Kinect v2 device. It contains roughly 15,000 frames and the rotations are ?75 ? for yaw, ?60 ? for pitch and ?50 ? for roll. A 3D model was fit to each individual's point cloud and the head rotations were tracked to produce the pose annotations. This dataset is commonly used as a benchmark for pose estimation using depth methods which attests to the precision of its labels. In our case we will not use the depth information nor the temporal information, only individual color frames. In Section 4.1 we compare to a very accurate state-of-the-art depth method to ascertain the performance gap between approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training on a Synthetically Expanded Dataset</head><p>We follow the path of <ref type="bibr" target="#b1">[2]</ref> which used synthetically expanded data to train their landmark detection model. One of the datasets they train on is the 300W-LP dataset which is a collection of popular in-the-wild 2D landmark datasets which have been grouped and re-annotated. A face model is fit on each image and the image is distorted to vary the yaw of the face which gives us pose across several yaw angles. Pose is accurately labeled because we have the 3D model and 6-D degrees of freedom of the face for each image.</p><p>We show in Section 4.1 that by carefully training on large amounts of synthetic data we can begin closing the gap with existing depth methods and can achieve very good accuracies on datasets with fine-grained pose annotations. We also test our method against other deep learning methods whose authors have graciously run on some of the test datasets that we use in Section 4.1. Additionally in the same Section, we test landmark-to-pose methods and other types of pose estimation methods such as 3D model fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">The Effects of Low-Resolution</head><p>Currently there is need for head pose estimation at a distance and there exist multiple example applications in areas such as video surveillance, autonomous driving and advertisement. Future head pose estimation methods should look to improve estimation for low-resolution heads.</p><p>We present an in-depth study of the effect of lowresolution on widely-used landmark detectors as well as state-of-the-art detectors. We contend that low-resolution should worsen the performance of landmark detection since estimating keypoints necessitates access to features which disappear at lower resolutions. We argue that although detailed features are important for pose estimation they are not as critical. Moreover this area is relatively untapped: there is scarce related work discussing head pose estimation at a distance. As far as we know there is no work discussing low-resolution head pose estimation using deep learning.</p><p>Deep networks which predict pose directly from image intensities are a good candidate method for this application because robustness can be built into them by modifying the network or augmenting its training data in smart ways. We propose a simple yet surprisingly effective way of developing robustness to low-resolution images: we augment our data by downsampling and upsampling randomly which forces the network to learn effective representations for varied resolutions. We also augment the data by blurring the images. Experiments are shown in Section 4.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>We perform experiments showing the overall performance of our proposed method on different datasets for pose estimation as well as popular landmark detection datasets. We show ablation studies for the multi-loss. Additionally, we delve into landmark-to-pose methods and shed light on their robustness. Finally we present experiments suggesting that a holistic approach to pose using deep networks outperforms landmark-to-pose methods when resolution is low even if the landmark detector is state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine-Grained Pose Estimation on the AFLW2000 and BIWI Datasets</head><p>We evaluate our method on the AFLW2000 and BIWI datasets for the task of fine-grained pose estimation and compare to pose estimated from landmarks using two different landmark detectors, FAN <ref type="bibr" target="#b1">[2]</ref> and Dlib <ref type="bibr" target="#b10">[11]</ref>, and groundtruth landmarks (only available for AFLW2000).</p><p>FAN is a very impressive state-of-the-art landmark detector described in <ref type="bibr" target="#b1">[2]</ref> by Bulat and Tzimiropoulos. It uses Stacked Hourglass Networks <ref type="bibr" target="#b15">[16]</ref> originally intended for human body pose estimation and switches the normal ResNet Bottleneck Block for a hierarchical, parallel and multi-scale block proposed in another paper by the same authors <ref type="bibr" target="#b0">[1]</ref>. We were inspired to train our pose-estimation network on 300W-LP from their work which trains their network on this dataset for the task of landmark detection. Dlib implements a landmark detector which uses an ensemble of regression trees and which is described in <ref type="bibr" target="#b10">[11]</ref>.</p><p>We run both of these landmark detectors on the AFLW2000 and BIWI datasets. AFLW2000 images are small and are cropped around the face. For BIWI we run a Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> face detector trained on the WIDER Face Dataset <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10]</ref> and deployed in a Docker container <ref type="bibr" target="#b23">[24]</ref>. We loosely crop the faces around the bounding box in order to conserve the rest of the head. We also retrieve pose from the ground-truth landmarks of AFLW2000. Results can be seen in <ref type="table">Tables 1 and 2.</ref> Additionally, we run 3DDFA <ref type="bibr" target="#b34">[35]</ref> which directly fits a 3D face model to RGB image via convolutional neutral networks. The primary task of 3DDFA is to align facial landmarks even for the occluded ones using a dense 3D model. As a result of their 3D fitting process, a 3D head pose is produced and we report this pose.</p><p>Finally, we compare our results to the state-of-the-art RGBD method <ref type="bibr" target="#b32">[33]</ref>. We can see that our proposed method considerably shrinks the gap between RGBD methods and ResNet50 <ref type="bibr" target="#b7">[8]</ref>. Pitch estimation is still lagging behind in part due to the lack of large quantities of extreme pitch examples in the 300W-LP dataset. We expect that this gap will be closed when more data is available.</p><p>We present two multi-loss ResNet50 networks with different regression coefficients of 1 and 2 trained on the 300W-LP dataset. For BIWI we also present a multi-loss ResNet50 (? = 1) trained on AFLW. All three networks were trained for 25 epochs using Adam optimization <ref type="bibr" target="#b11">[12]</ref> with a learning rate of 10 ?5 and ? 1 = 0.9, ? 2 = 0.999 and = 10 ?8 . We normalize the data before training by using the ImageNet mean and standard deviation for each color channel. Note that since our method bins angles in the ?99 ? range we discard images with angles outside of this range. Only 31 images are not used from the 2000 images of AFLW2000. In order to compare to Gu et al. <ref type="bibr" target="#b4">[5]</ref> we train on three different 70-30 splits of videos in the BIWI dataset and we average our mean average error for each split. For this evaluation we use weight decay with a coefficient of 0.04 because of the smaller amount of data available. We compare our result to their single-frame result which was trained in the same fashion and we show the results in <ref type="table" target="#tab_0">Table 3</ref>. Our method compares favorably to Gu et al. and lowers the sum of mean average errors by 1.29 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Landmark-To-Pose Study</head><p>In this set of experiments, we examine the approach of using facial landmarks as a proxy to head pose and investigate the limitations of its use for pose estimation. The commonly used pipeline for landmark-to-pose estimation involves a number of steps; 2D landmarks are detected, 3D human mean face model is assumed, camera intrinsic parameters are approximated, and finally the 2D-3D correspondence problem is solved. We show how this pipeline is affected by different error sources. Specifically, using the AFLW2000 benchmark dataset, we conduct experiments starting from the best available condition (ground truth 2D landmarks, ground truth 3D mean face model) and examine the final head pose estimation error by deviating <ref type="figure">Figure 3</ref>. We show the effects of using different number of landmark points for 3D head pose estimation using ground truth facial landmarks and the ground truth mean face model on the AFLW2000 dataset. from this condition. For all of these experiments, we assume zero lens distortion, and run iterative method based on Levenberg-Marquardt optimization to find 2D-3D correspondence which is implemented as the function SolvePnP in OpenCV.</p><p>We first run the pipeline only with ground truth landmarks, varying the number of points used in the optimization method. We observe that in this ideal condition, using all of the available 68 landmark points actually gives biggest error as shown in <ref type="figure">Figure 3</ref>. Then, we jitter the ground truth 2D landmarks by adding random noise independently in x, y direction per landmark. <ref type="figure">Figure 4</ref> shows the results of this experiment with up to 10 pixel of jittering. We repeat the experiment with the same set of keypoints selected for <ref type="figure">Figure 3</ref>. Finally, we change the mean face model by stretching the ground truth mean face in width and height up to 40% <ref type="figure">Figure 5</ref>. Additionally, we also report results based on estimated landmarks using FAN and Dlib in <ref type="figure">Figure 6</ref>.</p><p>The results suggest that with ground truth 2D landmarks, using less key points produces less error since it's less likely to be affected by pose-irrelevant deformation such as facial expression. However, the more points we use for correspondence problem, the more robust it becomes to random jittering. In other words, there exists a tradeoff; if we know the keypoints are very accurate we want to use less points for pose, but if there's error we want to use more points. With estimated landmarks, it's not clear how we can weigh these two, and we find that using more points can both help and worsen pose estimation as presented in <ref type="figure">Figure 6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">AFLW and AFW Benchmarking</head><p>The AFLW dataset, which is commonly used to train and test landmark detection methods, also includes pose anno-tations. Pose was obtained by annotating landmarks and using a landmark-to-pose method. Results can be seen in <ref type="table" target="#tab_1">Table 4</ref>.</p><p>AFW is a popular dataset, also commonly used to test landmark detection, which contains rough pose annotations. It contains 468 in-the-wild faces with absolute yaw degree's up to ?90 ? . Methods only compare mean average error for yaw. Methods usually output discrete predictions and round their output to the closest 15 ? multiple. As such at the 15 ? error margin, which is one of the main metrics reported in the literature, this dataset is saturated and methods achieve over 95% accuracy. Results are shown in <ref type="figure">Figure 7</ref>.</p><p>Using our joint classification and regression losses for AlexNet <ref type="bibr" target="#b12">[13]</ref> we obtain similar mean average error after training for 25 epochs. We compare our results to the KE-PLER <ref type="bibr" target="#b13">[14]</ref> method which uses a modified GoogleNet for simultaneous landmark detection and pose estimation and to <ref type="bibr" target="#b18">[19]</ref> which uses a 4-layer convolutional network. Multi-Loss ResNet50 achieves lower Mean Average Error than KEPLER across all angles in the AFLW test-set after 25 epochs of training using Adam and same learning parameters as in Section 4.1. These results can be observed in <ref type="table" target="#tab_1">Table 4</ref>.</p><p>We test the previously trained AlexNet and Multi-Loss ResNet50 networks on the AFW dataset and display the results in <ref type="figure">Figure 7</ref>. We evaluate the results uniquely on the yaw as all related work does. We constrain our networks to output discrete yaw in 15 degree increments and display the accuracy at two different yaw thresholds. A face is correctly classified if the absolute error of the predicted yaw is lower or equal than the threshold presented.</p><p>The same testing protocol is adopted for all compared methods and numbers are reported directly from the associated papers. Hyperface <ref type="bibr" target="#b19">[20]</ref> and All-In-One <ref type="bibr" target="#b20">[21]</ref> both use a single network for numerous facial analysis tasks. Hyperface uses an AlexNet pre-trained on ImageNet as a backbone and All-In-One uses a backbone 7-layer conv-net pretrained on the face recognition task using triplet probability constraints <ref type="bibr" target="#b24">[25]</ref>.</p><p>We show that by pre-training on ImageNet and finetuning on the AFLW dataset we achieve accuracies that are very close to the best results of the related work. We do not use any other supervisory information which might improve the performance of the network such as 2D landmark annotations. We do however use a more powerful backbone network in ResNet50. We show performance of the same network on both the AFLW test-set and AFW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">AFLW2000 Multi-Loss Ablation</head><p>In this section we present an ablation study of the multiloss. We train ResNet50 only using a Mean Squared Error (MSE) Loss and compare this to ResNet50 using a multiloss with different coefficients for the MSE component. The <ref type="figure">Figure 4</ref>. We show the effect of jittering landmark points around their ground truth position on the task of 3D head pose estimation on AFLW2000 to simulate the effects of noise in the facial keypoint detector. We repeat this experiment four times with different number of landmarks. For all experiments we use the ground truth mean face model for the landmark-to-pose alignment task. <ref type="figure">Figure 5</ref>. We show the effects of changing the 3D mean face model on the task of 3D head pose estimation from 2D landmarks. We use 2D ground truth landmarks and modify the mean face model by stretching its width and height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yaw</head><p>Pitch Roll MAE Multi-Loss ResNet50 (? = 1) 6. <ref type="bibr" target="#b25">26</ref>  weight of the Cross-Entropy loss is maintained constant at 1. We also compare this to AlexNet to discern the effects of having a more powerful architecture.</p><p>We observe the best results on the AFLW2000 dataset when the regression coefficient is equal to 2. We demonstrate increased accuracy when weighing each loss roughly with the same magnitude. This phenomenon can be observed in <ref type="table" target="#tab_2">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Low-Resolution AFLW2000 Study</head><p>We study the effects of downsampling all images from the AFLW2000 dataset and testing landmark-to-pose methods on these datasets. We compare these results to our method using different data augmentation strategies. We test the pose retrieved from the state-of-the-art landmark detection network FAN and also from Dlib. We test all methods on five different scales of downsampling x1, x5, x10 and x15. In general images are around 20-30 pixels wide and high when downsampled x15. We then upsample these images and run them through the detectors and deep networks. We use nearest neighbor interpolation for downsampling and upsampling. For our method we present a multi-loss ResNet50 with regression coefficient of 1 trained on normal resolution images. We also train three identical networks: for the first one we augment the dataset by randomly downsampling and upsampling the input image by x10, for next one we randomly downsample and upsample an image by an integer ranging from 1 to 10 and for the last one we randomly downsample <ref type="figure">Figure 6</ref>. Using estimated 2D landmark points, this experiment shows the 3D pose estimation error depending on how many facial keypoints are used. <ref type="figure">Figure 7</ref>. AFW pose benchmark result along with other methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>. and upsample an image by one of the following integers <ref type="bibr">1, 6, 11, 16, 21.</ref> We observe that from the get-go our methods show better performance than pose from the Dlib landmarks, yet pose from the FAN landmarks is acceptable. Pose from the FAN landmarks degrades as the resolution gets very low which is natural since landmarks are very hard to estimate at these resolutions especially for methods that rely heavily on appearance. Pose from the network without augmentation deteriorates strongly yet the networks with augmentation show much more robustness and perform decently at very low resolutions. Results are presented in <ref type="figure">Figure 8</ref>. This is exciting news for long-distance and low-resolution head pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>In this work we show that a multi-loss deep network can directly, accurately and robustly predict head rotation from image intensities. We show that such a network outperforms landmark-to-pose methods using state-of-the-art landmark detection methods. Landmark-to-pose methods are studied in this work to show their dependence on extraneous factors <ref type="figure">Figure 8</ref>. Mean average error for different methods on the downsampled AFLW2000 dataset in order to determine robustness of methods to low-resolution images. such as head model and landmark detection accuracy.</p><p>We also show that our proposed method generalizes across datasets and that it outperforms networks that regress head pose as a sub-goal in detecting landmarks. We show that landmark-to-pose is fragile in cases of very low resolution and that, if the training data is appropriately augmented, our method shows robustness to these situations.</p><p>Synthetic data generation for extreme poses seems to be a way to improve performance for the proposed method as are studies into more intricate network architectures that might take into account full body pose for example.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example pose detections in difficult scenarios using our proposed method. The blue axis points towards the front of the face, green pointing downward and red pointing to the side. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>ResNet50 architecture with combined Mean Squared Error and Cross Entropy Losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Yaw</cell><cell>Pitch</cell><cell></cell><cell>Roll</cell><cell>MAE</cell></row><row><cell cols="2">Multi-Loss ResNet50 (? = 2) 6.470</cell><cell cols="2">6.559</cell><cell>5.436</cell><cell>6.155</cell></row><row><cell cols="2">Multi-Loss ResNet50 (? = 1) 6.920</cell><cell cols="2">6.637</cell><cell>5.674</cell><cell>6.410</cell></row><row><cell>3DDFA [35]</cell><cell>5.400</cell><cell cols="2">8.530</cell><cell>8.250</cell><cell>7.393</cell></row><row><cell>FAN [2] (12 points)</cell><cell>6.358</cell><cell cols="3">12.277 8.714</cell><cell>9.116</cell></row><row><cell>Dlib [11] (68 points)</cell><cell cols="5">23.153 13.633 10.545 15.777</cell></row><row><cell>Ground truth landmarks</cell><cell>5.924</cell><cell cols="3">11.756 8.271</cell><cell>8.651</cell></row><row><cell cols="6">Table 1. Mean average error of Euler angles across different meth-</cell></row><row><cell cols="2">ods on the AFLW2000 dataset [35].</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Yaw</cell><cell>Pitch</cell><cell></cell><cell>Roll</cell><cell>MAE</cell></row><row><cell>Multi-Loss ResNet50 (? = 2)</cell><cell>5.167</cell><cell cols="2">6.975</cell><cell>3.388</cell><cell>5.177</cell></row><row><cell>Multi-Loss ResNet50 (? = 1)</cell><cell>4.810</cell><cell cols="2">6.606</cell><cell>3.269</cell><cell>4.895</cell></row><row><cell>KEPLER [14] ?</cell><cell>8.084</cell><cell cols="4">17.277 16.196 13.852</cell></row><row><cell cols="2">Multi-Loss ResNet50 (? = 1) ? 5.785</cell><cell cols="3">11.726 8.194</cell><cell>8.568</cell></row><row><cell>3DMM+ Online [33] *</cell><cell>2.500</cell><cell cols="2">1.500</cell><cell>2.200</cell><cell>2.066</cell></row><row><cell>FAN [2] (12 points)</cell><cell>8.532</cell><cell cols="2">7.483</cell><cell>7.631</cell><cell>7.882</cell></row><row><cell>Dlib [11] (68 points)</cell><cell cols="4">16.756 13.802 6.190</cell><cell>12.249</cell></row><row><cell>3DDFA [35]</cell><cell cols="4">36.175 12.252 8.776</cell><cell>19.068</cell></row><row><cell cols="6">Table 2. Mean average error of Euler angles across different meth-</cell></row><row><cell cols="6">ods on the BIWI dataset [6]. * These methods use depth informa-</cell></row><row><cell>tion.  ? Trained on AFLW</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Yaw Pitch Roll Sum of errors</cell></row><row><cell cols="3">Multi-Loss ResNet50 (? = 1) 3.29 3.39</cell><cell cols="2">3.00 9.68</cell></row><row><cell>Gu et al. [5]</cell><cell cols="2">3.91 4.03</cell><cell cols="2">3.03 10.97</cell></row></table><note>. Comparison with Gu et al. [5]. Mean average error of Euler angles averaged over train-test splits of the BIWI dataset [6].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Mean average errors of predicted Euler angles in the AFLW test set.</figDesc><table><row><cell></cell><cell></cell><cell>5.89</cell><cell cols="2">3.82 5.324</cell></row><row><cell>AlexNet (? = 1)</cell><cell>7.79</cell><cell>7.41</cell><cell cols="2">6.05 7.084</cell></row><row><cell>KEPLER [14]</cell><cell>6.45</cell><cell>5.85</cell><cell cols="2">8.75 7.017</cell></row><row><cell>Patacchiola, Cangelosi [19]</cell><cell cols="2">11.04 7.15</cell><cell>4.4</cell><cell>7.530</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Ablation analysis: MAE across different models and regression loss weights on the AFLW2000 dataset.</figDesc><table><row><cell></cell><cell>?</cell><cell>Yaw</cell><cell>Pitch Roll</cell><cell>MAE</cell></row><row><cell>ResNet50 regression only</cell><cell></cell><cell cols="3">13.110 6.726 5.799 8.545</cell></row><row><cell>Multi-Loss ResNet50</cell><cell>4</cell><cell>7.087</cell><cell cols="2">6.870 5.621 6.526</cell></row><row><cell></cell><cell>2</cell><cell>6.470</cell><cell cols="2">6.559 5.436 6.155</cell></row><row><cell></cell><cell>1</cell><cell>6.920</cell><cell cols="2">6.637 5.674 6.410</cell></row><row><cell></cell><cell>0.1</cell><cell cols="3">10.270 6.867 5.420 7.519</cell></row><row><cell></cell><cell cols="4">0.01 11.410 6.847 5.836 8.031</cell></row><row><cell></cell><cell>0</cell><cell cols="3">11.628 7.119 5.966 8.238</cell></row><row><cell>Multi-Loss AlexNet</cell><cell>1</cell><cell cols="3">27.650 8.543 8.954 15.049</cell></row><row><cell></cell><cell>0.1</cell><cell cols="3">30.110 9.548 9.273 16.310</cell></row><row><cell></cell><cell cols="4">0.01 25.090 8.442 8.287 13.940</cell></row><row><cell></cell><cell>0</cell><cell cols="3">24.469 8.350 8.353 13.724</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faceposenet: Making a case for landmark-free face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1599" to="1608" />
		</imprint>
	</monogr>
	<note>Computer Vision Workshop</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dynamic facial analysis: From bayesian filtering to recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G X Y S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random forests for real time 3d face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face pose discrimination using support vector machines (svm)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="154" to="156" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face detection with the faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="650" to="657" />
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kepler: Keypoint and pose estimation of unconstrained faces by learning efficient h-cnn regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active Appearance Models Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composite support vector machines for detection of faces across views and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training support vector machines: an application to face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Girosit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cangelosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural networkbased face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dockerface: an easy to install and use faster r-cnn face detector in a docker container</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04370</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Triplet probabilistic embedding for face verification and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>IEEE 8th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding pose discrimination in similarity space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face distributions in similarity space under varying head pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="807" to="819" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face alignment assisted by head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust and accurate 3d head pose estimation through 3dmm and online head model reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A F</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="711" to="718" />
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Head pose estimation in seminar room using multi view face detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Evaluation Workshop on Classification of Events, Activities and Relationships</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="299" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
