<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embodied Scene-aware Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Iwase</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Embodied Scene-aware Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: From a monocular video input in a known scene, we estimate 3D absolute body pose by controlling a simulated character to match 2D observation in a sequential manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose embodied scene-aware human pose estimation where we estimate 3D poses based on a simulated agent's proprioception and scene awareness, along with external third-person observations. Unlike prior methods that often resort to multistage optimization, non-causal inference, and complex contact modeling to estimate human pose and human scene interactions, our method is one-stage, causal, and recovers global 3D human poses in a simulated environment. Since 2D third-person observations are coupled with the camera pose, we propose to disentangle the camera pose and use a multi-step projection gradient defined in the global coordinate frame as the movement cue for our embodied agent. Leveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we simulate our agent in everyday environments (library, office, bedroom, etc.) and equip our agent with environmental sensors to intelligently navigate and interact with the geometries of the scene. Our method also relies only on 2D keypoints and can be trained on synthetic datasets derived from popular human motion databases. To evaluate, we use the popular H36M and PROX datasets and achieve high quality pose estimation on the challenging PROX dataset without ever using PROX motion sequences for training. Code and videos are available on the project page.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider a 3D human pose estimation setting where a calibrated camera is situated within a prescanned environment (with known 3D mesh geometries). Such a setting has a wide range of applications in mixed reality telepresence, animation, and gaming, at home robotics, video surveillance, etc., where a single fixed RGB camera is often used to observe human movement. However, even under such constraints, severe occlusion, low-textured garments, and human-scene interactions often cause pose estimation methods to fail catastrophically. Especially when estimating the global 3D human pose, state-of-the-art methods often resort to expensive multistage batch optimization <ref type="bibr" target="#b5">[6]</ref>, motion prior or infilling <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b47">48]</ref>, and depth information to recover reasonable global translation and orientation. Consequently, these methods have limited use cases in time-critical applications. Multistage optimization can also suffer from failure of convergence, where the optimizer cannot find a good solution due to occlusion, numerical instability, or poor initialization.</p><p>The effective use of scene information is also critical for human pose estimation. Unfortunately, the lack of datasets with paired ground-truth 3D body pose and annotated contact labels makes it difficult to incorporate scene information to estimate pose. Thus, popular methods often resort to 1) minimizing the distance between predefined contact points in the body and the scene <ref type="bibr" target="#b5">[6]</ref>, 2) using vertex-based PROXimity <ref type="bibr" target="#b48">[49]</ref> as a proxy scene representation, and 3) semantic labels as affordance cues <ref type="bibr" target="#b16">[17]</ref>. These methods, while effective in modeling human-scene interactions, are expensive to compute and heavily rely on a batch optimization procedure to achieve good performance. More complex scenes are also harder to model in a batch optimization framework, as the number of contact constraints increases significantly as the number of geometries increases.</p><p>We hypothesize that embodiment may be a key element in estimating accurate absolute 3D human poses. Here, we define embodiment as having a tangible existence in a simulated environment in which the agent can act and perceive. In particular, humans move according to their current body joint configuration (proprioception), movement destination (goal), and an understanding of their environment's physical laws (scene awareness and embodiment). Under these guiding principles, we propose embodied scene-aware human pose estimation as an alternative to optimization and regression-based methods. Concretely, we estimate poses by controlling a physically simulated agent equipped with environmental sensors to mimic observations from third-person in a streaming fashion. At each timestep, our agent receives movement goals from a third-person observation and computes its torque actuation based on (1) current agent state: body pose, velocities, etc., <ref type="bibr" target="#b1">(2)</ref> third-person observation, and (3) environmental features from the agent's point of view. Acting in a physics simulator, our agent can receive timely feedback about physical plausibility and contact to compute actions for the next time frame. Although there exist prior methods that also use simulated characters, we draw a distinction in how the next frame pose is computed. Previous work first regress the kinematic pose <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref> without considering egocentric features such as body and environmental states. As a result, when the kinematic pose estimation methods fail due to occlusion, these simulated agents will receive unreasonable target poses. Our method, on the other hand, predicts target poses by comparing the current body pose with third-person observations. When third-person observations are unreliable, the target poses will be extrapolated from environmental cues and current body states. Thus, our method is robust to occlusion and can recover from consecutive frames of missing body observations. Our method is also capable of estimating pose for long-term (several minutes to perpetual), in sub-real-time (? 10 fps), while creating physically realistic human scene interactions.</p><p>To achieve this, we propose a multi-step projection gradient (MPG) to provide movement cues to our embodied agent from 2D observations. At each time step, MPG performs iterative gradient descent starting from the current pose of the humanoid to exploit the temporal coherence of human movement and better avoid local minima. MPG also directly provides temporal body and root movement signals to our agent in the world coordinate frame. Thus, our networks does not need to take the 3D camera pose as input explicitly. To control a simulated character, we extend the Universal Humanoid Controller (UHC) and the kinematic policy proposed in previous work <ref type="bibr" target="#b20">[21]</ref> to support the control of humanoids with different body shapes in environments with diverse geometries. Finally, we equip our agent with an occupancy-based environmental sensor to learn about affordances and avoidance. Since only 2D keypoint coordinates are needed by our method to estimate body motion, our model can be learned from purely synthetic data derived from popular motion databases.</p><p>To summarize, our contribution is as follows: 1) we propose embodied scene-aware human pose estimation, where we control a simulated scene-aware humanoid to mimic visual observations and recover accurate global 3D human pose and human-scene interactions; 2) we formulate a multi-step projection gradient (MPG) to provide a camera-agnostic movement signal for an embodied agent to match 2D observations; and 3) we achieve state-of-the-art result on the challenging PROX dataset while our method remains sub-real-time, causal, and trained only on synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Local and Absolute 3D Human Pose Estimation. It is popular to estimate the 3D human pose in the local coordinate system (relative to the root) <ref type="bibr">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> where the translation is discarded and the rotation is defined in the camera coordinate frame. Among these methods, some focus on lifting 2D keypoints to 3D <ref type="bibr">[1,</ref><ref type="bibr" target="#b26">27]</ref>, while others use a template 3D human mesh (the SMPL <ref type="bibr" target="#b18">[19]</ref> body model and its extensions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>) and jointly recover body shape and joint angles. Among these methods, some <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46</ref>] make use of iterative optimization to better fit 2D keypoints. SPIN <ref type="bibr" target="#b14">[15]</ref> devises an optimization-in-the-loop fitting procedure to provide a better starting point for a pose regressor. NeuralGD <ref type="bibr" target="#b36">[37]</ref> and SimPOE <ref type="bibr" target="#b45">[46]</ref> both utilize a learned gradient update to iteratively refine body pose and shape, where optimization starts from either zero pose (NeuralGD) or estimated kinematic pose (SimPOE). Compared to these methods, our MPG starts the computation process from the previous time-step pose to leverage the temporal coherence in human motion. While estimating pose in the local coordinate frame is useful in certain applications, it is often desirable to recover the pose in the absolute/global space to fully leverage the estimated pose for animation or telepresence. However, recovering the absolute 3D body pose that includes root translation is challenging due to limb articulation, occlusion, and depth ambiguity. Among the work that attempt to estimate the absolute human pose <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43]</ref>, most take a two-stage approach where the root-relative body pose is first regressed from a single image and then a root translation is estimated from image-level features. Compared to these methods, we leverage physical laws and motion cues to autoregressively recover both root movement and body pose.</p><p>Scene-aware Human Pose and Motion Modeling. The growing interest in jointly modeling human pose and human-scene interactions has led to a growing popularity in datasets containing both human and scenes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref>. In this work, we focus on the PROX <ref type="bibr" target="#b6">[7]</ref> dataset, where a single human is recorded moving in daily indoor environments such as offices, libraries, bedrooms, etc. Due to the diversity of motion and human-scene interaction (sitting on chairs, lying on beds, etc.) and lack of ground-truth training data, the PROX dataset remains highly challenging. Consequently, state-of-the-art methods are mainly optimization-based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref> while leveraging the full set of observations (scene scans, camera pose, depth). HuMoR <ref type="bibr" target="#b30">[31]</ref> and LEMO <ref type="bibr" target="#b46">[47]</ref> achieves impressive result by using large-scale human motion datasets to learn strong prior of human movement to constrain or infill the missing human motion. However, multistage optimization can often become unstable due to occlusion, failures in 2D keypoint detections, or erroneous depth segmentation.</p><p>Physics-based humanoid control for pose estimation. Physical laws guard every aspect of human motion and there is a growing interest in enforcing physics prior in human pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. Some methods prefer to use physics as a postprocessing step and refine initial kinematic pose estimates <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, while ours follows the line of work where a simulated character is controlled to estimate pose inside a physics simulation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46]</ref>. Although both approaches have their respective strong suites, utilizing a physics simulation simplifies the process of incorporating unknown scene geometries and can empower real-time pose estimation applications. Among the simulated character control-based methods, Neural Mocon <ref type="bibr" target="#b8">[9]</ref> utilizes sampling-based control and relies on the learned motion distribution from the training split of the dataset to acquire quality motion samples. SimPOE <ref type="bibr" target="#b45">[46]</ref> first extracts the kinematic pose using off-the-shelf method <ref type="bibr" target="#b12">[13]</ref> and then uses a simulated agent for imitation. PoseTriplet <ref type="bibr" target="#b3">[4]</ref> proposes using a self-supervised framework to co-evolve a pose estimator, imitator, and hallucinator. Compared to ours, SimPoE and PoseTriplet both rely on a kinematic pose estimator as backbones <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> and require well-observed 2D keypoints for reliable pose estimates. They also require lengthy policy training time and do not model any scene geometries. Our work extends Kin_poly <ref type="bibr" target="#b20">[21]</ref>, where a simulated character is used to perform first-person pose estimation. While Kin_poly uses egocentric motion as the only motion cue, we propose a Multistep Projection Gradient to leverage thrid-person observations. Compared to their object-pose based scene feature, we use an occupancy-based scene feature inspired by the graphics community <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> to better model scene geometries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given a monocular video I 1:T taken using a calibrated camera in a known environment, our aim is to estimate, track, and simulate 3D human motion in metric space. We denote the 3D human  pose as q t (r R t , r T t , ? t ), consisting of the 3D root position r T t , the orientation r R t and the angles of the body joints ? t . Combining 3D pose and velocity, we have the character's state s t (q t ,q t ). As a notation convention, we use ? to represent kinematic poses (without physics simulation), ? to denote ground-truth quantities from Motion Capture (MoCap), and normal symbols without accents for values from the physics simulation. In Sec 3.1, we will first summarize our extension to the Universal Humanoid Controller <ref type="bibr" target="#b20">[21]</ref> to support controlling people with different body proportions. Then, in Sec.3.2, we will describe the state space of our character's control policy: the multi-step projection gradient (MPG) as a feature and occupancy based scene feature. Finally, we will detail the training procedure to learn our embodied pose estimator using synthetic datasets in Sec 3.3. Our pose estimation pipeline is visualized at <ref type="figure" target="#fig_1">Fig.2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries: Physics-based Character and Universal Humanoid Controller (UHC)</head><p>To simulate people with varying body shapes, we first use an automatic simulated character creation process similar to SimPoE <ref type="bibr" target="#b45">[46]</ref> to generate human bodies with realistic joint limits, body proportions and other physical properties based on the popular human mesh model SMPL <ref type="bibr" target="#b18">[19]</ref>. In SMPL, body joints are recovered from mesh vertices and a joint regressor, so the length of each limb may vary according to the body pose. This is undesirable as varying the length of the body limbs is physically inaccurate. Recovering body joints by first computing a large number of vertices (6890) followed by a joint regressor is also time-consuming. Therefore, we fix the limb lengths using those calculated from the T pose (setting the joint angles to zero) and use forward kinematics to recover the 3D location of the body joints J t from joint angles. Given the shape of the body ? and the pose q t , we define the forward kinematics function as FK ? (q t ) ? J t .</p><p>To learn a universal humanoid controller that can perform everyday human motion with different body types, we follow the standard formulation in physics-based character and model controlling a humanoid for motion imitation as a Markov Decision Process (MDP) defined as a tuple M = S, A, T , R, ? of states, actions, transition dynamics, reward function, and discount factor. The state s ? S, reward r ? R, and the transition dynamics T are determined by the physics simulator, while the action a ? A is given by the control policy ? UHC (a t |s t , q t+1 , ?). Here we extend the Universal Humanoid Controller proposed in Kin_poly <ref type="bibr" target="#b20">[21]</ref> and condition the control policy on the current simulation state s t , the reference motion q t , and the body shape ?. We employ Proximal Policy Optimization (PPO) <ref type="bibr" target="#b33">[34]</ref> to find the optimal control policy ? UHC to maximize the expected discounted reward E T t=1 ? t?1 r t . For more information on the UHC network architecture and training procedure, please refer to Kin_poly <ref type="bibr" target="#b20">[21]</ref> and our Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embodied Scene-Aware Human Pose Estimation</head><p>To estimate the global body pose from the video observations, we control a simulated character to follow the 2D keypoint observations in a sequential decision manner. Assuming a known camera pose and scene geometry, we first use an off-the-shelf 2D keypoint pose estimator, DEKR <ref type="bibr" target="#b2">[3]</ref>, to extract 2D keypoints kp 2D t at a per-frame level. Then, we use HuMoR <ref type="bibr" target="#b30">[31]</ref> to extract the first-frame 3D pose q 0 and body shape ? from the RGB observations as initialization. Given q 0 and ?, we initialize our simulated character inside the pre-scanned scene, and then we attempt to estimate poses which match observed 2D keypoints in a streaming fashion.</p><p>Multi-step Projection Gradient (MPG) as a Motion Feature. To match the motion of the 3D simulated humanoids to 2D keypoint observations, it is essential to provide 3D information about how poses should be modified in the global coordinate frame (coordinate frame of the scene geometry and the 3D humanoid). As 2D keypoints are coupled with camera pose (the same body pose corresponds to different 2D keypoints based on the camera pose), it is problematic to directly input 2D observations without a proper coordinate transform. To avoid this issue, prior arts that use simulated character control <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref> often estimate the kinematic human pose in the camera coordinate frame using a kinematic pose estimator. However, conducting physics simulation using the camera coordinate frame is challenging, since the up-vector of the camera is not always aligned with the direction of gravity. As a result, manual alignment of the character's up-vector to the gravity direction is often needed for a successful simulation <ref type="bibr" target="#b3">[4]</ref>.</p><p>To decouple the pose of the camera from the pose estimation pipeline, we propose using a multi-step projection gradient (MPG) feature as an alternative to estimated kinematic pose as an input to the policy. Intuitively, the MPG is a modified (learned) sum of 2D projection loss gradients that encodes how much the 3D pose should be adjusted for to fit the next frame 2D keypoints. Given the current global state of the humanoid q t and the camera projection function ? cam , we define MPG ? as:</p><formula xml:id="formula_0">? t = q K t ? q 0 t = K k=0 G( ?Lproj ?q k t , q k t , kp 2D t??:t+1 ), q k+1 t = q k t ? s ? G( ?Lproj ?q k t , q k t , kp 2D t??:t+1 )<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">L proj = ? cam (FK ? (q t )) ? kp 2D t+1 2 is the L 2 reprojection loss with respect to the observed 2D keypoints kp 2D</formula><p>t+1 . s is the step size of the gradient and k indexes into the total number of gradient descent steps K. The 2D project loss gradient ?L ?q t represents how the root pose and body joints should change to better match the 2D key points of the next frame and contains rich geometric information independent of the camera projection function ? cam .</p><p>Due to noise in the detection, occlusion, and depth ambiguity of 2D key points, a single-step instantaneous projection gradient ?Lproj ?q k t can be erroneous and noisy, especially the gradient of root translation and orientation. Thus, we augment ?Lproj ?q k t with a geometric and learning-based transform G that is intended to remove noise. In this way, the MPG feature is an aggregation of a collection of modified gradients to compute a more stable estimate of the 2D projection loss gradient.</p><p>To better recover root rotation, we use a temporal convolution model (TCN) similar to VideoPose3D <ref type="bibr" target="#b26">[27]</ref> and PoseTriplet <ref type="bibr" target="#b3">[4]</ref> to regress camera-space root orientation from 2D keypoints. We notice that while TCN may produce an unreasonable pose estimate during server occlusion, its root orientation estimation remains stable. We compute the camera space root orientation using a causal TCN F TCN and transform it into the world coordinate system r R t using the inverse camera rotation:</p><formula xml:id="formula_2">r R t = ? ?1 cam (F TCN (kp 2D t??:t+1 ))</formula><p>. We then calculate the difference between the current root rotation r R t and r R t as an additional movement gradient signal. Given the current 3D pose q k t and the projection function ? cam , we can obtain a better root translation r T t by treating the 3D keypoints obtained as a fixed rigid body and solving for translation using linear least squares. Combining the F TCN model and the geometric transformation step, we obtain the function G to obtain a better root translation and orientation gradient. By taking multiple steps, we avoid getting stuck in local minima and obtain better gradient estimates. For a more complete formulation of MPG, see our Appendix.</p><p>Scene Representation. To provide our embodied agent with an understanding of its environment, we provide our policy with two levels of scene-awareness: 1) we simulate the geometry of the scene inside a physics simulation, which implicitly provides contact and physics constraints for our policy. As the scanned meshes provided by the PROX dataset often have incorrect volumes, we use a semi-automatic geometric fitting procedure to approximate the scene by simple shapes such as rectangles and cylinders. The modified scene can be visualized together in Eq.2. For more  </p><formula xml:id="formula_3">? ?cam(q 1:T ) // compute paired 2D projection . ; for t ? 1...T do st ? (q t ,q t ) ? t ? K k=0 G( ?L proj ?q k t , q k t , kp 2D t??:t+1 ) // movement cues from 2D observations. ; q t+1 ? ?KIN(st, ? t , ot) + q t st+1 ? T (st+1|st, at)</formula><p>, at ? ?UHC(at|st, q t+1 ) // physics simulation and agent control using ?UHC store (st, at, rt, st+1, q t , q t+1 ) into memory M dyna end end end ?KIN ? Supervised learning update using experiences collected in M dyna for 10 epoches.</p><p>information about our scene simplification process, please refer to the Appendix. 2) We use a voxel-based occupancy map to indicate the location and size of the scene geometries, similar to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. Occupancy provides basic-level scene and object awareness to our agent and is robust to changing scene geometries. Concretely, we create a cube of 1.8 meters edge length as the sensing range of our agent's surroundings and create a 16 by 16 by 16 evenly spaced grid. At the center of each point on the grid, we sample the signed distance function of the scene F s to obtain how far the sampling point is from the geometries of the scene. Then, we threshold the gird feature to provide a finer scene information. In time step t, for threshold ? = 0.05, the value of point p i in the grid can be computed as o i t , and o t = {o i 0 , o i 1 , . . . , }: </p><formula xml:id="formula_4">o i t = ? ? ? 1 F s (p i ) ? 0 0 F s (p i ) &gt; ? 1 ? F s (p i )/? otherwise (2)</formula><p>Scene-aware Kinematic Policy. To train and control our embodied agent in the physics simulator, we extend a policy network similar to Kin_poly <ref type="bibr" target="#b20">[21]</ref>, with the important distinction that our networks perform third-person pose estimation in a real-world scene. Under the guiding principle of embodied human pose estimation, our kinematic policy ? KIN computes a residual 3D movementq t based on proprioception s t , movement goal ? t , and scene awareness o t :</p><formula xml:id="formula_5">q t = ? KIN (s t , ? t , o t ),q t+1 =q t +q t .<label>(3)</label></formula><p>Specifically, the input q t contains the current body joint configurations, ? t provides the movement signal per step, while o t provides awareness to the agent's surroundings. Within ? KIN , an agentcentric transform T AC is used to transform q t , ? t and s t into the agent-centirc coordinate system.</p><formula xml:id="formula_6">The outputq t , is defined asq t (r R t ,r T t ,? t )</formula><p>, which computes the velocity in root translationr R t and orientationr T t , as well as the joint angles? t . Notice thatq t computes the residual body movement instead of the full kinematic body poseq t+1 and establishes a better connection with the movement feature ? t . During inference, ? KIN works in tandem with ? UHC and computes the next-step target poseq t+1 for the humanoid controller to mimic. In turn, ? KIN receives timely feedback on physical realism and contact information based on the simulated agent state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning the embodied human pose estimator</head><p>Since ? KIN relies only on 2D keypoints, we can leverage large-scale human motion databases and train our pose estimation network using synthetic data. We use the AMASS <ref type="bibr" target="#b21">[22]</ref>, H36M <ref type="bibr" target="#b9">[10]</ref>, and Kin_poly <ref type="bibr" target="#b20">[21]</ref> datasets to dynamically generate 2D keypoints and 3D pose pairs for training. At the beginning of each episode, we sample a motion sequence and randomly change its direction and starting position in a valid range, similar to the procedure defined in NeuralGD <ref type="bibr" target="#b36">[37]</ref>. Then we use a randomly selected camera projection function ? cam to project 3D keypoints onto 2D images to obtain paired 2D keypoints and human motion sequences. During training, we initialize the agent with the ground-truth 3D location and pose. The H36M and Kin_poly dataset contains simple human-scene interactions, such as sitting on chairs and stepping on boxes. To better learn object affordance and collision avoidance, we also randomly sample scenes from the PROX dataset and pair them with motions from the AMASS dataset. Although these motion sequences do not correspond to the sampled scene, the embodied agent will hit the scene geometries while trying to follow the 2D observations. We stop the simulation when the agent deviates too far away from the ground-truth motion to learn scene affordance and avoidance. For more details about our synthetic data sampling procedure, please refer to the Appendix.</p><p>We train the kinematic policy as a per-step model together with physics simulation, following the dynamics-regulated training procedure in <ref type="bibr" target="#b20">[21]</ref> (Alg. 1.). Using this procedure, we can leverage the pre-learned motor skills from the UHC and let the kinematic policy act directly in a simulated physical space to obtain feedback on physical plausibility. In each episode, we connect the UHC and the kinematic policy using the result of the physics simulation q t+1 to compute the input features to the kinematic policy. Using q t+1 verifies that q t can be followed by our UHC and informs our ? KIN of the current humanoid state to encourage the policy to adjust its predictions to improve stability. Here, we only train our ? KIN using supervised learning, as third-person pose estimation is a more well-posed problem and requires less exploration than estimating pose from only egocentric videos <ref type="bibr" target="#b20">[21]</ref>. For each motion sequence, our loss function is defined as L1 distance between the predicted kinematic pose and the ground truth plus a prior loss using a learned motion transition prior p ? and the encoder q ? from HuMoR <ref type="bibr" target="#b30">[31]</ref>. The prior term encourages the generated motion to be smoother:</p><formula xml:id="formula_7">L = T t=0 wL1 q t ?q t 1 + wpriorDKL(q ? (zt|q t ,q t?1 )||p ? (zt|q t?1 )),<label>(4)</label></formula><p>4 Experiments Implementation details. We use the MuJoCo <ref type="bibr" target="#b39">[40]</ref> physics simulator and use modified scene geometries as environments. The training process or our embodied pose estimator takes around 2 days on a RTX 3090 with 30 CPU threads. During inference, our network is causal and runs at ?10 FPS on an Intel desktop CPU (not counting the time for 2D keypoint pose estimation, which runs at around 30 FPS <ref type="bibr" target="#b2">[3]</ref>). For more details on the implementation, please refer to the Appendix.</p><p>Datasets. Our Universal Humanoid Controller is trained on the AMASS dataset training split that contains high-quality SMPL parameters of 11402 sequences curated from the various MoCap datasets. For training our pose estimator, we use motions from the AMASS training split, the Kin_poly dataset, and H36M dataset. The Kin_poly dataset contains 266 motion sequences of sitting, stepping, pushing, and avoiding, and the H36M dataset consists of 210 indoor videos with 51 different unique actions. Note that we only use the motion sequences from these datasets and not the paired videos. For evaluation, we use videos from the PROX <ref type="bibr" target="#b6">[7]</ref> dataset and the test split of H36M.</p><p>Metrics and Baselines. We compare with a series of state-of-the-art methods HyperIK <ref type="bibr" target="#b15">[16]</ref>, Me-TRAbs <ref type="bibr" target="#b32">[33]</ref>, VIBE <ref type="bibr" target="#b12">[13]</ref>, and SimPOE <ref type="bibr" target="#b45">[46]</ref> in the H36M dataset. On the PROX dataset, we compare with the HuMoR <ref type="bibr" target="#b30">[31]</ref>, LEMO <ref type="bibr" target="#b46">[47]</ref>, and HyberIK with RootNet <ref type="bibr" target="#b23">[24]</ref>. We use the official implementation of the above methods. We use pose-based and plausibility-based metrics for evaluation. To evaluate pose estimation when there is ground-truth MoCap pose, we report the absolute/global and root-relative mean per joint position error: A-MPJPE and MPJPE. On the PROX dataset, since there is no ground truth 3D pose annotation, we report: success rate SR (whether the optimization procedure converges and whether the simulated agents lose balance), one-way average chamfer distance from the point cloud (ACD, acceleration (Accel.c), and scene penetration frequency (Freq.) and distance (Pen.). For a full definition of our evaluation metrics, please refer to the Appendix.  Results on H36M. We report the pose estimation on the H36M dataset in <ref type="table" target="#tab_1">Table 1</ref>. Here, we can see that our method achieves a reasonable pose estimation result on both root-relative and global pose metrics. Notice that our experiment with the H36M dataset is meant to be a sanity check on our pose estimation framework and is not a fair comparison. Although our method does not obtain the state-of-the-art result on H36M, we want to point out that 1) our method is trained on purely synthetic data and has never seen detected 2D keypoints from H36M videos, 2) we do not use any image-level information and only relies on a small set (12 in total) of 2D keypoints to infer global pose. Under such conditions, it is especially challenging when the person's movement direction is parallel to the forward direction of the camera (the person is moving along the depth direction in the image plane). We also evaluate a different subset of 3D keypoints, as our use of forward kinematics FK ? causes our joint definition to be different from prior work. For more clarification and visualization on the H36M evaluation, please refer to the Appendix and project site. <ref type="table">Table 2</ref>: Evaluation of plausibility on the PROX dataset. Since PROX has no ground truth annotation, we employ ACD (one-way Average Chamfer Distance) between the segmented point cloud and predicted body mesh as a proxy metric for pose estimation plausibility, as formulated in HuMoR <ref type="bibr" target="#b30">[31]</ref>. Notice that the ACD values correspond to a different number of sequences based on the success rate (out of 60 sequences from PROX). Since all of the metrics can only approximate plausibility, we encourage our readers to refer to our project website for visualization of all 60 sequences on PROX of our method. Time(s) is the per-frame average runtime evaluated on RTX 2080 Super. Note that only HybrIK <ref type="bibr" target="#b15">[16]</ref> w/ RootNet <ref type="bibr" target="#b23">[24]</ref> and ours are not multi-stage optimization based methods (indicated by the coloum "Opti.") and offers a relatively fair comparison. Results on PROX. <ref type="table">Table 2</ref> shows our result in the PROX dataset where we compare against SOTA multi-stage batch optimization based methods: LEMO <ref type="bibr" target="#b46">[47]</ref> and HuMoR <ref type="bibr" target="#b30">[31]</ref>. We also report the result using HyperIK with RootNet <ref type="bibr" target="#b15">[16]</ref> to show that severe occlusion and human scene interaction on the PROX dataset is also challenging to SOTA regression methods. From <ref type="table">Table 2</ref>, we can see that our method achieves the best ACD (chamfer distance) without using any depth information and is competitive in all other metrics against both RGB and RGB-D based methods. Note that LEMO and HuMoR directly optimize the chamfer distance and ground penetration during optimization, while our method relies only on 2D keypoints and laws of physics to achieve plausible pose estimates. Methods with exceedingly high ACD values (&gt; 1000) can be attributed to unstable optimization or unreliable depth estimation, which causes the humanoid to fly away for some sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>In <ref type="figure" target="#fig_4">Fig.4</ref>, we visualize our results in each of the scene from the PROX dataset. <ref type="figure" target="#fig_5">Fig.5</ref> compares our result with SOTA methods. We can see that our method can estimate accurate 3D absolute poses that align with the 3D scene. Compared to batch multistage optimization methods that utilize RGB-D input, PROX <ref type="bibr" target="#b6">[7]</ref> and LEMO <ref type="bibr" target="#b46">[47]</ref>, our method can achieve comparable results while running in near-real-time. Compared to the regression-based method, HybrIK <ref type="bibr" target="#b15">[16]</ref> with RootNet <ref type="bibr" target="#b23">[24]</ref>, our method can estimate much more accurate root translation and body pose. In terms of physical plausibility, optimization-based methods often perform well but can still exhibit floating and penetration. While optimization methods can take over a minute per-frame, catastrophic failure can still occur (as shown in the fourth row). For video quantitative comparison, please refer to our project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we ablate the main components of our framework. Row 1 (R1) corresponds to not using our modified scene geometry and uses the mesh provided by the PROX dataset; row 2 corresponds to not using any scene awareness (voxel features); R2 uses MPG with only one time-step, which results in nosier gradients; R3 trains a model without using the geometric translation step in G; R4 does not use the TCN model F TCN in G; R5 does not leverage modified 3D scenes and uses the original scene scans. R1 shows that our modified geometry leads to better overall performance, as the scanned mesh does not correspond well to the actual scene. R2 demonstrates that scene awareness further improves our result, where our agent learns important affordance information about chairs and sofas to improve the success rate. R3 highlights the importance of taking multiple gradient steps to obtain better gradients. When comparing R4 and R5 with R6, we can see that the learned function G is essential to obtain a better root translation and orientation, without which the agents drift significantly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head><p>Failure Cases. We show the failure cases of our proposed method in the last two rows of <ref type="figure" target="#fig_5">Fig. 5</ref>. Although our method can estimate human pose and human-scene interaction from the PROX dataset with a high success rate, there is still much room for improvement. We simplify all humanoids and scene geometries as rigid bodies and do not model any soft materials such as human skin, cushions, or beds. As a result, our sitting and lying motion can often result in jittery motion, as contact occurs only in a very small surface area on the simulated body and hard surfaces. Due to the nature of physics simulation, our simulated humanoid sometimes gets stuck on narrow gaps between chairs and tables, or when lying on beds. In these scenarios, our method can often recover quickly by jolting out of the current state and trying to follow the target kinematic pose. Nevertheless, if the drift of global translation is severe or many 2D keypoints are occluded, our agent may lose balance. Our causal formulation may also lead to jittery motion when 2D keypoint observations jitter. Currently, our agent has no freedom to deviate from the 2D observations and always tries to match the 2D motion even when it is stuck. More intelligent control modeling will be needed to improve these scenarios.</p><p>Conclusion and Future Work. We introduce embodied scene-aware human pose estimation, where we formulate pose estimation in a known environment by controlling a simulated agent to follow 2D visual observations. Based on the 2D observation, its current body pose, and environmental features, our embodied agent acts in a simulated environment to estimate pose and interact with the environment. A multi-step projection gradient (MPG) is proposed to provide global movement cues to the agent by comparing the current body pose and target 2D observations. The experimental result shows that our method, trained only on synthetic datasets derived from popular motion datasets, can estimate accurate global 3D body pose on the challenging PROX dataset while using only RGB input. Future work includes providing a movement prior for the embodied agent to intelligently sample body motion based on scene features to better deal with occlusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Broader Social Impact 20</head><p>In this Appendix, we provide extended qualitative results, and more details about our Multi-step Projection Gradient as feature, evaluation, and implementation. All code will be released for research purposes. For supplementary videos, please refer to our project page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Supplementary Video</head><p>As motion is best seen in videos, we provide a supplemental video at our project site to showcase our method. Specifically, we provide qualitative results on the PROX and H36M datasets and also compare with state-of-the-art RGB-based methods (HuMoR-RGB and HyberIK). From the visual result, we can see that our method can estimate physically valid human motion from 2D keypoint inputs in a causal fashion and control a simulated character to interact with the scenes. Compared to non-physics-based method, ours demonstrates less penetration and remain relatively stable under occlusion. For future work, incorporating more physical prior and simulating soft objects such as cushions can further improve motion quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Multi-step Projection Gradient (MPG)</head><p>In this section, we provide a detailed formulation for our multi-step projection gradient as a feature formulation. First, we provide our keypoint definition based on forward kinematics in Sec. B.1. Then, we will describe the geometric and learning-based transformation function G for augmenting MPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Forward Kinematics and Keypoint Definition</head><p>The SMPL body model defines 24 body joints (green dots visualized in <ref type="figure">Fig. 6</ref>), which correspond to the 24 angles of the joints. For datasets such as H36M <ref type="bibr" target="#b9">[10]</ref> and COCO <ref type="bibr" target="#b17">[18]</ref>, some joints of the SMPL human body do not have a one-to-one correspondence. Thus, prior SMPL-based human pose estimation methods often make use of a predefined joint regressor to recover the corresponding joints from the body vertices. As described in Sec.3.1, for the same body shape, the lengths of the limbs can change according to body pose, and changing the joint regressor may even increase performance <ref type="bibr" target="#b7">[8]</ref>. However, it is computationally expensive to regress 3D joint locations with linear blend skinning at every MPG step. Thus, we opt to use a skeleton with fixed limb length and forward kinematics for our embodied agent toward real-time humanoid control. To obtain limb length, we use the rest pose (T-pose) of the SMPL body and recover the joint positions using the pre-defined joint regressor. From the joint positions, we compute the fixed limb lengths following the parent-child relationship defined in the kinematics tree. <ref type="figure">Figure 6</ref>: Visualization of the 12 joints we use for pose estimation and the 2D keytpoins obtained from SMPL using forward kinematics. Red is the DEKR 2D keypoints detection result <ref type="bibr" target="#b2">[3]</ref>, white is our pose estimation result, and green (numbered) is the original 2D keypoints obtained from the projection joints of SMPL. For both the COCO 2D keypoints <ref type="bibr" target="#b17">[18]</ref> and H36M 3D joint positions of H36M <ref type="bibr" target="#b9">[10]</ref>, we use a subset of these points to obtain the corresponding points on the SMPL kinematics tree. Specifically, we use 12 joints, a similar joint definition to HuMoR <ref type="bibr" target="#b30">[31]</ref>, as visualized in <ref type="figure">Fig.  6</ref>. To obtain correspondences, we choose the following joint numbers from the 24 keypoints based on SMPL: #15, #16, #17, #20, #21, #22, #2, #3, #6, #7 without any processing. Then we average the position of joint #15 and #20 to obtain the mid-chest point. The pelvis point is obtained by averaging the 3 joints #0, #1, and #5. Following the process mentioned above, we obtain a sparse set of points (12) that we can directly obtain from the SMPL body pose parameters using forward kinematics to match visual observations and evaluation. Since not all 2D keypoints are confidently estimated, we use a confidence threshold of 0.1. For detected 2D keypoints of confidence &lt; 0.1, we do not consider them and omit them during MPG calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 MPG and Geometric and Learning-based Transform G</head><p>We define the 3D body pose for time step t as q t (r R t , r T t , ? t ) where q t ? R 75 . r R t ? R 3 and ? t ? R 69 are root and body joint orientations in axis-angles, while r T t ? R 3 is the global translation.</p><p>Geometric Transform of G for ?r R t . Due to depth ambiguity and severe occlusion, recovering global translation accurately from 2D observations alone is an ill-posed problem. Previous attempts that use image features <ref type="bibr" target="#b23">[24]</ref> and 2D keypoints <ref type="bibr" target="#b26">[27]</ref> can often result in unreliable translation estimates (as seen in Tab.2 and our quantitative video). Our pose gradient, on the other hand, provides temporal movement information instead of estimating global translation in one-shot. Using the temporal smoothness of human movement and the laws of physics, our pose gradient is more robust to occlusion and will not produce sudden jumps in pose estimates. To provide better root translation, we also take advantage of a geometric transformation step to further improve ?r T t . Given 2D keypoints kp 2D t+1 and body pose q k t in this gradient descent iteration, we can treat a human skeleton as a rigid body to obtain a better translation estimate. Specifically, we compute the world-space 3D keypoints using forward kinematics J t = FK ? (q k t ) and optimize the following reprojection error: arg min</p><formula xml:id="formula_8">?r T?k t ||? cam (J t + ?r T?k t ) ? kp 2D t+1 || 2 G( ?L proj ?q k t , q k t , kp 2D t??:t+1 )<label>(7)</label></formula><p>C Details about Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Evaluation Metrics</head><p>Here we provide a detailed list of our metric definition.</p><p>? SR: success rate measures whether the pose sequence has been estimated successfully. For optimization-based methods, LEMO <ref type="bibr" target="#b46">[47]</ref> and HuMoR <ref type="bibr" target="#b30">[31]</ref>, it indicates whether the optimization process has successfully converged or not. For physics-based methods (ours), it indicates whether the agent has lost balance or not during pose estimation. We measure losing balance as when the position of the simulated agent q t and the output of the kinematic policyq t deviates too much from each other (&gt; 0.3 meters per joint on average). For regression-based methods, e.g., HyberIK <ref type="bibr" target="#b15">[16]</ref>, we do not use this metric. ? ACD (mm): one-way average chamfer distance measures the chamfer distance between the estimated human mesh and the point cloud captured from the RGB-D camera. This metric is a proxy measurement of how well the policy can estimate global body translation, as used in HuMoR <ref type="bibr" target="#b30">[31]</ref>. Notice that RGB-D based methods such as HuMoR-RGBD and Prox-D both directly optimize this value, while our method does not use any depth information. Unstable optimization and degenerate solutions (where the humanoid flies away) can lead to a very large ACD value. ? Accel. (mm/frame 2 ): acceleration measures the joint acceleration and indicates whether the estimated motion is jittery. From the MoCap motion sequences from the AMASS dataset, a value of around 3 ? 10 mm/frame 2 indicates human motion that are smooth. When there is no ground-truth motion available, the joint acceleration serves as a proxy measurement for physical realism. ? Freq. (%): frequency of penetration measures how often a scene or ground penetration of more than 100 mm occurs. We obtain the scene penetration measurement by using the mesh vertices obtained from the SMPL body model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Modified Scenes</head><p>As described in Sec.3.2, we make modified scenes based on the scanned meshes to better approximate the actual 3D scene. Due to the inaccuracy of scanning procedures and point cloud to mesh algorithms, the scanned meshes are often bulkier than the real environment. As a result, the gaps between chairs and tables are often too small in the mesh form, as visualized in <ref type="figure">Fig. 7</ref>. These inaccuracies can often cause the humanoid to get stuck between chairs and tables when trying to follow the 2D observations. To resolve this issue, we created modified scenes from the scanned meshes to better match the videos. We create these scenes semi-automatically by selecting keypoints in the mesh vertices and create rectangular cuboids and cylinders. Another advantage of using these modified scenes is that the signed distance functions for these modified geoms can be computed efficiently. Modified scenes are created for all scenes from the Prox and H36M datasets. All scenes are visualized in <ref type="figure" target="#fig_4">Fig.4</ref>.</p><p>C.3 Clarification about evaluation on H36M <ref type="figure">Figure 7</ref>: Comparison between the scanned scene and our modified scene based on simple shapes. Notice that the scanned scene has much thicker chairs and tables, which leave smaller gaps for the agent to sit down.</p><p>We conduct quantitative evaluations on the H36M dataset, as shown in <ref type="table" target="#tab_1">Table 1</ref>. Since we do not make use of a joint regressor, we cannot perform the standard evaluation using the 14 joints commonly used for the evaluation on the H36M dataset. Instead, we use the 12 joints defined in Sec.B.1 to calculate MPJPE and A-MPJPE. As input, we use the same set of 12 2D keypoints. The 2D keypoints are estimated using a pre-trained DEKR <ref type="bibr" target="#b2">[3]</ref> without additional fine-tuning on the H36M dataset. Our method only uses a sparse set of detected 2D keypoints (12 keypoints) as input and does not directly use any image feature. We also evaluate a different subset of 3D keypoints, as our use of forward kinematics FK ? causes our joint definition to be different from prior works. For quantitative results, please refer to our supplementary video. This experiment also shows our method's ability to estimate pose in a generic setting where there are almost no scene elements (except for the ground and sporadically a chair).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Shape-based Universal Humanoid Controller</head><p>We extend the universal humanoid controller (UHC) proposed in Kin_poly <ref type="bibr" target="#b20">[21]</ref> to also support humanoids with different body shapes. Specifically, we added the shape and gender parameter H (?, gender) to the state space of UHC to form our body shape conditioned UHC: ? UHC (a t |s t , q t , H).</p><p>In the original UHC, the body shape information is discarded during training, and the policy is learned using a humanoid based on the mean SMPL body shape. In ours, we create different humanoids for each body shape during training using the ground-truth body shape annotation from AMASS. In this way, we can learn a humanoid controller that can control humanoids with different body shapes. The action space and policy network architecture, and the reward function remain the same.</p><p>Training procedure. We train on the training split of the AMASS dataset <ref type="bibr" target="#b21">[22]</ref> and remove motion sequences that involve human-object interactions (such as stepping on stairs or leaning on tables). This results in 11402 high-quality motion sequences. At the beginning of each episode, we sample a fixed-length sequence (maximum length 300 frames) to train our motion controller. Reference state initialization <ref type="bibr" target="#b28">[29]</ref> is uesed to randomly choose the starting point for our sampled sequence and we terminate the simulation if the simulated character deviates too far from the reference motion q p t . To choose which motion sequence to learn from, we employ a hard-negative mining technique based on the success rate of the sequence during training. For each motion sequence Q i = q 0 , . . . , q T , we maintain a history queue (of maximum length 50) for the most recent times the sequence is sampled: h i 1 , . . . h i 50 where each h i 0 is a Boolean variable indicating whether the controller has successfully imitated the sequence during training. We compute the expected success rate s i of the sequence based on the exponentially weighted moving average using the history s i = ewma(h i 1 , . . . h i 50 ). The probability of sampling sequence i is then P Q i = exp(?s i /? ) J i exp(?s i /? ) where ? is a temperature parameter. Intuitively, the more we fail at imitating a sequence, the more likely we will sample it. Here, we sample from the AMASS dataset (sequence name in the caption).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Embodied Scene-aware Kinematic Policy</head><p>Network Architecture and Hyperparameters Our kinematic policy is an RNN-based policy with multiplicative compositional control <ref type="bibr" target="#b27">[28]</ref>. It consists of a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b1">[2]</ref> based feature extractor to provide memory for our policy. Then, 6 primitives ? prim KIN are used to provide a diverse set of pose estimates based on 2D observation ? i (MPG) and current body state s t . We process the environmental features separately from the pose features due to its high dimensionality, using an MLP S t . The output of S t , concatenated with the RNN output, is fed into a composer ? comp KIN to produce the action weights. Intuitively, the primitive produces potential body movement based on the 2D observation and current body pose. Then, the composer chooses plausible action based on the environmental features. The detailed data flow of our policy is visualized in <ref type="figure">Fig. 8</ref>. The hyperparameters we used for training are shown in <ref type="table" target="#tab_5">Table 4</ref>. Training procedure. To train our kinematic policy, we sample motion sequences from AMASS <ref type="bibr" target="#b21">[22]</ref>, H36M <ref type="bibr" target="#b9">[10]</ref>, and Kin_poly <ref type="bibr" target="#b19">[20]</ref> to generate synthetic paired 2D keypoints. To learn a network that is more robust to 2D keypoints detected from real videos, we randomly sample confidence from a uniform [0, 1] distribution as the detection confidence. Keypoints with confidence &lt; 0.1 will be omitted in our model. For each episode, we randomly sample a sequence, randomly change its heading, and sample a valid starting point in the camera frustum as visualized in <ref type="figure">Fig.?</ref>?. We constrain the newly sampled sequence to be always visible in the image plane and re-sample if not. With a probability of 0.5, we also sample a random scene from the PROX dataset to let our agent learn human-scene interactions. Although the randomly sampled motion sequence does not always yield reasonable human-scene interactions (e.g. the reference motion may walk through a chair), collision and physics simulation will stop our agent automatically. In this way, our agent can learn to use the scene interaction term based on acting inside the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Broader Social Impact</head><p>This research focuses on estimating physically valid human poses from monocular videos. Such a method can be positively used for animation, telepresence, at home robotics, etc. where natural looking human pose can be served as the basis for higher-level methods. It can also lead to malicious use cases, such as illegal surveillance and video synthesis. As our method focuses on fast and causal inference, it is possible to significantly improve its speed and deploy it to real-time scenarios. Thus, it is essential to deploy these algorithms with care and make sure that the extracted human poses are with consent and not misused.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our embodied human pose estimation framework: given 2D keypoint features, our model computes the next-time frame pose based on third-person 2D observation (multi-step projection gradient ? t ), proprioception features (body state s t ), and scene (occupancy o t ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ALGORITHM 1 :</head><label>1</label><figDesc>Learning embodied pose estimator via dynamics-regulated training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Our modified scene for simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of our pose estimation result on the PROX and H36M dataset, along with our modified scenes. For each scene, we approximate the scene geometries with simple shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparision with SOTA methods. Last two rows are failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>? Pen (mm): penetration measures the mean absolute values of the penetrated vertices in the scene / ground, similar to the penetration metric in SimPOE<ref type="bibr" target="#b45">[46]</ref>. ? MPJPE (mm): mean per-joint position error is a popular pose-based metric that measures how well the estimated joint matches the ground-truth MoCap joint locations. This metric is calculated in a root-relative fashion where the root joint (#0, as shown inFig.6is aligned.? PA-MPJPE (mm): mean per-joint position error after procrustes alignment. ? A-MPJPE (mm): absolute mean per-joint position error measures the absolute (non rootrelative) joint position error in the world coordinate frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Network architecture for our scene-aware kinematic policy AMASS: ACCAD-sway AMASS: BioMotionLab_NTroje-kick AMASS: BMLhandball-handball Visualization of a randomly sampled motion sequence and paired 2D keypoints for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input: Pre-trained controller ?UHC, ground truth motion dataset Q ; while not converged do M dyna ? ? initialize sampling memory ; while M dyna not full do q 1:T ? sample random sequence of human pose ;</figDesc><table><row><cell>kp 2D 1:T</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>MeTRAbs [33]</cell><cell>223.9</cell><cell>49.3</cell><cell>34.7</cell></row><row><cell>VIBE [16]</cell><cell>-</cell><cell>61.3</cell><cell>43.1</cell></row><row><cell>SimPoE [46]</cell><cell>-</cell><cell>56.7</cell><cell>41.6</cell></row><row><cell>HybrIK [16] w/ RootNet [24]</cell><cell>135.9</cell><cell>65.2</cell><cell>43.4</cell></row><row><cell>Ours*</cell><cell>284.0</cell><cell>105.1</cell><cell>72.1</cell></row></table><note>Our pose estimation result on the H36M dataset. *Since our method does not directly use any image-level feature, uses a smaller set of 2D keypoints for observation, and evaluates on a different set of 3D keypoints, it is not a fair comparison against SOTA methods.Image-features A-MPJPE ? MPJPE ? PA-MPJPE ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Accel. ? Freq. ? Pen. ? Freq. ? Pen. ? Time(s) ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground</cell><cell cols="2">Scene</cell><cell></cell></row><row><cell cols="4">Phys. Scene Opti. ACD ? RGB-D SR ? PROX [7] 98.3% 181.1 HuMoR [31] 100% 325.5</cell><cell>157.4 3.4</cell><cell>10.6 % 2.4%</cell><cell>109.1 113.6</cell><cell>0.7 0.4 %</cell><cell>46.8 % 47.5</cell><cell>73.1 13.72</cell></row><row><cell></cell><cell>LEMO [47]</cell><cell cols="2">71.7% 173.2</cell><cell>3.0</cell><cell>0 %</cell><cell>14.5</cell><cell>0.2%</cell><cell>22.8</cell><cell>75.19</cell></row><row><cell></cell><cell>PROX [7]</cell><cell cols="2">90.0% &gt;1000</cell><cell>381.6</cell><cell cols="3">4.6 % &gt; 1000 0.39 %</cell><cell>36.7</cell><cell>47.64</cell></row><row><cell></cell><cell>HuMoR [31]</cell><cell>100%</cell><cell>311.5</cell><cell>3.1</cell><cell>0.1 %</cell><cell>38.1</cell><cell>0.3%</cell><cell>36.4</cell><cell>11.73</cell></row><row><cell>RGB</cell><cell>HybrIK [16] w/ RootNet [24]</cell><cell>-</cell><cell>&gt; 1000</cell><cell>310.6</cell><cell>4.4%</cell><cell>384.7</cell><cell>0.6 %</cell><cell>48.9</cell><cell>0.08</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">98.3% 171.7</cell><cell>9.4</cell><cell>0%</cell><cell>24.9</cell><cell>0 %</cell><cell>16.6</cell><cell>0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of different components of our framework.</figDesc><table><row><cell>Ground</cell><cell>Scene</cell></row></table><note>ACD ? Accel. ? Freq. ? Pen. ? Freq. ? Pen.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Supplementary Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Forward Kinematics and Keypoint Definition . . . . . . . . . . . . . . . . . . . . 15 B.2 MPG and Geometric and Learning-based Transform G . . . . . . . . . . . . . . . 16 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 C.2 Modified Scenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 C.3 Clarification about evaluation on H36M . . . . . . . . . . . . . . . . . . . . . . . 18 Shape-based Universal Humanoid Controller . . . . . . . . . . . . . . . . . . . . 18 D.2 Embodied Scene-aware Kinematic Policy . . . . . . . . . . . . . . . . . . . . . . 18</figDesc><table><row><cell>Appendix</cell><cell></cell></row><row><cell>A Qualitative Results</cell><cell>15</cell></row><row><cell>A.1 B Multi-step Projection Gradient (MPG)</cell><cell>15</cell></row><row><cell>B.1 C Details about Evaluation</cell><cell>17</cell></row><row><cell>C.1 D Implementation Details</cell><cell>18</cell></row><row><cell>D.1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for our scene-aware kinematic policy.</figDesc><table><row><cell></cell><cell>Batch Size</cell><cell>Learning Rate</cell><cell>Voxel grid</cell><cell>w L1</cell><cell>w prior</cell></row><row><cell>Value</cell><cell>5000</cell><cell>5 ? 10 ?4</cell><cell>16 ? 16 ? 16</cell><cell>5</cell><cell>0.0001</cell></row><row><cell></cell><cell cols="2">TCN window MPG -#steps</cell><cell>MLP-hidden</cell><cell cols="2">RNN-hidden # primitives</cell></row><row><cell>Value</cell><cell>81</cell><cell>5</cell><cell>[1024, 512, 256]</cell><cell>512</cell><cell>6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b4">(5)</ref> We solve this equation using SVD to obtain ?r T-k t and ?r T-k t is updated by Eq. 6.?r T-k t = ?r T-k t + ?r T-k t(6)Notice that ?r T-k t is guaranteed to lead to a 2D reprojection error no bigger than the original ?r T t , so we use a direct additive transform to obtain a better global root translation gradient.Learning-based Transform of G for ?r T t . Unlike global translation where depth ambiguity can cause sudden jumps in depth perception, we find that global rotation can be robustly estimated using a sequence of 2D keypoints only. Inspired by PoseTriplet<ref type="bibr" target="#b3">[4]</ref>, we use the temporal convolution model (TCN)<ref type="bibr" target="#b26">[27]</ref> to recover the camera space 3D body orientation. Unlike PoseTriplet<ref type="bibr" target="#b3">[4]</ref>, we</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , ?r R t , ?r T-k t , ?q t ] = K k=0</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This project is sponsored in part by the JST AIP Acceleration Research Grant (JPMJCR20U1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>only regress the 3D body orientation and do not regress the 3D body pose: the TCN model is not quite robust to occluded or incorrect 2D keypoints (e.g. missing leg joints, which is prevalent when people are moving behind furniture). However, we notice that the TCN can still recover a reasonable root orientation by reasoning about reliable body joints (such as arms and torsos). Thus, we use the TCN to regress the camera space root orientation r R t = ? ?1 cam (F TCN (kp 2D t??:t+1 )) based on a window of 2D keypoints. We then calculate the difference between current root rotationr R t and r R t :</p><p>We directly add ?r R t as an additional entry to MPG instead of modifying ?r R?k t since there is no guarantee that the 2D reprojection error from using ?r R t will be better. We compute ?r R t only once instead of K times. Combining the above two components, we obtain our complete MPG feature.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wending</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up human pose estimation via disentangled keypoint regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14671" to="14681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Posetriplet: Co-evolving 3d human pose estimation, imitation, and hallucination under self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2203.15625</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic scene-aware motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11354" to="11364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Resolving 3d human pose ambiguities with 3d scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2282" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Resolving 3D human pose ambiguities with 3D scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple method to boost human pose estimation accuracy by correcting the joint regressor for the human3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hedlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>6m dataset. ArXiv, abs/2205.00076, 2022</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural mocon: Neural motion control for physically plausible human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2203.14065</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5607" to="5616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5252" to="5262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spec: Seeing people in the wild with an estimated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><forename type="middle">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11015" to="11025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3383" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Putting humans in a scene: Learning affordance in 3d indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12360" to="12368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smpl: a skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamics-regulated kinematic policy for egocentric pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hachiuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5441" to="5450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">imapper: Interaction-guided joint scene and human motion mapping from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<idno>abs/1806.07889</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10132" to="10141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/2008.03713</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7745" to="7754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mcp: Learning composable hierarchical control with multiplicative compositional policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno>37:143:1-143:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pischulini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15185" to="15195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Humor: 3d human motion model for robust pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>abs/2201.02610</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MeTRAbs: metric-scale truncation-robust heatmaps for absolute 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="30" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural monocular 3d human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Physcap: Physically plausible monocular 3d motion capture in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>39:235:1-235:16</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Human body model fitting by learned gradient descent. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural state machine for character-scene interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering 3d human mesh from monocular images: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2203.01923</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2012-10" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geometric pose affordance: 3d human pose with scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno>abs/1905.07718</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Physics-based human motion estimation and synthesis from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shkurti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11512" to="11521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Glamr: Global occlusion-aware human mesh recovery with dynamic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d ego-pose estimation via imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ego-pose estimation and forecasting as real-time pd control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="10082" to="10092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simpoe: Simulated character control for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning motion priors for 4d human body capture in 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning motion priors for 4d human body capture in 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/2108.10399</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Place: Proximity learning of articulation and contact in 3d environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">For all authors</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
