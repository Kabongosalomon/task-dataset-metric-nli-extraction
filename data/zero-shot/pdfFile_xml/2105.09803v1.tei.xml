<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-Supervised Physically Unconstrained Gaze Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Kothari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>De Mello</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
							<email>spark@lunit.ioshalinig</email>
							<affiliation key="aff2">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-Supervised Physically Unconstrained Gaze Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge for physically unconstrained gaze estimation is acquiring training data with 3D gaze annotations for in-the-wild and outdoor scenarios. In contrast, videos of human interactions in unconstrained environments are abundantly available and can be much more easily annotated with frame-level activity labels. In this work, we tackle the previously unexplored problem of weaklysupervised gaze estimation from videos of human interactions. We leverage the insight that strong gaze-related geometric constraints exist when people perform the activity of "looking at each other" (LAEO). To acquire viable 3D gaze supervision from LAEO labels, we propose a training algorithm along with several novel loss functions especially designed for the task. With weak supervision from two large scale CMU-Panoptic and AVA-LAEO activity datasets, we show significant improvements in (a) the accuracy of semisupervised gaze estimation and (b) cross-domain generalization on the state-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation benchmark. We open source our code at https://github.com/NVlabs/weaklysupervised-gaze.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Much progress has been made recently in the task of remote 3D gaze estimation from monocular images, but most of these methods are constrained to largely frontal subjects viewed by cameras located within a meter of them <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b19">20]</ref>. To go beyond frontal faces, a few recent works explore the more challenging problem of so-called "physically unconstrained gaze estimation", where larger camera-to-subject distances and higher variations in head pose and eye gaze angles are present <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8]</ref>. A significant challenge there is in acquiring training data with 3D gaze labels, generally and more so outdoors. Fortunately, several 3D gaze datasets with large camera-to-subject dis-* Rakshit Kothari was an intern at NVIDIA during the project.  <ref type="figure">Figure 1</ref>. Overview of our weakly-supervised gaze estimation approach. We employ large collections of videos of people "looking at each other" (LAEO) curated from the Internet without any explicit 3D gaze labels, either by themselves or in a semi-supervised manner to learn 3D gaze in physically unconstrained settings.</p><p>tances and variability in head pose have been collected recently in indoor laboratory environments using specialized multi-cameras setups <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b27">28]</ref>. In contrast, the recent Gaze360 dataset <ref type="bibr" target="#b16">[17]</ref> was collected both indoors and outdoors, at greater distances to subjects. While the approach of Gaze360 advances the field significantly, it nevertheless requires expensive hardware and many co-operative subjects and hence can be difficult to scale.</p><p>Recently "weakly-supervised" approaches have been demonstrated on various human perception tasks, such as body pose estimation via multi-view constraints <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14]</ref>, hand pose estimation via bio-mechanical constraints <ref type="bibr" target="#b36">[37]</ref>, and face reconstruction via differentiable rendering <ref type="bibr" target="#b5">[6]</ref>. Nevertheless, little attention has been paid to exploring methods with weak supervision for frontal face gaze estimation <ref type="bibr" target="#b41">[42]</ref> and none at all for physically unconstrained gaze estimation. Eye gaze is a natural and strong nonverbal form of human communication <ref type="bibr" target="#b26">[27]</ref>. For instance, babies detect and follow a caregiver's gaze from as early as four months of age <ref type="bibr" target="#b37">[38]</ref>. Consequently, videos of hu-man interactions involving eye gaze are commonplace and are abundantly available on the Internet <ref type="bibr" target="#b9">[10]</ref>. Thus we pose the question: "Can machines learn to estimate 3D gaze by observing videos of humans interacting with each other?".</p><p>In this work, we tackle the previously unexplored problem of weakly supervising 3D gaze learning from videos of human interactions curated from the Internet <ref type="figure">(Fig. 1)</ref>. We target the most challenging problem within this domain of physically unconstrained gaze estimation. Specifically, to learn 3D gaze we leverage the insight that strong gazerelated geometric constraints exist when people perform the commonplace interaction of "looking at each other" (LAEO), i.e., the 3D gaze vectors of the two people interacting are oriented in opposite directions to each other. Videos of the LAEO activity can be easily curated from the Internet and annotated with frame-level labels for the presence of the LAEO activity and with 2D locations of the persons performing it <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref>. However, estimating 3D gaze from just 2D LAEO annotations is challenging and ill-posed because of the depth ambiguity of the subjects in the scene. Furthermore, naively enforcing the geometric constraint of opposing gaze vector predictions for the two subjects performing LAEO is, by itself, insufficient supervision to avoid degenerate solutions while learning 3D gaze.</p><p>To solve these challenges and to extract viable 3D gaze supervision from weak LAEO labels, we propose a training algorithm that is especially designed for the task. We enforce several scene-level geometric 3D and 2D LAEO constraints between pairs of faces, which significantly aid in accurately learning 3D gaze information. While training, we also employ a self-training procedure and compute stronger pseudo 3D gaze labels from weak noisy estimates for pairs of faces in LAEO in an uncertainty-aware manner. Lastly, we employ an aleatoric gaze uncertainty loss and a symmetry loss to supervise learning. Our algorithm operates both in a purely weakly-supervised manner with LAEO data only or in a semi-supervised manner along with limited 3D gaze-labeled data.</p><p>We evaluate the real-world efficacy of our approach on the large physically unconstrained Gaze360 <ref type="bibr" target="#b16">[17]</ref> benchmark. We conduct various within-and cross-dataset experiments and obtain LAEO labels from two large-scale datasets: (a) the CMU Panoptic <ref type="bibr" target="#b15">[16]</ref> with known 3D scene geometry and (b) the in-the-wild AVA-LAEO activity dataset <ref type="bibr" target="#b24">[25]</ref> containing Internet videos. We show that our proposed approach can successfully learn 3D gaze information from weak LAEO labels. Furthermore, when combined with limited (in terms of the variability of subjects, head poses or environmental conditions) 3D gaze-labeled data in a semi-supervised setting, our approach can significantly help to improve accuracy and cross-domain generalization. Hence, our approach not only reduces the burden of acquiring data and labels for the task of physically uncon-strained gaze estimation, but also helps to generalize better for diverse/naturalistic environments.</p><p>To summarize, our key contributions are: ? We propose a novel weakly-supervised framework for learning 3D gaze from in-the-wild videos of people performing the activity of "looking at each other". To our understanding, we are the first to employ videos of humans interacting to supervise 3D gaze learning. ? To effectively derive 3D gaze supervision from weak LAEO labels, we introduce several novel training objectives. We learn to predict aleatoric uncertainty, use it to derive strong pseudo-3D gaze labels, and further propose geometric LAEO 3D and 2D constraints to learn gaze from LAEO labels. ? Our experiments on the Gaze360 benchmark show that LAEO data can effectively augment data with strong 3D gaze labels both within and across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Gaze Estimation Recent developments in remote gaze estimation increasingly benefit from large-scale datasets with gaze direction <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref> or target <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13]</ref> labels. While earlier methods study the effect of different input facial regions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b44">45]</ref>, later methods attempt to introduce domain-specific insights into their solutions. For example by encoding the eye-shape into the learning procedure <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39]</ref>, or by considering the dependency between head orientation and gaze direction <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>, or modelling uncertainty or random effects <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17]</ref>. Other works propose few-shot adaptation approaches for improving performance for end-users <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23]</ref>. However, most such approaches restrict their evaluations to screenbased settings (with mostly frontal faces and subjects located within 1m of the camera) due to limitations in the diversity of available training datasets.</p><p>Recently proposed datasets such as RT-GENE <ref type="bibr" target="#b7">[8]</ref>, HUMBI <ref type="bibr" target="#b42">[43]</ref>, and ETH-XGaze <ref type="bibr" target="#b43">[44]</ref> attempt to allow for gaze estimation in more physically unconstrained settings such as from profile faces of subjects located further from the camera. As complex multi-view imaging setups are required, these datasets are inevitably collected in controlled laboratory conditions. A notable exception is Gaze360 <ref type="bibr" target="#b16">[17]</ref>, which uses a panoramic camera for collecting data from multiple participants at once, both outdoors and indoors. Yet, such collection methods are still difficult to scale compared to data sourced from the web, or via crowd-sourced participation such as done for the GazeCapture dataset <ref type="bibr" target="#b19">[20]</ref>.</p><p>In terms of learning a generalized gaze estimator using only small amounts of labeled data (without supervised pretraining), Yu et al. <ref type="bibr" target="#b41">[42]</ref> are the only prior art. However, their method is restricted to mostly frontal faces and assumes little to no movement of the head between pairs of samples from a given participant -an assumption that does not hold in less constrained settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaze Following and Social Interaction Labels</head><p>Given an image with a human, gaze following concerns the prediction of the human's gaze target position. Performing this task with deep neural networks was initially explored by Recasens et al. <ref type="bibr" target="#b32">[33]</ref>, with extensions to time sequence data and multiple camera views in <ref type="bibr" target="#b33">[34]</ref>. Chong et al. <ref type="bibr" target="#b2">[3]</ref> improve performance on the static gaze following task further by jointly training to predict 3D gaze direction using the EYEDIAP dataset <ref type="bibr" target="#b8">[9]</ref>, and by explicitly predicting whether the target is in frame. This work is also extended to video data in <ref type="bibr" target="#b3">[4]</ref>. Much like the task of physically unconstrained gaze estimation, gaze following also involves viewing human subjects in all head poses, in diverse environments, and from larger distances. However, gaze following datasets are complex to annotate, and do not lend themselves well to the task of learning to predict 3D gaze due to the lack of scene and object geometry information.</p><p>Alternatively, weak annotations for gaze-based interaction exist in the form of social interaction labels. One such condition is the commonplace "looking at each other" condition, also known as LAEO <ref type="bibr" target="#b25">[26]</ref>, where a binary label is assigned to pairs of human heads for when they are gazing at each other. This is a simpler quantity to annotate compared to mutual attention or visual focus of attention. The recently published AVA-LAEO dataset <ref type="bibr" target="#b24">[25]</ref> is an extension of the AVA dataset <ref type="bibr" target="#b9">[10]</ref> and demonstrates the ease of acquiring such annotations for existing videos. To the best of our knowledge, we are the first to show that social interaction labels such as LAEO can be used for weakly-supervised gaze estimation. Furthermore, adding LAEO-based constraints and objectives consistently improves performance in cross-dataset and semi-supervised gaze estimation, further validating the real-world efficacy of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakly-supervised Gaze Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition and Motivation</head><p>Our goal is to supervise 3D gaze learning with weak supervision from in-the-wild videos of humans "looking at each other". Such scenes contain the LAEO constraint, i.e., the 3D gazes of the two subjects are oriented along the same line, but in opposite directions to each other. We specifically target the challenging task of physically unconstrained gaze estimation where large subject-to-camera distances, and variations in head poses and environments are present. We assume that we have a large collection of videos containing LAEO activities available to us which can be acquired, for example, by searching the web with appropriate textual queries. We further assume that, by whatever means, the specific frames of a longer video sequence containing the LAEO activity have been located and that the 2D bounding boxes of the pair of faces in the LAEO condition are also available. We refer to these labels collectively as the "LAEO labels".</p><p>Acquiring LAEO data is a relatively quick and cost effective way to curate lots of diverse training data. Nevertheless, Internet videos with LAEO labels cannot provide precise 3D gaze supervision. This is because, for such videos neither the scene's precise geometry, nor the camera's intrinsic parameters are known a priori. Moreover, trivially enforcing the simple LAEO constraint of requiring the predicted gaze estimates of the two individuals to be opposite to each other is not sufficient for learning gaze. It quickly leads to degenerate solutions.</p><p>To address these various challenges, we design a novel weakly-supervised learning framework for 3D gaze estimation from LAEO data. Specifically, we propose a number of novel geometric scene-level LAEO losses, including a 3D and a 2D one, that are applied to pairs of faces in LAEO. For individual face inputs we also use an aleatoric gaze loss <ref type="bibr" target="#b17">[18]</ref>, which computes gaze uncertainty, along with a self-supervised symmetry loss. We further propose an uncertainty-aware self-training procedure to generate 3D gaze pseudo ground truth labels from pairs of faces exhibiting LAEO. Our training framework operates in two configurations: (a) a purely weakly-supervised one with LAEO data only and (b) a semi-supervised one, where LAEO data is combined with 3D gaze-labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Solution Overview</head><p>Our overall framework for weakly-supervised 3D gaze learning from LAEO data is shown in <ref type="figure">Fig. 2</ref>. We wish to train the function F(I, ?) with weights ? to estimate gaze by providing video sequences of pairs of people exhibiting LAEO. Inspired by <ref type="bibr" target="#b16">[17]</ref>, our gaze estimation network F(I, ?) consists of a ResNet-18 backbone followed by two bi-directional LSTM layers and a fully-connected (FC) layer, which estimates a gaze value? = {? ? ,? ? } along with an uncertainty value? corresponding to the central image in a sequence of 7 consecutive input frames. Here? ? and? ? indicate the estimates for the gaze pitch and yaw angles, respectively. In addition to this temporal version of our network, we also explore a static variant, which takes a single image as input and bypasses the LSTM layers to directly connect the output of the backbone CNN to the FC layer.</p><p>For LAEO data, the input to our network is a pair of head crops of size 224 ? 224 ? 3 each containing one of the two faces that exhibit LAEO along with the original scene image. No 3D gaze labels are available during training with LAEO data. If data containing explicit 3D gaze labels is additionally available for semi-supervised training, we extract single head crops from the scene images and input them along with their known ground truth 3D gaze labels into the network for training. <ref type="figure">Figure 2</ref>. An overview of our weakly-supervised approach to learning 3D gaze from the "looking at each other" human activity videos. From left to right, we show (a) the inputs to our gaze estimation network, i.e., pairs of head crops in LAEO with their scene images for weakly-supervised training and optionally single head crops with gaze labels for semi-supervised training (if available); (b) our gaze estimation network, which predicts gaze? and its uncertainty?; (c) the various weakly-supervised and fully-supervised losses used for training; (d) estimation of scene geometry for in-the-wild LAEO videos acquired from the web including that of the 2D and 3D positions of the cyclopean eyes of the subject pairs in LAEO used to compute the LAEO losses; and (e) details of our proposed LAEO losses including the geometric 2D LAEO loss (L 2D geom ), geometric 3D LAEO loss (L 3D geom ) and the pseudo gaze loss (L pseudo G ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Functions</head><p>We employ several end-to-end differentiable geometric loss functions, which are derived from the LAEO constraint to supervised 3D gaze learning. These include two scenelevel geometric 2D and 3D LAEO losses. We start by describing our technique for scene geometry estimation and 3D gaze origin determination for in-the-wild videos and then describe our geometric LAEO losses. We then describe our uncertainty-aware 3D gaze pseudo labeling procedure, followed by two additional losses -the aleatoric gaze and symmetry losses that are applied to individual face inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Geometry Estimation</head><p>The geometric LAEO loss functions can only be computed in a coordinate system common to both subjects, i.e., the camera coordinate system. For Internet videos, we cannot reliably recover camera parameters or the subjects' 3D poses. So we instead derive approximate values for them. We approximate the camera focal parameter f to be the size of the larger image dimension in pixels. The principal point is assumed to be at the center of the image. We detect the 2D facial landmarks of a subject using AlphaPose <ref type="bibr" target="#b6">[7]</ref> and refer to the midpoint of their left and right eye pixel locations as their "2D cyclopean eye" P 2D = (x, y). We assume it to be the point from where gaze originates for a subject on the 2D image plane. To find its 3D counterpart, i.e., the 3D cyclopean eye P 3D , we also estimate depth z per subject and back-project P 2D to 3D as (zx/f, zy/f, z). This procedure ensures that P 2D and P 3D lie on the same projection line originating from the camera's center.</p><p>To recover depth z of each subject, we first estimate their 2D-3D correspondences using DensePose <ref type="bibr" target="#b10">[11]</ref>. We use the predicted 2D facial key-points <ref type="bibr" target="#b6">[7]</ref> and an average gender neutral 3D SMPL head model <ref type="bibr" target="#b23">[24]</ref> to compute the 3D transformation required to fit the 3D head model to a particular subject using PnP <ref type="bibr" target="#b20">[21]</ref>. This allows for an estimation of the 3D head model's location and orientation in the camera coordinate system, which in turn provides us with depth estimates in meters (see <ref type="figure">Fig. 2</ref>) for each subject. Specifically, we utilize the depth z value of the mid points of the left and right eyes of the fitted 3D head model to recover depth of each subject. The end result is a shared 3D coordinate system for both subjects under LAEO (see <ref type="figure">Fig. 2</ref>). In Sec. D.3 of the supplementary we further discuss the effect of our various approximations employed to compute the scene geometry on the reliability of 3D gaze estimates derived from LAEO data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric 2D LAEO Loss For two subjects A and B</head><p>in LAEO, the projections of their predicted 3D gaze vectors onto the scene image plane, should lie along the line joining their 2D cyclopean eyes P 2D A and P 2D A (see <ref type="figure">Fig. 2</ref>). This intuition forms the basis of our geometric 2D LAEO loss L 2D geom . To compute this loss, we estimate the gaze angles? A for subject A in LAEO by forward propagating their head crop image I A through F(I, ?). We then transform it to a 3D unit gaze vector? 3D A originating from subject A's 3D cyclopean eye P 3D A in the camera coordinate system. Next, we forward project? 3D</p><p>A onto the observed scene image as the 2D gaze vector? 2D</p><p>A (see <ref type="figure">Fig. 2</ref>). To compute L 2D geom , we compute the angular cosine distance between two 2D unit vectors in the image plane: one along? 2D</p><p>A and another one along the line joining P 2D A and P 2D B . We repeat this process for subject B and average both losses to obtain the final loss L 2D geom . Note, however, that L 2D geom on its own cannot fully resolve the depth ambiguity present in videos obtained from the Internet and hence is not sufficient to learn 3D gaze (see <ref type="table">Table 1</ref>), but when combined with the other LAEO losses it helps to improve overall gaze estimation accuracy (see Sec. B.4 in supplementary). Thus, we additionally propose a geometric 3D LAEO loss which helps to resolve depth ambiguities and aids in learning 3D gaze more accurately. We describe it next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric 3D LAEO Loss The geometric 3D LAEO loss, L 3D</head><p>geom , explicitly provides 3D directional information to supervise gaze learning. We formulate it to enforce that the estimated 3D gaze vector originating from the cyclopean eye P 3D B of subject B in LAEO, must intersect the viewed subject A's 3D cyclopean eye P 3D</p><p>A (see <ref type="figure">Fig. 2</ref>). To achieve this, we first estimate the 3D facial plane ? A of the viewed subject A, and place it at their 3D cyclopean eye location P 3D</p><p>A perpendicular to their heading vector. We define the heading vector as the line joining the 3D midpoint of a subject's outer most 3D ear points, and 3D nose tip obtained from the fitted SMPL head model. Then the geometric 3D LAEO constraint for subject B is given by ||P 3D</p><formula xml:id="formula_0">A -P 3D A ||, where P 3D A is subject A's 3D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cyclopean eye position and P 3D</head><p>A is the intersection of subject B's 3D gaze vector? 3D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B</head><p>with subject A's face plane ? A (see <ref type="figure">Fig. 2</ref>). Here ||.|| denotes Euclidean distance. We repeat this process for subject A and average the losses computed for both subjects to obtain the final loss L 3D geom . Empirically we find that our formulation for L 3D geom performs better than an alternate cosine angle-based version (see Sec. B.3 in supplementary).</p><p>Pseudo Gaze LAEO Loss The LAEO activity also provides us with the self-supervised constraint that the ground truth 3D gaze vectors of two individuals A and B in LAEO, are oriented along the same 3D line, but in opposite directions to each other, i.e., g 3D</p><formula xml:id="formula_1">A = ?g 3D B .</formula><p>Hence, we leverage it in a self-training procedure and compute gaze pseudo ground truth labels for a pair of LAEO subjects continually while training. We observe that the LAEO activity often results in a clear frontal view of one subject while the other subject is turned away (see examples in <ref type="figure">Fig. 1 and Fig. 2</ref>). Moreover, gaze estimation errors generally increase with extreme head poses where features such as the eyes are less visible (see <ref type="figure">Fig. 2</ref> in the supplementary for a plot of gaze error versus gaze yaw). For example, in the extreme case of looking from behind a subject, facial features become completely occluded.</p><p>We find that the uncertainty measure estimated by our network is well correlated with gaze error (with a Spearman's rank correlation coefficient of value of 0.46). So to derive the gaze pseudo ground truth for a pair of faces in LAEO, we use the uncertainty measure to weigh more heavily the more reliable (less uncertain) of the two gaze estimates for a LAEO pair. Specifically, let {? 3D A ,? A } and {? 3D B ,? B } be the predicted 3D gaze vectors and their angular uncertainty values (in a common 3D coordinate system) for a pair of input face crops in LAEO, I A and I B , respectively. We compute the pseudo 3D gaze ground truth label g 3D pseudo for faces A and B as a weighted combination of their estimated 3D gaze vectors as:</p><formula xml:id="formula_2">g 3D pseudo = w A? 3D A + w B (?? 3D B ),<label>(1)</label></formula><p>where we compute w A and w B from the angular uncer-</p><formula xml:id="formula_3">tainty values? A and? B as w A =? B /? A +? B and w B = ? A /? A +? B</formula><p>predicted by the gaze network. We further compute cosine distances between each LAEO subjects' predicted gaze vectors? 3D and their respective pseudo ground truth values g 3D pseudo and ?g 3D pseudo . We average the cosine distances computed for both subjects to obtain the final L pseudo G loss. We find that this formulation of L pseudo G is superior to other variants of it (see Sec. B.2 in supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aleatoric Gaze Loss</head><p>We use an aleatoric loss function L G to supervise gaze estimation of individual face inputs, which regresses both the predicted gaze value and its uncertainty. This gaze uncertainty is helpful in deriving pseudo ground truths for pairs of faces in LAEO as described in the previous section. Aleatoric uncertainty models the distribution of the estimated gaze angles as a parametric Laplacian function and hence our gaze network F(I, ?) predicts their estimated mean {? ? ,? ? } and absolute deviation? values. We supervise the network by minimizing the negative loglikelihood of observing the ground truth gaze value {g ? , g ? } w.r.t. to this predicted Laplacian distribution as:</p><formula xml:id="formula_4">L ? G = log(?) + 1 ? |? ? ? g ? | L ? G = log(?) + 1 ? |? ? ? g ? | (2) L G = L ? + L ? .</formula><p>In practice, we predict the logarithm of the absolute deviation log(?) from our network. This formulation has been shown to be numerically stable and avoids a potential division by zero <ref type="bibr" target="#b17">[18]</ref>. Note that previously, in <ref type="bibr" target="#b16">[17]</ref>, the authors similarly employed a pinball loss to estimate the uncertainty of gaze predictions. We find that, in comparison to the pinball loss, the aleatoric loss improves the baseline accuracy of gaze estimation (see Sec. B.1 in supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetry Loss</head><p>We also exploit the left-right symmetry inherent to the gaze estimation task to enforce another selfsupervised gaze symmetry loss L sym . Specifically, we estimate gaze angles for an input face image I as? = {? ? ,? ? }, reverse the sign of its predicted gaze yaw angle to produce the altered prediction? * = {? ? , ?? ? } and use this altered gaze estimate as the ground truth to supervise the predicted gaze, using the aleatoric loss, for a horizontally flipped (mirrored) version I * of the input face image as:</p><formula xml:id="formula_5">L sym = L G (F(I * , ?),? * ).<label>(3)</label></formula><p>We repeat this process for the horizontally flipped image and average the two resultant losses. Note that here the gaze angles are assumed to be in a normalized eye coordinate system as described in <ref type="bibr" target="#b16">[17]</ref>, whose z axis passes through each subject's 3D cyclopean eye position P 3D . This loss prevents network over-fitting while improving accuracy of gaze estimation (see Sec. B.1 in supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>We adopt two training paradigms: purely weaklysupervised training with LAEO data only or semisupervised training where LAEO data augments data containing explicit 3D gaze labels. In both conditions, we initialize the ResNet-18 backbone of our model with weights pre-trained using ImageNet <ref type="bibr" target="#b4">[5]</ref>. We initialize the LSTM module and FC weights using a normal distribution. For semi-supervised training, we first train our model to convergence with images containing explicit 3D gaze labels only and then add weakly-supervised images with LAEO labels and continue training jointly to convergence. We fix the parameters of the batch normalization layers during initialization to those found in the ImageNet pre-trained weights. We optimize the model using the following objective function:</p><formula xml:id="formula_6">L = L G + ?L sym + ?L LAEO , L LAEO = (L 3D geom + L 2D geom + L pseudo G ).<label>(4)</label></formula><p>Here, ? and ? are scalar weights, which slowly ramp up the contribution of the symmetry and LAEO losses, respectively. The ramp operation is formulated as ( i/T 1 ) where i is the smallest iterative step to update our model while T is a threshold. We set T ? as 3000 and T ? as 2400. In experiments, which do not involve any gaze supervision, ? is always fixed at 1 and L G is not included. We use a batch-size of 80 frames/sequences to train our static/temporal gaze estimation network. We use a fixed learning rate of 10 ?4 with the ADAM optimizer <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Here we evaluate the real-world performance of our method in the fully weakly-supervised or semi-supervised settings for the task of physically unconstrained gaze estimation <ref type="bibr" target="#b16">[17]</ref>. We perform extensive experiments within and across datasets. Besides gaze estimation, in Sec. A of the supplementary, we also show the utility of adding LAEO labels to the task of in-the-wild visual target attention prediction <ref type="bibr" target="#b3">[4]</ref> in a semi-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAEO Datasets</head><p>We employ two LAEO datasets -CMU Panoptic <ref type="bibr" target="#b15">[16]</ref> and AVA <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. CMU Panoptic <ref type="bibr" target="#b14">[15]</ref> is collected with a multiple-camera system installed in a large indoor dome, wherein subjects perform various activities. It does not contain LAEO annotations but contains the subjects' 3D body-joint locations and camera calibration information, which we directly use in our experiments. From video sequences containing the haggling activity, we extract clips with the LAEO activity present via a semi-automatic procedure (described Sec. <ref type="figure">D.1 of supplementary)</ref>. This results in over 800k pairs of faces extracted from 485 unique subjects. For our experiments, we only utilize images from cameras that are parallel to the ground plane.</p><p>To acquire LAEO data from in-the-wild Internet videos, we leverage the large scale AVA human activity dataset <ref type="bibr" target="#b9">[10]</ref> with LAEO annotations <ref type="bibr" target="#b24">[25]</ref> provided by Marin-Jimenez et al. (called "AVA-LAEO"). It consists of annotated head bounding box pairs under LAEO in select frames across multiple video sequences, resulting in a wide variety of faces, backgrounds and lighting conditions. Unlike CMU Panoptic, AVA-LAEO does not provide access to accurate camera parameters or 3D human poses. We estimate the subjects' 3D poses using DensePose <ref type="bibr" target="#b10">[11]</ref> and AlphaPose <ref type="bibr" target="#b6">[7]</ref> (described in Sec. 3.3 and Sec. D.2 in supplementary). In all, this dataset contains 13,787 sequences of pairs of faces in LAEO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaze Datasets</head><p>We validate the efficacy of our weaklysupervised approach on the large-scale physically unconstrained in-the-wild Gaze360 <ref type="bibr" target="#b16">[17]</ref> dataset. It contains explicit 3D gaze labels and large variations in subject head poses and gaze angles, and lighting conditions and backgrounds. Its images are acquired in both indoor and outdoor environments using a Ladybug multi-camera system. It contain 127K training sequences from 365 subjects. For semi-supervised training, we additionally use two largescale gaze datasets with known 3D gaze ground-truth -GazeCapture <ref type="bibr" target="#b19">[20]</ref> and ETH-XGaze <ref type="bibr" target="#b43">[44]</ref>. GazeCapture contains nearly 2M frontal face images of 1474 subjects acquired in unconstrained environmental conditions. ETH-XGaze, on the other hand, was acquired indoors with controlled lighting on a standard green background with a multi-view camera system. It contains 756K frames of 80 subjects.</p><p>The gaze distribution plots of all these datasets and their example face images are shown in <ref type="figure">Fig. 3</ref>. For GazeCapture and ETH-XGaze, we use the normalization procedure described in <ref type="bibr" target="#b46">[47]</ref> to create normalized face crops. For all other datasets, we employ the procedure described in <ref type="bibr" target="#b16">[17]</ref> to create normalized head crops. For all evaluations, we report the angular error (in degrees) between the estimated and ground truth unit gaze vectors, averaged across the corresponding test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GazeCapture</head><p>XGaze Panoptic Gaze360 AVA <ref type="figure">Figure 3</ref>. Top Gaze direction distribution of the Gaze360 <ref type="bibr" target="#b16">[17]</ref>, AVA-LAEO <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10]</ref>, CMU Panoptic <ref type="bibr" target="#b14">[15]</ref>, GazeCapture <ref type="bibr" target="#b19">[20]</ref> and ETH-XGaze <ref type="bibr" target="#b43">[44]</ref> datasets. Note that here the approximate gaze for CMU Panoptic and AVA-LAEO is computed by joining the LAEO pair of subjects' 3D cyclopean eye locations. Bottom Example face or head crops (if available) from the individual datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>To verify the contributions of our individual losses, we conduct a purely weakly-supervised cross-dataset ablation study. We train our method with the CMU Panoptic or AVA-LAEO datasets and evaluate performance on the Gaze360 dataset's test partition. <ref type="table">Table 1</ref> highlights the effect of the various weakly-supervised LAEO losses in this crossdataset setting. All values reported are for the case when the symmetry loss was used by default. We train two configurations of our gaze estimation model -(a) a temporal version, which accepts 7 frames as input, and (b) a static variant, which predicts gaze from a single input frame.</p><p>We observe that among the individual weaklysupervised losses, L pseudo G and L 2D geom on their own or together, result in degenerate solutions. This is not surprising as it highlights the effects of depth ambiguity (see Sec. 3.3). Strong supervision can be provided by explicitly constraining the estimated gaze to intersect a 3D target, in our case, the viewed subject's head in the LAEO condition. This can be seen from the fact that L 3D geom significantly improves over its degenerate counterparts. We observe that the best performance is achieved by utilizing a combination of L 3D geom , L 2D geom and the L pseudo G losses, especially with the real-world AVA-LAEO dataset where the scene geometry is not known. We also find that removing the symmetry loss increases the overall gaze error of our best (temporal) model to 27.9 ? from 25.9 ? for CMU Panoptic and to 27.9 ? from 26.3 ? for AVA-LAEO (not listed in <ref type="table">Table 1</ref>). We provide additional ablation studies to explore the effect of the aleatoric and symmetric losses; other variants of the L pseudo G and L 3D geom losses; and the utility of L 2D geom in Sec. B of the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-supervised Evaluation</head><p>Despite successfully learning to estimate gaze, the performance of our purely weakly-supervised model (trained on the AVA-LAEO dataset and tested on the Gaze360 dataset) lags behind the fully-supervised model on Gaze360's training set <ref type="bibr" target="#b16">[17]</ref> as shown in LAEO data (as discussed in Sec. D.3 of the supplementary) and the other is the the existence of domain gap between the AVA-LAEO and Gaze360 datasets. The latter is evident from the gaze distribution plots shown in <ref type="figure">Fig. 3</ref>. LAEO data tends to be biased towards viewing individuals from larger profile angles (see <ref type="figure">Fig. 1 and Fig. 2</ref>) and contains less frontal face data. It also contains less diversity in the head's pitch (up/down rotation). Hence, in this experiment, we explore a semi-supervised setting, where we evaluate if weakly-supervised LAEO data can successfully augment limited gaze-labeled data and improve its generalization for the task of physically unconstrained gaze estimation in-the-wild. We conduct both cross-dataset and within-dataset experiments. For the crossdataset experiment, we train our model with several existing datasets other than Gaze360 and test on Gaze360's test partition. For the within-dataset experiment, we train on various subsets of Gaze360's training partition along with LAEO data and evaluate performance on Gaze360's test set. Unlike <ref type="bibr" target="#b43">[44]</ref>, which evaluates performance on only frontal faces from Gaze360, we evaluate performance on both (a) frontal and (b) all faces from Gaze360's test set (including large profile faces). datasets on Gaze360, with and without weak supervision from AVA-LAEO. Both these supervised gaze datasets, although large, are limited in some respect for the task of physically unconstrained gaze estimation in-the-wild. The GazeCapture dataset contains images acquired indoors and outdoors, but of mostly frontal faces with a narrow distribution of gaze angles <ref type="figure">(Fig. 3)</ref>. The ETH-XGaze dataset, on the other hand, has a broad distribution of gaze angles from 80 subjects <ref type="figure">(Fig. 3</ref>), but is captured indoors only. <ref type="table">Table 2</ref> highlights that on including weak gaze supervision from AVA-LAEO, the generalization performances of both GazeCapture and ETH-XGaze on Gaze360, for frontal and all faces is improved. For frontal faces, the addition of AVA-LAEO results in improvements of 7.4 ? for Gaze-Capture and 3.6 ? for ETH-XGaze. On all head crops, however, this improvement is even more pronounced -31.0 ? for GazeCapture and 27.6 ? for ETH-XGaze. <ref type="figure">Fig. 3</ref> shows that the AVA-LAEO dataset complements both the Gaze-Capture and ETH-XGaze datasets by expanding their underlying distributions via weak gaze labels (see more details in Sec. C of supplementary). In <ref type="table">Table 2</ref>, we also show the cross-dataset performance of jointly training with CMU Panoptic and AVA-LAEO with their weak gaze labels only. We find that the in-the-wild AVA-LAEO data also slightly improves the generalization performance of the indoor-only CMU Panoptic data on Gaze360. Finally, <ref type="table">Table 2</ref> shows that our model also outperforms the previously reported state-of-the-art performances <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref> on all benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-dataset In</head><p>Within-dataset Training data acquired from a larger number of subjects improves generalization of gaze estimators as shown in <ref type="bibr" target="#b19">[20]</ref>. However, recruiting more subjects requires additional cost and time. In <ref type="figure" target="#fig_1">Fig. 4</ref>, we evaluate the performance of training with progressively larger numbers of subjects from Gaze360's training set, without (labeled as "Gaze360" in <ref type="figure" target="#fig_1">Fig. 4</ref>) and with (labeled as "+AVA" in <ref type="figure" target="#fig_1">Fig. 4</ref>) AVA-LAEO. We use all available videos of a particular subject during training. We assess both our temporal and static models. For this within-domain semi-supervised setting, we find that training on a small number of subjects from Gaze360 along with weak supervision from AVA-LAEO, offers the same performance as using a larger number of subjects from Gaze360.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present the first exploration of a weaklysupervised 3D gaze learning paradigm from images/videos of people looking at each other (LAEO). This approach is trivially scalable due to the ease of acquiring LAEO annotations from Internet videos. To facilitate the learning of 3D gaze, we propose three training objectives, which exploit the underlying geometry native to the LAEO activity. Through many experiments we demonstrate that our approach is successful in augmenting gaze datasets limited in gaze distributions, subjects, or environmental conditions with unconstrained images of people under LAEO, resulting in improved physically unconstrained gaze estimation in the wild.</p><p>In this supplementary document, we show additional experimental results and provide more implementation details. Specifically, we demonstrate the advantage of using weak labels from LAEO data on an additional in-the-wild physically unconstrained gaze-related task besides gaze estimation. For this we incorporate our gaze estimation pipeline from AVA-LAEO into the current state-of-the-art visual target estimation network <ref type="bibr" target="#b3">[4]</ref> (termed "VATnet" here) and evaluate its performance. Next, for the task of physically unconstrained gaze estimation, we provide additional ablation experiments (besides those in Sec. 4.1 of the main paper), including for the aleatoric and symmetry losses; for various formulations of the pseudo gaze and geometric 3D LAEO losses; and for the utility of the geometric 2D LAEO loss. We show more performance details of the various training datasets used in the cross-dataset experiments (in Sec. 4.2 of the main paper) for different gaze yaw angles. Finally, we provide more details of pre-processing the CMU Panoptic and AVA-LAEO datasets, and analyze the reliability of the 3D gaze labels extracted from real-world LAEO data.</p><p>A. Weakly-Supervised Visual Target Estimation <ref type="figure">Figure 5</ref>. A simple modification of the VATnet architecture <ref type="bibr" target="#b3">[4]</ref>. Two fully connected layers serve as an auxiliary task to predict 3D gaze from the head conditioning branch of the original VATnet architecture. The LAEO losses (see Section 3.3 in the main paper) on the predicted gaze vectors for the AVA-LAEO dataset are then used to fine-tune the final layer of the head conditioning branch. Facial features extracted from the fine-tuned head conditioning branch then proceed to VATnet for the visual attention target prediction task. Please refer to Chong et al. <ref type="bibr" target="#b3">[4]</ref> for a full description of their network architecture.</p><p>Chong et al. <ref type="bibr" target="#b3">[4]</ref> proposed a novel spatio-temporal architecture (VATnet), which predicts fixation targets of subjects within a given video frame. In this experiment, we explore if LAEO-based weakly-supervised 3D gaze helps to estimate more accurate visual targets as well. We use LAEO 3D gaze estimation as an auxiliary task while training networks for visual target estimation in a semi-supervised setting. This provides additional weak gaze annotations from the noisy, in-the-wild AVA-LAEO dataset.</p><p>Method VATnet comprises of four modules, a head conditioning branch, which generates gaze-related features from an input head image; a main scene branch, which generates scene-related feature maps based on the saliency of an input scene image; a recurrent attention prediction module, which fuses gaze-and scene-related features across contiguous video frames; and lastly, a heatmap conditioning branch, which generates a visual target prediction heatmap (see <ref type="figure">Fig. 5</ref>). VATnet's head conditioning branch is a ResNet-50 module initialized with weights from a gaze estimation network trained on the EYEDIAP dataset <ref type="bibr" target="#b8">[9]</ref>. Utilizing this gaze estimator, Chong et al. <ref type="bibr" target="#b3">[4]</ref> demonstrate state-of-the-art results on a new dataset called VisualAtten-tionTarget, which comprises of annotated gaze target locations on the image plane. In our experiments we jointly train this VATNet architecture with both the training set of the original fully-supervised VAT dataset and with the AVA-LAEO dataset. To do so, we modify the VATNet architecture and add two fully connected layers to the output of the head conditioning branch, and train it to additionally predict weak 3D gaze vectors derived from the AVA-LAEO dataset (see <ref type="figure">Fig. 5</ref>). We train with samples from AVA-LAEO using the LAEO loss L SY M + L 2D geom + L 3D geom + L pseudo G only.</p><p>Data Preparation VATnet requires three input modalities. First, it requires a full scene image with known head bounding box locations for each annotated subject. Next, it requires a 2D pixel gaze target location on the image plane for the said subject and finally, an in-out label, which indicates if the target is within or out of a frame. For this task, to use the LAEO data we input the same 7-frame sequence centered around a LAEO annotation. We treat the 2D cyclopean eye P 2D (see the sub-section titled "Scene Geometry Estimation" within Sec. 3.3 of the main paper) of subject B as the target for subject A and vice versa for subject B. The nature of the AVA-LAEO data ensures that all target locations are within an image frame and we assume this to be the default in-out ground truth state. We do not pre-process or augment the AVA-LAEO data and directly re-train Chong et al.'s original implementation of VATnet with the two datasets with minimal modifications.</p><p>Results Following Chong et al. <ref type="bibr" target="#b3">[4]</ref>, we evaluate the area under the curve (AUC) for correct target location prediction (within a pre-specified distance threshold on the image  <ref type="table">Table 3</ref>. Improvements to the VATnet baseline <ref type="bibr" target="#b3">[4]</ref> by adding weak supervision from the AVA-LAEO dataset using the best configuration of LAEO loss functions described in <ref type="table">Table 1 of the main  paper.</ref> plane), the L2 distance between the predicted and ground truth target locations in the scene and the out-of-frame prediction's average precision (AP). We report the scores on the VAT test dataset, averaged across training epochs 2-30, both for the author's original method <ref type="bibr" target="#b3">[4]</ref> and our proposed modification. <ref type="table">Table 3</ref> shows the benefits of jointly training with the AVA-LAEO and VAT datasets. We notice an improvement in the AUC and L2 distance metrics for visual target prediction. These encouraging results suggest that weak supervision from noisily-labeled in-the-wild LAEO data can potentially also aid other gaze-related tasks, e.g., visual attention target prediction besides 3D gaze estimation. We also note a reduction in the out-of-frame AP, which is not surprising as all target locations for a given subject in the AVA-LAEO dataset lie within image bounds and hence it provides labels for only one (i.e., the in-frame) class.</p><formula xml:id="formula_7">AUC (?) L2 Dist (?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Ablation Studies</head><p>For the task of physically unconstrained gaze estimation, we provide additional ablation experiments besides those in Sec. 4.1 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Aleatoric and Symmetry Losses</head><p>In the normalized eye co-ordinate system <ref type="bibr" target="#b16">[17]</ref>, where the z axis passes through the 3D cyclopean eye center of each face, constraining gaze yaw prediction to be equal and opposite for a face and its symmetrically flipped version, is an intuitive constraint, which can be employed during training. Our experiments show that using this symmetry constraint and the aleatoric gaze loss improve the baseline performance of <ref type="bibr" target="#b16">[17]</ref> on both variants of the author's original fully-supervised ResNet-18-based gaze estimator (temporal and static), which use the pinball gaze loss. <ref type="table">Table 4</ref> shows a detailed comparison of the effects of adding the symmetry constraint to the pinball (from <ref type="bibr" target="#b16">[17]</ref>) and aleatoric (ours) loss functions for a within-dataset fully-supervised experiment on Gaze360. Here we train our gaze network with Gaze360's entire training set (with its gaze labels) and evaluate it on Gaze360's test set. Note that the symmetry constraint improves the performance of both the pinball and aleatoric losses.  <ref type="table">Table 4</ref>. Summary of performance gain by employing an aleatoric gaze loss (described in Sec. 3.3, "Aleatoric Gaze Loss" of the main paper) and the effects of incorporating a symmetry constraint (described in Sec. 3.3, "Symmetry Loss" of the main paper). All values reported are angular gaze errors in degrees (lower is better) for the fully-supervised within-dataset experiment on Gaze360. <ref type="figure">Figure 6</ref>. Gaze360 test error (in degrees) as a function of gaze yaw for the fully-supervised within-dataset experimental on Gaze360.</p><p>Note that gaze error increases as faces turn away from the camera.</p><p>We also observe that for this within-dataset experiment, the aleatoric loss consistently outperforms the pinball loss and that the combination of the aleatoric and symmetry losses results in the best overall performance ( <ref type="table">Table 4</ref>). In addition to this, we observe that the aleatoric loss also outperforms the pinball loss in the cross-domain purely weakly-supervised experimental setting. By replacing the aleatoric loss with the pinball loss (from <ref type="bibr" target="#b16">[17]</ref>), our best temporal network (trained with all the LAEO losses and corresponding to the last row of <ref type="table">Table 1</ref> in the main paper), generalizes less effectively to Gaze360. For AVA-LAEO its gaze error of 26.3 ? increases to 28.7 ? and for CMU Panoptic it increases from 25.9 ? to 26.1 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Variants of L pseudo G</head><p>The LAEO activity provides us with the constraint that the predicted 3D gaze from subjects A and B in LAEO must be equal and opposite in a shared camera coordinate system. There are multiple ways in which we can implement this constraint. As an ablation, we explore two additional formulations for this LAEO constraint besides the one described in Sec. 3.3 titled "Pseudo Gaze LAEO Loss" of the main paper: a) naive LAEO enforcement and b) using the most confident gaze prediction for a pair of faces in LAEO as the pseudo ground truth gaze direction. In either experiment, we replace the L pseudo G loss in our best (temporal) purely weakly-supervised cross-dataset configuration that is trained with all the LAEO losses L sym +L pseudo</p><formula xml:id="formula_8">G +L 2D geom + L 3D</formula><p>geom (corresponding to the last row in <ref type="table">Table 1</ref> of the main paper) with one of these losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naive LAEO Enforcement Here we naively enforce the predicted vectors? 3D</head><p>A and? 3D B to be equal and opposite by minimizing the resultant angular cosine distance between g 3D A and ?? 3D B . In this constraint, predictions for both faces could be modified by the network. In order to achieve this, our gaze estimation network could either improve its prediction for the difficult face in a LAEO pair (see <ref type="figure">Fig. 6</ref>, which show that gaze prediction error increased with extreme gaze angles), or it could deteriorate its prediction for the clearer frontal face to satisfy this naive LAEO objective. Our experiments show a reduction in cross-dataset performance on the entire Gaze360 test set (CMU Panoptic: 25.9 ? ? 28.2 ? and AVA-LAEO: 26.3 ? ? 26.9 ? ) with this naive variant of the LAEO loss versus the one described in Sec. 3.3 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confident Gaze Prediction</head><p>In this experiment, we regard the more confident of the two predicted gaze vectors for a LAEO pair as the pseudo ground truth g 3D pseudo gaze label as opposed to their weighted average used in Sec. 3.3 of the main paper. That is, </p><formula xml:id="formula_9">g 3D pseudo =? 3D A if W A ? W B (from</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Variant of L 3D geom</head><p>We also compare the performance of our L 3D geom loss formulation used in Sec. 3.3 of the main paper to a conventional 3D angular cosine loss, whose ground truth is assumed to be along the line joining LAEO subjects' estimated 3D eyes. Empirically, we observe that replacing L 3D geom with a cosine loss in our best (temporal) purely weakly-supervised configuration (last row of <ref type="table">Table 1 in</ref>   geom is always on). With the L 2D geom loss included, performance degrades more gracefully on increasing depth noise versus without. Plots show median values across 4 different training runs initialized with different network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Utility of L 2D</head><p>geom The 2D eye position on the image plane can be estimated without depth ambiguity and is more reliable than the 3D eye position. To quantify the contribution of L 2D geom to the overall performance of our system, we add increasing noise (z-only) as a ratio of the absolute ground truth depth of the 3D eye positions to subjects under LAEO in the CMU Panoptic dataset, train various purely weakly-supervised configurations (as described in Sec. 4.1 of the main paper) with and without L 2D geom and evaluate on Gaze360 <ref type="figure" target="#fig_4">(Fig. 7)</ref>. While we see gaze prediction accuracy deteriorate with increasing depth noise, the inclusion of L 2D geom constrains gaze ambiguity and reduces the degradation of gaze estimates. Besides this, we also observe that including L 2D geom makes gaze predictions more consistent and reduces the standard deviation of errors on Gaze360's test set (CMU Panoptic: 27.0 ? ? 23.7 ? and AVA-LAEO: 23.6 ? ? 19.8 ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed Cross-dataset Performance</head><p>For the cross-dataset experiment described in Sec. 4.2 and <ref type="table">Table 2</ref> of the main paper, we additionally analyze the variation in gaze errors with varying gaze yaw angles on the Gaze360 test set. We consider the case of training with (a) GazeCapture only (dashed curves in <ref type="figure" target="#fig_5">Fig. 8</ref>) or (b) with GazeCapture and AVA-LAEO in (solid curves <ref type="figure" target="#fig_5">Fig. 8)</ref>. The corresponding curves for training with (a) ETH-XGaze only or (b) with ETH-XGaze and AVA-LAEO are shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. The blue curves show performance on the entire Gaze360 test set, while the red curves are for its subset containing frontal faces only.</p><p>The AVA-LAEO dataset exhibits a large distribution of extreme gaze angles as the LAEO activity largely consists of people with side profiles fixating at each other (see <ref type="figure">Fig. 1</ref> and <ref type="figure">Fig. 2</ref> in main paper and <ref type="figure">Fig. 11</ref> in the supplementary for examples). This conveniently augments datasets with narrow gaze distributions, e.g., GazeCapture (dashed versus solid curves in <ref type="figure" target="#fig_5">Fig. 8</ref>), which is largely concentrated about gaze pitch and yaw values of zero (from <ref type="figure">Fig. 3</ref> of the main paper) and helps them generalize better to Gaze360. The AVA-LAEO dataset also contains a large appearance variability because of being collected from in-the-wild videos, which positively augments datasets collected indoors only, e.g., ETH-XGaze (dashed versus solid curves <ref type="figure" target="#fig_6">Fig. 9</ref>) and helps it generalize better to Gaze360 as well. On jointly training either the GazeCapture or ETH-XGaze dataset with AVA-LAEO, we see a significant boost in their performance on all head crops from Gaze360, including faces with large profile views (blue curves in <ref type="figure" target="#fig_5">Fig. 8 and Fig. 9</ref>). Interestingly, adding the AVA-LAEO dataset improves crossdomain performance of GazeCapture and ETH-XGaze on Gaze360's frontal face crops as well (red curves in <ref type="figure" target="#fig_5">Fig. 8</ref> and <ref type="figure" target="#fig_6">Fig. 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Pre-processing</head><p>We first describe in detail how we pre-process the CMU Panoptic (haggling activity subset) and the AVA-LAEO datasets. Then we analyze the effect of the simplifying assumptions that we employed to estimate scene geometry (as described in Sec. 3.3 of the main paper) on the reliability of 3D gaze annotations derived from real-world LAEO data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. CMU Panoptic</head><p>The CMU Panoptic dataset contains 31 views captured from high-definition cameras within a dome with available accurate body/facial 3D landmark locations and camera intrinsic and extrinsic parameters. This enables us to compute each subject's head position and orientation with respect to any scene camera. Such a convenient setup allows us to quickly gather our own large-scale gaze dataset by leveraging the LAEO constraint. However, this dataset does not contain explicit information about the presence or absence of the LAEO activity in video frames. So we use a semi-automatic procedure to label the video frames in it with LAEO activity labels. We use the pre-trained Gaze360 static network <ref type="bibr" target="#b16">[17]</ref> to estimate gaze for every subject from multiple frontal views (i.e., if a given face is oriented within ?90 ? of a camera's principal axis). These gaze estimates are then transformed to world co-ordinates and their pairwise cosine distance is computed between every subject pair present in a frame. A pair of gaze vectors for two subjects are assumed to be under LAEO when their angular separation (with one of the vectors being inverted) from each other and the 3D line joining their cyclopean 3D eyes is &lt; 20 ? . A pair of subjects is treated to be in LAEO when at least 4 of its gaze pairs from multiple views are classified as being in LAEO. The nature of the haggling activity ensures that only a single pair may ever exhibit LAEO. Frames with none or multiple LAEO pair detections are removed from the analysis.</p><p>We experience two corner cases: a) facial features of certain subjects can be blocked from view by another subject in the scene and b) multiple subjects may appear within the same head bounding box (see <ref type="figure" target="#fig_7">Fig. 10</ref>). To mitigate this issue, we first compute a facial bounding box surrounding a subject's ears, eyes and nose keypoints. Next, we compute a bounding box around every subject's body. Views with facial bounding boxes overlapping with body bound- ing boxes of other subjects (i.e., with a bounding box IOU score ?0.01) are discarded from the analysis. This in-turn results in missing gaze values in the central ?15 ? gaze pitch and yaw distribution region (see <ref type="figure">Fig. 3</ref> in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. AVA-LAEO</head><p>The availability of 3D head poses and landmarks is a vital requirement for computing our LAEO losses. These annotation, however are not available in the AVA-LAEO dataset. We utilize dense 2D-3D correspondence predictions derived from DensePose <ref type="bibr" target="#b10">[11]</ref> to fit the SMPL 3D head model to every detected subject within a LAEO annotated frame from the AVA training set <ref type="bibr" target="#b9">[10]</ref> with LAEO annotations provided by Marin-Jimenez et al. <ref type="bibr" target="#b24">[25]</ref>. To improve computational efficiency while deriving these correspondences, we utilize up to 1,000 2D pixels detected by Dense-Pose, which belong to a subject's head. To ensure that every detected facial region is well represented while computing 3D head pose, we uniformly sample 2D pixels based on their distance from the mean 2D head location. However, incorrect head-pose estimates due to incorrect 2D-3D correspondences are inevitable. See <ref type="figure">Fig. 11</ref> for a positive and a negative example of head pose fitting, where the latter results in noisy gaze labels for the AVA-LAEO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Reliability of LAEO 3D Gaze Labels</head><p>When scene geometry is unknown (e.g., in real-world LAEO datasets), 3D gaze labels derived from LAEO are indeed noisy. We introduce various constraints while training our system to counter this issue, and show results on both controlled (CMU Panoptic) and in-the-wild (AVA-LAEO) datasets. Yet, as a rough estimate, we compare the angular separation between 3D gaze derived from the approximate scene geometry (described in Sec. 3.3 of the main paper) and its ground truth values using a subset of 3495 images from the CMU Panoptic dataset. On average, we observe a 14.8 ? gaze label error and an absolute relative depth difference of 0.3 between the ground truth and estimated subject depths when both 2D cyclopean eye points and the subjects' z depths are estimated, and the focal length is assumed to be the largest image dimension. Replacing with accurate focal length reduces gaze label error to 10.1 ? and using accurate 2D cyclopean eye centers further reduces it to 8.84 ? . Additionally, the assumption that people look at each others' 3D eye centers introduces &lt; 4.3 ? gaze error for sub- <ref type="figure">Figure 11</ref>. (Top) a positive and (bottom) a negative example of scene geometry reconstruction from the AVE-LAEO dataset. Notice the incorrect 3D head placement for the right most subject in the bottom example with respect to z depth. The subject on the right is clearly closer to the camera (in terms of z depth) than the subject on the left, but is incorrectly estimated as being further from it. This, in turn, results in noisy 3D gaze labels. jects located &gt; 500mm apart. These label errors are significantly smaller than those encountered in cross-dataset (? 30 ? from <ref type="bibr" target="#b43">[44]</ref>) and semi-supervised (&gt; 25 ? from <ref type="figure" target="#fig_1">Fig. 4</ref> of the main paper) training for Gaze360 making LAEO data a reliable source of supervision for 3D gaze learning in physically unconstrained settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Gaze error (in degrees) on the Gaze360 test set on augmenting a reduced Gaze360 training set (with less subjects) with AVA-LAEO. We vary the number of Gaze360 training subjects along the horizontal axis. The shaded area corresponds to the standard error of the average metric evaluated over 5 repetitions of each experiment performed by picking a different random subset of subjects each time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Eq. 1 in the main paper) and vice versa for subject B. Our experiments show a reduction in cross-dataset performance with this variant of the LAEO pseudo ground truth label as well versus the one used in Sec. 3.3 of the main paper (CMU Panoptic: 25.9 ? ? 27.24 ? and AVA-LAEO: 26.3 ? ? 27.8 ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the main paper), results in consistently worse performance on Gaze360 (CMU Panoptic: 25.9 ? ? 30.0 ? and AVA-LAEO: 26.3 ? ? 29.63 ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Purely weakly-supervised performance of CMU Panoptic on Gaze360, with added relative depth noise (? = 0, ? = {0.1, 0.3, 0.5}), when trained with different combinations of LAEO losses (L 3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Reduction in gaze error on the Gaze360 test set on jointly training with GazeCapture and AVA-LAEO. The dashed curves are for training with GazeCapture only and the solid ones are for jointly training with GazeCapture and AVA-LAEO. Each curve represents the mean of samples in bins 1.8 ? wide and the bins with 20 samples or less are discarded. The vertical axis is represented in log scale. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Reduction in gaze error on the Gaze360 test set on jointly training with ETH-XGaze and AVA-LAEO. The dashed curves are for training with ETH-XGaze only and the solid ones are for jointly training with ETH-XGaze and AVA-LAEO. Each curve represents the mean of samples in bins 1.8 ? wide and the bins with 20 samples or less are discarded. The vertical axis is represented in log scale. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Examples of discarded CMU Panoptic frames from our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 (Table 1</head><label>21</label><figDesc><ref type="bibr" target="#b25">26</ref>.3 ? vs 13.2 ? for the temporal model). One reason for this discrepancy is the presence of noise in the gaze labels derived from</figDesc><table><row><cell>Temporal</cell><cell>Static</cell></row></table><note>. An ablation study to evaluate our individual weakly- supervised LAEO losses. The symmetry loss is always used. All numbers reported are using predictions from the temporal and static variants of our gaze estimation model, when evaluated on Gaze360's test set, measured in gaze angular error in degrees. Lower is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,Table 2</head><label>22</label><figDesc>we compare the generalization performance of the GazeCapture and the ETH-XGaze</figDesc><table><row><cell cols="2">Within dataset, Gaze + LAEO Labels</cell><cell></cell></row><row><cell>Training Data</cell><cell cols="2">Frontal face crops All head crops</cell></row><row><cell>Gaze360 [17]</cell><cell>11.1</cell><cell>13.5</cell></row><row><cell>Gaze360</cell><cell>10.1</cell><cell>13.2</cell></row><row><cell>Gaze360 + AVA</cell><cell>10.2</cell><cell>13.2</cell></row><row><cell cols="2">Cross dataset, Gaze + LAEO Labels</cell><cell></cell></row><row><cell>Training Data</cell><cell cols="2">Frontal face crops All head crops</cell></row><row><cell>GazeCapture [44]</cell><cell>30.2</cell><cell>-</cell></row><row><cell>GazeCapture</cell><cell>29.2</cell><cell>58.2</cell></row><row><cell>GazeCapture + AVA</cell><cell>19.5</cell><cell>27.2</cell></row><row><cell>ETH-XGaze [44]</cell><cell>27.3</cell><cell>-</cell></row><row><cell>ETH-XGaze</cell><cell>20.5</cell><cell>52.6</cell></row><row><cell>ETH-XGaze + AVA</cell><cell>16.9</cell><cell>25.0</cell></row><row><cell cols="2">Cross dataset, LAEO Labels</cell><cell></cell></row><row><cell>Training Data</cell><cell cols="2">Frontal face crops All head crops</cell></row><row><cell>AVA</cell><cell>29.0</cell><cell>26.3</cell></row><row><cell>CMU Panoptic</cell><cell>26.0</cell><cell>25.9</cell></row><row><cell>CMU Panoptic + AVA</cell><cell>22.5</cell><cell>24.4</cell></row></table><note>. Performance evaluation of our temporal model on Gaze360 dataset's test partition with various different training datasets ranging from those containing full gaze supervision (Gaze360, GazeCapture, ETH-XGaze), weak LAEO supervision only (the AVA-LAEO or CMU Panoptic datasets), or their com- binations. All reported values are gaze angular errors in de- grees (lower is better) on either (a) frontal face crops only or (b) all head crops from Gaze360's test set. Note that the addition of AVA-LAEO to GazeCapture or ETH-XGaze significantly im- proves their generalization performance on Gaze360.</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Offset calibration for appearance-based gaze estimation via gaze decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Appearancebased gaze estimation via evaluation-guided asymmetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="100" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connecting gaze, scene, and attention: Generalized attention estimation via joint modeling of gaze and scene saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="383" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting Attended Visual Targets in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rtgene: Real-time eye gaze estimation in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hyung Jin Chang, and Yiannis Demiris</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="352" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Alberto Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM ETRA. ACM</title>
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DensePose: Dense Human Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dmitry Lagun, and Vidhya Navalpakkam. On-device few-shot personalization for real-time gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiappan</forename><surname>Valliappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingmei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chase</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tabletgaze: Dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="445" to="461" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5243" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<title level="m">Shohei Nobuhara, and Yaser Sheikh. Panoptic Studio: A Massively Multiview System for Social Interaction Capture. TPAMI</title>
		<meeting><address><addrLine>Bart Nabbe, Iain Matthews, Takeo Kanade</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gaze360: Physically unconstrained gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="6911" to="6920" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in Bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5575" to="5585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eye Tracking for Everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchendra</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to personalize in appearance-based gaze tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Lind?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Sjostrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Proutiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A differential approach for gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Alberto Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Laeo-net: Revisiting people looking at each other in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Medina-Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3472" to="3480" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Detecting people looking at each other in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Physiological aspects of communication via mutual gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Faupel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Leen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Thurman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Sociology</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="74" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards end-to-end video-based eye-tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Few-shot adaptive gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molchanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Umar Iqbal, Otmar Hilliges, and Jan Kautz</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep pictorial gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Andreas Bulling, and Otmar Hilliges. Learning to find eye region landmarks for remote gaze estimation in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ETRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lightweight head pose invariant gaze tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2156" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Where are they looking? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Following gaze in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1435" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gaze Locking: Passive Eye Contact Detection for Human-Object Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM UIST</title>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly supervised 3d hand pose estimation via biomechanical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Social cognition in the first year</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tricia</forename><surname>Striano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A hierarchical generative model for eye image synthesis and eye gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalizing eye tracking with bayesian adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11907" to="11916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mixed effects neural networks (menets) with applications to gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyunwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7743" to="7752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning for gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7314" to="7324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Humbi: A large multiview dataset of human body expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2990" to="3000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Eth-xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Siyu Tang, and Otmar Hilliges</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Andreas Bulling, and Otmar Hilliges. Learning-based region selection for end-to-end gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="4511" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">It&apos;s written all over your face: Full-face appearancebased gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monocular free-head 3d gaze tracking with deep learning and geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoping</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
