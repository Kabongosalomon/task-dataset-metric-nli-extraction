<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminator Contrastive Divergence: Semi-Amortized Generative Modeling by Exploring Energy of the Discriminator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkai</forename><surname>Xu</surname></persName>
							<email>mkxu@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tie-yan.liu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminator Contrastive Divergence: Semi-Amortized Generative Modeling by Exploring Energy of the Discriminator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) have shown great promise in modeling high dimensional data. The learning objective of GANs usually minimizes some measure discrepancy, e.g., f -divergence (f -GANs [28]) or Integral Probability Metric (Wasserstein GANs <ref type="bibr" target="#b1">[2]</ref>). With f -divergence as the objective function, the discriminator essentially estimates the density ratio <ref type="bibr" target="#b36">[37]</ref>, and the estimated ratio proves useful in further improving the sample quality of the generator <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>. However, how to leverage the information contained in the discriminator of Wasserstein GANs (WGAN) [2] is less explored. In this paper, we introduce the Discriminator Contrastive Divergence, which is well motivated by the property of WGAN's discriminator and the relationship between WGAN and energy-based model. Compared to standard GANs, where the generator is directly utilized to obtain new samples, our method proposes a semi-amortized generation procedure where the samples are produced with the generator's output as an initial state. Then several steps of Langevin dynamics are conducted using the gradient of the discriminator. We demonstrate the benefits of significant improved generation on both synthetic data and several real-world image generation benchmarks. 1 * Equal contribution, with the order determined by flipping coins. 1 Code is available at https://github.com/MinkaiXu/Discriminator-Contrastive-Divergence.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[10]</ref> proposes a widely popular way to learn likelihood-free generative models, which have shown promising results on various challenging tasks. Specifically, GANs are learned by finding the equilibrium of a min-max game between a generator and a discriminator, or a critic under the context of WGANs. Assuming the optimal discriminator can be obtained, the generator substantially minimizes some discrepancy between the generated distribution and the target distribution.</p><p>Improving training GANs by exploring the discrepancy measure with the excellent property has stimulated fruitful lines of research works and is still an active area. Two well-known discrepancy measures for training GANs are f -divergence and Integral Probability Metric (IPM) <ref type="bibr" target="#b25">[26]</ref>. f -divergence is severe for directly minimization due to the intractable integral, f -GANs provide minimization instead of a variational approximation of f -divergence between the generated distribution p G ? and the target distribution p data . The discriminator in f -GANs serves as a density ratio estimator <ref type="bibr" target="#b36">[37]</ref>. The other families of GANs are based on the minimization of an Integral Probability Metric (IPM). According to the definition of IPM, the critic needs to be constrained into a specific function class. When the critic is restricted to be 1-Lipschitz function, the corresponding IPM turns to the Wasserstein-1 distance, which inspires the approaches of Wasserstein GANs (WGANs) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>No matter what kind of discrepancy is evaluated and minimized, the discriminator is usually discarded at the end of the training, and only the generator is kept to generate samples. A natural question to ask is whether, and how we can leverage the remaining information in the discriminator to construct a more superior distribution than simply sampling from a generator.</p><p>Recent work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> has shown that a density ratio can be obtained through the output of discriminator, and a more superior distribution can be acquired by conducting rejection sampling or Metropolis-Hastings sampling with the estimated density ratio based on the original GAN <ref type="bibr" target="#b9">[10]</ref>.</p><p>However, the critical limitation of previous methods lies in that they can not be adapted to WGANs, which enjoy superior empirical performance over other variants. How to leverage the information of a WGAN's critic model to improve image generation remains an open problem. In this paper, we do the following to address this:</p><p>? We provide a generalized view to unify different families of GANs by investigating the informativeness of the discriminators. ? We propose a semi-amortized generative modeling procedure so-called discriminator contrastive divergence (DCD), which achieves an intermediate between implicit and explicit generation and hence allows a trade-off between generation quality and speed.</p><p>Extensive experiments are conducted to demonstrate the efficacy of our proposed method on both synthetic setting and real-world generation scenarios, which achieves state-of-the-art performance on several standard evaluation benchmarks of image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Both empirical <ref type="bibr" target="#b1">[2]</ref> and theoretical <ref type="bibr" target="#b14">[15]</ref> evidence has demonstrated that learning a discriminative model with neural networks is relatively easy, and the neural generative model(sampler) is prone to reach its bottleneck during the optimization. Hence, there is strong motivation to further improve the generated distribution by exploring the remaining information. Two recent advancements are discriminator rejection sampling(DRS) <ref type="bibr" target="#b2">[3]</ref> and MH-GANs <ref type="bibr" target="#b35">[36]</ref>. DRS conducts rejection sampling on the output of the generator. The vital limitation that lies in the upper bound of D ? is needed to be estimated for computing the rejection probability. MH-GAN sidesteps the above problem by introducing a Metropolis-Hastings sampling procedure with generator acting as the independent proposal; the state transition is estimated with a well-calibrated discriminator. However, the theoretical justification of both the above two methods is based on the fact that the output of discriminator needs to be viewed as an estimation of density ratio pdata p G ? . As pointed out by previous work <ref type="bibr" target="#b40">[41]</ref>, the output of a discriminator in WGAN <ref type="bibr" target="#b1">[2]</ref> suffers from the free offset and can not provide the density ratio, which prevents the application of the above methods in WGAN.</p><p>Our work is inspired by recent theoretical studies on the property of discriminator in WGANs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>. <ref type="bibr" target="#b32">[33]</ref> proposes discriminator optimal transport (DOT) to leverage the optimal transport plan implied by WGANs' discriminator, which is orthogonal to our method. Moreover, turning the discriminator of WGAN into an energy function is closely related to the amortized generation methods in the context of the energy-based model (EBM) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b22">23]</ref> where a separate network is proposed to learn to sample from the partition function in <ref type="bibr" target="#b8">[9]</ref>. Recent progress <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref> in the area of EBM has shown the feasibility of generating high dimensional data with Langevin dynamics. From the perspective of EBM, our proposed method can be seen as an intermediary between an amortized generation model and an implicit generation model, i.e., a semi-amortized generation method, which allows a trade-off between speed and flexibility of generation. With a similar spirit, <ref type="bibr" target="#b10">[11]</ref> also illustrates the potential connection between neural classifier and energy-based model in supervised and semi-supervised scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Adversarial Networks</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[10]</ref> is an implicit generative model that aims to fit an empirical data distribution p data over sample space X . The generative distribution p G ? is implied by a generated function G ? , which maps latent variable Z to sample X, i.e., G ? : Z ? ? X . Typically, the latent variable Z is distributed on a fixed prior distribution p(z). With i.i.d samples available from p G ? and p data , the GAN typically learns the generative model through a min-max game between a discriminator D ? and a generator G ? : min</p><formula xml:id="formula_0">? max ? E x?Pdata [r(D ? (x))] ? E x?p G ? [m(D ? (x))] .<label>(1)</label></formula><p>With r and m as the function r(x) = m(x) = x and the D ? (x) is constrained as 1-Lipschitz function, the Eq. 1 yields the WGANs objective which essentially minimizes the Wasserstein distance between p data and p G ? . With r(x) = x and m(x) as the Fenchel conjugate <ref type="bibr" target="#b15">[16]</ref> of a convex and lowersemicontinuous function, the objective in Eq. 1 approximately minimize a variational estimation of f -divergence <ref type="bibr" target="#b27">[28]</ref> between p data and p G ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Energy Based Model and MCMC basics</head><p>The energy-based model tends to learn an unnormalized probability model implied by an energy function E ? (x) to prescribe the ground truth data distribution p data . The corresponding normalized density function is:</p><formula xml:id="formula_1">q ? (x) = e ?E ? (x) Z ? , Z ? = e ?E ? (x) dx,<label>(2)</label></formula><p>where Z ? is so-called normalization constant. The objective of training an energy-based model with maximum likelihood estimation is as:</p><formula xml:id="formula_2">L MLE (?; p) := ?E x?pdata(x) [log q ? (x)] .</formula><p>(3) The estimated gradient with respect to the MLE objective is as follows:</p><formula xml:id="formula_3">? ? L MLE (?; p) (4) = ? ? E x?pdata(x) [E ? (x)] ? e ?E ? (x) ? ? E ? (x)dx Z ? = E x?pdata(x) [? ? E ? (x)] ? E x?q ? (x) [? ? E ? (x)].</formula><p>The above method for gradient estimation in Equation 4 is called contrastive divergence (CD). Furthermore, we define the score of distribution with density function p(x) as ? x log p(x). We can immediately conclude that ? x log q ? (x) = ?E ? (x), which does not depend on the intractable Z ? .</p><p>Markov chain Monte Carlo is a powerful framework for drawing samples from a given distribution. An MCMC is specified by a transition kernel K(x |x) which corresponds to a unique stationary distribution p, i.e.,</p><formula xml:id="formula_4">q = p ? q(x) = q (x ) K (x|x ) dx , ?x.</formula><p>More specifically, MCMC can be viewed as drawing x 0 from the initial distribution x 0 and iteratively get sample x t at the t-th iteration by applied the transition kernel on the previous step, i.e., x t |x t?1 ? K(x t |x t?1 ). Following <ref type="bibr" target="#b23">[24]</ref>, we formalized the distribution q t of z t as obtained by a fixed point update of form q t (x) ? Kq t?1 (x), and Kq t?1 (x):</p><formula xml:id="formula_5">Kq t?1 (x) := q t?1 (x ) K (x|x ) dx .</formula><p>As indicated by the standard theory of MCMC, the following monotonic property is satisfied:</p><formula xml:id="formula_6">D KL (q t ||p) ? D KL (q t?1 ||p).<label>(5)</label></formula><p>And q t converges to the stationary distribution p as t ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Informativeness of Discriminator</head><p>In this section, we seek to investigate the following questions:</p><p>? What kind of information is contained in the discriminator of different kinds of GANs?</p><p>? Why and how can the information be utilized to further improved the quality of generated distribution?</p><p>We discuss the discriminator of f -GANs, and WGANs, respectively, in the following.  <ref type="figure">Figure 1</ref>: Discriminator Contrastive Divergence: After WGAN training, a fine-tuning for critics can be conducted with several MCMC steps, which leverages the gradient of discriminator by Langevin dynamics; after the fine-tuning, the discriminator could be viewed as a superior distribution of p G ? , hence sampling from p G ? can be implemented using the same Langevin dynamics as described in 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">f -GAN Discriminator</head><p>f -GAN <ref type="bibr" target="#b26">[27]</ref> is based on the variational estimation of f -divergence <ref type="bibr" target="#b0">[1]</ref> with only samples from two distributions available: Theorem 1. <ref type="bibr" target="#b26">[27]</ref> With Fenchel Duality, the variational estimation of f -divergence can be illustrated as follows:</p><formula xml:id="formula_7">D f (P Q) (6) = X q(x) sup t? dom f * t p(x) q(x) ? f * (t) dx ? sup T ?T X p(x)T (x)dx ? X q(x)f * (T (x))dx = sup T ?T (E x?P [T (x)] ? E x?Q [f * (T (x))]) ,</formula><p>where the T is the arbitrary class of function and f * denotes the Fenchel conjugate of f . And the supremum is achieved only when</p><formula xml:id="formula_8">T * (x) = f p(x) q(x) , i.e. p(x) q(x) = ?f * ?T (T * (x)</formula><p>).</p><p>In f -GAN <ref type="bibr" target="#b27">[28]</ref>, the discriminator D ? is actually the function T parameterized with neural networks. Theorem. 1 indicates the density ratio estimation view of f -GAN's discriminator, as illustrated in <ref type="bibr" target="#b36">[37]</ref>. More specifically, the discriminatorD ? in f -GAN is optimized to estimate a statistic related to the density ratio between p data and p G ? , i.e. pdata p G ? , and the pdata p G ? can be acquired easily with D ? . For example, in the original GANs <ref type="bibr" target="#b9">[10]</ref>, the corresponding f in f -GAN literature is f (x) = x log x ? (x + 1) log(x + 1) + 2 log 2. Assuming the discriminator is trained to be optimal, the output is D ? (x) = pdata pdata+p G ? , and we can get the density ratio pdata</p><formula xml:id="formula_9">p G ? = D ? (x) 1?D ? (x)</formula><p>. However, it should be noticed that the discriminator is hard to reach the optimality. In practice, without loss of generality, the density ratio implied by a sub-optimal discriminator can be seen as the density ratio between an implicitly defined distribution p * and the generated distribution p G ? . It has been studied both theoretically and empirically in the context of GANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, with the same inductive bias, that learning a discriminative model is more accessible than a generative model. Based on the above fact, the rejection-sampling based methods are proposed to use the estimated density ratio, e.g.,</p><formula xml:id="formula_10">D ? (x) 1?D ? (x)</formula><p>in original GANs, to conduct rejection sampling <ref type="bibr" target="#b2">[3]</ref> or Metropolis-Hastings sampling <ref type="bibr" target="#b35">[36]</ref> based on generated distribution p G ? . These methods radically modify the generated distribution p G ? to p * , the improvement in empirical performance as shown in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> demonstrates that we can construct a superior distribution p * to prescribe the empirical distribution p data by involving the remaining information in discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">WGAN Discriminator</head><p>Different from f -GANs, the objective of WGANs is derived from the Integral Probability Metric, and the discriminator can not naturally be derived as an estimated density ratio. Before leveraging the remaining information in the discriminator, the property of the discriminator in WGANs needs to be investigated first. We introduce the primal problem implied by WGANs objective as follows:</p><p>Let ? denote the joint probability for transportation between P and Q, which satisfies the marginality conditions,</p><formula xml:id="formula_11">dy?(x, y) = p(x), dx?(x, y) = q(y)<label>(7)</label></formula><p>The primal form first-order Wasserstein distance W 1 is defined as:</p><formula xml:id="formula_12">W 1 (P, Q) = inf ???(P,Q) E (x,y)?? [ x ? y 2 ]</formula><p>the objective function of the discriminator in Wasserstein GANs is the Kantorovich-Rubinstein duality of Eq. 7, and the optimal discriminator has the following property <ref type="bibr" target="#b12">[13]</ref>: Theorem 2. Let ? * as the optimal transport plan in Eq. 7 and</p><formula xml:id="formula_13">x t = tx + (1 ? t)y with 0 ? t ? 1.</formula><p>With the optimal discriminator D ? as a differentiable function and ? * (x, x) = 0 for all x, then it holds that:</p><formula xml:id="formula_14">P (x,y)?? * ? xi D * ? (x t ) = y ? x y ? x = 1</formula><p>Theorem. 2 states that for each sample x in the generated distribution p G ? , the gradient on the x directly points to a sample y in the p data , where the (x, y) pairs are consistent with the optimal transport plan ? * . All the linear interpolations x t between x and y satisfy that</p><formula xml:id="formula_15">? x k D * ? (x t ) = y?x y?x .</formula><p>It should also be noted that similar results can also be drawn in some variants of WGANs, whose loss functions may have a slight difference with standard WGAN <ref type="bibr" target="#b40">[41]</ref>. For example, the SNGAN uses the hinge loss during the optimization of the discriminator, i.e., r(?) and g(?) in Eq. 1 is selected as max(0, ?1 ? u) for stabilizing the training procedure. We provide a detailed discussion on several surrogate objectives in Appendix. E.</p><p>The above property of discriminator in WGANs can be interpreted as that given a sample x from generated distribution p G ? we can obtain a corresponding y in data distribution p data by directly conducting gradient decent with the optimal discriminator D * ? :</p><formula xml:id="formula_16">y = x + w x * ? x D * ? , w x ? 0<label>(8)</label></formula><p>It seems to be a simple and appealing solution to improve p G ? with the guidance of discriminator D ? . However, the following issues exist:</p><p>1) there is no theoretical indication on how to set w x for each sample x in generated distribution. We noticed that a concurrent work <ref type="bibr" target="#b32">[33]</ref> introduce a search process called Discriminator Optimal Transport(DOT) by finding the corresponding y * through the following:</p><formula xml:id="formula_17">y x = arg min y y ? x 2 ? D * ? (y)<label>(9)</label></formula><p>However, it should be noticed that Eq. 9 has a non-unique solution. As indicated by Theorem 2, all points on the connection between x and y are valid solutions. We further extend the fact into the following theorem: Theorem 3. With the ? * and D * ? as the optimal solutions of the primal problem in Eq. 7 and Kantorovich-Rubinstein duality of Eq. 7, the distribution p ot implied by the generated distribution p G ? and the discriminator D * ? is defined as(y x is defined in Eq. 9):</p><formula xml:id="formula_18">p ot (y) = dx?(y ? y x )p G ? (x)</formula><p>when p data = p G ? , there exists infinite numbers of p ot with p data as a special case.</p><p>Theorem 3 provides a theoretical justification for the poor empirical performance of conducting DOT in the sample space, as shown in their paper.</p><p>2) Another problem lies in that samples distributed outside the generated distribution (p G ? ) are never explored during training, which results in much adversarial noise during the gradient-based search process, especially when the sample space is high dimensional such as real-world images. Sample a batch of data samples {x t } m t=1 for empirical data distribution p data and {z t } m t=1 for the prior distribution p(z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for iteration l = 1, ? ? ? , K do 6:</p><formula xml:id="formula_19">Pixel Space: G ? (z t ) l = G ? (z t ) l?1 ? 2 ? x D ? G ? (z t ) l?1 + ? ?, ? ? N (0, I) or 7: Latent Space: z l t = z l?1 t ? 2 ? z D ? G ? (z t ) l?1 + ? ?, ? ? N (0, I) 8:</formula><p>end for 9:</p><p>Optimized the following objective w.r.t. ?:</p><p>10:</p><formula xml:id="formula_20">Pixel Space: L = 1 m t (D ? (x t ) ? D ? (G ? (z t ) K )) or 11: Latent Space: L = 1 m t (D ? (x t ) ? D ? (G ? (z K t ))) 12: end for</formula><p>To fix the issues mentioned above in leveraging the information of discriminator in Wasserstein GANs, we propose viewing the discriminator as an energy function. With the discriminator as an energy function, the stationary distribution is unique, and Langevin dynamics can approximately conduct sampling from the stationary distribution. Due to the monotonic property of MCMC, there will not be issues like setting w x in Eq. 8. Besides, the second issue can also be easily solved by fine-tuning the energy spaces with contrastive divergence. In addition to the benefits illustrated above, if the discriminator is an energy function, the samples from the corresponding energy-based model can be obtained through Langevin dynamics by using the gradients of the discriminator which takes advantage of the property of discriminator as shown in Theorem 2. With all the facts as mentioned above, there is strong motivation to explore further and bridge the gap between discriminator in WGAN and the energy-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-Amortized Generation with Langevin Dynamics</head><p>We first introduce the Fenchel dual of the intractable partition function Z ? in Eq. 2: Theorem 4. <ref type="bibr" target="#b38">[39]</ref> With H(q) = ? q(x) log q(x)dx, the Fenchel dual of log-partition Z ? is as follows:</p><p>A</p><formula xml:id="formula_21">(E ? ) = max q?P q(x), E ? (x) + H(q),<label>(10)</label></formula><p>where P denotes the space of distributions, and q(x), E ? (x) = E ? (x)q(x)dx.</p><p>We put the Fenchel dual of A(E ? ) back into the MLE objective in Eq. 3, we achieve the following min-max game formalization for training energy-based model based on MLE: min</p><formula xml:id="formula_22">q?P max E ? ?E E x?Pdata [E ? (x)] ? E x?q [E ? (x)] WGAN's objective for critic ? H(q) entropy regularization .<label>(11)</label></formula><p>The Fenchel dual view of MLE training in the energy-based model explicitly illustrates the gap and connection between the WGAN and Energy based model. If we consider the dual distribution q as the generated distribution p G ? , and the D ? as the energy function E ? . The duality form for training energy-based models is essentially the WGAN's objective with the entropy of the generator is regularized.</p><p>Hence to turn the discriminator in WGAN into an energy function, we may conduct several fine-tuning steps, as illustrated in Eq. 11. Note that maximizing the entropy of the p G ? is indeed a challenging task, which needs to either use a tractable density generator, e.g., normalizing Flows <ref type="bibr" target="#b6">[7]</ref>, or maximize the mutual information between the latent variable Z and the corresponding G ? (Z) when the G ? is a deterministic mapping. However, instead of maximizing the entropy of the generated distribution p G ? directly, we derive our method based on the following fact: Proposition 1.</p><p>[20] Update the generated distribution p G ? according to the gradient estimated through Equation. 11, essentially minimized the Kullback-Leibler (KL) divergence between p G ? and the distribution p D ? , which refers to the distribution implied by using D ? as the energy function, as illustrated in Eq. 2, i.e. D KL (p G ? ||p D ? ).</p><p>To avoid the computation of H(p G ? ), motivated by the monotonic property of MCMC, as illustrated in Eq. 5, we propose Discriminator Contrastive Divergence (DCD), which replaces the gradient-based optimization on q(p G ? ) in Eq. 11 with several steps of MCMC for finetuning the critic in WGAN into an energy function. To be more specific, we use Langevin dynamics <ref type="bibr" target="#b33">[34]</ref> which leverages the gradient of the discriminator to conduct sampling:</p><formula xml:id="formula_23">x k = x k?1 ? 2 ? x D ? (x k?1 ) + ? ?, ? ? N (0, I),<label>(12)</label></formula><p>Where refers to the step size. The whole finetuning procedure is illustrated in Algorithm 1. The GAN-based approaches are implicitly constrained by the dimension of the latent noise, which is based on a widely applied assumption that the high dimensional data, e.g., images, actually distribute on a relatively low-dimensional manifold. Apart from searching the reasonable point in the data space, we could also find the lower energy part of the latent manifold by conducting Langevin dynamics in the latent space which are more stable in practice, i.e.:</p><formula xml:id="formula_24">z l t = z l?1 t ? 2 ? z D ? G ? (z t ) l?1 + ? ?, ? ? N (0, I).<label>(13)</label></formula><p>Ideally, the proposal should be accepted or rejected according to the Metropolis-Hastings algorithm:</p><formula xml:id="formula_25">? := min 1, D ? (x k ) q (x k?1 |x k ) D ? (x k?1 ) q (x k |x k?1 ) ,<label>(14)</label></formula><p>where q refers to the proposal which is defined as:</p><formula xml:id="formula_26">q (x |x) ? exp ? 1 4? x ? x ? ? ? log ?(x) 2 2 .<label>(15)</label></formula><p>In practice, we find the rejection steps described in Eq. 14 do not boost performance. For simplicity, following <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>, we apply Eq. 12 in experiments as an approximate version.</p><p>After fine-tuning, the discriminator function can be approximated seen as an unnormalized probability function, which implies a unique distribution p D ? . And similar to the p * implied in the rejection sampling-based method, it is reasonable to assume that p D ? is a superior distribution of p G ? . Sampling from p D ? can be implemented through the Langevin dynamics, as illustrated in Eq. 12 with p G ? serves as the initial distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct extensive experiments on both synthetic data and real-world images to demonstrate the effectiveness of our proposed method. The results show that taking the optionally fine-tuned Discriminator as the energy function and sampling from the corresponding p D ? yield stable improvement over the WGAN implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Density Modeling</head><p>Displaying the level sets is a meaningful way to study learned critic. Following the <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>, we investigate the impacts of our method on two challenging low-dimensional synthetic settings: twentyfive isotropic Gaussian distributions arranged in a grid and eight Gaussian distributions arranged in a ring <ref type="figure" target="#fig_1">(Fig. 2a</ref>). For all different settings, both the generator and the discriminator of the WGAN model are implemented as neural networks with four fully connected layers and Relu activations. The Lipschitz constraint is restricted through spectral normalization <ref type="bibr" target="#b24">[25]</ref>, while the prior is a twodimensional multivariate Gaussian with a mean of 0 and a standard deviation of 1.</p><p>To investigate whether the proposed Discriminator Contrastive Divergence is capable of tuning the distribution induced by the discriminator as desired energy function, i.e. p D ? , we visualize both the value surface of the critic and the samples obtained from p D ? with Langevin dynamics. The results are shown in <ref type="figure">Figure.</ref> 2. As can be observed, the original WGAN <ref type="figure" target="#fig_1">(Fig. 2b)</ref> is strong enough to cover most modes, but there are still some spurious links between two different modes. The enhanced distribution p D ? <ref type="figure" target="#fig_1">(Fig. 2c)</ref>, however, has the ability to reduce spurious links and recovers the modes with underestimated density. More precisely, after the MCMC fine-tuning procedure <ref type="figure" target="#fig_1">(Fig. 2c)</ref>, the gradients of the value surface become more meaningful so that all the regions with high density in data distribution p data are assigned with high D ? value, i.e., lower energy(exp(?D ? )). By contrast, in the original discriminator <ref type="figure" target="#fig_1">(Fig. 2b)</ref>, the lower energy regions in p D ? are not necessarily consistent with the high-density region of p data . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-World Image Generation</head><p>To quantitatively and empirically study the proposed DCD approach, in this section, we conduct experiments on unsupervised real-world image generation with DCD and its related counterparts. On several commonly used image datasets, experiments demonstrate that our proposed DCD algorithm can always achieve better performance on different benchmarks with a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experimental setup</head><p>Baselines. We evaluated the following models as our baselines: we take PixelCNN <ref type="bibr" target="#b37">[38]</ref>, Pix-elIQN <ref type="bibr" target="#b28">[29]</ref>, and MoLM <ref type="bibr" target="#b29">[30]</ref> as representatives of other types of generative models. For the energybased model, we compared the proposed method with EBM <ref type="bibr" target="#b7">[8]</ref> and NCSN <ref type="bibr" target="#b31">[32]</ref>. For GAN models, we take WGAN-GP <ref type="bibr" target="#b12">[13]</ref>, Spectral Normalization GAN (SNGAN) <ref type="bibr" target="#b24">[25]</ref>, and Progressiv eGAN <ref type="bibr" target="#b18">[19]</ref> for comparison. We also take the aforementioned DRS <ref type="bibr" target="#b2">[3]</ref>, DOT <ref type="bibr" target="#b32">[33]</ref> and MH-GAN <ref type="bibr" target="#b35">[36]</ref> into consideration. The choices of EBM and GANs are due to their close relation to our proposed method, as analyzed in Section 4. We omit other previous GAN methods since as a representative of a state-of-the-art GAN model, SNGAN and Progressive GAN has been shown to rival or outperform several former methods such as the original GAN <ref type="bibr" target="#b9">[10]</ref>, the energy-based generative adversarial network <ref type="bibr" target="#b39">[40]</ref>, and the original WGAN with weight clipping <ref type="bibr" target="#b1">[2]</ref>.</p><p>Evaluation Metrics. For evaluation, we concentrate on comparing the quality of generated images since it is well known that GAN models cannot perform reliable likelihood estimations <ref type="bibr" target="#b34">[35]</ref>. We choose to compare the Inception Scores <ref type="bibr" target="#b30">[31]</ref> and Frechet Inception Distances (FID) <ref type="bibr" target="#b14">[15]</ref> reached during training iterations, both computed from 50K samples. A high image quality corresponds to high Inception and low FID scores. Specifically, the intuition of IS is that high-quality images should lead to high confidence in classification, while FID aims to measure the computer-vision-specific similarity of generated images to real ones through Frechet distance.</p><p>Data. We use CIFAR-10 <ref type="bibr" target="#b21">[22]</ref> and STL-10 <ref type="bibr" target="#b4">[5]</ref>, which are all standard datasets widely used in generative literature. STL-10 consists of unlabeled real-world color images, while CIFAR-10 is provided with class labels, which enables us to conduct conditional generation tasks. For STL-10, we also shrink the images into 32 ? 32 as in previous works.   Network Architecture. For all experiment settings, we follow Spectral Normalization GAN (SNGAN) <ref type="bibr" target="#b24">[25]</ref> and adopt the same Residual Network (ResNet) <ref type="bibr" target="#b13">[14]</ref> structures and hyperparameters, which presently is the state-of-the-art implementation of WGAN. Details can be found in Appendix. D.</p><p>We take their open-source code and pre-trained model as the base model for the experiments on CIFAR-10. For STL-10, since there is no pre-trained model available to reproduce the results, we train the SNGAN from scratch and take it as the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>Model Inception FID SNGAN <ref type="bibr" target="#b24">[25]</ref> 8.90 ? .12 18.73 SNGAN-DCD (Pixel) 9.25 ? .09 22.25 SNGAN-DCD (Latent) 9.33 ? .04 17.68 <ref type="table">Table 2</ref>: Inception and FID scores for STL-10</p><p>For quantitative evaluation, we report the inception score <ref type="bibr" target="#b30">[31]</ref> and FID <ref type="bibr" target="#b14">[15]</ref> scores on CIFAR-10 in Tab. 1 and STL-10 in Tab. 2. As shown in the Tab. 1, in pixel space, by introducing the proposed DCD algorithm, we achieve a significant improvement of inception score over the SNGAN. The reported inception score is even higher than most values achieved by classconditional generative models. Our FID score of 21.67 on CIFAR-10 is competitive with other top generative models. When the DCD is conducted in the latent space, we further achieve a 9.11 inception score and a 16.24 FID, which is a new state-of-the-art performance of IS. When combined with label information to perform conditional generation, we further improve the FID to 15.05, which is comparable with current state-of-the-art large-scale trained models <ref type="bibr" target="#b3">[4]</ref>. Some visualization of generated examples can be found in <ref type="figure" target="#fig_2">Fig 3,</ref> which demonstrates that the Markov chain is able to generate more realistic samples, suggesting that the MCMC process is meaningful and effective. Tab. 2 shows the performance on STL-10, which demonstrates that as a generalized method, DCD is not over-fitted to the specific data CIFAR-10. More experiment details and the generated samples of STL-10 can be found in Appendix. F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Future Work</head><p>Based on the density ratio estimation perspective, the discriminator in f -GANs could be adapted to a wide range of application scenarios, such as mutual information estimation <ref type="bibr" target="#b17">[18]</ref> and bias correction of generative models <ref type="bibr" target="#b11">[12]</ref>. However, as another important branch in GANs' research, the available information in WGANs discriminator is less explored. In this paper, we narrow down the scope of discussion and focus on the problem of how to leverage the discriminator of WGANs to further improve the sample quality in image generation. We conduct a comprehensive theoretical study on the informativeness of discriminator in different kinds of GANs. Motivated by the theoretical progress in the literature of WGANs, we investigate the possibility of turning the discriminator of WGANs into an energy function and propose a fine-tuning procedure of WGANs named as "discriminator contrastive divergence". The final image generation process is semi-amortized, where the generator acts as an initial state, and then several steps of Langevin dynamics are conducted. We demonstrate the effectiveness of the proposed method on several tasks, including both synthetic and real-world image generation benchmarks.</p><p>It should be noted that the semi-amortized generation allows a trade-off between the generation quality and sampling speed, which holds a slower sampling speed than a direct generation with a generator. Hence the proposed method is suitable to the application scenario where the generation quality is given vital importance. Another interesting observation during the experiments is the discriminator contrastive divergence surprisingly reduces the occurrence of adversarial samples during training, so it should be a promising future direction to investigate the relationship between our method and bayesian adversarial learning.</p><p>We hope our work helps shed some light on a generalized view to a method of connecting different GANs and energy-based models, which will stimulate more exploration into the potential of current deep generative models.</p><formula xml:id="formula_27">f (x t + h y?x y?x 2 ) ? f (x t ) h = lim h?0 f (x t+ h y?x 2 ) ? f (x t ) h = lim h?0 h y?x 2 ? k y ? x 2 h = k.</formula><p>Then we derive the formal proof of Theorem 2.</p><p>Proof. Assume p = 2, if f (x) is k-Lipschitz with respect to . 2 and f (x) is differentiable at x t , then ?f (x t ) 2 ? k. Let v be the unit vector y?x y?x 2 . We have</p><formula xml:id="formula_28">k 2 = k ?f (x t ) ?v = k v, ?f (x t ) = kv, ?f (x t ) ? kv 2 ?f (x t ) 2 = k 2 .<label>(17)</label></formula><p>Because the equality holds only when ?f (x t ) = kv = k y?x y?x 2 , we have that ?f (x t ) = k y?x y?x 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 3</head><p>Theorem. 3 states that following the following procedure as introduced in <ref type="bibr" target="#b32">[33]</ref>, there is non-unique stationary distribution. The complete procedure is to find the following y for x ? P G ? :</p><formula xml:id="formula_29">y * = arg min x { x ? y 2 ? D(x)}.<label>(18)</label></formula><p>To find the corresponding y * , the following gradient based update is conducted:</p><formula xml:id="formula_30">{x ? x ? ? x {||x ? y|| 2 ? D(x)} .<label>(19)</label></formula><p>For all the points x t in the linear interpolation of x and target y * as defined in the proof of Theorem 2,</p><formula xml:id="formula_31">? xt {||x t ? y|| 2 ? D(x t )} = y ? x y ? x 2 ? y ? x y ? x 2 = 0,<label>(20)</label></formula><p>which indicates all points in the linear interpolation satisfy the stationary condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Proposition 1</head><p>Proposition. 1 is the direct result of the following Lemma. 3. Following <ref type="bibr" target="#b23">[24]</ref>, we provide the complete proof as following. Lemma 3.</p><p>[6] Let q and r be two distributions for z 0 . Let q t and r t be the corresponded distributions of state z t at time t, induced by the transition kernel K. Then D KL [q t ||r t ] ? D KL [q t+1 ||r t+1 ] for all t ? 0.</p><p>Proof.</p><formula xml:id="formula_32">D KL [q t ||r t ] = E qt log q t (z t ) r t (z t ) = E qt(zt)K(zt+1|zt) log q t (z t )K(z t+1 |z t ) r t (z t )K(z t+1 |z t ) = E qt+1(zt+1)qt+1(zt|zt+1) log q t+1 (z t+1 )q(z t |z t+1 ) r t+1 (z t+1 )r(z t |z t+1 ) = D KL [q t+1 ||r t+1 ] + E qt+1 D KL [q t+1 (z t |z t+1 )||r t+1 (z t |z t+1 )].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Network architectures</head><p>ResNet architectures for CIFAR-10 and STL-10 datasets. We use similar architectures to the ones used in <ref type="bibr" target="#b12">[13]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Discussions on Objective Functions</head><p>Optimization of the standard objective of WGAN, i.e. with r(x) = m(x) = x in Eq. 1, are found to be unstable due to the numerical issues and free offset <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25]</ref>. Instead, several surrogate losses are actually used in practice. For example, the logistic loss(r(x) = m(x) = ? log(1 + e ?x )) and hinge loss(r(x) = m(x) = min(0, x)) are two widely applied objectives. Such surrogate losses are valid due to that they are actually the lower bounds of the Wasserstain distance between the two distributions of interest. The statement can be easily derived by the fact that ? log(1 + e ?x ) ? x and min(0, x) ? x. A more detailed discussion could also be found in <ref type="bibr" target="#b32">[33]</ref>.</p><p>Note that min(0, ?1 + x) and ? log(1 + e ?x ) are in the function family proposed in <ref type="bibr" target="#b40">[41]</ref>, and Theorem 4 in <ref type="bibr" target="#b40">[41]</ref> guarantees the gradient property of discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Experiment Details F.1 CIFAR-10</head><p>For the meta-parameters in DCD Algorithm 1, when the MCMC process is conducted in the pixel space, we choose 6?8 as the number of MCMC steps K, and set the step size as 10 and the standard deviation of the Gaussian noise as 0.01, while for the latent space we set K as 50, as 0.2 and the deviation as 0.1. Adam optimizer <ref type="bibr" target="#b20">[21]</ref> is set with 2 ? 10 ?4 learning rate with ? 1 = 0, ? 2 = 0.9. We use 5 critic updates per generator update, and a batch size of 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 STL-10</head><p>We show generated samples of DCD during Langevin dynamics in <ref type="figure" target="#fig_4">Fig. 4</ref>. We run 150 steps of MCMC steps and plot generated sample for every 10 iterations. The step size is set as 0.05 and the noise is set as N (0, 0.1). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Density modeling on synthetic distributions. Top: 8 Gaussian distribution. Bottom: 25 Gaussian distribution. Left: Distribution of real data. Middle: Distribution defined by the generator of SNGAN. The surface is the level set of the critic. Yellow corresponds to higher value while purple corresponds to lower. Right: Distribution defined by the SNGAN-DCD. The surface is the level set of the proposed energy function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Unconditional CIFAR-10 Langevin dynamics visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>z</head><label></label><figDesc>? R 128 ? N (0, I) dense, 4 ? 4 ? 256</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>STL-10 Langevin dynamics visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Discriminator Contrastive Divergence 1: Input: Pretrained generator G ? , discriminator D ? . 2: Set the step size , the length of MCMC steps K and the total iterations T .</figDesc><table /><note>3: for iteration i = 1, ? ? ? , T do4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Inception and FID scores for CIFAR-10.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Generator RGB image x ? R 32?32?3</figDesc><table><row><cell>ResBlock down 128</cell></row><row><cell>ResBlock down 128</cell></row><row><cell>ResBlock 128</cell></row><row><cell>ResBlock 128</cell></row><row><cell>ReLU</cell></row><row><cell>Global sum pooling</cell></row><row><cell>dense ? 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Discriminator</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 2</head><p>It should be noticed that Theorem. 2 can be generalized to that Lipschitz continuity with l 2 -norm (Euclidean Distance) can guarantee that the gradient is directly pointing towards some sample <ref type="bibr" target="#b40">[41]</ref>. We introduce the following lemmas, and Theorem. 2 is a special case.</p><p>Let (x, y) be such that y = x, and we define</p><p>Proof. As we know f (x) is k-Lipschitz, with the property of norms, we have</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A general class of coefficients of divergence of one distribution from another</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Syed Mumtaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06758</idno>
		<title level="m">Discriminator rejection sampling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08689</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03852</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03263</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bias correction of learned generative models using likelihood-free importance weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09531</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fundamentals of convex analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Hiriart-Urruty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Lemar?chal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08431</idno>
		<title level="m">Boundary-seeking generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep directed generative models with energy-based probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03439</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Maximum entropy generators for energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08508</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Approximate inference with amortised mcmc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08343</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Autoregressive quantile networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05575</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning implicit generative models with the method of learned moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11006</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06832</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Discriminator optimal transport. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Energy-based models for sparse overcomplete representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1235" to="1260" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<title level="m">A?ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11357</idno>
		<title level="m">Metropolis-hastings generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Generative adversarial nets from a density ratio estimation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02920</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiadong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05687</idno>
		<title level="m">Lipschitz generative adversarial nets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
