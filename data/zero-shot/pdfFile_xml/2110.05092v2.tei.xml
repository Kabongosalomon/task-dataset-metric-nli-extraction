<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Adaptive Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D human pose estimation</term>
					<term>Multi-view Fusing Transformer</term>
					<term>Temporal Fusing Transformer !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a unified framework dubbed Multi-view and Temporal Fusing Transformer (MTF-Transformer)  to adaptively handle varying view numbers and video length without camera calibration in 3D Human Pose Estimation (HPE). It consists of Feature Extractor, Multi-view Fusing Transformer (MFT), and Temporal Fusing Transformer (TFT). Feature Extractor estimates 2D pose from each image and fuses the prediction according to the confidence. It provides pose-focused feature embedding and makes subsequent modules computationally lightweight. MFT fuses the features of a varying number of views with a novel Relative-Attention block. It adaptively measures the implicit relative relationship between each pair of views and reconstructs more informative features. TFT aggregates the features of the whole sequence and predicts 3D pose via a transformer. It adaptively deals with the video of arbitrary length and fully unitizes the temporal information. The migration of transformers enables our model to learn spatial geometry better and preserve robustness for varying application scenarios. We report quantitative and qualitative results on the Human3.6M, TotalCapture, and KTH Multiview Football II. Compared with state-of-the-art methods with camera parameters, MTF-Transformer obtains competitive results and generalizes well to dynamic capture with an arbitrary number of unseen views.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HREE-dimensional human pose estimation (HPE) aims to predict 3D human pose information from images or videos, in which skeleton joint location is the primary output result to carry pose information. It plays a fundamental role in many applications, such as action recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, human body reconstruction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and robotics manipulation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>With the emergence of deep learning, 3D HPE has made considerable progress. Especially, 2D-to-3D <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> methods have superior performance owing to intermediate 2D supervision <ref type="bibr" target="#b11">[12]</ref>. In practice, the 2D-to-3D pipeline involves several variable factors deriving from different application scenarios, including the number of views, the length of the video sequence, and whether using camera calibration.</p><p>In the monocular scene, most works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> estimate body structure from a static image with elaborate networks such as Convolutional Neural Networks and Graph Convolutional Networks. This scheme is convenient since a single image is easy to obtain and process. Nevertheless, the information in a single image is insufficient considering the occlusion and depth ambiguity. For compensation, some works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> utilize temporal information from video sequences. Sequential variation in the video is conducive to revealing the human body's structure. However, continuous images contain more homogeneous information rather than complementary clues. In a word, monocular 3D HPE is convenient to implement, but recovering 3D structure from 2D images is always an ill-posed problem. <ref type="bibr">?</ref>  Recently, prevalent works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> tend to utilize multi-view geometric constraints. Most existing multi-view methods aggregate information from different views via projective geometry, depending on calibrated camera parameters. Camera parameters incorporate solid prior knowledge into the network but are difficult to calibrate accurately in dynamic capture. To this end, some works <ref type="bibr" target="#b25">[26]</ref> attempt to fuse multi-view features without calibration, but they have strict requirements on camera configuration and the number of views. In addition, massive computation in the geometric space hinders multi-view methods to deal with video sequences. Overall, most existing multi-view methods are more accurate than monocular methods, but camera calibration and computation overhead limit their application scenarios.</p><p>Each method, as mentioned above, targets one or a few particular combinations of those variable factors and is not compatible with others, limiting the flexibility of the 3D HPE algorithm. Thus, developing a unified framework that can adaptively handle all the factors is essential. The main obstacles are that (1) Most deep learning modules, such as fully connected layers, long and shortterm memory (LSTM), and GCN, are not friendly to variablelength input. Moreover, these modules still have generalization problems even with careful adjustments to handle variable-length input. <ref type="bibr" target="#b1">(2)</ref> Most methods rely on camera calibration to deal with multi-view information, but precise camera parameters are unrealistic to calibrate synchronously in dynamic capture. (3) Some methods are too computationally expensive to deal with multi-view videos. Accordingly, a unified framework needs to be compatible with monocular to multi-view, single-image to videos 3D HPE: (1) It should effectively integrate an arbitrary number of multi-view features in uncalibrated scenarios. <ref type="bibr">(</ref>2) It should adaptively fuse temporal features in the variable-length videos and be compatible with a single image. <ref type="bibr">(</ref>3) It should be lightweight enough and have generalization capability.</p><p>To satisfy these requirements, we propose a unified framework to deal with variable multi-view sequences without calibration, arXiv:2110.05092v2 [cs.CV] 4 Jul 2022 named Multi-view and Temporal Fusing Transformer (MTF-Transformer) because the transformer can perceive the global relationship of a varying number of tokens and aggregate them adaptively <ref type="bibr" target="#b26">[27]</ref>. MTF-Transformer consists of Feature Extractor, Multi-view Fusion Transformer (MFT), and Temporal Fusion Transformer (TFT). In the Feature Extractor, a pre-trained 2D detector predicts the 2D pose of each frame first. Then, coordinates and confidence are encoded into a vector via a feature embedding module, discarding the image features. It makes subsequent modules lightweight and focuses on lifting the 2D pose into the 3D pose. MFT is designed to fuse the features of multiple views into more informative ones. It integrates the relationship between the views into the procedure that calculates the key, query, and value in the Relative-Attention block, avoiding camera calibration. In TFT, we employ a conventional transformer to capture temporal information. It is worth mentioning that, to make the MTF-Transformer adaptive to the input of an arbitrary number of views and length of sequences, we design a random mask mechanism in both MFT and TFT, referring to the dropout mechanism <ref type="bibr" target="#b27">[28]</ref>.</p><p>We evaluate our method on Human3.6M <ref type="bibr" target="#b28">[29]</ref>, TotalCapture <ref type="bibr" target="#b29">[30]</ref>, and KTH Multiview Football II <ref type="bibr" target="#b30">[31]</ref> quantitatively and qualitatively. We also conduct detailed ablation study experiments to verify the effectiveness of each module. Experiment results demonstrate that MTF-Transformer outperforms camera parameter-free methods. Besides, MTF-Transformer can be directly applied to the scenarios with different configurations from the training stage, bridging the generalization gap significantly. In short, our contributions are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We proposed a unified framework (MTF-Transformer) for 3D HPE. It is adaptive to scenarios with videos of arbitrary length and from arbitrary views without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We design a novel Multi-view Fusing Transformer (MFT), where the relationship between views is integrated into the Relative-Attention block. MFT reconstructs the features from multiple views according to the estimated implicit relationship, avoiding the need for camera calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We introduce the random mask mechanism into MFT and Temporal Fusing Transformer (TFT) to make them robust to variable view number and video length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Not only does our model outperform camera parameterfree models in precision, but it also has a better generalization capability to handle diverse application scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This section firstly summarizes 3D human pose estimation works, including monocular and multi-view methods. Then, we review the transformer technology and introduce the methods that apply the transformer in 3D human pose estimation and some other related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Human Pose Estimation</head><p>Fundamentally, 3D HPE is to reconstruct the 3D body structure from 2D data. It is an ill-posed inverse task as one 2D image corresponds to many possible 3D poses, further amplified by occlusions, background clutters, etc. Thus, utilizing all kinds of clues, such as the mutual constraint between the joints in the image, the complementary information in videos, and the spatial geometric relationship from multiple viewpoints, to piece together the most likely 3D pose is the rationale for 3D HPE.</p><p>According to the different clues, 3D HPE methods are divided into categories and developed into several frameworks that handle specific application scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Monocular 3D Human Pose Estimation</head><p>With the pattern self-organizing and non-linear mapping capacity of deep neural networks, many approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> directly map pixel intensities to 3D poses from a single image. It forces DNNs to remember the pattern and infer the 3D pose. These networks are difficult to learn and rely on tremendous labeled samples, resulting in unsatisfactory performance and generalization capability. Therefore, prior constraints between joints are utilized to determine the special pose. Fang <ref type="bibr" target="#b4">[5]</ref> et al. incorporate kinematics, symmetry, and motor coordination grammar in 3D pose estimation. Some works employ GCN to model the constraints between the joints <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>. These methods are devoted to digging into the image's potential information, but such a manner is insufficient to solve an ill-posed problem. To solve the ambiguity of a single image, more works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> pay attention to temporal consistency in the video. For example, Pavllo et al. <ref type="bibr" target="#b15">[16]</ref> transform a sequence of 2D poses through temporal convolutions. Cai et al. propose a graphbased method to incorporate spatial dependencies and temporal consistences <ref type="bibr" target="#b16">[17]</ref>. Wang et al. <ref type="bibr" target="#b39">[40]</ref> employ a novel objective function to involve motion modeling in learning explicitly. Temporal information compensates for the incompleteness of 3D geometry, improving the performance of 3D HPE. In general, monocular methods are easy to implement as there is no need for camera calibration. However, to piece up the 3D structure from 2D images, it is evident that the clues from multiple viewpoints are better alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Multi-view 3D Human Pose Estimation</head><p>To tackle the occlusion and depth ambiguity, multi-view methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> exploit geometric information from multiple views to infer 3D pose. Most utilize intrinsic and extrinsic camera parameters to fuse 2D features from different views. For example, He et al. <ref type="bibr" target="#b20">[21]</ref> aggregate features on epipolar lines between different views, depending on camera parameters in specific camera configurations. Iskakov et al. <ref type="bibr" target="#b21">[22]</ref> utilize volumetric grids to fuse features from different views with camera parameters and regress root-centered 3D pose through a learnable 3D CNN. Despite predicting 3D poses reliably, volumetric approaches are computationally demanding. These methods require precise camera parameters but can not generalize to scenarios with new camera configurations, not to mention the dynamic capture. Huang et al. <ref type="bibr" target="#b25">[26]</ref> propose a new vision-IMU data fusion technique to avoid strict requirements on camera configuration and the number of views. FLEX <ref type="bibr" target="#b45">[46]</ref> introduce to predict joint angles and bone lengths invariant to the camera position rather than directly 3D positions, so calibration is obviated. Nevertheless, it is complicated, and its performance degenerates with only a few views. Multi-view pose estimation methods are more accurate due to adequate feature fusing via projective geometry. However, another side of the coin is that these methods rely on the restricted camera configuration explicitly or implicitly, limiting their application scene.</p><p>Monocular and multi-view methods exploit the clues from different aspects and fit particular application scenarios. Unlike these methods, we attempt to fuse all the clues adaptively in a unified network that can predict robust 3D poses in all the application scenarios. So, the critical component is to find a mechanism that organically integrates the information from different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer in 3D Pose Estimation</head><p>Transformer and self-attention have tremendously succeeded in Natural Language Processing and Computer Vision <ref type="bibr" target="#b46">[47]</ref>. The selfattention module can adaptively capture long-range dependencies and global correlations from the data. In 3D pose estimation, the core is to integrate the information from spatial 2D joints, temporal sequence, and multiple viewpoints. Thus, the transformer is suitable for handling these aspects of information, and some works utilizing the transformer models have recently emerged. Following the line of lifting 2D to 3D, some monocular methods improve the performance by introducing the transformer. Among them, METRO <ref type="bibr" target="#b47">[48]</ref> employs a multi-layer transformer architecture with progressive dimensionality reduction to regress the 3D coordinates of the joints and vertices. PoseGTAC <ref type="bibr" target="#b48">[49]</ref> proposes graph atrous convolution and graph transformer layer to extract local multi-scale and global long-range information, respectively. More works handle spatial-temporal clues with the transformer to alleviate occlusion and depth ambiguity in a single image. For example, LiftFormer <ref type="bibr" target="#b50">[50]</ref> estimates 3D pose from a sequence of 2D keypoints with self-attention on long-term information. MHFormer <ref type="bibr" target="#b51">[51]</ref> proposes a Multi-Hypothesis Transformer (MHFormer) to learn spatio-temporal representations of multiple plausible pose hypotheses and aggregates the multi-hypothesis into the final 3D pose. Strided Transformer <ref type="bibr" target="#b52">[52]</ref> incorporates the strided convolution into the transformer to aggregate long-range information in a hierarchical architecture at low computation. PoseFormer <ref type="bibr" target="#b19">[20]</ref> proposes a purely transformer-based approach to model the spatial relationships between 2D joints and temporal information in videos. Naturally, the transformer is also used to aggregate the multi-view clues, but it usually works with epipolar geometric while camera parameters are essential prerequisites. Epipolar transformer <ref type="bibr" target="#b20">[21]</ref> leverages the transformer to find the point-point correspondence in the epipolar line. TransFusion <ref type="bibr" target="#b24">[25]</ref> further proposes the concept of epipolar field to encode 3D positional information into the transformer.</p><p>This tendency demonstrates the potential of the transformer for feature fusing in 3D pose estimation. Moreover, the transformer is inherently adaptive to a variable number of input tokens. Thus, our concerns focus on generalization capability and calibration avoidance. Fortunately, the transformer generalizes well to the configurations different from the training phase in some other tasks. For example, Pooling-based Vision Transformer <ref type="bibr" target="#b53">[53]</ref> improves model capability and generalization performance via designing a pooling layer in ViT. Neural Human Performer <ref type="bibr" target="#b54">[54]</ref> synthesizes a free-viewpoint video of an arbitrary human performance, and it generalizes to unseen motions and characters at test time. It adaptively aggregates multi-time and multi-view information with temporal and multi-view transformer. However, Neural Human Performer fuses the multi-view features that are pixel-wisely matched by a parametric 3D body model (SMPL). Such pixelpixel correspondence between multiple viewpoints is stronger than pixel-epipolar correspondence. So, fusing multi-view features without camera calibration remains an open problem for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>The purpose of our framework is to adaptively handle features from an arbitrary number of views and arbitrary sequence length without camera calibration. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the basic idea is to embed 2D detections into vectors first, then fuse multi-view features, and finally aggregate temporal clues to predict 3D joints. This framework consists of Feature Extractor, Multi-view Fusing Transformer (MFT), and Temporal Fusing Transformer (TFT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extractor</head><p>Feature Extractor uses a pre-trained 2D pose detector (e.g., CPN <ref type="bibr" target="#b55">[55]</ref>) to obtain 2D predictions and then maps them into 1D feature vectors through a feature embedding module.</p><p>Taking multi-view sequences</p><formula xml:id="formula_0">I = {I i } N ?T i=1</formula><p>with N views and T frames as input, each frame is an image I ? R W ?H?3 . As the following operations are conducted on each frame, we omit N and T for simplicity here. For each frame, Feature Extractor first uses a pre-trained 2D pose detector D 2D to infer the 2D prediction: </p><formula xml:id="formula_1">Z = D 2D (I)<label>(1)</label></formula><formula xml:id="formula_2">where Z = {P 2D , C 2D }, P 2D = {p j } J j=1</formula><p>represents J coordinates of the 2D pose and C 2D = {c j } J j=1 represents the confidence of these coordinates. Then a feature embedding module encodes the predicted 2D pose into a feature vector (as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>). The movements of the limbs and head are relatively independent, so we divide the human body joints into five partitions and deal with them in five parallel branches. The five partitions are the head, left and right arms, and left and right legs:</p><formula xml:id="formula_3">P g 2D = {p k |k ? S g } (2) C g 2D = {c k |k ? S g }<label>(3)</label></formula><p>where g refers to the g-th partition, g ? {1, 2, 3, 4, 5}, P g 2D , C g 2D are subset of P 2D , C 2D , S g ? {1, 2, ..., J} represents the index set belongs to the g-th partition. For matrix multiplication, P g 2D ,</p><formula xml:id="formula_4">C g 2D are reshaped into vectors that p g 2D ? R 2J g , c g 2D ? R J g .</formula><p>Since the 2D pose inferred from the pre-trained detector is unreliable due to motion blur and occlusion, simply fusing them may lead to unstable performance. Previous works, such as FLEX <ref type="bibr" target="#b45">[46]</ref>, directly concatenate the 2D pose and confidence values together for aggregation but they ignore the effects of unreliable inputs on the features as the pose changes. In order to alleviate this issue, we utilize the confidence to modulate coordinates. Specifically, Confidence Attentive Aggregation (CAA) extracts local feature f g ? R C/2 for each part, C is the dimension of the output of Feature Extractor. It can be formulated as:</p><formula xml:id="formula_5">f g = F g p (p g 2D ) (4) a g = F g c (c g 2D ) (5) f g = F g res f g + a g ? p g 2D<label>(6)</label></formula><p>where F g p is a fully connected layer to map 2D coordinates p g 2D to initial feature vectorsf g ? R C/2 , F g c is another fully connected layer to learn a attention matrix a g ? R (C/2)?2J g from the confidence c g 2D . The third fully connected layer F g res aggregates initial feature vectorsf g with 2D coordinates p g 2D modulated by attention matrix a g . It consists of two res-blocks <ref type="bibr" target="#b7">[8]</ref>.</p><p>We further concatenate features of five partitions together and map them into a global feature f ? R C . This procedure can be described as:</p><formula xml:id="formula_6">f = F shrink Concat f 1 , f 2 , f 3 , f 4 , f 5 (7)</formula><p>where F shrink is another fully connected layer. It maps features from five branches to the global feature of each frame. For the input multi-view sequence I with N ?T frames, Feature Extractor extracts the feature X ? R C?N ?T for the subsequent pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-view Fusing Transformer</head><p>We target to measure the relationship between the features from an arbitrary number of views and fuse these features adaptively. Transformer models are characterized by the ability to model dependencies in the input tokens regardless of their distance and enable immediate aggregation of global information <ref type="bibr" target="#b26">[27]</ref>. Thus, the transformer is a candidate to complete this task. Nevertheless, the conventional transformer does not meet our requirements in position encoding, and Point Transformer <ref type="bibr" target="#b56">[56]</ref> has limitations in manipulating the value item. So, we design a Relative-Attention that measures the relative relationship between multiple viewpoints and employs a more elaborate value transform procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Revisit Transformer and Self-attention</head><p>The transformer is a family of models consisting of the selfattention block, appending the position encoding, and the mask block. The position encoding provides a unique coding for each input token. The mask block truncates some nonexistent connections based on prior knowledge. Self-attention operator transforms the input feature vectors</p><formula xml:id="formula_7">X = {x i } N i=1 into output feature vectors Y = {y i } N i=1</formula><p>, one output feature vector y i is a weighted sum of all the input feature vectors. Typically, self-attention operators can be classified into scalar attention and vector attention <ref type="bibr" target="#b56">[56]</ref>.</p><p>The scalar dot-product attention can be formulated as follows:</p><formula xml:id="formula_8">y i = xj ?X ? ? (x i ) ? (x j ) + ? ? (x j )<label>(8)</label></formula><p>where ?, ?, and ? are pointwise feature transformations, such as linear projections or MLPs, ? (x i ), ? (x j ), and ? (x j ) are called query, key, and value. ? is a position encoding function and ? is a normalization function such as softmax (mask block is optional). The scalar attention layer computes the scalar product between features transformed by ? and ?, and uses the output as an attention weight for aggregating features transformed by ?. Differently, in the vector attention, attention weights are vectors that can modulate individual feature channels:</p><formula xml:id="formula_9">y i = xj ?X ? (? (? (? (x i ) , ? (x j )) + ?)) ? (x j ) (9)</formula><p>where ? is a relation function (e.g., subtraction) and ? is a mapping function (e.g., an MLP) that produces attention vectors for feature aggregation, is element-wise product.</p><p>Nevertheless, scalar attention and vector attention do not perfectly satisfy our requirements. First, they both employ position encoding to indicate the absolute position of the input token, but we only need a relative relationship. Second, the value is only a derivative of x j , but we hope it can reflect the relative relationship between x i and x j as well. Point Transformer <ref type="bibr" target="#b56">[56]</ref> proposes a relative position encoding and adds the position encoding to the value item, alleviating the above two issues. However, its relative position encoding is additive. The addition represents the translation operation in the vector space, but we need a more flexible operation to manipulate the features from different views. Moreover, if we directly use Point Transformer in our task, we have to concatenate all the 2D joints and converse it into the More parameters but less flexibility often lead to the generalization problem, and this problem is verified in TABLE 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-view Fusing Transformer</head><p>To bridge the gap between our purpose and existing transformer models, we propose the Multi-view Fusing Transformer (MFT). As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, taking X ? R C?N ?T as input, MFT considers it as tokens of X = {x i } N i=1 , from the perspective of view. The dimension of T is omitted here as MFT equally operates in each time slice. In addition, different body parts go through a similar transformation between multiple viewpoints even after feature transformation. Inspired by Squeeze Reasoning <ref type="bibr" target="#b57">[57]</ref> that related components distribute in different groups along channels sparsely, we divide the dimension of C into K groups, and the same transform matrix manipulates each group. So, we get</p><formula xml:id="formula_10">x i ? R D?K , C = D ? K.</formula><p>The output of MFT is X :</p><formula xml:id="formula_11">X = RA (X) + X<label>(10)</label></formula><p>RA is Relative-Attention. In Relative-Attention, the input X triplicates the role of query, key, and value, the output is</p><formula xml:id="formula_12">Y = {y i } N i=1 . A ij = ? ( (x i , x j )) (11) T ij = ? ( (x i , x j ))<label>(12)</label></formula><formula xml:id="formula_13">y i = xj ?X ? (A ij ) (T ij x j )<label>(13)</label></formula><p>where (x i , x j ) measures the relationship between each pair of view {x i , x j }, ? and ? further transform (x i , x j ) into attention matrix A ij ? R D?K and transform matrix T ij ? R D?D via fully connected layers, ? consists of a block mask module and a softmax operation. The block mask module randomly sets all the values of A ij to ?inf at the rate of M , except the condition that i = j. Those values turn into zero after softmax. This mechanism ensures the MFT generalizes well to the scenario with an arbitrary number of views. For further regularization, we penalize the difference between the inferred T ij and T ij that derived from rotation matrix R ij between two viewpoints. R ij is the rotation matrix calculated from the 3D pose of i-th and j-th viewpoints via SVD. Then, it is flattened, transformed with MLP ?, and reshaped as T ij .</p><formula xml:id="formula_14">T ij = ?(R ij )<label>(14)</label></formula><p>? SVD <ref type="figure">Fig. 4</ref>. The architecture of relative relation encoding module</p><p>In the testing phase, this branch related to T ij is discarded. It is interesting to note that our framework is also compatible with the scenario with camera parameters. When utilizing camera extrinsic, we let T ij take the place of T ij in both the training and testing phase. In this circumstance, R ij is the rotation matrix of extrinsic parameters. The architecture of (x i , x j ) is shown in <ref type="figure">Fig. 4</ref>, formulated as:</p><formula xml:id="formula_15">h ij = F p (p i 2D ? p j 2D ) + F i (x i ) + F j (x j )<label>(15)</label></formula><formula xml:id="formula_16">(x i , x j ) = F ij (h ij ) + h ij<label>(16)</label></formula><p>where p i 2D and p j 2D are flatten 2D poses from the 2D detector. We add the offset between viewpoints to enhance the geometric awareness of the relative-attention module. F p , F i , F j and F ij are fully connected layers.</p><p>The Relative-Attention is a vector product-like operation. The difference between them is that (1) the explicit position encoding is discarded in query and value items, and (2) the relative relationship is also integrated into the value item in the form of a transform matrix. In brief, MFT reconstructs the feature of each view according to the relationship between them, formulated as: The Temporal Fusing Transformer (TFT) is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, it takes X as input and predicts the 3D pose of J joint points P 3D ? R 3?J?N in static scenes or dynamic scenes. Specifically, TFT utilizes a Transformer Encoder block <ref type="bibr" target="#b26">[27]</ref> of two encoder layers to get the 3D pose of the middle frame. As the temporal sequence has  <ref type="bibr" target="#b63">[63]</ref> (*,</p><formula xml:id="formula_17">X ? X , X ? R C?N ?T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Fusing Transformer</head><formula xml:id="formula_18">T = 1) - - - - - - - - - - - - - - - 21.</formula><p>7 Remeli et al. <ref type="bibr" target="#b64">[64]</ref> (*, T = 1) 27. a direction and the order of frames matters, the position encoding is employed here. In addition, TFT masks some frames during the training stage to be compatible with a single image in static scenes and multi-view videos in dynamic scenes. For example, when the input video sequence has 7 frames, the left and right frames are masked evenly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>The loss Function consists of two components. We employ the mean per joint position error (MPJPE) as the training loss and the testing metric. MPJPE first aligns the root joint (central hip) of predicted skeleton S = {p i } J i=1 and the ground truth skeleton S gt = {p gt i } J i=1 , and then calculates the average Euclidean distance between each joints of them. MPJPE is computed as:</p><formula xml:id="formula_19">L M (S) = 1 J J i=1 p i ? p gt i 2<label>(17)</label></formula><p>Besides, we utilize the rotation matrix between each pair of views to constrain the transform matrix T ij , an extra transform error is also used as:</p><formula xml:id="formula_20">L t = T ij ? T ij 1<label>(18)</label></formula><p>The total loss function is:</p><formula xml:id="formula_21">L = L M + ?L t<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>MTF-Transformer is an end-to-end method implemented with Pytorch. We employ a pretrained 2D detector with frozen weights in the training stage. During the training phase, batch size, learning rate, learning decay, and dropout rate are set to 720, 1e ?3 , 0.95, 0.1, respectively. Note that learning decay is executed after the end of every epoch. We adopt the same strategy for BN momentum decay as in <ref type="bibr" target="#b15">[16]</ref> and use Adam Optimizer for all modules. Besides, we set the channel C to 600 and the ? in the loss function to 0.5 and train the model with 60 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first report quantitative and qualitative results of MTF-Transformer on three datasets. Then, we conduct ablation studies to verify the effectiveness of our design in all modules.</p><p>Considering the clarity and brevity of this section, we place some ablation studies on the hyper-parameters in the appendix part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate MTF-Transformer on three datasets, including: Human3.6M (H36M) <ref type="bibr" target="#b28">[29]</ref> is a large publicly available 3D human pose benchmark for both monocular and multi-view setups. It consists of 3.6 million image frames from 4 synchronized 50Hz digital cameras, and the corresponding 2D pose and 3D pose are captured by the MoCap system in a constrained indoor studio environment. Each actor performs 15 everyday activities such as walking, discussing, etc. Following previous works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b65">[65]</ref>, <ref type="bibr" target="#b66">[66]</ref>, we use 5 subjects (S1, S5, S6, S7, S8) for training and 2 subjects (S9, S11) for testing, and report MPJPE <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b42">[43]</ref> as the evaluation metric. We simulate an additional virtual view when training MTF-Transformer to enhance its flexibility. The 2D pose in the virtual view is synthesized via random rotation and projection, following Cheng et al. <ref type="bibr" target="#b61">[61]</ref>. To verify the effectiveness of the virtual view, we also report the result of MTF-Transformer trained with no added view.</p><p>TotalCapture <ref type="bibr" target="#b29">[30]</ref> is captured from 8 calibrated full HD video cameras recording at 60Hz. It features five subjects. Each subject performs four diverse performances 3 times, involving ROM,  <ref type="table" target="#tab_3">W2  FS3  A3  W2  FS3  A3  W2  FS3  A3  W2  FS3  A3</ref> Multi-view methods with camera parameters Qiu et al. <ref type="bibr" target="#b23">[24]</ref> 19.0 28.0 21.0 32.0 54.0 33.0 Walking, Acting, and Freestyle. Accurate 3D human joint locations are obtained from a marker-based motion capture system. Following previous work, the training set consists of "ROM1,2,3", "Walking1,3", "Freestyle1,2", "Acting1,2", on subjects 1,2, and 3. The test set consists of "Walking2 (W2)", "Freestyle3 (FS3)", and "Acting3 (A3)" on subjects 1, 2, 3, 4, and 5. The number following each action indicates the video from which the action is. For example, Freestyle has three videos of the same action, of which 1 and 2 are used for training and 3 for testing. Camera 1,3,5,7 is used in both the training and testing set, but camera 2,4,6,8 only appear in the testing set. That is to say. The testing set has some unseen camera configuration. KTH Multiview Football II <ref type="bibr" target="#b30">[31]</ref> consists of 8000+ images of professional footballers during a match in the Allsvenskan league. It is filmed by moving cameras and contains 14 joints(top-head, neck, shoulders, hips, knees, feet, elbows, and hands). To match the topology of H36M, we create the root (pelvis) by averaging the hips, the nose by averaging the neck and top-head, and the spine by averaging the root and the neck.</p><formula xml:id="formula_22">29.0 -- -- -- -- -- -- -- Remeli</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>We report the quantitative results of MTF-Transformer on Human3.6M and TotalCapture. MTF-Transformer and MTF-Transformer+ represent the vanilla MTF-Transformer and the MTF-Transformer utilizing camera parameters, respectively. In MTF-Transformer+, we directly use the transform matrix T ij calculated from rotate matrix R ij between 3D ground truth. Human3.6M: The quantitative results of MTF-Transformer and competitive methods are shown in TABLE 1. When CPN is used as a 2D pose detector, MTF-Transformer outperforms all the monocular methods, and it decreases the MPJPE by 1.3 when increasing the length of sequence from 1 to 27, indicating that multi-view and temporal information benefits for 3D pose estimation. When we employ Ground Truth as 2D pose input, both <ref type="bibr" target="#b45">[46]</ref> and MTF-Transformer obtain significant improvement, indicating that 2D pose plays an essential role in 2D-to-3D methods.</p><p>Compared to multi-view methods with camera calibration, MTF-Transformer is superior to <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b64">[64]</ref>, and <ref type="bibr" target="#b20">[21]</ref> but inferior to others. It shows that MTF-Transformer is competitive, but camera calibration still has an obvious advantage. Compared to Multiview methods without calibration, MTF-Transformer achieves the best performance and demonstrates its superiority. Considering the difficulty of calibrating the camera in real-time, MTF-Transformer is a satisfactory attempt. Besides, when we extend to MTF-Transformer+, we further improve the result. MTF-Transformer+ is inferior to some calibration-need methods. Considering that our focus is on fusing multi-view features without calibration and those superior methods utilize extra sensors or 2D image features, the performance of the MTF-Transformer+ is acceptable. TotalCapture: The quantitative results of MTF-Transformer and competitive methods are shown in TABLE 2. MTF-Transformer series are trained on camera 1, 3, 5, 7 of the training set, and tested on camera 1, 3, 5, 7 (seen) and camera 2, 4, 6, 8 (unseen) of the testing set. The testing set includes both seen subjects and unseen subjects in the training set. From the vertical comparison, MTF-Transformer+ outperforms <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b64">[64]</ref> with arbitrary 2D detector and the length of sequence, and MTF-Transformer has superior result over <ref type="bibr" target="#b45">[46]</ref>. Besides, the 2D detector has an influence on the result, and increasing the length of the sequence improves the performance. Moreover, MTF-Transformer with ResNet101 as 2D detector obtains better result than <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b64">[64]</ref> demonstrating the superiority of our method. From horizontal analysis, all the methods achieve better performance on seen cameras than on unseen cameras, on seen subjects than on unseen subjects. It means that generalization is an important issue for 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Evaluation</head><p>Some results of FLEX and MTF-Transformer on Human3.6M are shown in <ref type="figure">Fig. 6</ref>. Both FLEX and MTF-Transformer improve the prediction as the number of views increases, but MTF-Transformer has better results when the number of views is low. The reason is that MTF-Transformer uses CAA to reduce the influence of 2D  <ref type="figure">Fig. 7</ref>. It demonstrates that MTF-Transformer can generalize well from an indoor lab scene to the wild environment because it stands free from camera parameters and measures the implicit relationship between views adaptively. Although FLEX is also parameter-free, it aggregates the features from multiple viewpoints into one feature and then splits it into different viewpoints. The viewpoint awareness is twisted in the procedure by FLEX while MTF-Transformer keeps each viewpoint's independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we verify the effectiveness of all modules of MTF-Transformer on Human3.6M. We train all the models with 5 views (4 cameras and an additional synthesized view) and test them with different views unless otherwise stated. To eliminate the effect of the 2D detector, we take 2D detection from CPN <ref type="bibr" target="#b55">[55]</ref> as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Analysis on Confidence Attentive Aggregation</head><p>MTF-Transformer employs the Confidence Attentive Aggregation (CAA) module in Feature Extractor to reduce the impact of the unreliable 2D pose. We report the results of MTF-Transformer with and without CAA. Besides, we also evaluate the technique of concatenating the 2D pose and confidence values. As shown in TABLE 3, concatenating can improve the performance, compared  </p><formula xml:id="formula_23">Method MPJPE f g =f g + a g ? p g 2D 27.121 f g = F g res (a g ? p g 2D )</formula><p>27.499</p><formula xml:id="formula_24">f g = F g res f g + a g ?f g 27.121 f g = F g res f g + a g ? p g 2D 27.056</formula><p>with the circumstance without confidence. When CAA takes the place of concatenating, MTF-Transformer can achieve better performance at all the number of views. We also conduct experiments to verify the design of CAA. When we remove the res-blocks, the result of CAA is f g =f g + a g ? p g 2D ; When we remove the shortcut connection, the result of CAA is f g = F g res (a g ? p g 2D ); If we letf g takes the place of p g 2D , the result of CAA is f g = F g res f g + a g ?f g . As shown in TABLE 4, when we modulate the 2D pose and employ both shortcut and res-blocks, CAA achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Analysis on Multi-view Fusing Transformer</head><p>The Multi-view Fusing Transformer (MFT) measures the relationship between each pair of views and reconstructs the features according to the relationship. To validate the effectiveness of MFT, we compare its result with other multi-view fusing methods on Human3.6M, in the aspects of precision and generalization capability.</p><p>In the aspect of precision, we compare MFT with conventional transformer (removing absolution position encoding), point Transformer, and MFT without transform matrix T ij . To adapt Point Transformer to our task, we replace the 3D coordinates of the point cloud with the flattened 2D pose from the 2D detector, resulting in more parameters to deal with relative position encoding. Results in TABLE 5 demonstrate that MFT outperforms other methods in the vast majority of cases. Notably, the performance of Point Transformer is only slightly inferior to MFT, reflecting the effectiveness of relative position encoding. Besides, MFT and MFT w/o T ij have little difference in results when only utilizing 1 view, but MFT achieves better performance when more viewpoints participate in multi-view fusing. It means the transform matrix T ij plays a vital role in multi-view feature fusing. In addition, we display some results of MFT with and without T ij in <ref type="figure" target="#fig_5">Fig. 8</ref>. The MFT with T ij can predict a more accurate 3D pose than the MFT with T ij , especially the position of hand and foot. The reason is that T ij can transform the feature from the source view to the target view. Then the transformed feature is fused with an elementwisely product. Without T ij , the feature always lies in the source view, and the element-wise product is not effective enough for multi-view fusing.</p><p>In the aspect of generalization capability, we compare MFT with transformer and point Transformer. We also report the performance of MTF-Transformer without MFT. As MFT has the input and output of the same shape, removing MFT does not affect subsequent modules. We train these models on two views (camera 0, 2) and test them in an increasing number of views from seen cameras (0, 2) to unseen cameras <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref>. As shown in TABLE 6, the transformer achieves the best result when only 1 view is used. MFT gets superior performance as the view number increases from 2 to 4.</p><p>To further explain the utility of relative attention modules, we  also display the contribution of the feature from each view to the final prediction in <ref type="figure" target="#fig_6">Fig. 9</ref>, inspired by the Grad-CAM <ref type="bibr" target="#b67">[67]</ref>. There is a slight generalization gap when we train models on 4 views and test on the same number of views. For the transformer, the feature from the target view makes almost the majority contribution to the final prediction. Instead, MFT and point transformer get uniform contributions from all the views, indicating that relative position is essential to fuse multi-view information. When we train MFT and point transformer on 2 views and test them on 4 views, there is a big generalization gap between the training and testing phase. We can find that MFT fuses the feature from other views more effectively. It is intuitive and verifies the effectiveness of the proposed Relative-Attention block.</p><p>In MFT, we divide the dimension C of input x i into K groups. To explore the effect of K on the results, we train the model with different settings of D because C = D ? K and D determines the shape of T ij directly. The results in TABLE 7 demonstrate MTF-Transformer achieves the best performance at the dimension of 4 with different sequence lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Analysis on Random Block Mask</head><p>Random Block Mask is designed to ensure the generalization capability of the MTF-Transformer. To verify the effectiveness of the Random Block Mask, we train MTF-Transformer on Human3.6M training set with 5 views and set the mask rate M at 0, 0.2, 0.4, 0.6, 0.8, 1.0, respectively. With M increasing, more features from different views are dropped in the training stage. M = 0 indicates that all the views participate in the feature fusing among all the views. Each view only fuses with itself when M = 1. In the testing stage, we test the MTF-Transformer counterparts with different mask rates via feeding testing samples with a different number of views (including 1, 2, 3, and 4 views). The results are shown in TABLE 8. From the vertical comparison, at most mask rates, the performance of the MTF-Transformer gets better as the number of views increases, except for the mask rate of 1. When the mask rate is set at 1, the MFT module fails to measure the relationship among the features since all the interconnections are masked. It verifies that fusing multi-view features can improve the performance of 3D pose estimation. From horizontal analysis, when the number of views is set at 4, MTF-Transformer achieves the best performance at the mask rate of 0. This number of views in the testing stage is close to that of the training stage (5 views). As the number of views for testing decreases, the difference between training and testing is enlarged, and MTF-Transformer achieves the best performance at a higher mask rate. It demonstrates that the Random Block Mask module is essential for scenes greatly different from the training stage. The purpose of MFT-Transformer is to handle the input from an arbitrary number of views adaptively, so we evaluate the mean value of the MPJPE at different mask rates. We find that the mask rate of 0.4 has the best result, and we will set the mask rate at 0.4 in all the experiments. Some predict results are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Analysis on Sequence Length</head><p>MTF-Transformer can adaptively handle videos with different sequence lengths. We evaluate it via feeding videos with the length from 1 to 27. The results are shown in TABLE. 9. The performance of the MTF-Transformer increases as the sequence length increases, at any number of views as input. It inflects that a more extended period of input benefits the pose estimation. Interestingly, MTF-Transformer converges on certain precision as the sequence length number increases. The more views are involved, MTF-Transformer converges on better precision and tends to saturate more quickly. We utilize multi-view and temporal clues to estimate the pose of the middle frame under each viewpoint. Geometric and temporal information is complementary to each other. Thus, when more multi-view clues are used, MTF-Transformer needs less temporal information to reconstruct the  3D pose. Moreover, multi-view clues have some information that does not exist in temporal clues, so more viewpoints lead to better convergence results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Analysis on added synthesized views</head><p>When training Human3.6M, we added a synthesized view to train MTF-Transformer as a data enhancement mechanism, following cheng et al. <ref type="bibr" target="#b61">[61]</ref>. To quantificat the effect of added views, we compare the results with a different number of added views in TABLE 10. MTF-Transformer achieves the best performance when we add 1 synthesized view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.6">Analysis on computational complexity</head><p>As shown in TABLE 11, we report the total number of parameters and estimated multiply-add operations (MACs) per frame (the 2D detector is not included). For comparison, we also report parameters and MACs of Iskakov et al. <ref type="bibr" target="#b21">[22]</ref>. Similar to MTF-Transformer, Iskakov et al. <ref type="bibr" target="#b21">[22]</ref> also infers the 3D pose via lifting multi-view 2D detections to 3D detections. MTF-Transformer has a slightly less number of parameters and orders of magnitude less computational complexity. The reason is that MTF-Transformer employs 1D convolution to manipulate the features instead of 3D convolution. We also report the time consumption of MTF-Transformer in the training and testing phase in TABLE 12. MFT-Transformer is a magnitude faster than FLEX for inference time, tested in the same device. Besides, increasing the number of views leads to a slight time consumption increment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present a unified framework MTF-Transformer to fuse multiview sequences in uncalibrated scenes with an arbitrary number of views. MTF-Transformer can adaptively measure the relationship between each pair of views with a relative-attention mechanism, avoiding the dependency on camera calibration. It is also computationally lightweight and can be directly applied to settings where the number of views and video frames varies. Extensive experimental results demonstrate the effectiveness and robustness of the MTF-Transformer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of MTF-Transformer. It consists of three successive modules: Feature Extractor, Multi-view Fusing Transformer (MFT), and Temporal Fusing Transformer (TFT). Feature Extractor predicts 2D pose (P 2D and C 2D ) first and then encodes 2D pose into a feature vector for each frame. MFT measures the implicit relationship between each pair of views to reconstruct the feature adaptively. TFT aggregates the temporal information of the whole sequence and predicts the 3D pose of the center frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The feature embedding module encodes the 2D prediction into a feature vector. It splits the 2D prediction into five partitions and then uses five branches to extract features. Finally, the features of five partitions are concatenated and mapped to a global feature f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of Multi-view Fusing Transformer position encoding. This procedure results in more parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The architecture of Temporal Fusing Transformer. It predicts the 3D pose of the middle frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Results of FLEX and MTF-Transformer with different view numbers on the Human3.6M Demonstration of transfer FLEX and MTF-Transformer trained on Human3.6M to KTH Multiview Football II detector errors and introduces Relative-Attention to improve the fusion efficiency between multi-view features. To further verify the generalization of MTF-Transformer under different camera configurations, we test the model trained on Human3.6M on more challenging KTH Multiview Football II. Some results of generalization experiments are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Demonstration of MFT with and without T ij on Human3.6M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>The contribution of the 2D pose (17 joint point) from 4 views to the 3D prediction (X, Y, Z coordinate) with different kind of Relative-Attention module. We measure the gradient of the predicted 3D coordinate to the features of each views and consider the maximum value as the contribution ratio. For better visualization, the values are normalized to the range of 0 to 1. (a) Transformer (b) Point Transformer trained on 4 views (c) MFT trained on 4 views (d) Point Transformer trained on 2 views (e) MFT trained on 2 views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Some predictions under different mask rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Quantitative results on Human3.6M. MTF-Transformer and MTF-Transformer+ are trained with 27 frames, where T is the length of the sequence for testing. We employ CPN<ref type="bibr" target="#b55">[55]</ref> as the 2D detector, and * means no 2D detector. . Disc. Eat. Greet Phone Photo Pose Purch. Sit. SitD. Smoke Wait WalkD. Walk WalkT. AvgMonocular methodsPavllo et al.<ref type="bibr" target="#b15">[16]</ref> (CPN, T = 243) 45.2 46.7 43.3 45.6</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Dir48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell cols="2">57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Chen et al. [58]</cell><cell>(CPN, T = 1)</cell><cell>43.8 48.6 49.1 49.8</cell><cell>57.6</cell><cell>61.5</cell><cell>45.9</cell><cell>48.3</cell><cell cols="2">62.0 73.4</cell><cell>54.8</cell><cell>50.6</cell><cell>56.0</cell><cell>43.4</cell><cell>45.5</cell><cell>52.7</cell></row><row><cell>Liu et al. [59]</cell><cell cols="2">(CPN, T = 243) 41.8 44.8 41.1 44.9</cell><cell>47.4</cell><cell>54.1</cell><cell>43.4</cell><cell>42.2</cell><cell cols="2">56.2 63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell>Wang et al. [18]</cell><cell cols="2">(CPN, T = 96) 40.2 42.5 42.6 41.1</cell><cell>46.7</cell><cell>56.7</cell><cell>41.4</cell><cell>42.3</cell><cell cols="2">56.2 60.4</cell><cell>46.3</cell><cell>42.2</cell><cell>46.2</cell><cell>31.7</cell><cell>31.0</cell><cell>44.5</cell></row><row><cell>Zeng et al. [60]</cell><cell cols="2">(CPN, T = 243) 46.6 47.1 43.9 41.6</cell><cell>45.8</cell><cell>49.6</cell><cell>46.5</cell><cell>40.0</cell><cell cols="2">53.4 61.1</cell><cell>46.1</cell><cell>42.6</cell><cell>43.1</cell><cell>31.5</cell><cell>32.6</cell><cell>44.8</cell></row><row><cell>Cheng et al. [61]</cell><cell cols="2">(CPN, T = 128) 38.3 41.3 46.1 40.1</cell><cell>41.6</cell><cell>51.9</cell><cell>41.8</cell><cell>40.9</cell><cell cols="2">51.5 58.4</cell><cell>42.2</cell><cell>44.6</cell><cell>41.7</cell><cell>33.7</cell><cell>30.1</cell><cell>42.9</cell></row><row><cell></cell><cell></cell><cell cols="5">Multi-view methods with camera parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pavlakos et al. [62]</cell><cell>(*, T = 1)</cell><cell>41.2 49.2 42.8 43.4</cell><cell>55.6</cell><cell>46.9</cell><cell>40.3</cell><cell>63.7</cell><cell>97.6</cell><cell>119</cell><cell>52.1</cell><cell>42.7</cell><cell>51.9</cell><cell>41.8</cell><cell>39.4</cell><cell>56.9</cell></row><row><cell>Qiu et al. [24]</cell><cell>(*, T = 1)</cell><cell>24.0 26.7 23.2 24.3</cell><cell>24.8</cell><cell>22.8</cell><cell>24.1</cell><cell>28.6</cell><cell cols="2">32.1 26.9</cell><cell>31.0</cell><cell>25.6</cell><cell>25.0</cell><cell>28.0</cell><cell>24.4</cell><cell>26.2</cell></row><row><cell>Iskakov et al. [22]</cell><cell>(*, T = 1)</cell><cell>19.9 20.0 18.9 18.5</cell><cell>20.5</cell><cell>19.4</cell><cell>18.4</cell><cell>22.1</cell><cell cols="2">22.5 28.7</cell><cell>21.2</cell><cell>20.8</cell><cell>19.7</cell><cell>22.1</cell><cell>20.2</cell><cell>20.8</cell></row><row><cell>He et al.(IMU) [21]</cell><cell>(*, T = 1)</cell><cell>25.7 27.7 23.7 24.8</cell><cell>26.9</cell><cell>31.4</cell><cell>24.9</cell><cell>26.5</cell><cell cols="2">28.8 31.7</cell><cell>28.2</cell><cell>26.4</cell><cell>23.6</cell><cell>28.3</cell><cell>23.5</cell><cell>26.9</cell></row><row><cell>Zhang et al. [23]</cell><cell>(*, T = 1)</cell><cell>17.8 19.5 17.6 20.7</cell><cell>19.3</cell><cell>16.8</cell><cell>18.9</cell><cell>20.2</cell><cell cols="2">25.7 20.1</cell><cell>19.2</cell><cell>20.5</cell><cell>17.2</cell><cell>20.5</cell><cell>17.3</cell><cell>19.5</cell></row><row><cell>Zhang et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Quantitative results on TotalCapture. We employ HRNet-W32(D1), ResNet50(D2), ResNet101(D3) as 2D pose detector. MTF-Transformer and MTF-Transformer+ are trained with 27 frames, where T is the length of the sequence for testing.</figDesc><table><row><cell></cell><cell cols="2">Seen Cameras(1,3,5,7)</cell><cell></cell><cell cols="2">Unseen Cameras(2,4,6,8)</cell><cell></cell></row><row><cell>Methods</cell><cell>Seen Subjects(S1, S2, S3)</cell><cell>Unseen Subjects(S4, S5)</cell><cell>Mean</cell><cell>Subjects(S1, S2)</cell><cell>Unseen Subjects(S4, S5)</cell><cell>Mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Results of different procedures to fuse the 2D pose and the confidence from 2D detector on Human3.6M.</figDesc><table><row><cell></cell><cell>Sequence</cell><cell cols="4">Number of Views N</cell><cell>Parameters</cell></row><row><cell></cell><cell>length T</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>(M)</cell></row><row><cell>no confidence</cell><cell></cell><cell>52.3</cell><cell>36.8</cell><cell>31.6</cell><cell>29.4</cell><cell>9.8</cell></row><row><cell>concatenate</cell><cell>1</cell><cell>52.2</cell><cell>36.7</cell><cell>31.4</cell><cell>29.2</cell><cell>9.9</cell></row><row><cell>CAA</cell><cell></cell><cell>50.7</cell><cell>35.3</cell><cell>30.1</cell><cell>28.0</cell><cell>10.1</cell></row><row><cell>no confidence</cell><cell></cell><cell>51.3</cell><cell>35.9</cell><cell>30.8</cell><cell>28.7</cell><cell>9.8</cell></row><row><cell>concatenate</cell><cell>3</cell><cell>51.3</cell><cell>35.9</cell><cell>30.7</cell><cell>28.6</cell><cell>9.9</cell></row><row><cell>CAA</cell><cell></cell><cell>49.8</cell><cell>34.5</cell><cell>29.4</cell><cell>27.3</cell><cell>10.1</cell></row><row><cell>no confidence</cell><cell></cell><cell>51.0</cell><cell>35.6</cell><cell>30.6</cell><cell>28.5</cell><cell>9.8</cell></row><row><cell>concatenate</cell><cell>5</cell><cell>50.9</cell><cell>35.6</cell><cell>30.4</cell><cell>28.3</cell><cell>9.9</cell></row><row><cell>CAA</cell><cell></cell><cell>49.4</cell><cell>34.2</cell><cell>29.2</cell><cell>27.1</cell><cell>10.1</cell></row><row><cell>no confidence</cell><cell></cell><cell>50.7</cell><cell>35.4</cell><cell>30.5</cell><cell>28.4</cell><cell>9.8</cell></row><row><cell>concatenate</cell><cell>7</cell><cell>50.8</cell><cell>35.4</cell><cell>30.3</cell><cell>28.2</cell><cell>9.9</cell></row><row><cell>CAA</cell><cell></cell><cell>49.2</cell><cell>34.1</cell><cell>29.1</cell><cell>27.1</cell><cell>10.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>The results of different design of CAA</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Results of different relative attention modules on Human3.6M. T is the length of sequence</figDesc><table><row><cell>Method</cell><cell>T</cell><cell>1</cell><cell cols="4">Number of Views N 2 3</cell><cell>4</cell><cell>Parameters (M)</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">50.8</cell><cell>38.7</cell><cell>34.0</cell><cell cols="2">31.9</cell><cell>10.4</cell></row><row><cell>point transformer MFT w/o T ij</cell><cell>1</cell><cell cols="2">51.2 50.6</cell><cell>36.1 37.8</cell><cell>30.8 32.8</cell><cell cols="2">28.4 30.5</cell><cell>11.7 9.7</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">50.7</cell><cell>35.3</cell><cell>30.1</cell><cell cols="2">28.0</cell><cell>10.1</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">49.9</cell><cell>37.8</cell><cell>33.1</cell><cell cols="2">31.0</cell><cell>10.4</cell></row><row><cell>point transformer MFT w/o T ij</cell><cell>3</cell><cell cols="2">50.2 49.8</cell><cell>35.2 37.0</cell><cell>30.1 32.0</cell><cell cols="2">27.9 29.7</cell><cell>11.7 9.7</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">49.8</cell><cell>34.5</cell><cell>29.4</cell><cell cols="2">27.3</cell><cell>10.1</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">49.6</cell><cell>37.5</cell><cell>32.8</cell><cell cols="2">30.7</cell><cell>10.4</cell></row><row><cell>point transformer MFT w/o T ij</cell><cell>5</cell><cell cols="2">49.9 49.5</cell><cell>35.0 36.6</cell><cell>29.9 31.6</cell><cell cols="2">27.7 29.4</cell><cell>11.7 9.7</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">49.4</cell><cell>34.2</cell><cell>29.2</cell><cell cols="2">27.1</cell><cell>10.1</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">49.4</cell><cell>37.3</cell><cell>32.6</cell><cell cols="2">30.6</cell><cell>10.4</cell></row><row><cell>point transformer MFT w/o T ij</cell><cell>7</cell><cell cols="2">49.7 49.3</cell><cell>34.8 36.4</cell><cell>29.8 31.5</cell><cell cols="2">27.6 29.3</cell><cell>11.7 9.7</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">49.2</cell><cell>34.1</cell><cell>29.1</cell><cell cols="2">27.1</cell><cell>10.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 6</cell><cell></cell><cell></cell></row><row><cell cols="8">Generalization capability of different relative attention modules. We</cell></row><row><cell cols="8">train all the models on Human3.6M with 2 views, test them with</cell></row><row><cell></cell><cell cols="5">different number of views.</cell><cell></cell></row><row><cell>Method</cell><cell>T</cell><cell>1</cell><cell cols="3">Number of views N 2 3</cell><cell></cell><cell>4</cell><cell>Parameters (M)</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">55.9</cell><cell>47.1</cell><cell>43.6</cell><cell cols="2">41.2</cell><cell>10.4</cell></row><row><cell>point transformer no MFT</cell><cell>1</cell><cell cols="2">58.0</cell><cell cols="2">51.8 57.0 49.9</cell><cell cols="2">48.7</cell><cell>11.7 7.5</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">56.4</cell><cell>46.1</cell><cell>41.5</cell><cell cols="2">39.0</cell><cell>10.1</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">55.1</cell><cell>46.3</cell><cell>42.7</cell><cell cols="2">40.2</cell><cell>10.4</cell></row><row><cell>point transformer</cell><cell></cell><cell cols="2">57.2</cell><cell>51.2</cell><cell>49.4</cell><cell cols="2">48.3</cell><cell>11.7</cell></row><row><cell>no MFT</cell><cell>3</cell><cell></cell><cell></cell><cell cols="2">56.2</cell><cell></cell><cell>7.5</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">55.7</cell><cell>45.4</cell><cell>40.9</cell><cell cols="2">38.4</cell><cell>10.1</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">54.9</cell><cell>46.0</cell><cell>42.4</cell><cell cols="2">40.0</cell><cell>10.4</cell></row><row><cell>point transformer</cell><cell></cell><cell cols="2">57.0</cell><cell>50.9</cell><cell>49.2</cell><cell cols="2">48.2</cell><cell>11.7</cell></row><row><cell>no MFT</cell><cell>5</cell><cell></cell><cell></cell><cell cols="2">56.0</cell><cell></cell><cell>7.5</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">55.4</cell><cell>45.1</cell><cell>40.6</cell><cell cols="2">38.2</cell><cell>10.1</cell></row><row><cell>transformer</cell><cell></cell><cell cols="2">54.7</cell><cell>45.9</cell><cell>42.3</cell><cell cols="2">39.8</cell><cell>10.4</cell></row><row><cell>point transformer</cell><cell></cell><cell cols="2">56.8</cell><cell>50.8</cell><cell>49.1</cell><cell cols="2">48.0</cell><cell>11.7</cell></row><row><cell>no MFT</cell><cell>7</cell><cell></cell><cell></cell><cell cols="2">55.7</cell><cell></cell><cell>7.5</cell></row><row><cell>MFT</cell><cell></cell><cell cols="2">55.2</cell><cell>45.0</cell><cell>40.5</cell><cell cols="2">38.0</cell><cell>10.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Results on Human3.6M with different setting of D. T is the sequence length.</figDesc><table><row><cell>D</cell><cell>T</cell><cell>1</cell><cell cols="3">Number of views N 2 3</cell><cell>4</cell><cell>Parameter (M)</cell></row><row><cell>1</cell><cell></cell><cell cols="2">51.8</cell><cell>37.0</cell><cell>31.7</cell><cell>29.6</cell><cell>10.074</cell></row><row><cell>2</cell><cell></cell><cell cols="2">51.3</cell><cell>35.9</cell><cell>30.6</cell><cell>28.4</cell><cell>10.076</cell></row><row><cell>3 4</cell><cell>1</cell><cell cols="2">51.2 50.7</cell><cell>35.9 35.3</cell><cell>30.7 30.1</cell><cell>28.5 28.0</cell><cell>10.079 10.083</cell></row><row><cell>5</cell><cell></cell><cell cols="2">50.9</cell><cell>35.8</cell><cell>30.6</cell><cell>28.4</cell><cell>10.088</cell></row><row><cell>6</cell><cell></cell><cell cols="2">51.3</cell><cell>35.5</cell><cell>30.3</cell><cell>28.1</cell><cell>10.095</cell></row><row><cell>1</cell><cell></cell><cell cols="2">50.8</cell><cell>36.0</cell><cell>30.8</cell><cell>28.8</cell><cell>10.074</cell></row><row><cell>2</cell><cell></cell><cell cols="2">50.4</cell><cell>35.1</cell><cell>29.9</cell><cell>27.8</cell><cell>10.076</cell></row><row><cell>3 4</cell><cell>3</cell><cell cols="2">50.3 49.8</cell><cell>35.1 34.5</cell><cell>29.9 29.4</cell><cell>27.8 27.3</cell><cell>10.079 10.083</cell></row><row><cell>5</cell><cell></cell><cell cols="2">50.1</cell><cell>35.0</cell><cell>29.9</cell><cell>27.8</cell><cell>10.088</cell></row><row><cell>6</cell><cell></cell><cell cols="2">50.4</cell><cell>34.7</cell><cell>29.6</cell><cell>27.5</cell><cell>10.095</cell></row><row><cell>1</cell><cell></cell><cell cols="2">50.5</cell><cell>35.6</cell><cell>30.5</cell><cell>28.6</cell><cell>10.074</cell></row><row><cell>2</cell><cell></cell><cell>50</cell><cell></cell><cell>34.8</cell><cell>29.7</cell><cell>27.6</cell><cell>10.076</cell></row><row><cell>3 4</cell><cell>5</cell><cell cols="2">49.9 49.4</cell><cell>34.7 34.2</cell><cell>29.6 29.2</cell><cell>27.5 27.1</cell><cell>10.079 10.083</cell></row><row><cell>5</cell><cell></cell><cell cols="2">49.7</cell><cell>34.7</cell><cell>29.7</cell><cell>27.6</cell><cell>10.088</cell></row><row><cell>6</cell><cell></cell><cell>50</cell><cell></cell><cell>34.4</cell><cell>29.4</cell><cell>27.3</cell><cell>10.095</cell></row><row><cell>1</cell><cell></cell><cell cols="2">50.4</cell><cell>35.5</cell><cell>30.4</cell><cell>28.5</cell><cell>10.074</cell></row><row><cell>2</cell><cell></cell><cell cols="2">49.8</cell><cell>34.6</cell><cell>29.5</cell><cell>27.5</cell><cell>10.076</cell></row><row><cell>3 4</cell><cell>7</cell><cell cols="2">49.7 49.2</cell><cell>34.6 34.1</cell><cell>29.5 29.1</cell><cell>27.5 27.1</cell><cell>10.079 10.083</cell></row><row><cell>5</cell><cell></cell><cell cols="2">49.5</cell><cell>34.6</cell><cell>29.6</cell><cell>27.5</cell><cell>10.088</cell></row><row><cell>6</cell><cell></cell><cell cols="2">49.8</cell><cell>34.2</cell><cell>29.3</cell><cell>27.2</cell><cell>10.095</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Results of different mask rate M on Human3.6M. MTF-Transformer is trained on the training set with 5 views at different mask rate. We evaluate these models with different number of views as input.</figDesc><table><row><cell>Mask rate M</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell cols="6">1 205.2 52.2 49.2 49.2 48.8 49.4</cell></row><row><cell>Number of views</cell><cell cols="6">2 78.7 35.3 34.1 34.7 36.1 117.5</cell></row><row><cell>N</cell><cell cols="6">3 44.5 29.3 29.1 30.1 31.7 127.5</cell></row><row><cell></cell><cell cols="6">4 25.6 26.8 27.1 28.2 30.0 134.5</cell></row><row><cell>Mean</cell><cell cols="6">89.0 35.9 34.9 35.5 36.7 107.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Results of different sequence length T on Human3.6M.</figDesc><table><row><cell>Sequence length</cell><cell></cell><cell cols="2">Number of views N</cell><cell></cell></row><row><cell>T</cell><cell>1 view</cell><cell>2 views</cell><cell>3 views</cell><cell>4 views</cell></row><row><cell>1</cell><cell>51.62</cell><cell>35.13</cell><cell>29.74</cell><cell>27.46</cell></row><row><cell>3</cell><cell>50.70</cell><cell>34.28</cell><cell>28.99</cell><cell>26.77</cell></row><row><cell>5</cell><cell>50.28</cell><cell>33.91</cell><cell>28.69</cell><cell>26.51</cell></row><row><cell>7</cell><cell>50.01</cell><cell>33.71</cell><cell>28.55</cell><cell>26.41</cell></row><row><cell>9</cell><cell>49.82</cell><cell>33.57</cell><cell>28.44</cell><cell>26.32</cell></row><row><cell>11</cell><cell>49.66</cell><cell>33.47</cell><cell>28.38</cell><cell>26.28</cell></row><row><cell>13</cell><cell>49.53</cell><cell>33.42</cell><cell>28.35</cell><cell>26.27</cell></row><row><cell>15</cell><cell>49.45</cell><cell>33.39</cell><cell>28.35</cell><cell>26.27</cell></row><row><cell>17</cell><cell>49.39</cell><cell>33.37</cell><cell>28.34</cell><cell>26.28</cell></row><row><cell>19</cell><cell>49.33</cell><cell>33.34</cell><cell>28.33</cell><cell>26.27</cell></row><row><cell>21</cell><cell>49.27</cell><cell>33.32</cell><cell>28.32</cell><cell>26.26</cell></row><row><cell>23</cell><cell>49.23</cell><cell>33.29</cell><cell>28.30</cell><cell>26.24</cell></row><row><cell>25</cell><cell>49.21</cell><cell>33.27</cell><cell>28.28</cell><cell>26.22</cell></row><row><cell>27</cell><cell>49.19</cell><cell>33.26</cell><cell>28.27</cell><cell>26.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10</head><label>10</label><figDesc>Results on Human3.6M with different number of added view.</figDesc><table><row><cell>T</cell><cell>Added view</cell><cell cols="4">The number of views N 1 2 3 4</cell><cell>Parameters (M)</cell></row><row><cell></cell><cell>0</cell><cell>51.0</cell><cell>35.7</cell><cell>30.6</cell><cell>28.4</cell></row><row><cell>1</cell><cell>1</cell><cell>50.7</cell><cell>35.3</cell><cell>30.1</cell><cell>28.0</cell></row><row><cell></cell><cell>2</cell><cell>51.4</cell><cell>35.8</cell><cell>30.4</cell><cell>28.1</cell></row><row><cell></cell><cell>0</cell><cell>50.2</cell><cell>35.0</cell><cell>30.0</cell><cell>27.9</cell></row><row><cell>3</cell><cell>1</cell><cell>49.8</cell><cell>34.5</cell><cell>29.4</cell><cell>27.3</cell></row><row><cell></cell><cell>2 0</cell><cell>50.5 49.9</cell><cell>35.0 34.7</cell><cell>29.7 29.7</cell><cell>27.5 27.7</cell><cell>10.1</cell></row><row><cell>5</cell><cell>1</cell><cell>49.4</cell><cell>34.2</cell><cell>29.2</cell><cell>27.1</cell></row><row><cell></cell><cell>2</cell><cell>50.1</cell><cell>34.8</cell><cell>29.5</cell><cell>27.2</cell></row><row><cell></cell><cell>0</cell><cell>49.7</cell><cell>34.6</cell><cell>29.6</cell><cell>27.6</cell></row><row><cell>7</cell><cell>1</cell><cell>49.2</cell><cell>34.1</cell><cell>29.1</cell><cell>27.1</cell></row><row><cell></cell><cell>2</cell><cell>49.9</cell><cell>34.6</cell><cell>29.3</cell><cell>27.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 11</head><label>11</label><figDesc>Computational complexity of MTF-Transformer. We use THOP 1 to represent the number of parameters and MACs (multiply-add operations). T is the length of sequence and N is the number of views.</figDesc><table><row><cell>Method</cell><cell>T</cell><cell>N</cell><cell>Parameters(M)</cell><cell>MACs(G)</cell></row><row><cell>Iskakov et al. [22]</cell><cell>1</cell><cell>4</cell><cell>11.9</cell><cell>155</cell></row><row><cell>FLEX [46]</cell><cell>27</cell><cell>4</cell><cell>70.6</cell><cell>4.27</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>0.01</cell></row><row><cell></cell><cell>1</cell><cell>2 3</cell><cell>10.1</cell><cell>0.03 0.05</cell></row><row><cell>MTF-Transformer</cell><cell></cell><cell>4 1</cell><cell></cell><cell>0.07 0.27</cell></row><row><cell></cell><cell>27</cell><cell>2 3</cell><cell>10.1</cell><cell>0.68 1.23</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell></cell><cell>1.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 12</head><label>12</label><figDesc>The time consumption in training and testing phase.T is the length of sequence and N is the number of views.</figDesc><table><row><cell>Methods</cell><cell>Device</cell><cell>T</cell><cell>N</cell><cell>time</cell></row><row><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MTF-Transformer</cell><cell>2?2080Ti</cell><cell>7 27</cell><cell>--</cell><cell>12h(60epochs) 34h(60epochs)</cell></row><row><cell></cell><cell>Testing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FLEX [46]</cell><cell></cell><cell>27</cell><cell>4</cell><cell>30.2ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>8.4ms</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2 3</cell><cell>8.6ms 8.6ms</cell></row><row><cell>MTF-Transformer</cell><cell>1?2080Ti</cell><cell></cell><cell>4 1</cell><cell>8.8ms 9.2ms</cell></row><row><cell></cell><cell></cell><cell>27</cell><cell>2 3</cell><cell>9.3ms 9.5ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>9.9ms</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Natural Science Foundation of China (61825601, U21B2044), and Science and Technology Program of Jiangsu Province (BK20192004B).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining 3d key-pose-motifs for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for real-time 3d human pose estimation and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2752" to="2764" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A human-robot interaction interface for mobile and stationary robots based on real-time 3d human body and handfinger pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehlers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Trajectory planning of upper limb rehabilitation robot based on human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 17th International Conference on Ubiquitous Robots (UR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="333" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human shape and pose from dense body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust 3d human pose estimation from single images or video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1227" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Locally connected network for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1429" to="1442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11536</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">445</biblScope>
			<biblScope unit="page" from="11" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adafuse: Adaptive multiview fusion for accurate human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision<address><addrLine>1, 2, 6, 7</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transfusion: Cross-view fusion with transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepfuse: An imu-aware network for real-time 3d human pose estimation from multi-view image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout from higher education: A theoretical synthesis of recent research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of educational research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="125" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-view body part recognition with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 24th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2013-09-09" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>British Machine Vision Association</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-toend recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="523" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-dynamic hypergraph neural network for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="782" to="788" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motionet: 3d human motion reconstruction from monocular video with skeleton consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Limb pose aware networks for monocular 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on computer vision and Pattern recognition</title>
		<meeting>the IEEE/CVF Conference on computer vision and Pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Trajectory space factorization for deep videobased 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiahao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Gim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A generalizable approach for multi-view 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Flex: Parameter-free multi-view 3d human motion reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Azov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01937</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Posegtac: Graph transformer encoder-decoder with atrous convolution for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">International Joint Conferences on Artificial Intelligence Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/188</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2021/1883" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1359" to="1365" />
		</imprint>
	</monogr>
	<note>main Track</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Liftformer: 3d human pose estimation using attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Llopart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00348,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Mhformer: Multihypothesis transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12707</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploiting temporal contexts with strided transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">945</biblScope>
			<biblScope unit="page" from="11" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural human performer: Learning generalizable radiance fields for human performance rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="16" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Towards efficient scene understanding via squeeze reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="7050" to="7063" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9887" to="9895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fusing wearable imus with multi-view images for human pose estimation: A geometric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2200" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Lightweight multiview 3d pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
