<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale Matching Networks for Semantic Correspondence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Nebula AI Group</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Nebula AI Group</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Nebula AI Group</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangming</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Nebula AI Group</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-scale Matching Networks for Semantic Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Target (b) Source (f) Ours (g) Ground Truth (e) DHPF (c) ANC-Net (d) SCOT Figure 1. Dense correspondence prediction produced by state-of-the-art algorithms, including ANC-Net [26], SCOT [30], DHPF [35] and our multi-scale matching network. With the predicted key point pairs, images are warped with thin-plate splines algorithm [3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Deep features have been proven powerful in building accurate dense semantic correspondences in various previous works. However, the multi-scale and pyramidal hierarchy of convolutional neural networks has not been well studied to learn discriminative pixel-level features for semantic correspondence. In this paper, we propose a multiscale matching network that is sensitive to tiny semantic differences between neighboring pixels. We follow the coarse-to-fine matching strategy and build a top-down feature and matching enhancement scheme that is coupled with the multi-scale hierarchy of deep convolutional neural networks. During feature enhancement, intra-scale enhancement fuses same-resolution feature maps from multiple layers together via local self-attention and cross-scale enhancement hallucinates higher-resolution feature maps along the top-down pathway. Besides, we learn complementary matching details at different scales thus the overall matching score is refined by features of different semantic levels gradually. Our multi-scale matching network can be trained end-to-end easily with few additional *Corresponding author: wfge@fudan.edu.cn learnable parameters. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on three popular benchmarks with high computational efficiency. The code has been released at https: //github.com/wintersun661/MMNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) Target (b) Source (f) Ours (g) Ground Truth (e) DHPF (c) ANC-Net (d) SCOT <ref type="figure" target="#fig_1">Figure 1</ref>. Dense correspondence prediction produced by state-of-the-art algorithms, including ANC-Net <ref type="bibr" target="#b25">[26]</ref>, SCOT <ref type="bibr" target="#b29">[30]</ref>, DHPF <ref type="bibr" target="#b35">[35]</ref> and our multi-scale matching network. With the predicted key point pairs, images are warped with thin-plate splines algorithm <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Deep features have been proven powerful in building accurate dense semantic correspondences in various previous works. However, the multi-scale and pyramidal hierarchy of convolutional neural networks has not been well studied to learn discriminative pixel-level features for semantic correspondence. In this paper, we propose a multiscale matching network that is sensitive to tiny semantic differences between neighboring pixels. We follow the coarse-to-fine matching strategy and build a top-down feature and matching enhancement scheme that is coupled with the multi-scale hierarchy of deep convolutional neural networks. During feature enhancement, intra-scale enhancement fuses same-resolution feature maps from multiple layers together via local self-attention and cross-scale enhancement hallucinates higher-resolution feature maps along the top-down pathway. Besides, we learn complementary matching details at different scales thus the overall matching score is refined by features of different semantic levels gradually. Our multi-scale matching network can be trained end-to-end easily with few additional *Corresponding author: wfge@fudan.edu.cn learnable parameters. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on three popular benchmarks with high computational efficiency. The code has been released at https: //github.com/wintersun661/MMNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finding pixel-wise correspondences between a pair of semantically similar images has been a longstanding fundamental problem in computer vision. They have been proven useful for many tasks including optical flow <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46]</ref>, geometric matching <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">50]</ref>, disparity estimation <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b61">60]</ref>, object recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b59">58]</ref>, semantic segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> and etc. Due to large intra-class variations in color, scale, orientation, illumination and non-rigid deformations, the problem of semantic correspondence remains very challenging. With the breakthrough in representation learning, semantic correspondence has achieved impressive improvements in various scenarios.</p><p>Despite that deep features have improved matching accuracy significantly, the multi-scale and hierarchical structures of deep convolutional neural networks have not been explored thoroughly for semantic correspondence. It is well-known that convolutional neural networks can extract features of different semantic levels in a bottom-up manner <ref type="bibr" target="#b58">[57]</ref>. Bottom convolutional layers close to the input image act like low level feature descriptors, and are sensitive to colors, edges, textures and other low level statistics. Top convolutional layers contain high level semantics which are similar among neighboring points in feature maps. Methods like NC-Net <ref type="bibr" target="#b40">[40]</ref>, DualRC-Net <ref type="bibr" target="#b26">[27]</ref> and GOCor <ref type="bibr" target="#b49">[49]</ref> use the features from the topmost layer as the feature representation. However, in semantic correspondence, the ambiguity between neighboring pixels in the topmost layer leads to inferior performance. Hyperpixel flow <ref type="bibr" target="#b20">[21]</ref> and its extension <ref type="bibr" target="#b35">[35]</ref> combine features at different semantic levels to generate reliable feature representation and achieve improved results. However, they have not thoroughly exploited the multi-scale and hierarchical structure of deep convolutional neural networks.</p><p>Given an image pair, human usually tends to glance at the whole images, and then compare details carefully to establish semantic correspondence. It is similar to a coarse-tofine matching scheme. In a convolutional neural network, neurons in the top layers have larger receptive fields while neurons in the bottom layers have relatively small receptive fields, which means top layers are rich in semantics but have relatively weak localization ability while the bottom layers are strong in localization but have less semantics. It will be helpful to follow the multi-scale and hierarchical structure of convolutional neural networks to find semantic correspondence in a coarse-to-fine manner.</p><p>In this paper, we propose a new multi-scale matching network to produce reliable semantic correspondence by integrating features of different semantic levels hierarchically and learn complementary matching details in a coarse-tofine manner. The multi-scale matching network consists of an encoder and a decoder. The encoder is a typical convolutional neural network pretrained on the ImageNet ILSVRC dataset <ref type="bibr" target="#b43">[43]</ref>. It contains many layers to capture semantic information at different levels. We divide the feature maps in the encoder into five convolutional groups with respect to their resolutions. The decoder has two top-down hierarchical enhancement pathways across different scales. The first one is the feature enhancement pathway which upsamples spatially coarser, but semantically stronger feature maps and fuse them with features from lateral connections to hallucinate higher resolution features. The second one is the matching enhancement pathway that learns finer and complementary matching details to enhance coarser matching results from a lower resolution. We start from the first layer in the decoder to generate the coarsest matching results, and upsample and enhance them with complementary matching details at different semantic levels.</p><p>To increase fine-grained details in feature maps, during intra-scale feature enhancement, we fuse all feature maps from the same convolutional group in the encoder not just the feature map of the last layer. We also design a transformer with a local self-attention mechanism to enhance features that are discriminative among neighboring pixels. Besides, we supervise matching detail learning at different scales to make sure the network learns reliable semantic correspondences. Our multi-scale matching network adds relatively few learnable parameters with little extra computational cost, and can be trained in an end-to-end manner easily.</p><p>In summary, the main contributions of this work can be summarized as follows:</p><p>? We propose a multi-scale matching network that utilizes the multi-scale and hierarchical structure of deep convolutional neural network to learn semantic correspondences in a coarse-to-fine manner. Two top-down pathways in the decoder are built to couple the backbone encoder. The feature enhancement pathway increases the representation power of feature maps with intra-scale enhancement and cross-scale enhancement. The matching enhancement pathway learns matching details that are complementary to matching results from coarser levels.</p><p>? We design a novel intra-scale feature enhancement module that simultaneously fuses all the feature maps in each convolutional group and further increases the discriminative ability of the fused feature map with a local transformer.</p><p>? Experimental results demonstrate that our multi-scale matching network achieves state-of-the-art performance on multiple popular benchmarks, including PF-PASCAL <ref type="bibr" target="#b10">[11]</ref>, CUB <ref type="bibr" target="#b54">[53]</ref> and SPair-71k <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Correspondence. Methods for semantic correspondence can be roughly categorized into several groups: handcrafted feature based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">48]</ref>, learnable feature based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b49">49]</ref>, graph matching and optimization based methods <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b57">56]</ref>, methods focusing on geometry displacement <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b50">50]</ref>, and etc. Hand crafted features, such SIFT <ref type="bibr" target="#b30">[31]</ref>, HOG <ref type="bibr" target="#b47">[47]</ref> and DAISY <ref type="bibr" target="#b48">[48]</ref>, design robust feature descriptors with low level statistics. In NC-Net <ref type="bibr" target="#b40">[40]</ref>, DualRC-Net <ref type="bibr" target="#b26">[27]</ref> and GO-Cor <ref type="bibr" target="#b49">[49]</ref>, high level semantic features of convolutional neural networks are used to build dense correspondences beween image pairs. SCOT <ref type="bibr" target="#b29">[30]</ref> and DeepEMD <ref type="bibr" target="#b59">[58]</ref> formulate the semantic correspondence as an optimal transport problem and give closed-form solutions. PCA-GM <ref type="bibr" target="#b52">[52]</ref> and other graph matching based methods focus on solving a general quadratic assignment programming (QAP) problem to get matching results. Besides, PHM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref> and SCNet <ref type="bibr" target="#b11">[12]</ref> develop the probabilistic Hough matching in a Bayesian probability framework to model the geometry displacement of objects between two images. In this paper, we learn semantic correspondence in a coarse-to-fine manner with a top-down matching enhancement scheme. By such hierarchical matching scheme, semantics at different levels and scales are fused and enhanced to get accurate pixel-wise correspondences. Multi-scale Feature Fusion. Multi-scale feature fusion can improve the representation ability of features in many tasks, including object detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">61]</ref>, semantic segmentation <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b28">29]</ref> and semantic correspondence <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">35]</ref>. Feature pyramid networks (FPN <ref type="bibr" target="#b27">[28]</ref>) build a decoder with a top-down pathway and lateral connections, and achieve impressive results on object detection. Hyperpxiel flow (HPF <ref type="bibr" target="#b32">[33]</ref>) searches the most informative convolutional feature to get superior results on semantic correspondence. Its extension Hypercolumns <ref type="bibr" target="#b35">[35]</ref> designs a learning algorithm to select convolutional features in different layers in a much more efficient learning scheme. Different from FPN <ref type="bibr" target="#b27">[28]</ref>, in the top-down pathway of our multi-scale matching network, feature maps in every layer of a convolutional group are fused to generate the lateral connection, not just the output of the last layer. Compared with HPF <ref type="bibr" target="#b32">[33]</ref> and Hypercolumns <ref type="bibr" target="#b35">[35]</ref>, our multiscale learning scheme is much more flexible with simple top-down and lateral connections, and can benefit from the multi-scale and pyramid hierarchy more efficiently.. 4-D Correlation. The 4-D correlation between two feature volumes is popular in semantic correspondence learning which calculates the matching scores densely. NC-Net <ref type="bibr" target="#b40">[40]</ref> analyzes neighborhood consensus patterns in the 4-D correlation space to get reliable dense correspondences. ANC-Net <ref type="bibr" target="#b25">[26]</ref> introduces a set of non-isotropic 4-D convolution layers to capture adaptive neighborhood consensus. In this paper, we don't normalize the feature maps with L 2 nor-malization as that in NC-Net <ref type="bibr" target="#b40">[40]</ref> and ANC-Net <ref type="bibr" target="#b25">[26]</ref>. We simply use the 4-D correlation tensor of two feature maps which stores the pair-wise scalar products as the matching score tensor. Then this 4-D correlation tensor is normalized with softmax to get the matching probability of every feature point. Experiments demonstrate that our proposed method without L 2 normalization can achieve impressive results. Transformer. Transformers have led to a series of breakthroughs in computer vision <ref type="bibr" target="#b63">[62,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b60">59]</ref> and natural language processing <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1]</ref>. In <ref type="bibr" target="#b51">[51]</ref>, elements in a sequence are encoded with a self-attention mechanism uniformly. While, in our local self attention, only neighborhood pixels are considered to enhancement the local patterns. Experimental results show that this local property in our local self attention works quite well for semantic correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-scale Matching Networks</head><p>Multi-scale matching networks utilize the multi-scale and hierarchical structures of deep convolutional neural networks to get discriminative pixel-level semantics for semantic correspondence. <ref type="figure" target="#fig_0">Figure 2</ref> gives an overview of the proposed method. Given a pair of images I s , I t and the ground truth of their matched key points M gt = {m i = (p s i , p t i ) |i = 1, ..., K}, our multi-scale matching network adopts a top-down feature enhancement scheme and a coarse-to-fine matching enhancement scheme respectively. In the feature enhancement, we design a local self attention which models dependencies between neighborhood neurons to reduce the semantic ambiguity. In the matching enhancement, we learn the matching details that are complementary at different semantic levels. Besides, different from that in <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b25">26]</ref>, feature volumes of two images are multiplied together directly without L 2 normalization to get 4-D matching scores. This scheme ensures the pixellevel similarity can be learnt with deep neural networks efficiently. Then we can train the multi-scale matching network by adding supervision at different scales in an end-to-end manner.</p><p>Network Structure. A typical convolutional neural network for image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> usually has five convolutional groups with different resolutions. Here we adopt ResNet <ref type="bibr" target="#b13">[14]</ref> as our backbone and follow the feature pyramid network (FPN <ref type="bibr" target="#b27">[28]</ref>) to broadcast semantics from high level layers to low level layers with a top-down pathway and lateral connections as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (a). Then in the decoding part, we get 4 feature maps with successive increasing resolutions by a scaling factor 2.</p><p>Feature Enhancement. There are feature enhancements both in the same scale and across different scales. During intra-scale enhancement, unlike FPN <ref type="bibr" target="#b27">[28]</ref> which only takes the feature map of the last residual block in each convolutional group, we simultaneously fuse the feature maps of all residual blocks in the same group to capture semantics at different levels. To enlarge the receptive field of a single neuron and capture semantics at different scales, every feature map passes a scale enhancement module (SEM <ref type="bibr" target="#b12">[13]</ref>) before fusion. Then these feature maps are simply added together and go into a local self attention module to finish intra-scale enhancement.</p><p>The local self attention module is designed to specialize every feature point. As common transformers <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b60">59]</ref>, our local self attention operates on queries(Q), keys(K) and values(V) with an input feature map X ? R C?H?W , and output a transformed version X with the same shape as X. For every location in X, we select its r ? r neighborhood to conduct the self-attention. Feature vectors of the neighborhood locations in X are collected, and then we get a neighborhood feature tensor X ? ? R C?H?W ?r?r . We send the input feature map X into the query transformation function F q , and send the neighborhood feature tensor X ? into the key and value transformation functions F k /F v respectively. The query/key/value transformation functions are implemented with 1?1 convolutions followed by ReLU activations respectively. The local interaction of our transformer is written as,</p><formula xml:id="formula_0">X i = X i + G F v X ? i ? F q (X i ) T F k X ? i T ,<label>(1)</label></formula><p>where X i is the feature vector of the i th grid cell in X, X ? i is the neighborhood feature tensor of the i th grid cell,</p><formula xml:id="formula_1">F q (X i ) ? R C ? is i th query, F k X ? i ? R C ? ?r 2 and F v X ? i ? R C ? ?r 2</formula><p>is i th key/value pair, ? is the SoftMax operation, and G is a transformation function implemented with 1 ? 1 convolution.</p><p>After local self feature enhancement, we perform cross scale feature enhancement. Like the top-down pathway in FPN <ref type="bibr" target="#b27">[28]</ref>, the feature map from the previous matching stage are upsampled with a deconvolutional layer and are concatenated with X before going through another convolutional layer. Then the enhanced feature map can be used to calculate the matching scores of an image pair and enhance features in the next stage.</p><p>Matching Enhancement. Unlike many cascaded methods such as FPN <ref type="bibr" target="#b27">[28]</ref> and BDCN <ref type="bibr" target="#b12">[13]</ref> where results of different scales are jointly fused, we enhance the matching result in a top-down manner by learning matching complement at different scales. For a pair of images I s , I t , denote their feature maps at the l th scale with X s l and X t l respectively. X s l and has the resolution H s l ?W s l , and X t l has the resolution H t l ? W t l . We calculate the exhaustive pair-  wise products between X s l and X t l , and store the results in a 4-D tensor S l ? R H s l ?W s l ?H t l ?W t l referred to as the matching score. S l (i, j, m, n) is the match score between the (i, j) grid cell in X s l and the (m, n) grid cell in X t l . Given the matching score</p><formula xml:id="formula_2">S l+1 ? R H s l+1 ?W s l+1 ?H t l+1 ?W t l+1</formula><p>from the previous scale, the matching complementation is conducted as follows,</p><formula xml:id="formula_3">S l = S l + U (S l+1 ) ,<label>(2)</label></formula><p>where U is the 4-D bicubic upscaling interpolation. Then the l-th scale just learns the matching residuals that are complementary with the matching results in the (l + 1)-th scale. Note that we start from the 5-th scale with the highest semantic level where S 5 = S 5 , and end at the 2-nd scale. <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the improvements caused by the matching enhancement. Given these four matching results, we test their performance on the validation set, and select the scale that has the best performance to conduct testing.</p><p>Correspondence Learning with Rich Supervision. Labeling dense semantic correspondences of image pairs requires huge amount of human labours which is impractical in real applications. We evaluate the effectiveness of the proposed method on existing datasets with sparse keypoint annotations including PF-PASCAL <ref type="bibr" target="#b10">[11]</ref>, CUB <ref type="bibr" target="#b54">[53]</ref> and SPair-71k <ref type="bibr" target="#b33">[34]</ref>. The sparse key-point annotation stores many one-tn-one mapping between images, where each mapping can be viewed as a probability distribution of a pixel in a source image I s matched with all pixels in a target image I t . Then these annotations can be utilized in a straightforward way to train a CNN model for semantic matching by minimizing the distance between the matching distribution and the ground-truth distribution. To benefit from the multi-scale matching mechanism further, we design loss functions at every scale to supervise the learning process. This rich supervision style leads to much more accurate matching results as stated in <ref type="table">Table 3</ref>.</p><p>For a key point p s i in the source image, the matching score S l (p s i ) ? R H t l ?W t l with the target image in the l-th scale is denoted in a 2-D form. Since deep features have very strong discriminative ability, we simply get the matching probability matrix P l (p s i ) by applying the SoftMax operation spatially. We first rescale the key-points in M gt to the same resolution as the feature maps at different scales. Then following ANC-Net <ref type="bibr" target="#b25">[26]</ref>, we pick its four nearest neighbours and set their probability according to distance to establish the 2-D ground-truth matching probabilities at every scale. Then we apply 2-D Gaussian smoothing of size 3 on that probability map. Our training objectives for semantic matching is then,</p><formula xml:id="formula_4">L = l ? l B P l (p s i ), P l (p s i ) + B P l (p t i ), P l (p t i ) ,<label>(3)</label></formula><p>where ? l (=1) is the weight at the l scale, B is the binary cross entropy loss, and P l (p s i ) and P l (p t i ) are the groundtruth probability map of the key-point pair (p s i , p t i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset. We conduct experiments on three popular benchmarks for semantic correspondence: PF-PASCAL <ref type="bibr" target="#b10">[11]</ref>, CUB <ref type="bibr" target="#b54">[53]</ref> and SPair-71k <ref type="bibr" target="#b33">[34]</ref>. The PF-PASCAL contains 1351 image pairs which are selected from all the 20 categories in PASCAL VOC <ref type="bibr" target="#b9">[10]</ref>. We split the dataset as done in <ref type="bibr" target="#b11">[12]</ref> where approximately 700 image pairs are used for training, 300 image pairs are used for validation and 300 image pairs are used for test. The CUB dataset <ref type="bibr" target="#b54">[53]</ref> contains 11,788 images of 200 bird species with large intra-class variations. Each image is annotated with the locations of 15 key-parts. Following the protocol in <ref type="bibr" target="#b25">[26]</ref>, we randomly sample 10,000 pairs from the CUB as training data and use the same test set provided by <ref type="bibr" target="#b21">[22]</ref>. SPair-71k is composed of total 70,958 image pairs in 18 categories with large view-point and scale variations. We use the same split proposed in <ref type="bibr" target="#b33">[34]</ref> where 53340, 5384, 12234 image pairs are used for training, validation and testing respectively.</p><p>Evaluation metric. Performances of different methods are evaluated using the percentage of correct key-points (PCK@?). A point is considered correct if the predicted point is within the circle of radius ? ? d centering at the ground-truth point, where d is the longer side of an image or an object bounding box as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">35]</ref>.</p><p>Implementation Details For fair comparison with stateof-the art methods, we use four different backbones including ResNet-50 <ref type="bibr" target="#b13">[14]</ref>, ResNet-101 <ref type="bibr" target="#b13">[14]</ref>, ResNeXt-101 <ref type="bibr" target="#b56">[55]</ref> and ResNet101-FCN <ref type="bibr" target="#b13">[14]</ref>. All backbone networks are pretrained on Image-Net1k classification set <ref type="bibr" target="#b22">[23]</ref> and then finetuned for corrrespondence task. The multi-scale matching network structure is visualized in <ref type="figure" target="#fig_0">Figure 2</ref>. We only introduce additional parameters in feature enhancement. As shown in <ref type="figure" target="#fig_0">Figure 2</ref> (b), we have many SEMs each of which is followed by a 1 ? 1 convolutional layer. We upscale the low resolution feature with a 4 ? 4 deconvolutional layer whose stride is 2 at different scales. Then the upsampled feature map is concatenated with the output of the intra-scale feature enhancement, and pass a 3 ? 3 convolution layer. Note that the SEM in our MM-Net is in the same settings as in BDCN <ref type="bibr" target="#b12">[13]</ref>, and the output channel numbers of both the convolutional layers and the deconvolutional layers is set to 21 to save computation cost.</p><p>During the training, we adopt SGD with momentum as our optimizer. The learning rate is set to 0.0005 for initialization and is decreased by 10 times every 10000 iterations. Momentum and weight decay are set to 0.9 and 0.0002 respectively. Learning rate is decreased by 10 times every 10,000 iterations. The batch-size is set to 5 for all experiments. The training will converge within 10000, 32000 and 30000 iterations for PF-PASCAL <ref type="bibr" target="#b10">[11]</ref>, CUB <ref type="bibr" target="#b54">[53]</ref> and SPair-71k <ref type="bibr" target="#b33">[34]</ref> respectively. All experiments are implemented with PyTorch <ref type="bibr" target="#b37">[37]</ref>, and run on NVidia TITAN RTX GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons with State-of-the-art Methods</head><p>For PF-PASCAL <ref type="bibr" target="#b10">[11]</ref>, our MMNet with ResNet101-FCN as the backbone outperforms all previous state-of-art methods with 81.1% PCK@0.05, 91.6% PCK@0.1, and 95.9% PCK@0. <ref type="bibr" target="#b14">15</ref>. When compared with ANC-Net <ref type="bibr" target="#b25">[26]</ref> which also conducts end-to-end training with 4-D correlation, MMNet gets 5.4%, 1.6%, and 5.5% increases on PCK@0.1 with three different backbones respectively. It demonstrates the effectiveness of the multi-scale feature learning and matching complementation. When compared with previous best SCOT <ref type="bibr" target="#b29">[30]</ref> with ResNet101-FCN as the backbone, we achieve a significant improvement on PCK@0.05 by 13.8%. We attribute this to that the end-to-end training of deep neural networks has a higher efficiency that optimization based methods to enforce one-on-one matching with discriminative features. We also compare with the multi-scale feature fusion based methods HPF <ref type="bibr" target="#b32">[33]</ref> and DHPF <ref type="bibr" target="#b35">[35]</ref>. MMNet with ResNet101-FCN as the backbone outperforms HPF with the same backbone by <ref type="bibr" target="#b16">17</ref>  <ref type="table" target="#tab_4">Table 2</ref>. Comparisons on SPair-71k <ref type="bibr" target="#b33">[34]</ref> with state-of-art methods. The backbone in methods listed is ResNet101 <ref type="bibr" target="#b13">[14]</ref>. The best results are reported in bold.</p><p>PCK@0.1 and 0.5% PCK@0.15. Since HPF doesn't conduct end-to-end training, it is reasonable that our MMNet gets better results. DHPF <ref type="bibr" target="#b35">[35]</ref> selects features from the backbone, and get slightly better results on PCK@0.1 and PCK@0.15 with ResNet50, ResNet101 backbone. However, when ? = 0.05, our MMNet get 1.9% improvement on PCK. It may because MMNet are much more sensitive to small difference between neighborhood, and thus get better matching results with much more strict matching criteria. For CUB <ref type="bibr" target="#b54">[53]</ref>, our MMNet outperforms all state-of-art algorithms with 87.0% on PCK@0.1 and achieve prominent betters on three different backbone compared with ANC-Net <ref type="bibr" target="#b25">[26]</ref>.For SPair-71k <ref type="bibr" target="#b33">[34]</ref>, our MMNet with ResNet101-FCN backbone outperforms the state-of-art algorithms by at least 13.1% on PCK@0.1, which is a huge improvement. Among all listed methods in <ref type="table" target="#tab_0">Table 1</ref>, our algorithm surpasses all state-of-art by a large margin on 15 of the 18 classes. This proves the effectiveness and robustness of our MMNet in establishing reliable matching. <ref type="figure" target="#fig_3">Figure 4</ref> shows the results in comparison with state-ofart method with varying ?. When ? is small, only points matched with the destination point closely is treated as a correct match, otherwise failure. When we increase ?, lager matching displacement will be allowed. It can be found that our MMNet achieves the best performance when ? varies from 0.02 to 0.3. When ? varies from 0.02 to 0.1, all algorithms will get improvements on PCK fast. It means the allowed match displacement influences the performance greatly. When ? varies from 0.15 to 0.3, all methods get almost the same results with very high matching accuracy. This indicates too large ? can not be used to measure the performance of different methods accurately. When ? varies from 0.01 to 0.05, our MMNet outperforms other state-of-art methods by a clear margin all the time. It indicates the strong ability of MMNet in identifying neighborhood pixels.</p><p>Besides, we also report the running speed to compare the computational efficiency of state-of-art methods. In Table 1, we get the comparable test speed with the previous best DHPF <ref type="bibr" target="#b35">[35]</ref>, and are much faster than other methods.  <ref type="table">Table 3</ref>. Ablation results on various setting on PF-PASCAL <ref type="bibr" target="#b10">[11]</ref> with ResNet101-FCN as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Module Analysis with Ablations</head><p>To investigate the effectiveness of different modules, we conduct ablation study by replacing or removing a single component. All experiments are conducted on PF-PASCAL <ref type="bibr" target="#b10">[11]</ref> with ResNet101-FCN as the backbone. PCKs are evaluated with ? = 0.05, 0.1 and 0.15. Results are listed in <ref type="table">Table 3</ref>. First, we remove the local self attention, the performance drops by 1.2% on PCK@0.05 which indicates that the contextual information around neighborhood pixels is very important. Then we remove the dense connections and only get the output of the last layer in every convolutional group, the performance of PCK@0.05 is 74.9% which is 6.2% lower than MMNet with dense connections. It shows fusing information at different layers in a convolutional group is greatly helpful. When we remove the cross-scale feature fusion, PCK@0.05, PCK@0.10 and PCK@0.15 decrease by 2.6%, 1.9% and 1.6% respectively. If we don't conduct the complementary matching learning and get the matching score at every layer independently, PCK@0.05 drops by 1%. At last, if we only supervise the learning at the last matching complementation layer with the highest resolution, the performance drops to 31.4% on PCK@0.05 which is 49.7% lower than the MMNet with rich supervision during training. It may indicate that supervision in multiple scales are very important to learn semantics at different levels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results and Visualization</head><p>We visualize the correspondence result by drawing the point-to-point matches and warping images with the predicted key point pairs respectively. In <ref type="figure" target="#fig_5">Figure 5</ref>, the pointto-point matches are drawn by linking key point pairs with line segments. The ground truth matching is given at first as reference for visual comparison. It can be find that our MM-Net matches all key points on horses correctly. Other stateof-the art methods, such as DCC-Net <ref type="bibr" target="#b14">[15]</ref>, ANC-Net <ref type="bibr" target="#b25">[26]</ref>, SCOT <ref type="bibr" target="#b29">[30]</ref> and DHPF <ref type="bibr" target="#b35">[35]</ref>, will lead to large mismatch displacement or many-to-one match. In <ref type="figure" target="#fig_1">Figure 1</ref>, images are warped based on matched key point pairs. For convenience, we warp the ground truth annotations as the reference. It can be found that our MMNet can matches the objects in images accurately. Especially in the first row, tables in the source and target images has very large viewpoint and appearance variations. But our MMNet can still match the corresponding key points accurately when other methods fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a multi-scale matching network that are coupled with the multi-scale and pyramidal hierarchy of deep convolutional neural networks to match semantic meaningful points in a coarse-to-fine manner. Our multiscale matching network learn discriminative pixel-level semantics by a top-down feature and matching enhancement scheme. Thus discriminative features are identified and fused during the complementary matching learning process. To strengthen the representatve ability of individual pixels, we introduce a local self attention module to encode local contextual information to disambiguate the feature representation of neighborhood pixels. Extensive experiments on several popular benchmarks demonstrate that the proposed MMNet outperform existing state-of-the-art semantic correspondence algorithms. However, how to get reliable correspondence that can handle drastically changes in real applications still remains to be an open problem.  <ref type="table" target="#tab_0">Table 1</ref>. Implementation details of MMNet, and numbers of parameters and FLOPs introduced by different modules. In this table, 'Conv Intra-fused' and 'Conv Cross-fused' denote the convolution operation in the intra-scale and cross-scale feature enhancements separately, 'Deconv' indicates the deconvolution operation to upscale the feature maps during the cross-scale feature enhancement, and 'LSA' is short for the local self attention module used at the end of the intra-scale feature enhancement.</p><p>Analysis. MMNet introduces additional parameters only in its feature enhancement module. We follow BDCN <ref type="bibr" target="#b12">[13]</ref> to set the parameters of scale enhancement modules. Each scale enhancement module contains four 3 ? 3 convolution operations with dilation 1, 4, 8, 12 respectively. The output channel numbers of these convolution operations are set to 32. The output channel numbers of other convolution and deconvolution operations are set to 21 to reduce the computational cost. We compare the additional parameters and FLOPs introduced by MMNet with four different backbones including ResNet-50, ResNet-101, ResNeXt-101 and ResNet-101-FCN. The additional parameters are no larger than 25% of that of any backbone. For the FLOPs, we add at most 26.3% computational cost when using ResNet-50 as the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head><p>Results with other backbones. We adapt our MMNet design with other backbones: VGG-16 and DeepLab-V3, the results are listed in 2. As can be seen, our model with ResNet-101-FCN backbone performs significantly best than others.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the multi-scale matching network. Our multi-scale matching network contains a convolutional backbone, and a top-down feature and matching enhancement pathway. In feature enhancement, coarser but semantically richer features are upscaled and combined with finer but semantically weaker features to enhance the discrminaitve ability of pixel-level features. Then the matching enhancement module directly upsamples the matching results of the previous scale and adds it with the current matching details to learn complementary correspondences across scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(a) Matching at Scale 1 (</head><label>1</label><figDesc>b) Matching at Scale 2 (c) Matching at Scale 3 (d) Matching at Scale 4 (e) Ground Truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Matching results at different feature resolutions. From left to right, displacements between predictions and the destination points are reduced with the increase of the feature resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The PCK-? curves of our method and compared works on PF-PASCAL<ref type="bibr" target="#b10">[11]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of the semantic correspondence. The odd rows are the source image, and the even rows are the target images. Destination key point are denoted with crosses. From left to right: (a) DCC-Net [15], (b) ANC-Net [26], (c) SCOT [30], (d) DHPF [35], (e) ours MMNet and (f) the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 .</head><label>1</label><figDesc>Key-point matching results on SPair-71k dataset<ref type="bibr" target="#b33">[34]</ref> compared with SCOT<ref type="bibr" target="#b29">[30]</ref> and DHPF<ref type="bibr" target="#b35">[35]</ref>. The odd rows are the source images, and the even rows are the target images. Destination key points are denoted with crosses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 .</head><label>3</label><figDesc>Warped images by thin-plate splines with the predicted key point pairs on SPair-71k dataset<ref type="bibr" target="#b33">[34]</ref> compared with SCOT<ref type="bibr" target="#b29">[30]</ref> and DHPF<ref type="bibr" target="#b35">[35]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 .</head><label>4</label><figDesc>Warped images by thin-plate splines with the predicted key point pairs on SPair-71k dataset<ref type="bibr" target="#b33">[34]</ref> compared with SCOT<ref type="bibr" target="#b29">[30]</ref> and DHPF<ref type="bibr" target="#b35">[35]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="3">PF-PASCAL 0.05 0.1 0.15</cell><cell cols="2">CUB time 0.1 (ms)</cell></row><row><cell>PF HOG [11]</cell><cell cols="3">31.4 62.5 79.5</cell><cell>-</cell><cell>-</cell></row><row><cell>CNNGeo ResNet-101 [38]</cell><cell cols="3">41.0 69.5 80.4</cell><cell>-</cell><cell>-</cell></row><row><cell>A2Net ResNet-101 [44]</cell><cell cols="3">42.8 70.8 83.3</cell><cell>-</cell><cell>-</cell></row><row><cell>SFNet ResNet-101 [25]</cell><cell cols="3">53.6 81.9 90.6</cell><cell>-</cell><cell>-</cell></row><row><cell>DCTM CAT-FCSS [20]</cell><cell cols="3">34.2 69.6 80.2</cell><cell>-</cell><cell>-</cell></row><row><cell>WeakAlign ResNet-101 [39]</cell><cell cols="3">49.0 74.8 84.0</cell><cell>-</cell><cell>-</cell></row><row><cell>SCNet VGG-16 [12]</cell><cell cols="3">36.2 72.2 82.0</cell><cell>-</cell><cell>-</cell></row><row><cell>RTNs ResNet-101 [19]</cell><cell cols="3">55.2 75.9 85.2</cell><cell>-</cell><cell>-</cell></row><row><cell>UCN GoogLeNet [6]</cell><cell>-</cell><cell>55.6</cell><cell>-</cell><cell>48.3</cell><cell>-</cell></row><row><cell>UCN ResNet-101 [6]</cell><cell>-</cell><cell>75.1</cell><cell>-</cell><cell>52.1</cell><cell>-</cell></row><row><cell>NC-Net ResNet-101 [40]</cell><cell cols="4">54.3 78.9 86.0 64.7</cell><cell>393</cell></row><row><cell>DCCNet ResNet-101 [15]</cell><cell cols="4">55.6 82.3 90.5 66.1</cell><cell>-</cell></row><row><cell>HPF ResNet-50 [33]</cell><cell cols="3">60.5 83.4 92.1</cell><cell>-</cell><cell>-</cell></row><row><cell>HPF ResNet-101 [33]</cell><cell cols="3">60.1 84.8 92.7</cell><cell>-</cell><cell>-</cell></row><row><cell>HPF ResNet-101-FCN [33]</cell><cell cols="3">63.5 88.3 95.4</cell><cell>-</cell><cell>-</cell></row><row><cell>DHPF ResNet-50 [35]</cell><cell cols="3">72.6 88.9 94.3</cell><cell>-</cell><cell>55</cell></row><row><cell>DHPF ResNet-101 [35]</cell><cell cols="3">75.7 90.7 95.0</cell><cell>-</cell><cell>95</cell></row><row><cell>SCOT ResNet-101 [30]</cell><cell cols="3">63.1 85.4 92.7</cell><cell>-</cell><cell>180</cell></row><row><cell>SCOT ResNet-101-FCN [30]</cell><cell cols="3">67.3 88.8 95.4</cell><cell>-</cell><cell>109</cell></row><row><cell>ANC-Net ResNet-101 [26]</cell><cell>-</cell><cell>83.7</cell><cell>-</cell><cell>69.6</cell><cell>600</cell></row><row><cell>ANC-Net ResNeXt-101 [26]</cell><cell>-</cell><cell>88.7</cell><cell>-</cell><cell>74.1</cell><cell>-</cell></row><row><cell>ANC-Net ResNet-101-FCN [26]</cell><cell>-</cell><cell>86.1</cell><cell>-</cell><cell>72.4</cell><cell>-</cell></row><row><cell>MMNet ResNet-50</cell><cell cols="4">75.3 88.0 93.2 80.6</cell><cell>51</cell></row><row><cell>MMNet ResNet-101</cell><cell cols="4">77.6 89.1 94.3 81.8</cell><cell>86</cell></row><row><cell>MMNet ResNeXt-101</cell><cell cols="4">78.9 90.3 94.4 83.1</cell><cell>101</cell></row><row><cell>MMNet ResNet-101-FCN</cell><cell cols="4">81.1 91.6 95.9 87.0</cell><cell>87</cell></row></table><note>Comparison with state-of-the-art algorithms in PCK and speed on PF-PASCAL [11] and CUB [53] dataset. Subscripts of the method names indicates the backbone used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>WeakAlign [39] 22.2 17.6 41.9 15.1 38.1 27.4 27.2 31.8 12.8 26.8 22.6 14.2</figDesc><table><row><cell>Methods</cell><cell>aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="7">cat chair cow dog horse mbike person plant sheep train</cell><cell>tv</cell><cell>all</cell></row><row><cell>CNNGeo [38]</cell><cell cols="5">23.4 16.7 40.2 14.3 36.4 27.7 26.0 32.7 12.7 27.4 22.8 13.7</cell><cell>20.9</cell><cell>21.0</cell><cell>17.5</cell><cell>10.2</cell><cell>30.8 34.1 20.6</cell></row><row><cell>A2Net [44]</cell><cell cols="5">22.6 18.5 42.0 16.4 37.9 30.8 26.5 35.6 13.3 29.6 24.3 16.0</cell><cell>21.6</cell><cell>22.8</cell><cell>20.5</cell><cell>13.5</cell><cell>31.4 36.5 22.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20.0</cell><cell>22.2</cell><cell>17.9</cell><cell>10.4</cell><cell>32.2 35.1 20.9</cell></row><row><cell>NC-Net [40]</cell><cell cols="3">17.9 12.2 32.1 11.7 29.0 19.9 16.1 39.2</cell><cell>9.9</cell><cell>23.9 18.8 15.7</cell><cell>17.4</cell><cell>15.0</cell><cell>14.8</cell><cell>9.6</cell><cell>24.2 31.1 20.1</cell></row><row><cell>HPF [33]</cell><cell cols="5">25.3 18.5 47.6 14.6 37.0 22.9 18.3 51.1 16.7 31.5 30.8 19.1</cell><cell>23.7</cell><cell>23.8</cell><cell>23.5</cell><cell>14.4</cell><cell>30.8 37.2 27.2</cell></row><row><cell>HPF [33]</cell><cell cols="5">25.2 18.9 52.1 15.7 38.0 22.8 19.1 52.9 17.9 33.0 32.8 20.6</cell><cell>24.4</cell><cell>27.9</cell><cell>21.1</cell><cell>1.9</cell><cell>31.5 35.6 28.2</cell></row><row><cell>DHPF [35]</cell><cell cols="5">38.4 23.8 68.3 18.9 42.6 27.9 20.1 61.6 22.0 46.9 46.1 33.5</cell><cell>27.6</cell><cell>40.1</cell><cell>27.6</cell><cell>28.1</cell><cell>49.5 46.5 37.3</cell></row><row><cell>SCOT [30]</cell><cell cols="5">34.9 20.7 63.8 21.1 43.5 27.3 21.3 63.1 20.0 42.9 42.5 31.1</cell><cell>29.8</cell><cell>35.0</cell><cell>27.7</cell><cell>24.4</cell><cell>48.4 40.8 35.6</cell></row><row><cell>MMNet</cell><cell cols="5">43.5 27.0 62.4 27.3 40.1 50.1 37.5 60.0 21.0 56.3 50.3 41.3</cell><cell>30.9</cell><cell>19.2</cell><cell>30.1</cell><cell>33.2</cell><cell>64.2 43.6 40.9</cell></row><row><cell>MMNet-FCN</cell><cell cols="5">55.9 37.0 65.0 35.4 50.0 63.9 45.7 62.8 28.7 65.0 54.7 51.6</cell><cell>38.5</cell><cell>34.6</cell><cell>41.7</cell><cell>36.3</cell><cell>77.7 62.5 50.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">.6% PCK@0.05, 3.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Supplementary Material for Multi-scale Matching Networks for Semantic Correspondence A. Parameter Table Images are resized to 224 ? 320 for all datasets both in training and testing. MMNet 14.9 ? 10 9 27.0 ? 10 9 51.6 ? 10 9 140.3 ? 10 9</figDesc><table><row><cell></cell><cell></cell><cell>ResNet-50</cell><cell>ResNet-101</cell><cell>ResNeXt-101</cell><cell>ResNet-101-FCN</cell></row><row><cell>Conv Intra-fused</cell><cell cols="3">conv: output channel = 21, kernel size = 1?1 , stride = 1</cell><cell></cell><cell></cell></row><row><cell>Conv Cross-fused</cell><cell cols="4">conv: output channel = 21, kernel size = 3?3, stride = 1, padding = 1</cell><cell></cell></row><row><cell>Deconv</cell><cell cols="4">transpose conv: output channel = 21, kernel size = 4?4, stride = 2, padding = 1 , bias = False</cell><cell></cell></row><row><cell></cell><cell>conv key</cell><cell cols="3">conv: output channel = 10, kernel size = 1?1</cell><cell></cell></row><row><cell>LSA</cell><cell>conv value conv query</cell><cell cols="3">conv: output channel = 10, kernel size = 1?1 conv: output channel = 10, kernel size = 1?1</cell><cell></cell></row><row><cell></cell><cell>conv aggregate</cell><cell cols="3">conv: output channel = 21, kernel size = 1?1</cell><cell></cell></row><row><cell></cell><cell>backbone</cell><cell>25.6 ? 10 6</cell><cell>44.5.0 ? 10 6</cell><cell>88.8 ? 10 6</cell><cell>54.4 ? 10 6</cell></row><row><cell># Params</cell><cell>MMNet w/o backbone</cell><cell>4.8 ? 10 6</cell><cell>10.3 ? 10 6</cell><cell>10.3 ? 10 6</cell><cell>10.3 ? 10 6</cell></row><row><cell></cell><cell>MMNet</cell><cell>30.4 ? 10 6</cell><cell>54.8 ? 10 6</cell><cell>99.1 ? 10 6</cell><cell>64.7 ? 10 6</cell></row><row><cell></cell><cell>backbone</cell><cell>11.8 ? 10 9</cell><cell>22.4 ? 10 9</cell><cell>47.0 ? 10 9</cell><cell>127.6 ? 10 9</cell></row><row><cell>FLOPs</cell><cell>MMNet w/o backbone</cell><cell>3.1 ? 10 9</cell><cell>4.6 ? 10 9</cell><cell>4.6 ? 10 9</cell><cell>12.7 ? 10 9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Experiments on different backbones. All experiments is conducted on PF-PASCAL.</figDesc><table><row><cell></cell><cell>PF-PASCAL</cell></row><row><cell></cell><cell>0.05 0.1 0.15</cell></row><row><cell>MMNet ResNet-101-FCN</cell><cell>81.1 91.6 95.9</cell></row><row><cell>MMNet VGG-16</cell><cell>69.5 82.0 88.4</cell></row><row><cell cols="2">MMNet DeepLabV3-ResNet101 73.3 85.3 92.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 2. Key-point matching results on SPair-71k dataset<ref type="bibr" target="#b33">[34]</ref> compared with SCOT<ref type="bibr" target="#b29">[30]</ref> and DHPF<ref type="bibr" target="#b35">[35]</ref>. The odd rows are the source images, and the even rows are the target images. Destination key points are denoted with crosses.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finetuning pre-trained transformer language models to distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>H?bner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<ptr target="Asso-ciationforComputationalLinguistics.4" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1388" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="567" to="585" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A graphmatching kernel for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1792" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Rafael S Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bi-directional cascade network for perceptual edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint optical flow and temporally consistent semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="163" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Warpnet: Weakly supervised matching for singleview reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sang Ryul Jeon, Dongbo Min, and Kwanghoon Sohn. Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lin</surname></persName>
		</author>
		<idno>De- cember 2018. 6</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page" from="6126" to="6136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dctm: Discrete-continuous transformation matching for semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A cross-season correspondence dataset for robust semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Mans Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9532" to="9542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2278" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10196" to="10205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dualresolution correspondence networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dgc-net: Dense geometric correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iaroslav</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1034" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spair-71k: A large-scale benchmark for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arxiv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020-16th European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Adam Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soumith Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Endto-end weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6917" to="6925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attentive semantic alignment with offset-aware correlation kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deunsol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Models matter, so does training: An empirical study of cnns for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1408" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4246" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Daisy: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="815" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GOCor: Bringing globally optimized correspondence volumes into your neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Glunet: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6258" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc. 4</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning combinatorial embedding networks for deep graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3056" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010- 001</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3109" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A continuation method for graph matching based feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1809" to="1822" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Feature pyramid transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="323" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Activestereonet: End-to-end self-supervised learning for active stereo systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schoenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9259" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
