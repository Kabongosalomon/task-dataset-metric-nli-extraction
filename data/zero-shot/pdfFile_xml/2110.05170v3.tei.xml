<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DOMAIN ADAPTIVE SEMANTIC SEGMENTATION VIA REGIONAL CONTRASTIVE CONSISTENCY REGULARIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuyun</forename><surname>Zhuang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Yi</surname></persName>
							<email>ranyi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
							<email>xuequan.lu@deakin.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">Deakin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DOMAIN ADAPTIVE SEMANTIC SEGMENTATION VIA REGIONAL CONTRASTIVE CONSISTENCY REGULARIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Domain Adaptation</term>
					<term>Semantic Segmen- tation</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) for semantic segmentation has been well-studied in recent years. However, most existing works largely neglect the local regional consistency across different domains, and are less robust to changes in outdoor environments. In this paper, we propose a novel and fully end-to-end trainable approach, called regional contrastive consistency regularization (RCCR) for domain adaptive semantic segmentation. Our core idea is to pull the similar regional features extracted from the same location of different images, i.e., the original image and augmented image, to be closer, and meanwhile push the features from the different locations of the two images to be separated. We propose a region-wise contrastive loss with two sampling strategies to realize effective regional consistency. Besides, we present momentum projection heads, where the teacher projection head is the exponential moving average of the student. Finally, a memory bank mechanism is designed to learn more robust and stable region-wise features under varying environments. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Semantic segmentation aims to assign a semantic class to each pixel for a given image. Despite the great success in recent years, most methods <ref type="bibr" target="#b1">[1]</ref> heavily require a large amount of training data, and labeling such pixel-wise data is extremely expensive, and time-consuming <ref type="bibr" target="#b2">[2]</ref>. A natural idea is using synthetic data <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref> to supervise the segmentation model instead of real data. However, such data cannot fully match the real-world distributions to guarantee reliable performance due to the existing domain shifts. Thus, it is necessary to reduce the labeling cost and improve the generalization ability of the segmentation models under different distributions.</p><p>To cope with this problem, unsupervised domain adaptation (UDA) for semantic segmentation has been recently ? Corresponding authors.  <ref type="figure">Fig. 1</ref>. Previous domain adaptation methods overlook the regional consistency across different domains. To address this problem, our key idea builds on region-level contrastive learning by maximizing the inter-region differences and minimizing intra-region disagreement. 1). On the output space, the predicted label should be invariant to cross-domain environmental augmentations, e.g., CutMix <ref type="bibr" target="#b14">[14]</ref>. 2). On the feature space, we pull the similar regional embeddings extracted on the same location from the target image and mixed image to be closer, and push the dissimilar embeddings from the different locations of the two images to be separated.</p><p>explored. This task aims to bridge domain gaps between the labeled source domain and the unlabeled target domain. Many approaches perform the adaptation in input-level <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>, feature-level <ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref>, and output-level <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>. However, most of them heavily depend on the adversarial objectives <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>, offline self-training <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b13">13]</ref> and image translation <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>, which make the training process too complicated and hard to converge.</p><p>Recently, consistency regularization <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>, also known as Mean Teacher, tackles this problem by employing the consistency constraint on the target prediction between the student model and the teacher model, respectively. This kind of method usually performs the feature-level domain alignment between the student model and the teacher model with an online ensemble. The teacher model is an exponential moving average (EMA) of the student model, and then the teacher model could transfer the learned knowledge to the student.</p><p>Unfortunately, such methods usually employ an inconsistent penalty for the prediction map on the global level, while largely neglecting the region-wise consistency on the local level, i.e., some contextual object occurrence should be consistent regardless of the changes of outdoor environments. We observe that only capturing the pattern information from the global level is not powerful enough to enhance the featurelevel representation in consistency regularization. If lacking such local regional information, the segmentation result of objects will inevitably suffer from a non-marginal performance drop in the target domain. To prevent the model from abusing the contexts, we aim to make the learned representations more robust to the changing environments by exploring the regional consistency in a fine-grained manner.</p><p>Motivated by the above facts, we propose a regional contrastive consistency regularization (RCCR) framework for domain adaptive semantic segmentation, which is fully endto-end trainable. As shown in <ref type="figure">Fig. 1</ref>, our key idea builds on region-level contrastive learning by maximizing the interregion differences and minimizing intra-region disagreement. 1) On the output label space, the predicted label should be invariant to cross-domain environmental augmentations, e.g., CutMix <ref type="bibr" target="#b14">[14]</ref>. 2) On the feature space, we pull the similar regional features extracted from the same location of the target image and the augmented image to be closer, and simultaneously push the features from the different locations of the two images to be separated.</p><p>Specifically, we first present the region-wise contrastive (RWC) loss between the target embeddings and the augmented embeddings, keeping the regional consistency in a fine-grained manner. Secondly, to fully utilize the spatial and local semantic cues, e.g., spatial layout and local context, we design momentum projection heads for aiding the regionlevel contrastive learning. The teacher projection head is the exponential moving average (EMA) of the student, and both of them are after the encoder architecture to produce the lowdimensional embeddings. Besides, we present two sampling strategies for positive and negative samples, avoiding treating every pixel-wise sample equally. Finally, we introduce a memory bank mechanism to store the negative features created in the last few batches to learn more robust and stable region-wise features under varying environments.</p><p>In a nutshell, our contributions are three-fold:</p><p>? We propose a regional contrastive consistency regularization (RCCR) framework for domain adaptive semantic segmentation, which keeps the local regional consistency on the feature space and label space, respectively, under the crossdomain environmental augmentations.</p><p>? We present a region-wise contrastive loss, sampling strategies, and momentum projection heads to realize effective regional consistency in domain adaptation. We also introduce a memory bank mechanism to learn more robust and stable region-wise features under varying environments.</p><p>? We provide extensive experiments with analysis to demonstrate the state-of-the-art performance on two challenging benchmark datasets of domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Domain Adaptive Semantic Segmentation</head><p>Unsupervised domain adaptation (UDA) aims to learn a generalized model on the labeled source domain and the unlabeled target domain. This problem has been well-studied in semantic segmentation. Many recent approaches are proposed to reduce the domain gap between the source data and the target data on three different levels, namely, the input-level <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b17">17]</ref>, feature-level <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>, and output-level adaptation <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>. However, most recent methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b17">17]</ref> involve many sophisticated sub-components, e.g., computationally-involved adversarial objectives <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>, offline self-training <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b13">13]</ref> and image translation models <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b17">17]</ref>, which are hard to converge, and cannot be trained in an end-to-end manner. Besides, most methods are less robust to changing of outdoor environments. In contrast, our method is effective and does not require any fine-tuning, and we explore the regional consistency across domains. On top of <ref type="bibr" target="#b16">[16]</ref>, we realize fully end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning</head><p>Great progress in contrastive learning has been achieved by encouraging the positive pairs to get closer and pulling the negative pairs apart. For semantic segmentation tasks, the definition of positive pairs and negative pairs can be various to fit the dense pixel prediction requirements. <ref type="bibr" target="#b18">[18]</ref> treated the same category samples as the positive pairs and others as the negative pairs. <ref type="bibr" target="#b19">[19]</ref> divided the positive pairs and negative pairs according to the label distribution similarity between different patches. There are also some works that investigated the contrastive learning methods <ref type="bibr" target="#b20">[20]</ref> in Semi-Supervised Semantic Segmentation (SSS). Our method differs from these methods in several aspects. Firstly, we tackle a more complicated task UDA rather than SSS, where the domain shifts exist between the source and the target domain. Secondly, most of them only consider category-wise contrastive learning patterns while largely neglecting the region-wise consistency across domains. In contrast, we attempt to keep the regional consistency in the feature space and label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations and Overview</head><p>In the UDA task, we have access to the source domain with labels, denoted as  <ref type="figure">Fig. 2</ref>. Overview of the regional contrastive consistency regularization (RCCR) architecture. Firstly, to produce cross-domain environmental changes, we cut a region from the target and paste it onto the source image to generate CutMix images. Then, we design momentum projection heads, namely, the student and teacher projector, to extract CutMix embeddings and target embeddings, respectively. Given these embeddings, we perform the region-level contrastive learning. Concretely, we compute the proposed region-wise contrastive (RWC) loss by maximizing the inter-region differences and minimizing intra-region disagreement. Moreover, we introduce sampling strategies and memory banks to further enhance our contrastive learning paradigm.</p><formula xml:id="formula_0">D s = {(x s , y s ) | x s ? R H?W ?3 , y s ? R H?W , y s ? [1, C] ,</formula><p>without labels is D t = {(x t ) | x t ? R H?W ?3 . Our primary goal is to bridge the domain gap between D s and D t . <ref type="figure">Fig. 2</ref> describes the whole pipeline of the RCCR in the end-to-end training procedure. Our framework includes a Mean Teacher <ref type="bibr" target="#b15">[15]</ref>, momentum projection heads, and regionwise contrastive learning. To produce cross-domain environmental changes, we cut a random region from the target and paste it onto the source image to generate CutMix image x cut . Then, we design the student and teacher projection head F proj to extract CutMix embeddings e cut and target embeddings e t , respectively. Given these embeddings, we perform the regionlevel contrastive learning by maximizing the inter-region differences and maximizing intra-region agreement. Moreover, we introduce sampling strategies and memory banks to further learn more robust and stable region-wise features under varying environments. Following DACS <ref type="bibr" target="#b16">[16]</ref>, we compute a Cross-Entropy loss L CE for supervising the student model with source labels, and a consistency loss L cons between the ClassMix prediction p class and the ClassMix label y class . ClassMix <ref type="bibr" target="#b16">[16]</ref> is a kind of augmentation by pasting some selective classes of the source onto the target image. In the inference phase, we only keep the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regional Contrastive Consistency Regularization</head><p>Momentum Projection Head. As shown in <ref type="figure">Fig. 2</ref>, we design momentum projection heads, namely student projector and teacher projector, where the teacher projector is an exponential moving average (EMA) of the student projec-</p><formula xml:id="formula_1">tor,? (t) proj = ?? (t?1) proj + (1 ? ?)? (t)</formula><p>proj . The EMA of the model is more stable and more precise than the student projector, thus can provide high-quality embeddings for the contrastive learning. The projection heads F proj behind the feature extractor F enc map the latent high-dimensional features z ? R h?w?D of F enc to low ones e = F proj (z) ? R h?w?K , where channel number K&lt;D. Instead of using the outputs z of the feature extractor, the main intuition of the projection head is to prevent losing too many spatial and local semantic cues in the adaptation. Such rich information, e.g., spatial layout and local context, can be well used to aid the region-wise contrative learning. We implement F proj with two consecutive convolutional layers and ReLU. Region-wise Contrastive Loss. The core idea of regionwise contrastive (RWC) loss is to keep the regional consistency on a fine-grained level by maximizing the inter-region differences and minimizing the intra-region disagreements. In other words, we aim to pull the embedding on the same location of the overlap region between the CutMix embedding e cut and the target embedding e t to be closer, and push the embeddings on other locations to be separated. The proposed contrastive loss function L cont is defined as:</p><formula xml:id="formula_2">L cont = ? 1 N N i=1 1 |P i | j?Pi log exp e t i ? e cut j /? exp e t i ? e cut j /? + k?Ni exp (e t i ? e cut k /? ) ,<label>(1)</label></formula><p>where e j denotes the positive embeddings from the student and e k denotes the negative embeddings from the teacher and student. P i and N i denotes the corresponding positive set and negative set on the i-th region. N is the total number of regions and ? represents the temperature. On one hand, the discrepancy between the elements of the positive pair should be minimized, thus maximizing the intra-region agreements and performing intra-domain adaptation. On the other hand, the discrepancy between the elements of the negative pair should be constrained, thus maximizing the inter-region difference and performing inter-domain adaptation. RWC loss is an asymmetric contrastive learning since the teacher model tends to output higher confident prediction than the student due to EMA, and thus we only make e cut close to e t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling Strategies.</head><p>To avoid treating every sample equally, we introduce two sampling strategies of positive and negative samples in region-level contrastive learning. (1) For negative samples, we introduce random sampling and category-aware sampling strategies. Specifically, we first randomly select half samples from the original negatives. Secondly, we gather the negative embeddings that have the same label or pseudo label with e cut . Note that we do not enlarge the embedding distance for the same category. (2) For positive sampling, we set a threshold to select positive samples, for every e cut , and only the prediction probability in the same location of corresponding target image larger than will be included in contrastive learning.</p><p>Memory Bank Mechanism. To further enhance the contrastive learning scheme, we develop a memory bank mechanism. To be specific, the embedding outputs produced in the last few batch images are also considered as negatives in the current iteration, and we construct a memory bank to save useful information: the projection embedding and their corresponding label or pseudo label. The introduced memory bank is driven by the motivation that the environments between different images may have similar distributions and can be semantically related due to the fact that they come from the same dataset. Therefore, this strategy can enable learning more robust and stable region-wise features under varying environments. Besides, it can also reduce the memory occupation and computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Following common UDA protocols <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>, our experiments are conducted on two widely-used UDA benchmarks, i.e., GTAV <ref type="bibr" target="#b3">[3]</ref> ? Cityscapes <ref type="bibr" target="#b2">[2]</ref> and SYNTHIA <ref type="bibr" target="#b4">[4]</ref> ? Cityscapes <ref type="bibr" target="#b2">[2]</ref>, and we adopt Deeplabv2 <ref type="bibr" target="#b1">[1]</ref> framework with a ResNet101 backbone as our model. The backbone is pretrained on ImageNet. Following <ref type="bibr" target="#b16">[16]</ref>, we also apply Color jittering and Gaussian blurring on the mixed images . The projection head F proj is constructed by two consecutive convolutions (2048 hidden layer channels and 128 output channels) with one intermediate ReLU layer. The temperature ? is set to 0.1, and the positive threshold is set to 0.75 by default. We use batchsize = 2 for 250000 iterations in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to State-of-the-Art Methods</head><p>We compare our method with the state-of-the-art UDA methods on two common UDA benchmarks, i.e., GTAV <ref type="bibr" target="#b3">[3]</ref> ? Cityscapes <ref type="bibr" target="#b2">[2]</ref> in <ref type="table" target="#tab_1">Table 1</ref> and the results of SYNTHIA <ref type="bibr" target="#b4">[4]</ref> ? Cityscapes <ref type="bibr" target="#b2">[2]</ref> in <ref type="table">Table 2</ref>, respectively.</p><p>As shown in these two tables, our proposed RCCR method outperforms the state-of-the-art approaches by 5% ? 6% on two challenging tasks. It also surpasses the baseline ("Source Only") by around 22% and 25%, respectively. Specifically, we obtain a 55.3% mIoU on GTAV <ref type="bibr" target="#b3">[3]</ref> and achieve the best per-class IoU performance for 14 classes among the total 19 classes. For SYNTHIA <ref type="bibr" target="#b4">[4]</ref> dataset, we observe a 59.0% mIoU of 13 classes and 51.1% mIoU of 16 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we perform ablation studies to investigate the role of RCCR components, including the region-wise contrastive (RWC) loss, random sampling (NS(R)) and categorywise sampling (NS(C)) for negative samples, sampling strategies for positive samples (PS), and memory bank (MB). Effect of each component. As shown in <ref type="table">Table 3</ref>, by adding the RWC loss, we boost the strong baseline <ref type="bibr" target="#b16">[16]</ref> with an additional +1.2%, achieving 53.3%, showing the effectiveness of region-level alignment under different environments. When taking the different sampling strategies, i.e., NS(R) and NS(C), into account, we find the gradual and non-marginal improvement by 0.6%, which reveals that combining the category information derived from the label or target segmentation output can lead to a more powerful contrastive learning scheme. Finally, we take the memory bank to store the negative samples created from the last three batches, and this mechanism makes a further improvement by +1.1%. Effect of momentum projection heads. <ref type="table" target="#tab_3">Table 4</ref> illustrates the effect of the momentum projection head F proj . If removing F proj , the results on two benchmarks are 52.1% and 54.8%, respectively. If directly using the feature map z from the encoder for the proposed region-wise contrastive (RWC) loss L cont (z), the improvements are limited with +0.5% and +0.2% in two benchmarks. In contrast, with using the embeddings e for computing L cont (e), we can yield larger improvements by +3.2% and +4.2%, respectively, which confirms the effectiveness of our F proj .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we proposed regional contrastive consistency regularization (RCCR) for domain adaptive semantic segmentation. By maximizing the inter-region differences and minimizing intra-region disagreements, we could effectively manage to keep the regional consistency in a fine-grained manner, i.e., feature space and label space, regardless of the changing of outdoor environments. Firstly, a region-wise contrastive (RWC) loss, momentum projection heads, and two sampling strategies are proposed to realize the regional consistency. Then, we introduce a memory bank mechanism to learn more robust and stable region-wise features under varying environments. Extensive experimental results demonstrate the stateof-the-art performance of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>and the target domain Consistency loss Student encoders Student projector Teacher projector CE loss Teacher encoder Teacher Segmentation head Target flow Source flow CutMix flow ClassMix flow EMA EMA EMA Source image Target image Memory bank Sampling strategy ClassMix image Region-level contrastive learning Region-wise contrastive loss CutMix image Source prediction ClassMix prediction Source label ClassMix label Target pseudo label CutMix label Different domain Different region Student Segmentation head DACS CutMix embedding Target embedding Momentum Projection Head</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison results (mIoU) from GTAV to Cityscapes. Source Only 63.3 15.7 59.4 8.6 15.2 18.3 26.9 15.0 80.5 15.3 73.0 51.0 17.7 59.7 28.2 33.1 3.5 23.2 16.7 32.9</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motocycle</cell><cell>bike</cell><cell>mIoU 19</cell></row><row><cell>BDL [5]</cell><cell cols="20">91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5</cell></row><row><cell>SIM [11]</cell><cell cols="20">90.6 44.7 84.8 34.3 28.7 31.6 35.0 37.6 84.7 43.3 85.3 57.0 31.5 83.8 42.6 48.5 1.9 30.4 39.0 49.2</cell></row><row><cell>FDA [7]</cell><cell cols="20">92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.4 50.5</cell></row><row><cell cols="21">WLabel [21] 91.6 47.4 84.0 30.4 28.3 31.4 37.4 35.4 83.9 38.3 83.9 61.2 28.2 83.7 28.8 41.3 8.8 24.7 46.4 48.2</cell></row><row><cell>FADA [22]</cell><cell cols="20">92.5 47.5 85.1 37.6 32.8 33.4 33.8 18.4 85.3 37.7 83.5 63.2 39.7 87.5 32.9 47.8 1.6 34.9 39.5 49.2</cell></row><row><cell>LDR [6]</cell><cell cols="20">90.8 41.4 84.7 35.1 27.5 31.2 38.0 32.8 85.6 42.1 84.9 59.6 34.4 85.0 42.8 52.7 3.4 30.9 38.1 49.5</cell></row><row><cell>CCM [23]</cell><cell cols="20">93.5 57.6 84.6 39.3 24.1 25.2 35.0 17.3 85.0 40.6 86.5 58.7 28.7 85.8 49.0 56.4 5.4 31.9 43.2 49.9</cell></row><row><cell>ASA [24]</cell><cell cols="20">89.2 27.8 81.3 25.3 22.7 28.7 36.5 19.6 83.8 31.4 77.1 59.2 29.8 84.3 33.2 45.6 16.9 34.5 30.8 45.1</cell></row><row><cell cols="21">CLAN [12] 88.7 35.5 80.3 27.5 25.0 29.3 36.4 28.1 84.5 37.0 76.6 58.4 29.7 81.2 38.8 40.9 5.6 32.9 28.8 45.5</cell></row><row><cell>DAST [13]</cell><cell cols="20">92.2 49.0 84.3 36.5 28.9 33.9 38.8 28.4 84.9 41.6 83.2 60.0 28.7 87.2 45.0 45.3 7.4 33.8 32.8 49.6</cell></row><row><cell cols="21">BiMaL [25] 91.2 39.6 82.7 29.4 25.2 29.6 34.3 25.5 85.4 44.0 80.8 59.7 30.4 86.6 38.5 47.6 1.2 34.0 36.8 47.3</cell></row><row><cell>Ours</cell><cell cols="20">93.2 54.7 86.7 42.1 34.9 37.9 44.4 42.8 87.5 51.2 86.1 65.5 37.8 88.5 47.2 62.2 5.4 35.5 47.0 55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison results (mIoU) from SYNTHIA to Cityscapes. Source Only 19.6 12.8 70.4 10.8 0.1 24.5 5.9 11.7 67.4 76.0 52.1 16.3 59.4 19.7 14.6 16.5 29.9 34.0 Ablation of components in GTAV to Cityscapes. get the best per-class IoU in 13 classes among the total 16 classes. These results reveal the effectiveness of our RCCR among different classes, e.g., building, traffic light, traffic sign, person, rider, car, etc.</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall  *</cell><cell>fence  *</cell><cell>pole  *</cell><cell>light</cell><cell>sign</cell><cell>vegetation</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motocycle</cell><cell>bike</cell><cell>mIoU 16</cell><cell>mIoU 13</cell></row><row><cell>BDL [5]</cell><cell cols="3">86.0 46.7 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>-</cell><cell>51.4</cell></row><row><cell>SIM [11]</cell><cell cols="3">83.0 44.0 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">17.1 15.8 80.5 81.8 59.9 33.1 70.2 37.3 28.5 45.8</cell><cell>-</cell><cell>52.1</cell></row><row><cell>FDA [7]</cell><cell cols="3">79.3 35.0 73.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">19.9 24.0 61.7 82.6 61.4 31.1 83.9 40.8 38.4 51.1</cell><cell>-</cell><cell>52.5</cell></row><row><cell cols="8">WLabel [21] 92.0 53.5 80.9 11.4 0.4 21.8 3.8</cell><cell cols="11">6.0 81.6 84.4 60.8 24.4 80.5 39.0 26.0 41.7 44.3 51.9</cell></row><row><cell>CCM [23]</cell><cell cols="18">79.6 36.4 80.6 13.3 0.3 25.5 22.4 14.9 81.8 77.4 56.8 25.9 80.7 45.3 29.9 52.0 45.2 52.9</cell></row><row><cell>LDR [6]</cell><cell cols="3">85.1 44.5 81.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">16.4 15.2 80.1 84.8 59.4 31.9 73.2 41.0 32.6 44.7</cell><cell>-</cell><cell>53.1</cell></row><row><cell cols="4">CLAN [12] 82.7 37.2 81.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">17.1 13.1 81.2 83.3 55.5 22.1 76.6 30.1 23.5 30.7</cell><cell>-</cell><cell>48.8</cell></row><row><cell>ASA [24]</cell><cell cols="7">91.2 48.5 80.4 3.7 0.3 21.7 5.5</cell><cell cols="11">5.2 79.5 83.6 56.4 21.9 80.3 36.2 20.0 32.9 41.7 49.3</cell></row><row><cell>DAST [13]</cell><cell cols="18">87.1 44.5 82.3 10.7 0.8 29.9 13.9 13.1 81.6 86.0 60.3 25.1 83.1 40.1 24.4 40.5 45.2 52.5</cell></row><row><cell cols="19">BiMaL [25] 92.8 51.5 81.5 10.2 1.0 30.4 17.6 15.9 82.4 84.6 55.9 22.3 85.7 44.5 24.6 38.8 46.2 53.7</cell></row><row><cell>Ours</cell><cell cols="18">92.5 58.7 83.7 15.1 1.3 34.7 26.6 27.1 82.6 87.3 66.0 34.9 86.5 50.5 23.6 47.4 51.1 59.0</cell></row><row><cell>ID</cell><cell cols="8">RWC NS(R) NS(C) PS MB mIoU 19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baseline</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>52.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>53.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>II</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>53.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>53.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>54.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>classes, and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Effect of momentum projector F proj . F proj ) L cont (z) L cont (e) mIoU 19</figDesc><table><row><cell>Ours</cell><cell></cell><cell></cell><cell>mIoU 13</cell></row><row><cell cols="3">(w/o (GTAV)</cell><cell>(SYN)</cell></row><row><cell>-</cell><cell>-</cell><cell>52.1</cell><cell>54.8</cell></row><row><cell></cell><cell>-</cell><cell>52.6</cell><cell>55.0</cell></row><row><cell>-</cell><cell></cell><cell>55.3</cell><cell>59.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">National Natural Science Foundation of China (72192821, 61972157), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), Shanghai Science and Technology Commission (21511101200, 22YF1420300), and Art major project of</title>
	</analytic>
	<monogr>
		<title level="j">National Social Science Fund</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">22</biblScope>
			<date>2019YFC1521104</date>
		</imprint>
		<respStmt>
			<orgName>National Key Research and Development Program of China</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label-driven reconstruction for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinliang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Category-level adversarial adaptation for semantic segmentation using purified features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dast: Unsupervised domain adaptation in semantic segmentation based on discriminator attention and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dacs: Domain adaptation via crossdomain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring crossimage pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11939</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Domain adaptation for semantic segmentation via patch-wise contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Zebedin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<idno>arXiv preprint:2104.11056</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with directional context-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation using weak labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Affinity space adaptation for semantic segmentation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajia</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bimal: Bijective maximum likelihood approach to domain adaptation in semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Dat</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><forename type="middle">Lam</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chase</forename><surname>Rainwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
