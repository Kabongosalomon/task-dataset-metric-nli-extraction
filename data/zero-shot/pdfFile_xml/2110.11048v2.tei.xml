<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hee</forename><surname>Paek</surname></persName>
							<email>donghee.paek@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">CCS Graduate School of Mobility</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Hyung</forename><surname>Kong</surname></persName>
							<email>skong@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">CCS Graduate School of Mobility</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kevin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tirta</forename><surname>Wijaya</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Robotics Program Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* equal contribution ? corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection is a critical function for autonomous driving. With the recent development of deep learning and the publication of camera lane datasets and benchmarks, camera lane detection networks (CLDNs) have been remarkably developed. Unfortunately, CLDNs rely on camera images which are often distorted near the vanishing line and prone to poor lighting condition. This is in contrast with Lidar lane detection networks (LLDNs), which can directly extract the lane lines on the bird's eye view (BEV) for motion planning and operate robustly under various lighting conditions. However, LLDNs have not been actively studied, mostly due to the absence of large public lidar lane datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's first and the largest public urban road and highway lane dataset for Lidar. K-Lane has more than 15K frames and contains annotations of up to six lanes under various road and traffic conditions, e.g., occluded roads of multiple occlusion levels, roads at day and night times, merging (converging and diverging) and curved lanes. We also provide baseline networks we term Lidar lane detection networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the spatial characteristics of lane lines on the point cloud, which are sparse, thin, and stretched along the entire ground plane of the point cloud. From experimental results, LLDN-GFC achieves the state-of-the-art performance with an F1score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong performance under various lighting conditions, which is unlike CLDNs, and also robust even in the case of severe occlusions, unlike LLDNs using the conventional CNN. The K-Lane, LLDN-GFC training code, pretrained models, and complete development kits including evaluation, visualization and annotation tools are available at https://github.com/kaist-avelab/k-lane.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous driving depends on a number of critical functions that are realized with state-of-the-art (SOTA) technologies. Among them, lane detection function is to detect the accurate location and curvature of the ego lane and neighboring lanes, and provide necessary input to the path planning function. Therefore, the lane detection function should be robust to various conditions (e.g., night, day times) and challenging situations (e.g., lane line occlusion). However, the conventional lane detection techniques based on image processing are vulnerable to situations when lane lines are partially missing or occluded, because the techniques rely on heuristic methods such as denoising, edge detection, and line fitting with detected edges <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Recently, lane detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref> has been largely improved due to the deep learning. When a large dataset with accurate label is available for training, deep learning networks can produce high-quality predictions that are almost indistinguishable to the ground truths. This is the case for camera lane detection networks (CLDNs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>, which show superior performances compared to the conventional (i.e., heuristic) lane detection techniques when given abundant training samples from public datasets such as CULane <ref type="bibr" target="#b18">[19]</ref> and TuSimple <ref type="bibr" target="#b29">[30]</ref>.</p><p>However, CLDNs still have a few inherent problems. First, cameras suffer from poor lighting conditions, such as low or harsh lights <ref type="bibr" target="#b18">[19]</ref>. Second, it is often necessary to project front camera images into 2-dimensional (2D) bird's eye view (BEV) for motion planning, which often causes lane line distortions <ref type="bibr" target="#b0">[1]</ref>. For example, BEV-projection of the detected lanes near the vanishing line of a front camera image <ref type="bibr" target="#b31">[32]</ref> may result in inaccurate and distorted lane lines so that the motion planning should be limited to a shorter distance.</p><p>On the other hand, Lidar has multiple advantages over the camera in the lane detection; lane detection from a Lidar point cloud does not suffer from the distortion in the BEVprojection and is not affected by lighting conditions. However, there have been a little studies introduced in the literature, mostly because there are not enough public dataset and benchmark for Lidar lane detection network (LLDN).</p><p>In this paper, we introduce KAIST-Lane (K-Lane) dataset, the world's first and the largest open Lidar lane dataset for Lidar lane detection in urban roads and highways. We also provide an easy-to-use development kits (devkits) for the training, evaluation, dataset development, and visualization. K-Lane has more than 15K annotated frames, and contains a maximum of six lanes under various road and traffic conditions, such as roads at night and day times, merging (converging and diverging) and curved lanes as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Each annotation consists of the lane lines segmentation label, driving condition, lane shape, and occlusion level. As such, the performance of developed LLDNs in different challenging conditions can be evaluated easily, e.g., when driving in the night time or with significant measurement loss due to high occlusions. The segmentation label is accurately annotated with one pixel width on the BEV image, which translates to a 4cm ? 4cm area in the real world. The label consists of a class id, which represent the relative position of the lane line to the ego-lane. This enables the LLDNs to be trained directly to infer the location of ego-lane, which is crucial for the motion planning. Moreover, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, front camera images have been elaborately calibrated with Lidar point clouds, enabling intuitive visualization and may pave the way for further lane detection studies using multi-modal sensors (e.g., sensor fusion).</p><p>To demonstrate the viability of developing LLDNs with K-Lane, we propose a baseline model, Lidar lane detection network utilizing global feature correlator (LLDN-GFC), which fully exploits the spatial characteristics of the lane lines in point clouds. This is in contrast to most of the CNNbased LLDNs introduced in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>, which are mostly a modification of the CNN-based CLDNs developed for camera images. We observe that the CNN-based LLDNs are not suitable for detecting lane lines in Lidar point cloud. For example, lane lines on the front view image have decreasing thickness with the distance from the ego vehicle and are heading to the same vanishing point (on a straight road), whereas lane lines in a BEV image have a constant thickness and stretch long in parallel over the entire BEV image. These spatial characteristics of the lane lines in Lidar point cloud are not appropriately exploited by the CNN-based lane detection networks, in contrast to our proposed LLDN-GFC. The proposed LLDN-GFC can be implemented with Transformer <ref type="bibr" target="#b4">[5]</ref> and Mixer <ref type="bibr" target="#b28">[29]</ref> blocks to perform an effective global feature correlation for lane lines. In the experimental results, we show that the proposed baseline achieves a superior performance than LLDNs using the conventional CNN. The contribution of this paper can be summarized as;</p><p>? K-Lane: we introduce the world first and the largest (15382 frames) public Lidar lane dataset for urban roads and highways under various conditions and scenarios. ? We also provide a complete devkits for training, evaluation, annotation, and visualization. ? We show that lane lines in the Lidar point cloud have a special characteristics not found in traditional RGB images, and provide appropriate baseline net- work we term LLDN-GFC, which significantly outperforms LLDNs using the conventional CNN in the F1-score.</p><p>This paper is organized as follows. Section 2 introduces prior studies related to this paper and the topic of this paper, Section 3 introduces K-Lane dataset, and the proposed baseline, LLDN-GFC. Section 4 shows the experiment setup and results. We draw our conclusion in Section 5, and introduce more information for both dataset and baseline, such as detailed network structure of LLDN-GFC in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Lane Detection Datasets and Benchmarks. Lane detection with data-driven approaches such as deep learning has seen tremendous advancements in recent years. One key enabler towards such advancements is the availability of large public lane dataset, as shown in <ref type="table" target="#tab_0">Table 1</ref>. TuSimple <ref type="bibr" target="#b29">[30]</ref> is one of the earliest publicly available camera-based lane dataset. It has 6,408 number of frames collected in the highway during the day. The dataset is further divided into 3,626 frames for training, 358 frames for validation, and 2,782 frames for testing. CULane <ref type="bibr" target="#b18">[19]</ref> introduces a more diverse and challenging camera-based lane dataset, with 133,235 number of frames divided into 88,880 frames for training, 9,675 frames for validation, and 34,680 frames for testing. CULane provides diverse driving conditions, both in urban and highway environments, in the day and night, and with various road structures. Comparing to the vibrant field of camera-based lane detection, Lidar lane detection dataset has not been explored as much. One of the earliest Lidar lane dataset is DeepLane <ref type="bibr" target="#b0">[1]</ref>, which contains 55,168 frames of Lidar and camera data collected in both urban and highway environments. Another dataset, RoadNet <ref type="bibr" target="#b16">[17]</ref>, consists of 5,200 frames of Lidar data collected only in the highway environments. Unfortunatelly, both datasets are not public, as such not many derivative works on Lidar lane detection have been conducted. In contrast, our proposed dataset, K-Lane, contains 15,382 frames of Lidar and camera data, collected in both urban and highway environements. As we make K-Lane public, we pave the way for a new research direction in lane detection approaches using Lidar. Lane Detection Networks for Camera. As labeled cam-era dataset <ref type="bibr" target="#b18">[19]</ref> for various roads become available, there have been a significant advancement in the CLDNs. Compared with the early rule-based techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, CLDNs are more adaptive to various road environments. In these techniques, lanes are predicted based on local features extracted by CNN <ref type="bibr" target="#b7">[8]</ref>, and the performance is improved with lane detection heads that exploit the features of lane lines. For example, Qin et al. <ref type="bibr" target="#b21">[22]</ref> proposes a row-wise detectionbased network that divides the entire image into grids, and recognize lanes from each row of grids. Liu et al. <ref type="bibr" target="#b15">[16]</ref> proposes a two-stage lane detection network that combines the conditional convolution <ref type="bibr" target="#b34">[35]</ref> with the row-wise detection in the detection head and achieves SOTA performance in several datasets. However, CLDNs have some inherent problems. In the CULane benchmark, most of the CLDNs show significant performance drop (about 20%) for night time and dazzling light conditions from their daytime performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. Early Lane Detection Techniques for Lidar. In early studies, lane points are detected by thresholding the measured intensity (or reflectivity). Lindner et al. <ref type="bibr" target="#b14">[15]</ref> uses a fixed polar grid map to store point intensities and filter the lane candidates with thresholding along azimuth angles. Hernandez et al. <ref type="bibr" target="#b8">[9]</ref> introduces a clustering approach, where the filtered lane points are clustered using DBSCAN <ref type="bibr" target="#b5">[6]</ref>. However, these heuristic techniques rely on pre-defined thresholding parameters, and, therefore, it is not very adaptive to different environments. Lane Detection Networks for Lidar. Deep learning-based lane detection studies for Lidar have not been actively conducted due to the absence of large open datasets, and only some studies with their private Lidar datasets are introduced in the literature. Bai et al. <ref type="bibr" target="#b0">[1]</ref> proposes an LLDN that combines 2D BEV images developed with the Lidar point cloud and the front camera image for lane detection. And Martinek et al. <ref type="bibr" target="#b16">[17]</ref> proposes a CNN-based LLDN that uses BEV images from point clouds to detect ego-lanes, and tests the network in an uncrowded highway. Self-Attention in Vision. Self-attention is a scheme to lead a neural network to pay more attention to the patches of the input image, between which there is high correlation score. Convolutional block attention module (CBAM) <ref type="bibr" target="#b32">[33]</ref> introduces per-channel and per-space self-attention mechanisms by adding MLP (Multilayer Perceptron) and convolu- <ref type="bibr">Figure 2</ref>. Data distribution (in number of data frames) of the K-Lane. The pie-chart in the middle shows data distribution in road types (i.e., urban roads or highways), time (i.e., daytimes or night), and usage (i.e., training or test). The four pie-charts on the left (for training) and right (for test) shows data distribution with respect to the six levels of lane occlusion (from zero occluded lanes to six occluded lanes) and that with respect to the lane shape (gentle curve, sharp curve and merging). tional operations, respectively, to the traditional CNN-based feature extractor. Since Transformer <ref type="bibr" target="#b30">[31]</ref> shows significant improvement in the self-attention mechanism by applying three independent MLPs for query, key, and value (i.e., Transformer block), it has been used actively for images and point cloud. As an example, ViT (Vision Transformer) <ref type="bibr" target="#b4">[5]</ref> greatly improves image classification performance using Transformer, where ViT divides input image is into unit patches and applies Transformer encoder to each patch for image classification. However, ViT employs three independent MLPs for each attention mechanism, for which high computational cost and large model size are inevitable. On the other hand, MLP-Mixer <ref type="bibr" target="#b28">[29]</ref> implements the attention mechanism with a simple MLP scheme (i.e., Mixer block), which results in a fast inference with a small model size and achieves comparable performance to ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">K-Lane and LLDN-GFC</head><p>In this section, we introduce K-Lane dataset, benchmark, and the proposed baseline, LLDN-GFC. <ref type="figure" target="#fig_0">Fig. 1</ref>. Data Distribution. As shown in <ref type="figure">Fig. 2</ref>, there are a total of 15382 data frames, divided into 7687 frames for training and 7695 frames for testing. Each set contains various road conditions and challenging scenarios including (a) different lighting conditions such as day and night times, (b) crowded traffic with lane occlusions by other vehicles, and (c) merging (converging, diverging) and curved lanes, which are further classified into gentle curves and sharp curves. Note that K-Lane has maximum six lanes and occlusions are divided into six levels representing 0, 1, 2, 3, and 4?6 occluded lanes. The benchmark kit provides evaluation tools for calculating the metrics per each condition, and given condi-tions are annotated for each frame under a clear criterion, which are described in Appendix A. Sensor Suite. The K-Lane is collected using Ouster OS2-64 Lidar sensor <ref type="bibr" target="#b17">[18]</ref> that has 64 channels with a maximum range of 240m, placed on the roof of the vehicle, and a front camera that has 1920 ? 1200 resolution, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>a. Front camera images have been carefully calibrated with Lidar point clouds, which makes it easy to visualize and may enable further lane detection studies with multi-modal sensors. Dataset Development. The ground truth labels are produced by projecting the Lidar point cloud into BEV, thresholding the intensity measurements to extract keypoints (i.e., candidates of lane lines), and drawing one-pixel-wide line for each lane as shown in <ref type="figure" target="#fig_1">Fig 3-</ref>b. As such, high resolution and accurate labels are produced, which is critical for deep learning-based methods. Metrics. To standardize the evaluation of the network being developed, we choose to use the F1-score metric for both confidence and classification, which evaluates perpixel presence of lane and per-pixel correct classification of the lane, respectively. The F1 metric represents a harmonic mean between precision and recall, and can be expressed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">K-Lane</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-Lane is the first large open LiDAR lane dataset that consists of Lidar point clouds and their corresponding RGB images for urban roads and highways under various conditions and scenarios as shown in</head><formula xml:id="formula_0">F1 = 2 1 Precision + 1 Recall = TP TP + 0.5(FP + FN) ,<label>(1)</label></formula><p>where TP, FP, and FN are the numbers of true positives, false positives, and false negatives, of the output of the detection head, respectively. Since the width of a lane line in the label is only one pixel-wide, we allow up to one pixel deviation between the prediction and the label. This is comparable to the evaluation metric used in the CULane <ref type="bibr" target="#b18">[19]</ref> dataset, where a lane line label is 30-pixels wide and a true positive is counted when the prediction and the ground truth have at least an IoU of 0.5.</p><p>To formally describe the evaluation metric, let x conf ? R M ?N be the confidence map label with M number of rows and N number of columns, with x conf m,n ? {0, 1}. Furthermore, letx conf ? R M ?N be the confidence map predic- Suppose that a thresholding operation is applied to the confidence map prediction such that</p><formula xml:id="formula_1">x conf,thr m,n = 1x conf m,n &gt; ? conf 0 otherwise ,<label>(2)</label></formula><p>where the ? conf is the confidence threshold for a prediction to be considered as a lane point. In our evaluation metric, a true positive occurs if for a positive prediction (pixel value equals to 1) atx conf,thr m,n , there exists at least one positive label at the grid neighborhood centered around x conf m,n . Conversely, if there is no positive label at the grid neighborhood, the prediction counts as a false positive. A false negative occurs if for a positive label at x conf m,n , there is no positive prediction at the grid neighborhood centered aroundx conf,thr m,n . For classification predictions, we transform the classification map label into a one-hot-encoding label. In addition, we also transform the classification prediction map into a one-hot-encoding prediction where the class with the highest probability is assigned a value of 1 and the rest are set as 0. The true positives, false positives, and false negatives of the classification predictions can be calculated with the previously mentioned procedure, accumulated for every possible classes.</p><p>The F1-score on classification is an evaluation of networks based on both lane line localization and lane class prediction. Therefore, F1-score on classification is a strict evaluation metric and, as a result, performance degradation can be found for all models compared to the confidence prediction, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Complete Devkits. Additionally, we provide a comprehensive devkits of K-Lane that includes training, evaluation, dataset development, and visualization. In particular, data development tools, such as labeling and annotation tools, are provided through the Graphic User Interface (GUI) for easy-to-use. This enables the research community to readily increase the dataset regardless of the Lidar sensor models, and thus to activate areas of LLDN with diverse datasets and benchmarks, as well as CLDN. Appendix A presents a full description of all of the specifics. Summary. In summary, compared to the conventional lane detection datasets, K-Lane has multiple advantages; (1) K-Lane is collected in urban roads and highways under various conditions and scenarios as stated above, while TuSimple <ref type="bibr" target="#b29">[30]</ref> and RoadNet <ref type="bibr" target="#b16">[17]</ref> include only highway, (2) K-Lane distinguishes lane classes and labels with precise lane location (pixel level), whereas TuSimple <ref type="bibr" target="#b29">[30]</ref> and DeepLane <ref type="bibr" target="#b0">[1]</ref> have labels without distinction between lane classes (3) K-Lane has larger number of labeled lanes (e.g., maximum 6 lanes), while TuSimple <ref type="bibr" target="#b29">[30]</ref>, and CULane <ref type="bibr" target="#b18">[19]</ref> have only up to 5 and 4 lanes, respectively, and (4) Above all, among Lidar lane datasets, K-Lane is the only publicly available dataset, which allows more studies on Lidar-based lane detection to be conducted. In addition, the well-calibrated camera images may also be used in future works for multimodal lane detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LLDN-GFC</head><p>In this section, we focus on the overall structure and necessity of our baseline for LLDNs while the details, such as the exact neural network structure, functions (i.e., (1)?(5) in <ref type="figure" target="#fig_2">Fig. 4</ref>), and mathematical expression for loss, are described in Appendix B. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the proposed baseline consists of a BEV encoder, a GFC as backbone,  and a lane detection head, which are introduced in the following subsections. BEV Encoder. The BEV encoder projects a 3D point cloud into a 2D pseudo-image and process it further to produce a 2D BEV feature map. We provide two variants of BEV encoder for the LLDN-GFC, namely point projector and pillar encoder.</p><p>The primary BEV encoder is the point projector <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> that projects point clouds into the xy-horizontal plane and produces a BEV feature map using CNN. In order to maintain both high-resolution lane information and real-time speed, we design a ResNet-based CNN to output a feature map that is 1/8 2 of the pseudo-image input.</p><p>An alternative for low computational 2D BEV encoder is the pillar encoder based on Point Pillars that has relatively small network size <ref type="bibr" target="#b12">[13]</ref>. Pillar encoder has slightly lower performance but faster inference speed than the CNN-based point projector. Therefore, in this paper, pillar encoder is presented as an alternative for real-time applications. Details are in Appendix B. GFC as Backbone. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>-a, lane lines on the road are thin, stretched along the entire point cloud, and only occupy a small number of pixels (i.e., sparse). Due to such thinness and sparsity, it is necessary to perform feature extraction in high resolution. In addition, the feature extractor should consider the correlation between distant grids within the BEV feature map. As such, we design our proposed GFC to calculate global correlations of the features in high resolution by utilizing patch-wise self-attention networks. We propose two variants of GFC: GFC-T (the main proposal based on Transformer blocks <ref type="bibr" target="#b4">[5]</ref>) and GFC-M (the low computational alternative based on Mixer blocks <ref type="bibr" target="#b28">[29]</ref>)</p><p>A major advantage of using patch-wise self attention networks is their capability to find correlations between distant grids (or patches) right from the early stages of backbone, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>-c. As such, the high-resolution information can be preserved (i.e., N 0 = N 1 ). This is in contrast with CNN-based feature extractors, which may find correlations between distant grids after several layers of convolutions and down samplings, thus lowering the resolution of information (i.e., N 0 ? N 2 ), as shown in <ref type="figure" target="#fig_3">Fig. 5</ref></p><formula xml:id="formula_2">-b.</formula><p>Quantitatively, we observe that patch-wise self-attention networks have higher performance compared to their CNNbased counterparts <ref type="bibr" target="#b16">[17]</ref>. In addition, we visualize the qualitative results of intermediate feature maps and attention scores in <ref type="figure">Fig</ref> As the number of lane samples are significantly smaller than the number of background samples on each frame, we incorporate the soft dice loss <ref type="bibr" target="#b25">[26]</ref> for the confidence loss that inherently handles the imbalance problem. For the classification head, we choose the grid-wise cross-entropy loss <ref type="bibr" target="#b22">[23]</ref> that has been widely used for multi-class classification problems, leading the network to learn to maximize the probability of the correct lane class during training. The total loss function is the summation of both the soft-dice loss and the cross-entropy loss as expressed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Comparison</head><p>In this section, we provide detailed performance comparisons between LLDN-GFC and conventional CNN-based LLDNs. In addition, we also discuss recent CLDNs for a general comparison to the LLDN-GFC performance. Implementation Details. We evaluate two variants of LLDN-GFC, Proj28-GFC-T3 and Pillars-GFC-M5, which we observe during experiments (i.e., ablations in Appendix C) to have the best accuracy and speed-accuracy tradeoff, respectively. Proj28-GFC-T3 stands for LLDN-GFC with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">LLDN-GFC vs. CNN-based LLDN</head><p>We consider three types of CNN-based backbone to be compared with the proposed GFC, namely: RNF-S13, RNF-D23, and RNF-C13, where (1) RNF represent ResNet <ref type="bibr" target="#b7">[8]</ref> with feature pyramid network (FPN) <ref type="bibr" target="#b13">[14]</ref>, (2) S13, D23, and C13 represent residual blocks implemented with strided convolution, dilated convolution, and CBAM <ref type="bibr" target="#b32">[33]</ref> of 13 or 23 layers, respectively. The model capacities of the counterparts are also determined with experiments (i.e., ablations in Appendix C). The FPN is applied to synthetically consider feature maps of different levels, and the dilated convolution increase the receptive field without loss of resolution, which is utilized in the existing LLDN <ref type="bibr" target="#b16">[17]</ref>. The CBAM performs self-attention mechanism similar to the proposed LLDN-GFC, but applied with per-channel convolution operation, meaning that it does not perform global correlations for all patches as in LLDN-GFC. For this reason, LLDN with RNF-C shows lower performance than the proposed LLDN-GFC, as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>As summarized in <ref type="table" target="#tab_1">Table 2</ref>, the proposed LLDN-GFC shows superior performance than the LLDNs with conventional CNN-based backbone of various depths. In particular, LLDN-GFC shows robust performance against sever occlusions, where four or more lanes are occluded.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">LLDN-GFC Attention Visualization</head><p>In this subsection, we discuss the robustness of the proposed LLDN-GFC, Proj28-GFC-T3, against the occlusion scenario using the visualization of attention score.</p><p>The proposed GFC is based on the self-attention mechanism that utilizes correlations between data units to make the network give more attention to the meaningful region on the feature map. As such, we can see the region that is considered as important by the GFC-T3 by visualizing the attention score prouced by each Transformer blocks, as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>From the visualization, we can see that the network give more attention to the regions which contain lane lines by attenuating the magnitude of non-lane-lines (irrelevant) features. As the layers get deeper, the network expands its region of interests, indicated by the increasing area with high attention scores. We observe that utilizing three blocks of transformer for GFC-T3 is sufficient to ensure the selfattention mechanism to cover the entire region of the point cloud which contains lane lines. Additionally, note that for the query location (the yellow box in <ref type="figure" target="#fig_7">Fig 7)</ref>, the network produce high attention scores to regions in which lane lines are present, even if the query location is occluded. Such phenomena indicates the robustness of LLDN-GFC to occlusions, where predictions are made by considering the entire point cloud such that occluded lane lines can still be estimated accurately. This may not be possible for CNNbased LLDN, which features are recognized through local convolutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">LLDN-GFC vs. Camera Lane Detection</head><p>Most state-of-the-art lane detection networks in the literature are for front-view camera images. This means most CLDNs are trained to detect lane lines in the front-view map. On the other hand, LLDN-GFC is trained to detect lane lines in the BEV map. In addition, the environment in which the data are collected is different. CULane is composed of data mostly for urban roads, while K-Lane consists of data for both urban roads and highways. Since these Lidar and camera datasets do not use the same representations and are collected in different environments, we cannot compare the CLDNs simply using the reported performance in the literature.</p><p>However, recent CLDNs show a significant performance drop for the night time data comparing to the daytime data. For example, CondLaneNet-Large <ref type="bibr" target="#b15">[16]</ref>, LaneATT-Large <ref type="bibr" target="#b27">[28]</ref>, and CurveLane-NAS-L <ref type="bibr" target="#b33">[34]</ref> show 18.67%, 20.93%, and 21.8% drops between daytime and nighttime conditionss, respectively. In contrast, as shown in <ref type="table" target="#tab_0">Table 1</ref>, the proposed LLDN-GFC shows almost no performance degradation (only 0.2% difference). This is because the Lidar is robust to light conditions, which demonstrates that LLDN is a reliable function for autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce K-Lane which, to the best of our knowledge, is the world's first publicly available dataset for Lidar lane detection. K-Lane consists of over 15K high-quality annotated Lidar data in diverse and challenging driving conditions, along with well-callibrated front-view RGB images. The driving conditions include various lighting (daytime and nighttime), lane occlusions (up to 6 occluded lane lines), and road structures (merging, diverging, curved lanes). In addition, we provide the development kits for K-Lane including the annotation, visualization, training, and benchmarking tools. We also introduce a baseline network for Lidar lane detection, which we term LLDN-GFC. LLDN-GFC utilizes self-attention mechanisms to extract lane features via global correlation, and show superior performance compared to the conventional CNN-based LLDNs. In addition, we show the importance of Lidar lane detection networks, where there is only little performance degradation in between detection in the daytime and detection in the nighttime, in contrast to camera-based lane detection networks. As such, we expect this work to pave the way for further studies in the field of Lidar lane detection, and improve the safety aspects in autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide a detailed description of the K-Lane dataset and the development kits (devkits), and detailed structure of proposed LLDN-GFC with its CNN-based counterparts, in Section A, and B, respectively. In addition, Section C shows ablation study for the network hyper-parameters of the proposed LLDN-GFC (i.e., Proj28-GFC-T3), low computational alternative (i.e., Pillars-GFC-M5), and the counterparts. Furthermore, Section D shows qualitative lane detection results for K-Lane, and visualization of both heatmap of features and the attention score. Lastly, Section E shows the comparison between LLDN-GFC and heuristic lane detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. K-Lane and Devkits</head><p>Section A contains technical details that may helps researchers in using the K-Lane datasets and devkits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Details of K-Lane and Devkits</head><p>In this section, we present three additional details about the K-Lane: sequence distributions, compositions, and the criteria of driving conditions annotations of the dataset. Sequence Distribution. K-Lane dataset consists of fifteen sequences that have different set of road conditions. The details of the sequences are shown in <ref type="table" target="#tab_3">Table 3</ref>. For the test data, we provide additional driving conditions annotations on each frame (i.e., curve, occlusion, merging, and number of lanes) with annotation tool shown in Section A.2. Conditions Criteria. To evaluate the LLDN performance depending on data characteristics, we provide 13 different categories of driving conditions as shown in <ref type="table">Table 4</ref>. Examples of each condition are shown <ref type="figure" target="#fig_0">Fig. 1</ref>, and each frame can have two or more conditions, for example, day time and occlusion. Dataset Composition. The K-lane is divided into fifteen directories, each representing a sequence. Each directory has one associated file that describe the driving condition of the frames in the sequence, and contains files for the collected point cloud data, BEV point cloud tensor (i.e., stacked pillars shown in <ref type="figure" target="#fig_0">Fig. 10</ref>), BEV label, front (camera) images, and calibration parameters, as shown in <ref type="table">Table 5</ref>. Pedestrians' faces are blurred on the front images for privacy protection. Interface for pre-processing the files is provided in Section A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details of Development Kits</head><p>In addition to the K-Lane dataset, we also provide the devkits which can be used to expand the dataset, and to develop further LLDNs. The devkits are available to the public in the form of three programs: (1) TPC -Total Pipeline Code for training and evaluation, (2) GLT -Graphic User Interface (GUI)-based Labeling Tools, and (3) GDT -GUI-  based development Tools for evaluation, visualization, and additional conditions annotations. Total Pipeline Code. TPC is a complete neural network development code that supports pre-processing of the input data and label, train the network, and perform evaluation based on the F1-metric. TPC handles input and output as Python dictionaries and support modularization of the LLDN (BEV encoder, GFC, detection head), therefore, providing comprehensive and flexible support to developers. GUI-based Labeling Tools. GLT provides an easy way to develop a labeled dataset for a Lidar and a front view camera, regardless of the Lidar and camera models. As shown in <ref type="figure" target="#fig_8">Fig. 8 (left)</ref>, GLT provides an easy way for labeling by showing the intensity of point cloud in a BEV image. <ref type="figure" target="#fig_8">Fig. 8</ref> (middle) shows a synchronized front camera image for easy labeling of point cloud, and <ref type="figure" target="#fig_8">Fig. 8 (right)</ref> shows the saved labeled point cloud. GUI-based Development Tools. GDT is a GUI program used together with TPC. GDT provides visualization of inference results for each scene as point cloud or camera image with projected lanes ( <ref type="figure" target="#fig_9">Fig. 9-b</ref>), high-accuracy calibration of camera and Lidar sensors with specific points of the lanes <ref type="figure" target="#fig_9">(Fig. 9-c)</ref>, and annotation of each frame with set of buttons ( <ref type="figure" target="#fig_9">Fig. 9-d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of LLDNs</head><p>This section provides a detailed neural network structure of the LLDN-GFC proposed in Section 3.2 of the main paper and its counterparts, CNN-based LLDN.  baseline LLDN-GFC, first shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We divide the LLDN-GFC structure into three parts: the BEV encoder, the global feature corrector (GFC), and the detection head. The functions (1)?(5) of <ref type="figure" target="#fig_2">Fig. 4</ref> are equivalent to the functions (1)?(5) of <ref type="figure" target="#fig_0">Fig. 10?12</ref>. (e.g., (1) of <ref type="figure" target="#fig_2">Fig. 4</ref> is equivalent to (1-1) and (1-2) of <ref type="figure" target="#fig_0">Fig. 10</ref>.) BEV Encoder. BEV encoder projects 3D point cloud into a horizontal plane to produce 2D pseudo-BEV image. A large number of heuristic path planning algorithms, such as A* <ref type="bibr" target="#b6">[7]</ref>, RRT* <ref type="bibr" target="#b9">[10]</ref>, and End-to-End autonomous driving algorithms <ref type="bibr" target="#b1">[2]</ref> require lane lines on 2D BEV images. The proposed LLDN-GFC variants use one of the two most common 2D BEV encoders, as shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. The primary 2D BEV encoder for the LLDN-GFC is the point projector <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> that projects point clouds into xyhorizontal plane and produces pseudo-BEV images using CNN. In this case, three additional information (z, intensity, and reflectivity) other than x and y of the point cloud is used  to generate three channels of the produced pseudo-BEV image. In order not to lose lane information while maintaining the real-time speed, we use only to a depth of the CNN where the feature map becomes the 1/8 2 of the pseudo-BEV image input. To this end, we may use the first 14, 28, and 41 convolutional layers of the ResNet-18, ResNet-34, and ResNet-50 <ref type="bibr" target="#b7">[8]</ref>, respectively. Note that we denote these partial ResNets as ResNet14, ResNet28, and ResNet41 in the ablation studies in Section C, and that ResNet28 is the one used for the proposed LLDN-GFC. An alternative for low computational 2D BEV encoder is the pillar encoder based on Point Pillars <ref type="bibr" target="#b12">[13]</ref> that has relatively small network size. Pillar encoder has slightly lower performance but faster inference speed than the CNN-based point projector. Therefore, in this paper, pillar encoder is presented for real-time applications. As shown in <ref type="figure" target="#fig_0">Fig.  10</ref>, the pillar encoder aligns the point cloud in each grid of the horizontal plane to generate stacked pillars of size N g ? N c ? N p , where N g is the total number of grids, N c is the point feature components, and N p is the maximum number of points present on the grid. Then, a simplified version of PointNet <ref type="bibr" target="#b20">[21]</ref> consisting of shared MLP's of size N c ? C is applied to each grid to extract pseudo-BEV image of size H bev ? W bev ? C. In this paper, considering that a lane in the real-world has a width of about 16cm and stretches long in the longitudinal direction of the road, the grid size in the pseudo-BEV is set to 32cm in the longitudinal direction and 16cm in the lateral direction. Global Feature Correlator. Due to the advantage of patchwise self attention networks (i.e., calculating the correlation in high resolution between distant grids within the feature map) for Lidar lane detection, we utilize two types of patchwise self-attention network for global correlation, ViT <ref type="bibr" target="#b4">[5]</ref> and MLP-Mixer <ref type="bibr" target="#b28">[29]</ref> to propose two possible GFCs, GFC-T (the main proposal) and GFC-M (the computational alternative), respectively. In this section, we provide the structure of those GFCs in detail. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the details of the two types of GFC, GFC-T and GFC-M. Both of the two GFCs employ (2-1), (2-2), (4-1), and (4-2) functions, while GFC-T employs (A) Transformer blocks and, a low computational alternative, GFC- M uses (B) Mixer blocks for global correlation. In <ref type="figure" target="#fig_0">Fig.  11, function (2-1)</ref> reshapes the pseudo-BEV image into a 2D tensor for global correlation. Function (2-2) performs per-patch linear transform, and functions (3-1) and (3-2) perform global correlation through per-channel MLPs (i.e., Multi-head attention or Token-mixing MLP) and per-patch MLPs (i.e., Feed forward or Channel-mixing MLP), respectively. The Transformer encoder block in (A) performs global correlation between image patches using three MLPs calculating query, key, and value and utilizes the global correlation result to pay more attention (i.e., larger attention score) to the important patches to improve the global feature extraction. In addition, the Transformer encoder block allows visualization of the attention score, which can be used for analyzing the network inference, as shown in Section 4.2. However, since the Transformer encoder block becomes a large network for the three MLPs and repeats calculating the attention score for every query (i.e., patch), the total computational cost increases in quadratic with the number of patches. On the other hand, Mixer block in (B) replace the multi-head attention, (3-1) in (A), with a single MLP, (3-1) in (B), which allows smaller network size and lower computational cost but it becomes difficult to analyze the network through attention score and causes lower model inductive bias than the Transformer block. Nonetheless, the two types of GFCs (GFC-T and GFC-M) show strong performance improvement in the Lidar lane detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Details of LLDN-GFC</head><p>Function (4-1) reshapes the last output of Transformer encoder and Mixer block to the size required for the lane detection head. Note that ViT and MLP-Mixer for the conventional image classification compress the detected feature with a classification token and global average pooling, respectively, but the proposed GFC reshapes the size up to H bev ? W bev that is the input size to the function (2-1). This is how the proposed GFC provides inductive bias to the output feature map, which is testified with the visualized heatmap in Section 4.1, where high activation result is obtained in the resolution of pixels (much smaller resolution than patches). Note that the number of total pixels after the reshape becomes H p ? W p times the number of total patches (N patch = H bev /H p ? W bev /W p ) before the reshape, which means that each pixel of the reshaped feature map has N h /(H p ? W p ) dimension as a result. Since the channel size of the reshaped output image depends on the hidden dimension N h , it can be smaller than that of the input BEV image, C bev . This may cause bottleneck <ref type="bibr" target="#b26">[27]</ref>, so function (4-2) applies 1x1 convolution and produces the final output feature map for the detection head.</p><p>Detection Head. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the detection head introduced in Section 3.2. There are two segmentation heads: the classification head and the confidence head. Given an input of H bev ? W bev ? C out feature map from the GFC, we employ two sequential shared-MLPs to create the final prediction maps output. The first shared-MLP expands the dimension of the feature map from C out to 2C out for both classification and confidence heads to increase the representation capacity. The second shared-MLP then transforms the feature maps from 2C out to N cls and from 2C out to 1 for the classification head and confidence head, respectively, resulting in a classification map and confidence map predictions. We then apply a grid-wise softmax to the classification map to get the H bev ? W bev ? N cls classification map output, and a grid-wise sigmoid to the confidence map to get the H bev ?W bev ?1 confidence map output. The classification map shows per-class-probabilities of each grid, while the confidence map only shows the probability of the grid being a lane or not. The implementation of both classification and confidence tasks in parallel enables the LLDN to simultaneously predict the lane shape and the lane class.</p><p>As stated in Section 3.2, we use the soft dice loss <ref type="bibr" target="#b25">[26]</ref>  for supervising the confidence loss L conf , defined as</p><formula xml:id="formula_3">L conf = 1 ? 2 N i M j x conf i,jx conf i,j N i M j x 2 conf i,j + N i M jx 2 conf i,j + ? ,<label>(3)</label></formula><p>where ? is set to be 10 ?12 to prevent division by zero. The grid-wise cross-entropy loss <ref type="bibr" target="#b22">[23]</ref> is used as the classification loss L cls , defined as</p><formula xml:id="formula_4">L cls = 1 N M N i M j log(p(x clsi,j )),<label>(4)</label></formula><p>where p(x clsi,j ) is the softmax of the classification prediction for class k = x clsi,j at grid (i, j), defined as</p><formula xml:id="formula_5">p(x clsi,j ) = exp(x cls i,j,k ) C k ? =1 exp(x cls i,j,k ? ) .<label>(5)</label></formula><p>The grid-wise cross-entropy loss penalizes the network based on the deviation of p(x clsi,j ) from 1, which is equivalent to maximizing the probability of the correct class for each grid on the final classification map output. The total loss function L total is the summation of both classification loss and the confidence loss as</p><formula xml:id="formula_6">L total = L conf + L cls .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Details of CNN-based LLDN</head><p>As we introduce in Section 4.1, we consider three types of CNN-based backbone as the counterparts of GFC, ResNet and FPN (RNF)-based backbone: (1) RNF-S, (2) RNF-D, and (3) RNF-C, where S, D, and C represent residual blocks implemented with strided convolution, dilated convolution, and convolutional block attention module (CBAM) <ref type="bibr" target="#b32">[33]</ref>, respectively. As shown in <ref type="figure" target="#fig_0">Fig. 13</ref>, there are 5 residual blocks composed of 3, 5, 5, 5, and 5 convolutional layers in the ResNet side. Each block produces a feature map that is 2 2 times smaller than the input feature map, and the feature pyramid network (FPN) concatenates the feature maps from each block to produce the final output feature map. <ref type="figure" target="#fig_0">Figure 13</ref>. Overall structure of the conventional CNN-based backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablations</head><p>In this section, we perform several ablation studies on the proposed LLDN-GFC, the low computational alternative, and the the conventional CNN-based LLDNs. Ablations on Network Depth. Since hyperparameters related to the depth of LLDN are BEV encoder depth, the depth of backbone, we provide ablation studies in the following tables <ref type="table">(Table 6?9</ref>) to compare the performance of the LLDN with various BEV encoder, such as Proj14, Proj28, Proj41, and Pillars, and the depth of backbone. Tables in this subsection shows F1-score on the confidence (upper value) and that on the classification (lower value) in each table bin. FPS stands for frame per second, representing the overall computational cost of inference.</p><p>As shown in the <ref type="table" target="#tab_5">Table 7</ref>, when we use Proj28 and increase the depth of GFC-T from GFC-T1 to GFC-T3, the performance increases by +2.3 and +2.3 in F1-scores on the confidence and the classification, respectively. In addition, as shown in the <ref type="table" target="#tab_5">Table 6 and Table 7</ref>, when we use GFC-T3 and varies BEV depth from Proj14 to Proj28, the performance increases by +1.1 and +2.2 in F1-scores on the confidence and the classification, respectively. However, performance degradations are observed when the depth of GFC-T is increased from GFC-T3 to GFC-T5 for Proj28 and when depth of BEV encoder is increased from Proj28 to Proj41 for GFC-T3. From our ablation studies, we find that the model with an appropriate capacity can be Proj28-GFC-T3 for the proposed LLDN-GFC, Pillars-GFC-M5 for the low-computational alternative, and Proj28-RNF-S13, Proj28-RNF-C13, Proj28-RNF-D23 for the LLDNs using the conventional CNN-based backbones.</p><p>Note that for models with larger capacities, some regularization methods or more sophisticated learning techniques may be applied to reduce overfitting. However, those learning techniques are out of the scope of our study, since we focus on the network architecture and dataset. Ablations on Hidden Dimension. As shown in the <ref type="table" target="#tab_0">Table  10</ref> and 11, we perform ablation studies on different hidden dimension size for Proj28-GFC-T3 and Pillars-GFC-M5, which are the best performing model of the proposed LLDN-GFC and its low computational alternative, respectively. As denoted in Section B.1, the hidden dimension N h is the number of channels for each patch after the per-patch linear transform, indicating that higher value of hidden dimension leads to higher model capacity per each grid. <ref type="table" target="#tab_0">Table 10</ref> shows the performance for various hidden dimension N h of Proj28-GFC-T3; N h =512 outperforms other variants, such as N h =128 and N h =2048. On the other hand, since Pillars-GFC-M5 requires more model capacity per each grid than Proj-GFC-T, Pillars-GFC-M with N h =2048 outperforms that with N h =512. Ablations on Patch Size. We also perform ablation studies on the patch size of the Proj28-GFC-T3 and Pillars-GFC- M5. The results in <ref type="table" target="#tab_0">Table 10</ref> and 11 show that there is a significant performance drop as P is increased from 8 to 16. This is because when the patch size is increased to 16 (from 8), the number of grids covered by a patch increases four times, so that the GFC has to extract global features from a map with four times lower resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results Visualization</head><p>In addition to the numerical results in Section 4.1, we provide qualitative results of the proposed LLDN-GFC, Proj28-GFC-T3, its low computational alternative, Pillars-GFC-M5, and the conventional CNN-based LLDNs, such as Proj28-RNF-S13, Proj28-RNF-C13, and Pillars-RNF-C13, using visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 13?16 in this subsection has 4 rows and 5 columns,</head><p>where each row shows inference results for different scenes (conditions) and each column shows inference results for different GFCs. Each inference result has upper and lower plots for the projection of inference results into the front view image with true labels in the upper left corner and for the inference on top of the BEV point cloud, respectively. <ref type="figure" target="#fig_0">Fig. 13 and 14</ref> show inference results of LLDNs with Proj28 for scenes with moderate (e.g., normal, no occlusion, and gentle curve) and severe (e.g., occlusion, merging, and sharp curve) lane detection difficulties, respectively, while </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Qualitative Heatmaps</head><p>We emphasize the performance of the proposed GFC, GFC-T, and its low computational alternative, GFC-M, using the visualization of the heatmaps for occlusion scenes as shown in <ref type="figure" target="#fig_5">Fig. 6</ref> in Section 4.1. In addition to the results in <ref type="figure" target="#fig_5">Fig. 6</ref>, we provide more visualization of heatmaps for various difficult scenes, such as curved lanes, merging lanes, and other severe occlusion cases, to emphasize the superior performance of the proposed GFC and its lowcomputational alternative GFC.</p><p>All of the figures in this subsection follow the same format used for <ref type="figure" target="#fig_5">Fig. 6</ref>   <ref type="table" target="#tab_0">Table 11</ref>. Performance of Pillars-GFC-M5 for various hidden dimension and patch sizes; where P and N h represent the patch size and the hidden dimension size with default value 8 and 512, respectively.      inference results for (a) Proj28-GFC-T3, (b) Proj28-GFC-M3, (c) Proj28-RNF-C13, and (d) Proj28-RNF-S13. The first row shows the projection of inference results into the front view image with true labels in the upper left corner, and the second row shows the inference on top of the BEV point cloud. From the 3rd to 5th row, we show the heatmap of the 1st, 2nd, and 3rd block output feature map of the GFC (e.g., 1st, 2nd, and 3rd Transformer block of Proj28-GFC-T3). Output feature maps at different blocks are resized or reshaped (i.e., in the same way to the function (4-1) in <ref type="figure" target="#fig_0">Fig.  11</ref>) and two heatmaps are sampled along the channels. As shown in <ref type="figure" target="#fig_0">Fig. 17, Fig. 18, and Fig. 19</ref>, both the proposed LLDN-GFC, Proj28-GFC-T3, and the LLDN with the low computational alternative GFC, GFC-M3, demonstrate three advantages described in Section 4.1 for curved </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. LLDN vs Heuristic Method</head><p>In the heuristic Lidar lane detection techniques, we first project pointcloud into a BEV image and apply thresholding to remove low-intensity points <ref type="bibr" target="#b8">[9]</ref>. The remaining points are then clustered using, for example, DBSCAN <ref type="bibr" target="#b5">[6]</ref> and then fitted by the first order polynomial to create smooth lane lines.</p><p>In the experiments, we observe multiple instances when the heuristic technique is unreliable; First, when a strong light illuminates a spot on the ground, as shown in <ref type="figure">Fig</ref>  not infer the presence of lane marks, leading to a high false negatives (FNs), as shown in <ref type="figure" target="#fig_22">Fig. 20 (d)</ref>. However, the proposed LLDN-GFC can produce reliable lane detection results for the two scenarios. As the LLDN-GFC learns global context features of the scene, a bright illuminated road spot or partial occlusion of lane lines hardly degrade the lane detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of frames under various conditions for K-Lane, where each column shows one of the conditions: Each column consists of a total of three rows. Each row shows an upper plot for the projection of true BEV labels into the front view image with true BEV labels in the upper left corner and a lower plot for the lane labels on top of the BEV point cloud, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Sensor suite, vehicle setup, and dataset development process of K-Lane. tion with M number of rows and N number of columns, with x conf m,n ? [0, 1]. Additionally, we define a grid neighborhood centered at the grid x m,n as a set of grids {x i,j |i = {m ? 1, m, m + 1}, j = {n ? 1, n, n + 1}}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Overall structure of the LLDN-GFC. There are five functions: (1), (2), (3), (4), and (5) indicate the BEV encoder, reshape &amp; per-patch linear transform, Transformer or Mixer block, reshape &amp; shared MLP, and detection head, respectively. H bev , W bev , C bev , Cout, N 2 p , and Cp, are the height, width, num. of channel of the pseudo-BEV image, num. of channel of the output pseudo-BEV image, the num. of total patches, and the num. of channel per patch in the global correlation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>A comparison of global feature correlation between CNN and the proposed GFC, where N0, N1, N2 and C0, C1, C2 represent the size and the number of channels of the feature maps at three layers in depth order. (a) an example of two separated grids to calculate the global correlation, (b) the two grids contained in a feature map developed by CNN, and (c) correlation of the two grids in a feature map developed by GFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. 6 and 7, respectively. Both quantitative and qualitative results further indicate the aptness of using patch-wise self-attention networks for Lidar lane detection even on a relatively small number of data (i.e. 7687 training frames). Detection Head and Loss Function. To design the detection head, we formulize the lane detection problem as a multi-class segmentation problem, where each pixel is assigned a class and a confidence score. The multi-class segmentation formulation enables the detection head to predict both lane classes and various lane shapes, which are important for motion planning where the ego vehicle need to plan inter-lane motions or recognize lane merging and separation. The LLDN-GFC detection head consists of two segmentation heads, each of which consists of a sequence of two-layer shared-MLP with a non-linear activation inbetween.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>show qualitative assessment of the robustness of LLDN-GFC based on the visualization of intermediate feature maps. We can observe on the heatmaps that both Proj28-GFC-T3 (a) and Proj28-GFC-M3 (b) clearly extract lanes with better resolution, especially on the deeper layers. This is in contrast with CNN-based LLDN, shown in (c) and (d), where the lanes tend to blur. In other words, the lane features extracted by Proj28-GFC-T3 and Proj28-GFC-M3 are more distinctive to the backgrounds compared to the lane features extracted by CNN-based LLDNs. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of the lane detection performance between the LLDN-GFC and CNN-based LLDNs for occluded lanes condition. The four columns are inference results of (a) Proj28-GFC-T3, (b) Proj28-GFC-M3, (c) Proj28-RNF-C13, and (d) Proj28-RNF-S13. The first row shows the projection of inference results onto the front view image with labels in the upper left corner, while the second row shows the inference on the BEV point cloud. From the third to fifth row, we show the heatmaps sampled along the channels of the 1st, 2nd, and 3rd block output feature map of the backbones (e.g., 1st , 2nd, and 3rd Transformer blocks of GFC or residual blocks of RNF). Heatmaps for various scenarios such as curved and merging lane lines are introduced in Appendix D. addition, even in the presence of occlusions, Proj28-GFC-T3 and Proj28-GFC-M3 are capable of predicting the lane shapes through correlations with the non-occluded lanes, which are not observed in the CNN-based LLDNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Attention score visualization of Proj28-GFC-T3. (aupper) shows the projection of inference results onto the front view image with labels in the upper left corner, while (a-lower) shows the inference result and the current query patch (yellow box) on top of the BEV point cloud. (b) to (d) show the point cloud in BEV, lane inference results, query patch, labels in cyan, and attention score in purple for block 1, 2, and 3 of the GFC, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>GUI-based Labeling Tool (GLT): (a) Overall components of GLT, (b) Labeling process of a point cloud, (c) Finalizing and saving the label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>GUI-based Development Tools (GDT): (a) overall components of GDT and loading a data, (b) visualization of the LLDN inference results, (c) calibrating Lidar with camera (d) annotating a frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Detail structure of two BEV encoder: Point Projector and Pillar Encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Details of proposed Global Feature Correlators; the input size is expressed with height H bev , width W bev , and number of channels C, and a patch size has height Hp and width Wp. The input and output size in Mixer block and Transformer encoder block is the same and the block repeats ND times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Detection head of the proposed LLDN-GFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15</head><label>15</label><figDesc>and 16 show inference results of LLDNs with Pillars. In all figures shown in this subsection, LLDNs based on GFC-T and GFC-M (shown in (a) and (b) of figures, respectively) show better performance than other LLDNs (in (c), (d), and (e) of figures) regardless of the lane detection difficulties and BEV encoders (i.e., Proj28 and Pillars). For example, plots in (a) and (b) show a strong lane detection performance even for images of severe occlusion, where a good portion of point cloud data are missing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Lane detection performance comparison for LLDNs with Proj28 for images with moderate difficulty (e.g., normal, no occlusion, and gentle curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 .</head><label>14</label><figDesc>Lane detection performance comparison for LLDNs with Proj28 for images with high difficulty (e.g., occlusion, merging, and sharp curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 .</head><label>15</label><figDesc>Lane detection performance comparison for LLDNs with Pillars for images with moderate difficulty (e.g., normal, no occlusion, and gentle curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 .</head><label>16</label><figDesc>Lane detection performance comparison for LLDNs with Pillars for images with high difficulty (e.g., occlusion, merging, and sharp curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 .</head><label>17</label><figDesc>Comparison of the lane detection performance of the proposed LLDN-GFC, Proj28-GFC-T3, to the Proj28-GFC-T3 and other CNN-based LLDNs (Proj28-RNF-C13 and Proj28-RNF-S13) for curved lanes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 .</head><label>18</label><figDesc>Comparison of the lane detection performance of the proposed LLDN-GFC, Proj28-GFC-T3, to the Proj28-GFC-M3 and other CNN-based LLDNs (Proj28-RNF-C13 and Proj28-RNF-S13) for merging lanes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 .</head><label>19</label><figDesc>Comparison of the lane detection performance of the proposed LLDN-GFC, Proj28-GFC-T3, to the Proj28-GFC-M3 and other CNN-based LLDNs (Proj28-RNF-C13 and Proj28-RNF-S13) for occluded lanes. lanes, merging lanes, and other occluded lanes. The three advantages are (1) better resolution as the network deepens, (2) distinctive color difference between lane and non-lane positions, and (3) predicting the shape of the lane even in presence of occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>. 20 (b), it results in false positives (FPs). Second, when lane marks are occluded, the heuristic Lidar lane detection can-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 20 .</head><label>20</label><figDesc>Comparison between the proposed LLDN-GFC (Proj28-GFC-T3) (in (a) and (c)) and heuristic Lidar lane detection (in (b) and (d)). When a strong source of illumination appears on the scene, (b) the heuristic method fails, but (a) the proposed LLDN-GFC is not affected. When lane marks are occluded, (d) the heuristic method cannot infer the lanes, but (c) the proposed LLDN-GFC is able to infer the occluded lane lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of previous lane datasets and ours (n/a denotes not available).</figDesc><table><row><cell>Dataset</cell><cell>Year</cell><cell>Sensor</cell><cell cols="4">Num. Lanes Class Info. Num. Frames Public</cell><cell>Road Type</cell></row><row><cell cols="2">TuSimple [30] 2017</cell><cell>Camera</cell><cell>5</cell><cell>No</cell><cell>6408</cell><cell>Yes</cell><cell>Highway</cell></row><row><cell>CULane [19]</cell><cell>2018</cell><cell>Camera</cell><cell>4</cell><cell>Yes</cell><cell>133235</cell><cell>Yes</cell><cell>Urban &amp; Highway</cell></row><row><cell>DeepLane [1]</cell><cell cols="2">2019 Lidar, Camera</cell><cell>n/a</cell><cell>No</cell><cell>55168</cell><cell>No</cell><cell>Urban &amp; Highway</cell></row><row><cell>RoadNet [17]</cell><cell>2020</cell><cell>Lidar</cell><cell>n/a</cell><cell>Yes</cell><cell>5200</cell><cell>No</cell><cell>Highway</cell></row><row><cell cols="3">K-Lane (ours) 2022 Lidar, Camera</cell><cell>6</cell><cell>Yes</cell><cell>15382</cell><cell>Yes</cell><cell>Urban &amp; Highway</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>/81.1 82.2/81.4 82.0/80.7 81.7/80.6 82.5/81.7 78.0/76.7 82.9/81.9 75.9/75.5 11.6 GFC-M3 79.7/78.8 79.9/79.0 79.6/78.4 78.9/77.7 80.8/80.0 74.6/72.6 80.4/79.4 72.5/71.7 13.3 RNF-S13 73.2/70.5 72.6/70.1 74.0/71.0 73.1/70.4 73.3/70.6 70.5/68.1 74.9/72.3 63.5/59.0 13.1 RNF-C13 78.0/75.3 77.6/75.1 78.5/75.5 77.7/74.8 78.3/76.01 76.0/73.1 79.6/77.0 69.3/65.3 13.0 RNF-D23 72.1/68.8 71.3/68.3 73.0/69.4 71.9/68.7 72.3/69.0 69.6/66.5 74.0/70.7 61.9/57.6 12.7 F1-score of confidence/classification for the proposed LLDN-GFC and various CNN-based LLDNs. Enc, Shp, Occ stands for BEV Encoder, sharp curve, and occlusion cases, respectively. We show no occlusion and severe occlusion (4?6 lanes occluded) cases, while other occlusion levels are presented in Appendix C. FPS stands for frame per second, which represents the overall computational cost (FLOPs, data efficiency, etc.) of the networks during inference, similar to throughput in<ref type="bibr" target="#b28">[29]</ref>. Note that we only show F1-score of confidence for the heuristic technique. point projector encoder with 28 layers and GFC with three Transformer blocks. Pillars-GFC-M5 stands for LLDN-GFC with pillar encoder and GFC with five Mixer blocks.We use RTX3090 GPUs for training the networks on the K-Lane for 60 epochs using Adam optimizer<ref type="bibr" target="#b10">[11]</ref> with a batch size 4 and a learning rate of 0.0002. All training and evaluations are implemented with PyTorch 1.7.1<ref type="bibr" target="#b19">[20]</ref> on an Ubuntu 18.04 machine.</figDesc><table><row><cell>Enc</cell><cell>Backbone</cell><cell>Total</cell><cell>Day</cell><cell>Night</cell><cell>Urban</cell><cell>Highway</cell><cell>Shp Curve</cell><cell>No Occ</cell><cell>Occ4?6</cell><cell>FPS</cell></row><row><cell cols="6">Proj-28 82.1Pillars GFC-T3 GFC-T5 78.5/77.3 78.5/77.6 78.4/77.0 77.8/76.4 GFC-M5 74.8/73.5 74.8/73.6 74.9/73.4 72.0/70.5 RNF-S13 64.6/18.2 62.9/16.4 66.5/20.4 59.4/15.6</cell><cell>79.2/78.4 78.2/77.1 70.7/21.4</cell><cell>72.5/70.2 64.6/62.2 51.1/13.1</cell><cell cols="3">79.4/78.2 70.2/69.5 13.8 75.5/74.2 65.2/62.3 16.3 65.5/19.2 44.9/4.7 15.7</cell></row><row><cell></cell><cell cols="5">RNF-C13 76.8/40.6 75.9/39.1 77.8/42.4 74.5/40.6</cell><cell>79.6/40.6</cell><cell>67.6/32.5</cell><cell cols="3">77.9/43.6 62.5/20.4 15.5</cell></row><row><cell></cell><cell cols="5">RNF-D23 63.2/19.0 61.6/17.0 65.0/21.2 58.6/17.2</cell><cell>68.6/21.1</cell><cell>51.1/13.5</cell><cell>64.2/20.2</cell><cell>43.9/4.9</cell><cell>15.2</cell></row><row><cell></cell><cell>Heuristic</cell><cell>26.4</cell><cell>26.8</cell><cell>26.0</cell><cell>23.3</cell><cell>29.6</cell><cell>27.6</cell><cell>28.1</cell><cell>16.7</cell><cell>18.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Sequences in K-Lane.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This section describes the sub-structure of the proposed</figDesc><table><row><cell>Conditions</cell><cell></cell><cell></cell><cell>Explanation</cell><cell>Num. Frames</cell></row><row><cell>Urban</cell><cell cols="3">Data acquired from city or university</cell><cell>8607</cell></row><row><cell>Highway</cell><cell cols="3">Data acquired on Highway</cell><cell>6775</cell></row><row><cell>Night</cell><cell cols="3">Data acquired at night (approximately 20:00-2:00)</cell><cell>7139</cell></row><row><cell>Daytime</cell><cell cols="3">Data acquired during the daytime (about 12:00-16:00)</cell><cell>8243</cell></row><row><cell>Normal</cell><cell cols="4">Data without curved or merging lanes (mostly straight lanes)</cell><cell>11065</cell></row><row><cell>Gentle Curve</cell><cell cols="4">Data with curved lanes whose radius of curvature is greater than 160 [m]</cell><cell>1804</cell></row><row><cell>Sharp Curve</cell><cell cols="4">Data with curved lanes whose radius of curvature is less than 160 [m]</cell><cell>1431</cell></row><row><cell>Merging</cell><cell cols="4">Data with a converging or diverging lane at the rightmost or leftmost lane</cell><cell>982</cell></row><row><cell cols="4">No Occlusion Data without occluded lanes based on the lane label</cell><cell>9443</cell></row><row><cell>Occlusion 1</cell><cell cols="3">Data with one occluded lane based on the lane label</cell><cell>2660</cell></row><row><cell>Occlusion 2</cell><cell cols="3">Data with two occluded lanes based on the lane label</cell><cell>2112</cell></row><row><cell>Occlusion 3</cell><cell cols="3">Data with three occluded lanes based on the lane label</cell><cell>793</cell></row><row><cell></cell><cell cols="4">Data with four to six occluded lanes based on the lane label;</cell></row><row><cell>Occlusion 4-6</cell><cell cols="4">Since there are few samples of data with five or six occluded lanes,</cell><cell>374</cell></row><row><cell></cell><cell cols="4">they are integrated as a single condition (i.e., four to six occluded lanes).</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Table 4. Condition details</cell></row><row><cell>Datum Type</cell><cell></cell><cell>Extension</cell><cell>Format</cell><cell>Comment</cell></row><row><cell>Point cloud</cell><cell></cell><cell>.pcd</cell><cell>Point cloud with 131072 points</cell><cell>Input to point projector and heuristic technique</cell></row><row><cell cols="2">BEV point cloud tensor</cell><cell>.pickle</cell><cell>Ng ? Nc ? Np size array</cell><cell>Input to pillar encoder</cell></row><row><cell>BEV label</cell><cell></cell><cell>.pickle</cell><cell>H</cell></row></table><note>bev ? (W bev + 6) size array Table 5. Dataset Composition</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Proj28-based LLDN performance for backbones with various depth.</figDesc><table><row><cell>Back-bone</cell><cell cols="3">Total Day Night</cell><cell>Ur-ban</cell><cell>High-way</cell><cell>Nor-mal</cell><cell>Gentle Curve</cell><cell>Sharp Curve</cell><cell>Mer-ging</cell><cell>No Occ</cell><cell>Occ 1</cell><cell>Occ 2</cell><cell>Occ 3</cell><cell>Occ 4-6</cell><cell>FPS</cell></row><row><cell>GFC -T1</cell><cell>77.5 76.1</cell><cell>77.3 76.1</cell><cell>77.8 76.1</cell><cell>76.1 74.6</cell><cell>79.1 77.9</cell><cell>78.2 76.9</cell><cell>78.1 76.6</cell><cell>70.3 68.4</cell><cell cols="6">77.3 78.5 76.9 76.5 73.1 69.8 12.7 76.5 77.1 74.9 75.7 73.1 69.0</cell></row><row><cell>GFC -T3</cell><cell>81.0 80.1</cell><cell>81.0 80.4</cell><cell>80.9 79.7</cell><cell>80.1 79.0</cell><cell>82.0 81.3</cell><cell>82.0 81.0</cell><cell>81.7 81.0</cell><cell>75.2 74.0</cell><cell cols="6">79.8 81.8 80.5 80.1 77.2 75.7 12.6 79.1 80.8 79.2 79.8 76.4 75.4</cell></row><row><cell>GFC -M1</cell><cell>74.8 73.4</cell><cell>74.7 73.6</cell><cell>74.9 73.3</cell><cell>72.9 71.2</cell><cell>77.1 76.1</cell><cell>74.2 74.2</cell><cell>77.0 75.7</cell><cell>66.1 63.7</cell><cell cols="6">73.1 75.5 74.5 75.2 70.6 65.0 16.1 72.2 74.1 72.2 74.8 69.3 64.1</cell></row><row><cell>GFC -M3</cell><cell>78.9 77.8</cell><cell>79.0 78.0</cell><cell>78.9 77.6</cell><cell>77.5 76.2</cell><cell>80.6 79.8</cell><cell>79.5 78.4</cell><cell>80.4 79.4</cell><cell>72.2 70.5</cell><cell cols="6">77.6 79.8 78.3 78.5 74.7 70.1 15.4 76.7 78.7 76.8 77.8 73.8 69.4</cell></row><row><cell>RNF -S8</cell><cell>74.6 58.0</cell><cell>73.3 58.0</cell><cell>76.0 58.0</cell><cell>73.2 58.8</cell><cell>76.2 57.1</cell><cell>74.8 58.3</cell><cell>76.1 57.3</cell><cell>69.6 54.9</cell><cell cols="6">75.8 76.5 73.9 71.7 66.3 61.8 16.5 62.0 60.8 55.0 54.2 50.3 42.6</cell></row><row><cell>RNF -C8</cell><cell>77.7 63.7</cell><cell>76.4 62.5</cell><cell>79.3 65.0</cell><cell>76.3 62.6</cell><cell>79.4 65.0</cell><cell>78.0 63.9</cell><cell>79.6 64.7</cell><cell>72.2 58.1</cell><cell cols="6">78.0 79.5 77.0 75.0 70.7 66.9 15.5 65.9 66.3 60.9 59.6 56.7 48.6</cell></row><row><cell>RNF -D8</cell><cell>74.8 55.4</cell><cell>73.4 55.5</cell><cell>76.5 55.4</cell><cell>73.1 56.1</cell><cell>77.0 54.6</cell><cell>74.8 55.5</cell><cell>77.6 55.3</cell><cell>70.1 52.7</cell><cell cols="6">75.9 76.7 74.6 72.2 65.9 62.0 15.3 60.7 57.7 53.1 53.0 47.4 42.1</cell></row><row><cell>RNF -S13</cell><cell>67.4 62.0</cell><cell>65.9 60.9</cell><cell>69.2 63.3</cell><cell>66.4 61.4</cell><cell>68.7 62.7</cell><cell>67.3 61.8</cell><cell>69.6 64.1</cell><cell>63.9 59.1</cell><cell cols="6">69.6 69.2 67.0 64.3 59.6 56.8 15.0 65.6 63.9 61.0 59.0 55.2 50.2</cell></row><row><cell>RNF -C13</cell><cell>78.0 69.2</cell><cell>77.1 69.2</cell><cell>79.0 69.3</cell><cell>77.1 68.5</cell><cell>79.2 70.2</cell><cell>78.4 69.7</cell><cell>79.2 70.4</cell><cell>72.2 62.7</cell><cell cols="6">78.7 79.5 77.4 76.1 71.6 66.6 14.9 70.6 71.2 67.1 67.4 63.0 54.6</cell></row><row><cell>RNF -D13</cell><cell>76.9 60.4</cell><cell>75.7 60.2</cell><cell>78.2 60.7</cell><cell>75.5 61.7</cell><cell>78.5 58.9</cell><cell>77.0 60.8</cell><cell>79.0 59.7</cell><cell>71.5 56.21</cell><cell cols="6">77.8 78.7 76.2 74.3 69.1 63.6 14.8 65.5 62.9 57.6 57.1 53.8 46.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="10">Table 6. Proj14-based LLDN performance for backbones with various depth.</cell><cell></cell></row><row><cell>Back-bone</cell><cell cols="3">Total Day Night</cell><cell>Ur-ban</cell><cell>High-way</cell><cell>Nor-mal</cell><cell>Gentle Curve</cell><cell>Sharp Curve</cell><cell>Mer-ging</cell><cell>No Occ</cell><cell>Occ 1</cell><cell>Occ 2</cell><cell>Occ 3</cell><cell>Occ 4-6</cell><cell>FPS</cell></row><row><cell>GFC -T1</cell><cell>79.8 78.8</cell><cell>79.6 78.9</cell><cell>80.0 78.7</cell><cell>79.4 78.3</cell><cell>80.3 79.4</cell><cell>80.2 79.3</cell><cell>80.6 75.2</cell><cell>75.2 73.5</cell><cell cols="4">79.4 80.8 79.1 78.7 78.4 79.8 77.7 78.2</cell><cell>74.9 73.9</cell><cell>72.8 11.8 72.5</cell></row><row><cell>GFC -T3</cell><cell>82.1 81.1</cell><cell>82.2 81.4</cell><cell>82.0 80.7</cell><cell>81.7 80.6</cell><cell>82.5 81.7</cell><cell>82.5 81.5</cell><cell>82.2 83.0</cell><cell>78.0 76.7</cell><cell cols="4">81.0 82.9 81.4 82.3 80.1 81.9 81.4 81.3</cell><cell>78.7 78.7</cell><cell>75.9 11.6 75.5</cell></row><row><cell>GFC -T5</cell><cell>81.1 79.5</cell><cell>81.0 79.5</cell><cell>81.2 79.4</cell><cell>80.6 78.7</cell><cell>81.7 80.4</cell><cell>82.0 80.0</cell><cell>82.3 80.7</cell><cell>76.0 73.0</cell><cell cols="4">80.0 82.1 80.5 79.6 78.8 80.3 78.5 78.6</cell><cell>77.3 75.3</cell><cell>77.2 11.2 76.2</cell></row><row><cell>GFC -M1</cell><cell>78.5 77.3</cell><cell>78.5 77.6</cell><cell>78.4 77.0</cell><cell>77.8 76.4</cell><cell>79.3 78.4</cell><cell>78.9 77.8</cell><cell>80.0 79.1</cell><cell>72.5 70.2</cell><cell cols="4">78.0 79.4 77.8 77.7 76.9 78.2 77.8 77.7</cell><cell>74.5 74.5</cell><cell>70.2 13.4 69.5</cell></row><row><cell>GFC -M3</cell><cell>79.7 78.8</cell><cell>79.9 79.0</cell><cell>79.6 78.4</cell><cell>78.9 77.7</cell><cell>80.8 80.0</cell><cell>80.1 79.2</cell><cell>81.3 80.4</cell><cell>74.6 72.6</cell><cell cols="4">79.0 80.4 79.6 79.4 78.1 79.4 78.3 78.9</cell><cell>76.1 74.9</cell><cell>72.5 13.3 71.7</cell></row><row><cell>GFC -M5</cell><cell>78.7 79.2</cell><cell>77.3 79.5</cell><cell>78.8 78.9</cell><cell>78.0 78.4</cell><cell>79.6 80.1</cell><cell>79.0 79.0</cell><cell>80.5 81.1</cell><cell>73.5 73.6</cell><cell cols="4">77.6 79.6 78.2 78.1 78.4 80.1 78.6 78.6</cell><cell>74.7 75.3</cell><cell>69.9 13.1 71.4</cell></row><row><cell>RNF -S8</cell><cell>74.6 63.0</cell><cell>73.9 62.8</cell><cell>75.4 63.3</cell><cell>74.4 63.4</cell><cell>74.9 62.6</cell><cell>74.9 63.5</cell><cell>75.3 62.7</cell><cell>70.5 57.6</cell><cell cols="4">76.0 76.5 73.5 71.8 66.7 65.3 60.3 60.9</cell><cell>66.9 54.7</cell><cell>64.8 13.2 49.8</cell></row><row><cell>RNF -C8</cell><cell>78.1 70.3</cell><cell>77.3 69.7</cell><cell>79.1 71.0</cell><cell>77.6 69.8</cell><cell>78.7 70.9</cell><cell>78.2 70.4</cell><cell>79.7 71.6</cell><cell>74.9 66.6</cell><cell cols="4">79.0 79.7 77.3 76.1 72.0 72.2 68.0 69.0</cell><cell>71.5 62.8</cell><cell>68.6 13.1 58.5</cell></row><row><cell>RNF -S13</cell><cell>73.2 70.5</cell><cell>72.6 70.1</cell><cell>74.0 71.0</cell><cell>73.1 70.4</cell><cell>73.3 70.6</cell><cell>73.3 70.4</cell><cell>74.0 71.9</cell><cell>70.5 68.1</cell><cell cols="4">74.8 74.9 72.2 70.9 72.5 72.3 68.4 69.0</cell><cell>65.7 63.3</cell><cell>63.5 13.1 59.0</cell></row><row><cell>RNF -C13</cell><cell>78.0 75.3</cell><cell>77.6 75.1</cell><cell>78.5 75.5</cell><cell>77.7 74.8</cell><cell>78.3 76.0</cell><cell>77.9 75.0</cell><cell>80.0 77.9</cell><cell>76.0 73.1</cell><cell cols="4">78.9 79.6 76.9 76.0 76.5 77.0 73.1 74.1</cell><cell>71.9 69.2</cell><cell>69.3 13.0 65.3</cell></row><row><cell>RNF -D13</cell><cell>69.5 65.5</cell><cell>68.4 64.9</cell><cell>70.8 66.2</cell><cell>69.5 65.6</cell><cell>69.6 65.3</cell><cell>69.6 65.5</cell><cell>70.1 65.8</cell><cell>67.3 62.9</cell><cell cols="4">72.1 71.6 68.5 66.2 68.7 67.6 63.1 63.0</cell><cell>61.5 58.5</cell><cell>58.5 13.1 54.6</cell></row><row><cell>RNF -D23</cell><cell>72.1 68.8</cell><cell>71.3 68.3</cell><cell>73.0 69.4</cell><cell>71.9 68.7</cell><cell>72.3 69.0</cell><cell>72.2 68.8</cell><cell>72.9. 69.8</cell><cell>69.6 66.5</cell><cell cols="6">74.0 74.0 70.9 69.5 63.8. 61.9 12.7 71.8 70.7 66.8 67.2 61.1 57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Proj41-based LLDN performance for backbones with various depth.</figDesc><table><row><cell>Back-bone</cell><cell cols="3">Total Day Night</cell><cell>Ur-ban</cell><cell>High-way</cell><cell>Nor-mal</cell><cell>Gentle Curve</cell><cell>Sharp Curve</cell><cell>Mer-ging</cell><cell>No Occ</cell><cell>Occ 1</cell><cell>Occ 2</cell><cell>Occ 3</cell><cell>Occ 4-6</cell><cell>FPS</cell></row><row><cell>GFC -T1</cell><cell>77.8 76.4</cell><cell>77.7 76.7</cell><cell>77.9 76.1</cell><cell>77.5 76.2</cell><cell>78.2 76.6</cell><cell>78.3 77.0</cell><cell>78.5 76.9</cell><cell>72.3 70.4</cell><cell cols="5">78.0 78.9 76.7 76.5 74.2 77.2 77.4 75.0 75.6 73.0</cell><cell>70.0 69.7</cell><cell>11.0</cell></row><row><cell>GFC -T3</cell><cell>80.5 79.1</cell><cell>80.6 79.4</cell><cell>80.5 78.8</cell><cell>80.4 79.1</cell><cell>80.7 79.1</cell><cell>81.2 79.8</cell><cell>80.9 79.4</cell><cell>75.2 73.4</cell><cell cols="5">79.3 81.4 79.8 79.4 77.3 78.3 79.9 78.2 78.5 75.9</cell><cell>75.0 74.5</cell><cell>10.9</cell></row><row><cell>GFC -M1</cell><cell>78.2 77.1</cell><cell>78.3 77.4</cell><cell>78.1 76.7</cell><cell>77.0 75.7</cell><cell>79.7 78.9</cell><cell>78.8 77.8</cell><cell>79.6 78.4</cell><cell>71.5 69.4</cell><cell cols="5">77.2 79.0 77.6 78.1 73.9 76.3 77.9 77.5 77.5 72.7</cell><cell>71.2 70.6</cell><cell>11.7</cell></row><row><cell>GFC -M3</cell><cell>79.9 78.8</cell><cell>80.1 79.2</cell><cell>79.7 78.4</cell><cell>79.2 77.8</cell><cell>80.9 80.1</cell><cell>80.5 79.4</cell><cell>80.6 72.5</cell><cell>74.9 72.5</cell><cell cols="5">79.0 80.9 79.0 79.5 76.1 77.9 79.7 77.4 79.0 75.0</cell><cell>72.2 71.2</cell><cell>11.6</cell></row><row><cell>GFC -M5</cell><cell>79.7 78.8</cell><cell>79.8 79.0</cell><cell>79.7 78.6</cell><cell>79.0 77.9</cell><cell>80.7 79.9</cell><cell>80.1 79.2</cell><cell>81.1 80.3</cell><cell>74.6 72.7</cell><cell cols="5">79.6 80.6 79.4 79.1 75.7 78.6 79.6 78.3 78.7 74.8</cell><cell>70.6 69.9</cell><cell>11.4</cell></row><row><cell>RNF -S8</cell><cell>75.1 61.8</cell><cell>73.8 61.7</cell><cell>76.5 62.0</cell><cell>74.4 62.8</cell><cell>75.8 60.7</cell><cell>75.0 61.9</cell><cell>76.3 61.3</cell><cell>72.3 59.5</cell><cell cols="5">77.1 77.0 74.6 71.8 66.8 67.0 64.0 59.5 59.4 54.2</cell><cell>63.6 48.6</cell><cell>11.1</cell></row><row><cell>RNF -C8</cell><cell>76.0 69.4</cell><cell>75.0 68.7</cell><cell>77.1 70.1</cell><cell>75.1 68.1</cell><cell>77.1 70.9</cell><cell>76.0 69.3</cell><cell>77.5 71.2</cell><cell>72.4 65.7</cell><cell cols="5">77.8 77.8 74.9 73.7 68.1 71.7 71.4 66.8 67.5 62.5</cell><cell>65.9 56.9</cell><cell>10.7</cell></row><row><cell>RNF -D8</cell><cell>72.2 65.9</cell><cell>70.6 64.7</cell><cell>74.0 67.2</cell><cell>71.4 65.3</cell><cell>73.1 66.6</cell><cell>72.0 65.5</cell><cell>73.8 67.7</cell><cell>70.3 63.8</cell><cell cols="5">74.5 74.3 71.3 68.7 63.3 69.3 68.1 63.8 63.1 57.6</cell><cell>58.9 51.5</cell><cell>10.9</cell></row><row><cell>RNF -S13</cell><cell>69.8 66.3</cell><cell>68.8 65.5</cell><cell>70.9 67.1</cell><cell>69.7 66.3</cell><cell>69.9 66.2</cell><cell>70.0 65.1</cell><cell>70.7 66.5</cell><cell>67.1 63.8</cell><cell cols="5">72.4 71.7 69.1 66.6 61.7 69.6 68.5 64.2 63.8 58.5</cell><cell>57.6 51.4</cell><cell>10.8</cell></row><row><cell>RNF -C13</cell><cell>77.6 73.9</cell><cell>76.8 73.4</cell><cell>78.5 74.5</cell><cell>76.7 73.0</cell><cell>78.6 75.0</cell><cell>78.0 74.0</cell><cell>79.3 75.9</cell><cell>73.9 70.2</cell><cell cols="6">78.1 79.2 76.8 75.5 70.5 75.1 75.8 71.7 72.3 66.9 61.9. 67.6</cell><cell>10.5</cell></row><row><cell>RNF -D13</cell><cell>69.9 65.4</cell><cell>68.7 64.5</cell><cell>71.3 66.3</cell><cell>69.5 65.3</cell><cell>70.4 65.5</cell><cell>69.8 79.2</cell><cell>70.9 80.3</cell><cell>67.6 63.4</cell><cell cols="5">72.7 71.8 69.1 66.6 62.7 69.4 67.3 63.5 63.2 58.8</cell><cell>59.2 52.8</cell><cell>10.8</cell></row><row><cell>RNF -S23</cell><cell>69.9 66.4</cell><cell>68.7 65.6</cell><cell>71.4 67.3</cell><cell>69.6 66.0</cell><cell>70.3 66.9</cell><cell>70.0 66.0</cell><cell>70.7 67.8</cell><cell>67.2 63.0</cell><cell cols="5">72.1 71.9 69.1 66.7 62.0 69.1 68.5 64.3 63.6 60.0</cell><cell>58.7 54.6</cell><cell>10.3</cell></row><row><cell>RNF -D23</cell><cell>70.1 66.1</cell><cell>69.7 65.1</cell><cell>72.7 67.2</cell><cell>70.4 65.4</cell><cell>70.6 67.3</cell><cell>70.1 66.1</cell><cell>71.2 67.3</cell><cell>67.1 63.4</cell><cell cols="5">72.9 72.3 70.4 66.9 62.5 70.4 68.9 64.2 63.3 59.2</cell><cell>59.3 52.6</cell><cell>10.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>in Section 4.1. The four columns are Pillars-based LLDN performance for backbones with various depth. Performance of Proj28-GFC-T3 for various hidden dimension and patch sizes, where P and N h represent the patch size and the hidden dimension size with default value 8 and 512, respectively.</figDesc><table><row><cell>Back-bone</cell><cell cols="3">Total Day Night</cell><cell>Ur-ban</cell><cell>High-way</cell><cell>Nor-mal</cell><cell>Gentle Curve</cell><cell>Sharp Curve</cell><cell>Mer-ging</cell><cell>No Occ</cell><cell>Occ 1</cell><cell>Occ 2</cell><cell>Occ 3</cell><cell>Occ 4-6</cell><cell>FPS</cell></row><row><cell>GFC -T1</cell><cell>64.3 62.4</cell><cell>63.7 61.9</cell><cell>64.9 62.9</cell><cell>61.0 58.9</cell><cell>68.2 68.2</cell><cell>65.3 63.5</cell><cell>66.2 64.5</cell><cell>51.4 48.5</cell><cell cols="6">64.6 64.9 63.2 65.6 59.8 56.1 14.0 63.3 63.1 60.3 64.4 58.1 54.4</cell></row><row><cell>GFC -T3</cell><cell>76.2 74.7</cell><cell>76.2 75.0</cell><cell>76.1 74.4</cell><cell>73.6 72.0</cell><cell>79.2 78.1</cell><cell>76.8 75.4</cell><cell>78.7 77.3</cell><cell>67.0 64.8</cell><cell cols="6">75.0 76.7 75.5 77.1 72.2 69.2 13.9 74.0 75.3 73.1 76.5 71.0 68.6</cell></row><row><cell>GFC -T5</cell><cell>78.5 77.3</cell><cell>78.5 77.6</cell><cell>78.4 77.0</cell><cell>77.8 76.4</cell><cell>79.2 78.4</cell><cell>77.9 77.6</cell><cell>79.0 77.8</cell><cell>72.5 70.2</cell><cell cols="6">78.0 79.4 77.8 77.7 74.5 70.2 13.8 76.9 78.2 76.3 77.2 73.2 69.5</cell></row><row><cell>GFC -M1</cell><cell>64.5 62.9</cell><cell>63.7 62.1</cell><cell>65.5 63.8</cell><cell>59.1 57.2</cell><cell>71.0 69.7</cell><cell>65.3 63.8</cell><cell>69.8 68.4</cell><cell>50.6 47.7</cell><cell cols="6">59.9 65.1 63.9 66.7 60.0 50.4 16.6 59.0 63.6 61.3 65.6 58.4 48.7</cell></row><row><cell>GFC -M3</cell><cell>70.0 60.6</cell><cell>69.8 61.2</cell><cell>70.1 59.8</cell><cell>66.0 56.8</cell><cell>74.7 65.1</cell><cell>71.0 62.0</cell><cell>73.7 63.6</cell><cell>56.7 44.3</cell><cell cols="6">66.4 70.6 69.4 72.0 64.6 57.5 16.4 57.8 61.6 58.1 63.0 56.3 42.9</cell></row><row><cell>GFC -M5</cell><cell>74.8 73.5</cell><cell>74.8 73.6</cell><cell>74.9 73.4</cell><cell>72.0 70.5</cell><cell>78.2 77.1</cell><cell>75.6 74.4</cell><cell>77.9 76.6</cell><cell>64.6 62.2</cell><cell cols="6">72.0 75.5 74.4 76.0 69.3 65.2 16.3 71.1 74.2 72.3 75.5 67.6 62.3</cell></row><row><cell>RNF -C8</cell><cell>70.1 22.1</cell><cell>69.3 22.3</cell><cell>71.1 21.9</cell><cell>67.6 25.1</cell><cell>73.2 18.5</cell><cell>69.9 23.2</cell><cell>75.7 19.1</cell><cell>62.9 17.4</cell><cell cols="6">70.2 71.5 70.9 69.0 61.9 50.9 15.9 23.1 25.8 16.9 16.6 12.7 8.5</cell></row><row><cell>RNF -S13</cell><cell>64.6 18.2</cell><cell>62.9 16.4</cell><cell>66.5 20.4</cell><cell>59.4 15.6</cell><cell>70.7 21.4</cell><cell>51.1 13.1</cell><cell>72.1 22.6</cell><cell>51.2 13.1</cell><cell cols="6">63.4 65.5 65.9 65.0 56.9 44.9 15.7 16.0 19.2 18.7 16.8 16.0 4.7</cell></row><row><cell>RNF -C13</cell><cell>76.8 40.6</cell><cell>75.9 39.1</cell><cell>77.8 42.4</cell><cell>74.5 40.6</cell><cell>79.6 40.6</cell><cell>67.6 32.5</cell><cell>81.4 43.6</cell><cell>67.6 32.5</cell><cell cols="6">76.6 77.9 77.9 75.7 69.5 62.5 15.5 42.6 43.6 38.3 35.7 32.8 20.4</cell></row><row><cell>RNF -D13</cell><cell>61.4 16.5</cell><cell>60.0 14.8</cell><cell>62.9 18.4</cell><cell>56.8 13.5</cell><cell>66.8 20.0</cell><cell>61.0 15.8</cell><cell>69.5 22.6</cell><cell>51.8 12.2</cell><cell cols="6">60.0 62.3 62.7 61.2 53.8 42.6 15.4 13.9 17.4 16.7 14.5 14.2 4.5</cell></row><row><cell>RNF -S23</cell><cell>58.5 31.9</cell><cell>56.9 29.9</cell><cell>60.4 34.2</cell><cell>52.6 28.4</cell><cell>65.6 36.1</cell><cell>59.2 32.0</cell><cell>64.9 42.2</cell><cell>42.2 23.2</cell><cell cols="6">55.8 59.1 59.1 59.9 51.5 43.6 15.3 30.2 32.6 32.1 32.0 28.5 17.3</cell></row><row><cell>RNF -D23</cell><cell>63.2 19.0</cell><cell>61.6 17.0</cell><cell>65.0 21.2</cell><cell>58.6 17.2</cell><cell>68.6 21.1</cell><cell>51.1 13.5</cell><cell>70.7 23.7</cell><cell>51.3 13.5</cell><cell cols="6">62.2 64.2 64.3 63.1 55.1 43.9 15.2 17.2 20.2 20.2 19.1 16.7 4.9</cell></row><row><cell>Back-bone</cell><cell cols="3">Total Day Night</cell><cell>Ur-ban</cell><cell>High-way</cell><cell>Nor-mal</cell><cell>Gen. Cur.</cell><cell>Sha. Cur.</cell><cell>Mer-ging</cell><cell>No Occ</cell><cell>Occ 1</cell><cell>Occ 2</cell><cell>Occ 3</cell><cell>Occ 4-6</cell><cell>FPS</cell></row><row><cell>P 8 N h 512</cell><cell>82.1 81.1</cell><cell>82.2 81.4</cell><cell>82.0 80.7</cell><cell>81.7 80.6</cell><cell>82.5 81.7</cell><cell>82.5 81.5</cell><cell cols="8">82.2 78.0 81.0 82.9 81.4 82.3 78.7 75.9 11.6 83.0 76.7 80.1 81.9 81.4 81.3 78.7 75.5</cell></row><row><cell>P 16 N h 512</cell><cell>80.2 78.1</cell><cell>79.3 77.6</cell><cell>81.2 78.7</cell><cell>78.4 75.8</cell><cell>82.4 80.9</cell><cell>81.0 79.0</cell><cell cols="8">82.4 73.3 78.9 80.7 80.5 79.7 76.6 75.6 11.9 80.5 70.4 76.9 78.5 77.7 78.5 75.1 74.6</cell></row><row><cell>P 8 N h 128</cell><cell>81.5 75.3</cell><cell>81.5 75.7</cell><cell>81.5 74.9</cell><cell>81.1 74.5</cell><cell>82.0 76.4</cell><cell>81.9 76.2</cell><cell cols="8">82.8 76.9 80.2 82.5 81.2 80.6 76.0 74.3 11.8 76.6 66.2 74.6 76.3 73.9 75.5 70.3 68</cell></row><row><cell>P 8 N h 2048</cell><cell>76.6 61.2</cell><cell>75.9 60.4</cell><cell>77.4 62.1</cell><cell>75.5 59.6</cell><cell>77.8 63.1</cell><cell>77.4 62.3</cell><cell cols="8">78.3 67.3 76.1 77.8 76.7 74.9 71.3 64.7 10.7 61.9 50.2 62.6 63.1 58.5 60.1 55.5 44.9</cell></row><row><cell>Back-bone</cell><cell cols="3">Total Day Night</cell><cell>Ur-ban</cell><cell>High-way</cell><cell>Nor-mal</cell><cell>Gen. Cur.</cell><cell>Sha. Cur.</cell><cell>Mer-ging</cell><cell>No Occ</cell><cell>Occ 1</cell><cell>Occ 2</cell><cell>Occ 3</cell><cell>Occ 4-6</cell><cell>FPS</cell></row><row><cell>P 8 N h 512</cell><cell>74.8 73.5</cell><cell>74.8 73.6</cell><cell>74.9 73.4</cell><cell>72.0 70.5</cell><cell>78.2 77.1</cell><cell>77.9 74.4</cell><cell cols="8">75.6 64.6 72.0 75.5 74.4 76.0 69.3 65.2 16.3 76.6 62.2 71.1 74.2 72.3 75.5 67.6 62.3</cell></row><row><cell>P 16 N h 512</cell><cell>72.2 65.2</cell><cell>72.0 65.2</cell><cell>72.4 65.2</cell><cell>68..8 61.8</cell><cell>76.2 69.3</cell><cell>67.0 73.0</cell><cell cols="8">75.9 58.4 70.2 72.6 71.5 73.4 68.9 63.4 16.6 67.2 49.5 64 66.2 62.8 66.8 60.5 55.8</cell></row><row><cell>P 8 N h 128</cell><cell>70.9 63.1</cell><cell>70.9 62.7</cell><cell>71.0 63.4</cell><cell>67.7 59.4</cell><cell>74.8 67.4</cell><cell>71.7 64.1</cell><cell cols="8">74.6 59.4 68.4 71.6 70.2 72.5 65.7 60.2 16.5 66.0 49.2 61.7 64.1 60.7 65.4 56.7 48.5</cell></row><row><cell>P 8 N h 2048</cell><cell>75.6 74.1</cell><cell>75.5 74.4</cell><cell>75.6 73.8</cell><cell>72.9 71.2</cell><cell>78.8 77.6</cell><cell>76.4 75.0</cell><cell cols="8">78.8 64.5 73.5 76.2 74.8 76.6 71.2 66.5 14.7 77.2 62.6 72.6 74.8 72.5 76.1 69.7 68.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shrinidhi Kowshika Lakshmikanth, and Raquel Urtasun. Deep multi-sensor lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gellert</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3102" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning by cheating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno>PMLR, 2020. 10</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Double lane line edge detection method based on constraint conditions hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganlu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking for lane departure warning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 International Conference on Computational Problem-Solving (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">kdd</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A formal basis for the heuristic determination of minimum cost paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">J</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raphael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Systems Science and Cybernetics</title>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lane surface identification based on reflectance using laser range finder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">Caceres</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van-Dung</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang-Hyun</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/SICE International Symposium on System Integration</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sampling-based algorithms for optimal motion planning. The international journal of robotics research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Frazzoli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="846" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-channel lidar processing for lane detection and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerd</forename><surname>Wanielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyokazu</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Isogai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Condlanenet: a top-to-down lane detection framework based on conditional convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lidar-based deep neural network for reference lane generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Martinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gheorghe</forename><surname>Pucea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udhayaraj</forename><surname>Sivalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ouster</surname></persName>
		</author>
		<idno>2020. Accessed: 2021-09-08. 4</idno>
		<ptr target="https://ouster.com/products/os2-lidar-sensor/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ultra fast structureaware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="276" to="291" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIV 16</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single camera lane detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alefs</forename><surname>Bram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Clabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE Intelligent Transportation Systems</title>
		<meeting>2005 IEEE Intelligent Transportation Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="302" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Complex-yolo: An euler-region-proposal for real-time 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Milzy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Amendey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Jorge</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Keep your eyes on the lane: Real-time attention-guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tusimple</forename><surname>Tusimple</surname></persName>
		</author>
		<idno>2017. Ac- cessed: 2021-09-08. 1</idno>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark" />
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic parking based on a bird&apos;s eye view vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengrun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunzhao</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Mechanical Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">847406</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lanesensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV 16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch? Buc, Edward A. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="1305" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
