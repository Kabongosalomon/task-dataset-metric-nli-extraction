<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Binary Representation for Event-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Undri</forename><surname>Innocenti</surname></persName>
							<email>simone.undri@unifi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Florence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Becattini</surname></persName>
							<email>federico.becattini@unifi.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Florence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pernici</surname></persName>
							<email>federico.pernici@unifi.it</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Florence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
							<email>alberto.delbimbo@unifi.it</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Florence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Binary Representation for Event-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present an event aggregation strategy to convert the output of an event camera into frames processable by traditional Computer Vision algorithms. The proposed method first generates sequences of intermediate binary representations, which are then losslessly transformed into a compact format by simply applying a binary-to-decimal conversion. This strategy allows us to encode temporal information directly into pixel values, which are then interpreted by deep learning models. We apply our strategy, called Temporal Binary Representation, to the task of Gesture Recognition, obtaining state of the art results on the popular DVS128 Gesture Dataset. To underline the effectiveness of the proposed method compared to existing ones, we also collect an extension of the dataset under more challenging conditions on which to perform experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action Recognition has gained increasing importance in recent years, due to applications in several fields of research such as surveillance, human computer interaction, healthcare and automotive. Despite the significant steps forward made since the diffusion of deep learning, there are still challenges yet to be solved. Certain applications, for instance, have extremely high time constraints. This is the case when recognition must be performed from fast moving vehicles (e.g. drones or cars), or when the pattern to be recognized is extremely fast (e.g. eye glimpses). Indeed, standard RGB cameras might even fail to capture a rich enough signal to enable recognition due to low frame-rates and motion blur.</p><p>These limitations of RGB cameras have been addressed with event cameras. Event cameras, also known as neuromorphic cameras, are sensors that capture illumination changes, producing asynchronous events independently for each pixel. These sensors have several desirable properties such as high dynamic range, low latency, low power consumption, absence of motion blur and, last but not least, they operate at extremely high frequencies, generating events at a ?s temporal scale. The output of an event camera therefore is highly different from the one of a regular RGB camera, making the applicability of computer vision algorithms not so straightforward. In particular, Deep Learning methods such as Convolutional Neural Networks (CNN), work with frames of synchronous data. Asynchronous events need to be aggregated into synchronous frames to be fed to a CNN.</p><p>Several event aggregation strategies have been proposed in literature, allowing the usage of frame-based algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. These techniques however approximate the signal by quantizing time into aggregation intervals, yielding to a loss of information. The aggregation time can be lowered to limit this phenomena, but this will result in an extremely high number of frames to be processed, making real-time analysis prohibitive.</p><p>In this paper we present an event aggregation strategy named Temporal Binary Representation (TBR). Compared to existing strategies, TBR generates compact representations without losing information up to an arbitrarily small quantization time. In fact, we first aggregate events to generate intermediate binary representations with small quantization times and then losslessly combine sequences of intermediate representations into a single frame. This allows us to lower the amount of data to be processed while retaining information at fine temporal scales. TBR is specifically tailored for fast moving actions or gestures and can be directly used for training and evaluating standard CNNs. Indeed, we exploit two models based on Alexnet+LSTM and Inception 3D for action recognition, reporting state of the art results on the IBM DVS128 Gesture Dataset <ref type="bibr" target="#b5">[6]</ref>. Furthermore, we highlight the benefits of the proposed strategy by collecting an extension of the dataset in more challenging scenarios, namely higher execution speed, multiple scales, camera pose and background clutter.</p><p>To summarize, the main contributions of this paper are the following:</p><p>? We propose a compact representation of event data dubbed Temporal Binary Representation, exploiting a conversion of binary event sequences into frames that encode both spatial and temporal information. ? Our formulation allows to tune information loss and memory footprint, making it suitable for real-time applications. <ref type="bibr">?</ref> We collected an extension of the popular DVS128 Gesture Dataset under challenging conditions, which we plan to release upon publication.</p><p>The paper is organized as follows: in Sec. II a literature review is reported to frame the work in the current state of the art; in Sec. III our Temporal Binary Representation is presented; in Sec. IV we provide an overview of the models used for classifying gestures; in Sec. V we present the dataset used for evaluating our approach and introduce the additional benchmark that we have collected; in Sec. VI we discuss the training details; in Sec. VII and VIII we report the results of our approach; finally in Sec. IX we draw the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Action and Gesture Recognition</head><p>Several formulations have been adopted in literature for the task of action recognition. Early works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have treated it as a classification task, while more recent works have provided a finer level of detail adding a temporal dimension (action detection) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> or spatial information (action localization) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Action detection aims at recognizing actions and determining their starting and ending points in untrimmed videos. These approaches are often based on temporal proposals <ref type="bibr" target="#b10">[11]</ref>, i.e. a set of frame intervals that are likely to contain a generic action, which are then classified or refined <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>. This concept has been extended in the spatio-temporal action localization formulation, where the temporal boundaries of the action still need to be determined, but at the same time the actor needs to be precisely localized in each frame, as in an object detection task. The output of such systems is a spatiotemporal tube <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, i.e. a list of temporally adjacent bounding boxes enclosing the action.</p><p>Several works have been focusing on a specific subset of actions, referred to as gestures. Gestures can be divided into the three categories of body, hand and head gestures <ref type="bibr" target="#b21">[22]</ref>. The interest in gestures often stems from the need to establish some form of interaction between humans and machines, which indeed can happen interpreting human behaviors <ref type="bibr" target="#b22">[23]</ref>. To reduce the reaction time to observed gestures, sensors with high frame-rate have been exploited <ref type="bibr" target="#b23">[24]</ref>. Of particular interest is the usage of event cameras, which have been largely used for gesture recognition in the recent years <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Some approaches rely on architectures specifically tailored to handle event data, such as spiking neural networks, which however require specialized hardware to be implemented <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Most approaches, however, in order to exploit traditional computer vision algorithms, adopt an event aggregation strategy that allows the conversion of streams of asynchronous events into a set of synchronous frames. Most of these approaches, though, perform a temporal quantization in the form of histograms <ref type="bibr" target="#b2">[3]</ref> or event subsampling <ref type="bibr" target="#b25">[26]</ref>. To avoid information loss, the bins into which events are quantized can be shrinked, with the side effect of generating a large amount of data that has to be processed. Differently from these works, we propose an aggregation strategy that is lossless up to an arbitrarily small time interval. Our proposed approach in fact compacts several representations in a single frame, allowing to generate less data without discarding information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVENT REPRESENTATION</head><p>Events generated by an event camera are temporally and spatially localized respectively by a timestamp t and pixel coordinates x and y. Each event is also associated to a polarity p ? {?1, +1}, indicating the sign of the pixel illumination change. The output of an event camera is therefore a stream of tuples E = (t, x, y, p). To make events interpretable by standard Computer Vision algorithms, they must be aggregated into frames. In general, an aggregation algorithm is a function that maps asynchronous events into a a stream of synchronous frames. Each generated frame f i aggregates all the events in the interval [t i ; t i + ?t] spanning from an initial timestamp t i and covering a temporal extent ?t, known as accumulation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal Binary Representation</head><p>Given a fixed ?t, we build an intermediate binary representation b i by simply checking the presence or absence of an event for each pixel during the accumulation time. The value in position (x, y) is obtained as b i</p><p>x,y = 1(x, y), where 1(x, y) is an indicator function returning 1 if an event is present in position (x, y) and 0 otherwise.</p><p>We then consider N temporally consecutive binary representations by stacking them together into a tensor B ? R H?W ?N . Each pixel can be considered as a binary string of N digits</p><formula xml:id="formula_0">[b 0 x,y b 1 x,y ... b N ?1 x,y ]</formula><p>with the most significant digit corresponding to the most recent event. We then convert into a decimal number each binary string, as shown in <ref type="figure" target="#fig_0">Fig.  1</ref>. This procedure allows us to compact the representation of N consecutive accumulation times into a single frame without any loss of information. The frame is then normalized in [0, 1], dividing its values by N . We refer to this event representation as Temporal Binary Representation (TBR).</p><p>Compared to standard event aggregation strategies that generate a single frame for each ?t, TBR reduces the memory footprint by a factor of N . This also leads to less data to be processed by Computer Vision algorithms, enabling timeconstrained applications. At the same time, the accumulation time can be significantly reduced to capture events at finer temporal scales, without increasing the total number of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL</head><p>We adopt our Temporal Binary Representation for event camera data to the task of Action Recognition. To process frames, we use two different architectures. First, we combine a Convolutional Neural Network to extract spatial features with a Recurrent Neural Network to process sequences of frames. As a feature extractor, we train an AlexNet <ref type="bibr" target="#b31">[32]</ref>, replacing the final fully connected layers using a single layer with 512 neurons. The output for each frame in the sequence is directly fed to a Long Short Term Memory (LSTM) with 2 layers with hidden dimension 256 each. Finally, a fully connected layer with softmax activation performs the classification.</p><p>The second model that we adopt is the Inception 3D model <ref type="bibr" target="#b32">[33]</ref>, a state of the art architecture widely adopted with RGB data for action recognition. Based on Inception-V1 <ref type="bibr" target="#b33">[34]</ref>, the model relies on inflated 3D convolutions by adding a third dimension to filters and pooling kernels to learn spatiotemporal feature extractors. The model originally has two separate streams for RGB and Optical Flow data. Here we simply remove one branch and retrain the model with event camera data aggregated with TBR.</p><p>To process videos, we follow two different approaches, depending on the network. For the AlexNet+LSTM model we simply feed the whole sequence of frames to the model and collect the final output. With Inception 3D instead, we use as input non-overlapping blocks of F frames stacked together, which are independently evaluated. To provide a final classification for the whole video, we adopt a majority-voting strategy among predictions for each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASETS</head><p>We train our model on the the IBM DVS128 Gesture Dataset <ref type="bibr" target="#b5">[6]</ref>. The dataset contains a total of 1342 hand gestures with a variable duration spanning from approximately 2 to 18 seconds (6 seconds on average). Gestures are divided in 10 classes plus an additional random class for unknown gestures. Each of these actions are performed by 29 subjects under different illumination conditions (natural, fluorescent and led lights). The data is acquired using a DVS128 camera, i.e. an event camera with a sensor size of 128 ? 128 pixels <ref type="bibr" target="#b34">[35]</ref>.</p><p>We follow the split proposed by the authors, comprising 23 subjects for training and 6 for validation.</p><p>To increase the variability of the DVS128 Gesture Dataset we recorded an additional test benchmark using a Prophesee GEN 3S VGA-CD event camera 1 . The camera has a sensor with a higher resolution of 640 ? 480 pixels (VGA). The recorded actions still belong to the 11 classes of the DVS128 dataset but are performed under more challenging conditions. In particular, the actors were asked to perform the actions at different speeds, in order to demonstrate the capacity of event cameras to capture high speed movements. In addition the actions have been recorded at different scales and camera orientations and also under uneven illumination which is likely to cast shadows on the body and the surroundings, generating spurious events. The dataset was recorded by 7 different actors of different age, height and gender for a total of 231 videos. All the videos are used for testing, still using the DVS128 Gesture Dataset as training set. We refer to the newly collected data as the MICC-Event Gesture Dataset, which will be released upon publication. In <ref type="figure" target="#fig_1">Fig. 2</ref> a few samples from the dataset are shown, highlighting the different execution speeds, scales and orientations at which actions are recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. TRAINING</head><p>We train the models using the SGD optimizer with momentum. We use a learning rate equal to 0.01, which is then decreased to 0.001 after 25 epochs. As loss we adopt the Binary Cross-Entropy Loss, regularized with weight decay. Overall, the training of Inception 3D took 13 hours on an NVIDIA Titan Xp, while AlexNet+LSTM required approximately 30 hours.</p><p>For the DVS128 Gesture Dataset, to make the frames compatible with the input layer of the models, we apply a zeropadding up to 227?227 for AlexNet+LSTM and 224?224 for Inception 3D. For the MICC-Event Gesture Dataset instead, which is recorded with the higher resolution of 640 ? 480, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head><p>In Tab. I we report the results on the DVS128 Gesture Dataset for the two models AlexNet+LSTM and Inception 3D, trained with frames generated by our Temporal Binary Representation. The results are compared with state of the art approaches. Following prior work, we report the classification accuracy both including and excluding the Other Gesture class, respectively referred to as "10 classes" and "11 classes".</p><p>In our models, events are aggregated with the proposed Temporal Binary Encoding, stacking N = 8 binary representations with an accumulation time ?t = 2.5ms. Therefore, we use an 8 bit representation for each pixel, covering 20 ms with each frame. It is important to notice that this allows the model to observe events without any loss of information up to a precision of 2.5 ms, even if a single frame stores data covering an 8 times bigger time interval. Since the Inception 3D model takes as input chunks of videos as a tensor of temporally stacked frames, we feed to the model chunks of 500 ms, i.e. chunks of 25 frames encoded with TBR. With classic event aggregation strategies that use the same ?t of 2.5 ms, this would lead to 200 frames per chunk, increasing considerably the computational burden.</p><p>Overall, the Inception 3D model achieves the best results, reporting approximately a 2% improvement respect to AlexNet+LSTM. Interestingly, both our architectures are capable to obtain a perfect classification of the Other Gesture class, making the accuracy in the 11 classes settings higher than the 11 classes one. This behavior is the opposite compared to the baselines that adopt the 10 classes setting, which consistently lowers the accuracy.</p><p>To better assess the benefits of adopting our Temporal Binary Representation, we report results on the MICC-Event Gesture Dataset. We use the whole dataset for testing the Inception 3D model, which is trained on DVS128. To provide a comparison with other approaches, we have trained 2 baseline  variants using event aggregation strategies from the literature: Polarity <ref type="bibr" target="#b0">[1]</ref> and Surface of Active Events <ref type="bibr" target="#b35">[36]</ref>. The Polarity [1] approach simply assigns a different value to events with different polarities. Therefore, the final representation is an image I p , where each pixel (x, y) is given by:</p><formula xml:id="formula_1">I p (x, y) = ? ? ? ? ? 0,</formula><p>if event polarity is negative 0.5, if no events happen in ?t 1, if event polarity is positive</p><p>If multiple events are detected in the accumulation time, the most recent one is considered. The Surface of Active Events (SAE) <ref type="bibr" target="#b35">[36]</ref> instead, for each pixel measures the time between the last observed event and the beginning of the accumulation time t 0 . The values are then normalized between 0 and 255, similarly to TBR with 8 bits. Polarity is discarded. The representation I SAE is obtained as:</p><formula xml:id="formula_3">I SAE (x, y) = 255 ? tp ? t0 ?t<label>(2)</label></formula><p>Samples using TBR, Polarity and SAE are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>In Tab. II we show the results obtained by Inception 3D trained with the three different aggregation strategies. All three strategies are used with an accumulation time ?t = 2.5ms. We also report the results on the original DVS128 Gesture Dataset test set obtained by our model with the baseline aggregation strategies. Interestingly, on DVS218 the three variants still obtain higher performances than the existing methods from the literature reported in Tab. I. This confirms the choice of Inception 3D, which proves to be suitable for the task of action/gesture recognition using event data. ?t = 1ms ?t = 2.5ms ?t = 5ms ?t = 10ms ?t = 20ms The results on the MICC-Event Gesture Dataset overall are much lower due to the challenging scenarios that we have collected. However, the gap between the proposed aggregation strategy and the baselines increases considerably, suggesting that the Temporal Binary Representation is capable of representing event data more effectively. At the same time, since we are using N = 8 bits, TBR generates 8 times less data to be processed since N frames are losslessly condensed into a single representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ABLATION STUDIES</head><p>We perform a series of ablation studies, showing the performance of the proposed method varying the parameters of the Temporal Binary Representation strategy. In particular, we observe how the accuracy of the system is affected when varying the accumulation time ?t, the number of bits used for the binary representation and the length of the video chunk fed to the Inception 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Accumulation time</head><p>Varying the accumulation time ?t, we can adjust the temporal quantization made by TBR. Higher accumulation times lead to more compact representations, which however carry less information. It can be seen from <ref type="figure">Fig. 5</ref> that this information loss comes with a drop in accuracy for accumulation times bigger than 2.5 ms. Interestingly, lowering ?t beneath this threshold does not bring any improvement for the task at hand. In the plot, the best result from the state of the art <ref type="bibr" target="#b2">[3]</ref>, is shown as reference. <ref type="figure" target="#fig_3">Fig. 4</ref>  high ?t, both the spatial and temporal nature of the encoding appears clearly visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Number of bits</head><p>Along with ?t, the number of bits N used for the proposed Temporal Binary Representation, defines how much information gets condensed into a single frame. <ref type="figure">Fig. 6</ref> shows the accuracy of Inception 3D on the DVS128 Gesture Dataset using N = 4, 8, 16. Similarly to ?t, when N becomes too small, the accuracy of the model saturates. Throughout the paper we have taken N = 8 bits as reference for building our representations since it offers a trade-off between accuracy and data compactness. Furthermore, the choice of N = 8 simplifies data storage since events can be saved as unsigned integers grayscale images with lossless compression.  <ref type="figure">Fig. 7</ref>. Accuracy of Inception 3D on the DVS 128 Gesture Dataset, varying the chunk size fed to Inception 3D. The best results from the state of the art <ref type="bibr" target="#b2">[3]</ref> is also shown as reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Chunk length</head><p>Here we vary the length of the chunks fed to Inception 3D. Since the model exploits 3D inflated convolutions, it can process multiple frames concatenated together, therefore taking into account the temporal dimension. In the case of TBR, the temporal dimension is already encoded covering a timespan of N ? ?t. By staking frames together we are extending the observation timespan by a factor equal to the number of frames. This setting is equivalent to the one adopted in <ref type="bibr" target="#b2">[3]</ref>, where the classifier performs a majority voting after having observed several chunks of various dimensions. In <ref type="figure">Fig. 7</ref>, we report the results for both methods, varying the chunk length from 100 ms to 500 ms. For our Temporal Binary Encoding we use ?t = 2.5ms and N = 8, hence covering with each frame a temporal interval of 20 ms. The accuracy of the system improves when the chunk length increases, up to 500 ms. We did not observe significant improvements when increasing it further by adding more frames. It has to be noted however that increasing the chunk length will also increase the latency of the model, since a longer part of the gesture needs to be observed before emitting the first classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSIONS</head><p>In this paper we have presented an accumulation strategy called Temporal Binary Representation for converting the out-put of event cameras from raw events to frames, making them processable by Computer Vision algorithms. The proposed approach generates highly compact data, thanks to a lossless conversion of intermediate binary representations into a single decimal one. The effectiveness of the proposed approach has been validated on the commonly used DVS128 Gesture Dataset, reporting state of the art results. In addition a new test benchmark for event-based gesture recognition has been collected and will be publicly released.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Temporal Binary Representation. Events are first stacked together into intermediate binary representations which are then grouped into a single frame thanks to a binary to decimal conversion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Samples from the MICC-Event Gesture Dataset. Slow and fast execution of the action air drum (first row) and different scale and orientation of the action arm roll (second row). A 1 second snippet is shown for each sample, where events are color-coded according to the timestamps from blue (start -0s) to yellow (end -1s). The actors are shown both frontal (left) and sideways (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Events aggregated with our Temporal Binary Representation (left), Polarity<ref type="bibr" target="#b0">[1]</ref> (middle) and Surface of Active Events<ref type="bibr" target="#b35">[36]</ref> (right). All three representations are made using an accumulation time ?t = 2.5ms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Temporal Binary Representations with different accumulation times ?t with a number of bits n = 8. Each frame represents all the events in the interval [0; n ? ?t]. Three different gestures are shown: Right Hand Clockwise (top); Arm Roll (middle); Other Gesture (bottom). Pixels are color-coded according to intensity, from 0 (blue -no events) to 255 (red -an event registered for each bit of the representation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON THE DVS128 GESTURE DATASET.</figDesc><table><row><cell></cell><cell>10 classes</cell><cell>11 classes</cell></row><row><cell>Time-surfaces [25]</cell><cell>96.59</cell><cell>90.62</cell></row><row><cell>SNN eRBP[26]</cell><cell>-</cell><cell>92.70</cell></row><row><cell>Slayer [27]</cell><cell>-</cell><cell>93.64</cell></row><row><cell>CNN [6]</cell><cell>96.49</cell><cell>94.59</cell></row><row><cell>Space-time clouds [28]</cell><cell>97.08</cell><cell>95.32</cell></row><row><cell>DECOLLE [29]</cell><cell>-</cell><cell>95.54</cell></row><row><cell>Spatiotemporal filt. [3]</cell><cell>-</cell><cell>97.75</cell></row><row><cell>RG-CNN [30]</cell><cell>-</cell><cell>97.20</cell></row><row><cell>Ours -AlexNet+LSTM</cell><cell>97.50</cell><cell>97.73</cell></row><row><cell>Ours -Inception3D</cell><cell>99.58</cell><cell>99.62</cell></row><row><cell cols="3">perform a central crop of 350 ? 350 pixels and then reshape</cell></row><row><cell cols="3">it to 128 ? 128 to match the size of DVS128. Reshape is</cell></row><row><cell cols="3">done with Nearest Neighbor interpolation to a avoid unwanted</cell></row><row><cell cols="3">artifacts that may introduce noise in the event representation.</cell></row><row><cell cols="3">Frame values are normalized in [?1; 1] before being fed to the</cell></row><row><cell cols="3">models. During training we also perform data augmentation</cell></row><row><cell cols="3">applying random scaling, translation and rotation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON THE DVS128 GESTURE DATASET AND THE MICC-EVENT GESTURE DATASET FOR INCEPTION 3D TRAINED WITH THREE DIFFERENT AGGREGATION STRATEGIES: TBR (OURS), POLARITY [1] AND SAE [36].</figDesc><table><row><cell></cell><cell cols="2">TBR (ours) Polarity</cell><cell>SAE</cell></row><row><cell>DVS128 Gesture Dataset</cell><cell>99.62</cell><cell>98.86</cell><cell>98.11</cell></row><row><cell>MICC-Event Gesture Dataset</cell><cell>73.16</cell><cell>68.40</cell><cell>70.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>shows samples of Temporal Binary Representations for different accumulation times. Especially for sufficiently Accuracy of Inception 3D on the DVS 128 Gesture Dataset, varying the accumulation time ?t. The best results from the state of the art<ref type="bibr" target="#b2">[3]</ref> is also shown as reference.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Temporal Binary Representation</cell><cell>Spatiotemporal Filtering</cell></row><row><cell></cell><cell>100</cell><cell>99,62</cell><cell>99,62</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">99,24</cell></row><row><cell>Accuracy [%]</cell><cell>98 99</cell><cell></cell><cell></cell><cell></cell><cell>98,48</cell></row><row><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell>96,59</cell></row><row><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 ms</cell><cell>2,5 ms</cell><cell>5 ms</cell><cell>10 ms</cell><cell>20 ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Accumulation Time (?t)</cell></row><row><cell>Fig. 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Accuracy of Inception 3D on the DVS 128 Gesture Dataset, varying the number of bits for the Temporal Binary Representation. The best results from the state of the art<ref type="bibr" target="#b2">[3]</ref> is also shown as reference.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Temporal Binary Representation</cell><cell>Spatiotemporal Filtering</cell></row><row><cell></cell><cell>100</cell><cell>99,62</cell><cell cols="2">99,62</cell></row><row><cell>Accuracy [%]</cell><cell>98,5 99,25</cell><cell></cell><cell></cell><cell></cell><cell>98,86</cell></row><row><cell></cell><cell>97,75</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">4 bits</cell><cell>8 bits</cell><cell></cell><cell>16 bits</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Number of Bits (n)</cell></row><row><cell>Fig. 6. Accuracy [%]</cell><cell>97 98,5 100</cell><cell cols="2">98,86 Temporal Binary Representation 97,35</cell><cell>99,24 Spatiotemporal Filtering</cell><cell>99,62</cell></row><row><cell></cell><cell>95,5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">100 ms</cell><cell>200 ms</cell><cell>260 ms</cell><cell>500 ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Length of Videos [ms]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.prophesee.ai/event-based-evk/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the Italian MIUR within PRIN 2017, Project Grant 20172BH297: I-MALLimproving the customer experience in stores by intelligent computer vision.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time 6dof pose relocalization for event cameras with stacked spatial lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Tsagarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neuromorphic vision datasets for pedestrian detection, action recognition, and fall detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neurorobotics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Spatiotemporal filtering for event-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thakor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07067</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A differentiable recurrent surface for asynchronous event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asynchronous convolutional networks for object detection in neuromorphic cameras</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A low power, fully event-based gesture recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Melano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Di</forename><surname>Nolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7243" to="7252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human activity analysis: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno>16:1-16:43</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guess where? actor-supervision for spatiotemporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page">102886</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5502" to="5511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for detecting multiple space-time action tubes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online realtime multiple spatiotemporal action localisation and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Am i done? predicting action progress in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Becattini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TOMM</publisher>
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmentation free object discovery in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cuffaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Becattini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gesture recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="324" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gesture recognition for human-robot collaboration: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Industrial Ergonomics</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="355" to="367" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ohajiki interface: flicking gesture recognition with a high-speed camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Entertainment Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Event-based gesture recognition with dynamic background suppression using smartphone computational capabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Maro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Embodied neuromorphic vision with event-driven random backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roennau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04805</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Slayer: Spike layer error reassignment in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Space-time event clouds for gesture recognition: from rgb cameras to event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1826" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Synaptic plasticity dynamics for deep continuous local learning (decolle)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">424</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph-based spatial-temporal feature learning for neuromorphic vision sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bourtsoulatze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andreopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03579</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Realtime classification and sensor fusion with a spiking deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A 128 x 128 120db 30mw asynchronous vision sensor that responds to relative intensity change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Solid State Circuits Conference-Digest of Technical Papers</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2060" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fast event-based corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
