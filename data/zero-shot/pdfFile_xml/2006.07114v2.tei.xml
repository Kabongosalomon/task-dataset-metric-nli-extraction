<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Distillation Meets Self-Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Distillation Meets Self-Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation, which involves extracting the "dark knowledge" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting "richer dark knowledge" from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and crossarchitecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art CRD [42] by an average of 2.3% in accuracy rate on CIFAR100 across six different teacher-student pairs. The code and models are available at: https://github.com/xuguodong03/SSKD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The seminal paper by Hinton et al. <ref type="bibr" target="#b14">[16]</ref> show that the knowledge from a large ensemble of models can be distilled and transferred to a student network. Specifically, one can raise the temperature of the final softmax to produce soft targets of the teacher for guiding the training of the student. The guidance is achieved by minimizing the Kullback-Leibler (KL) divergence between teacher and student outputs. An interesting and inspiring observation is that despite the teacher model assigns probabilities to incorrect classes, the relative probabilities of incorrect answers are exceptionally informative about generalization of the trained model. <ref type="figure">Fig. 1</ref>. Difference between conventional KD <ref type="bibr" target="#b14">[16]</ref> and SSKD. We extend the mimicking on normal data and on a single classification task to the mimicking on transformed data and with an additional self-supervision pretext task. The teacher's self-supervision predictions contain rich structured knowledge that can facilitate more rounded knowledge distillation on the student. In this example, contrastive learning on transformed images serves as the self-supervision pretext task. It constructs a single positive pair and several negative pairs through image transformations t(?), and then encourages the network to recognize the positive pair. The backbone of the teacher and student are represented as ft and fs, respectively, while the corresponding output is given as t and s with subscript representing the index The hidden knowledge encapsulated in these secondary probabilities is sometimes known as "dark knowledge".</p><p>In this work, we are fascinated on how one could extract richer "dark knowledge" from neural networks. Existing studies focus on what types of intermediate representations of teacher networks should student mimic. These representations include attention map <ref type="bibr" target="#b44">[46]</ref>, gram matrix <ref type="bibr" target="#b42">[44]</ref>, gradient <ref type="bibr" target="#b37">[39]</ref>, pre-activation <ref type="bibr" target="#b13">[15]</ref>, and feature distribution statistics <ref type="bibr" target="#b15">[17]</ref>. While the intermediate representations of the network could provide more fine-grained information, a common characteristic shared by these medium of knowledge is that they are all derived from a single task (typically the original classification task). The knowledge is highly task-specific, and hence, such knowledge may only reflect a single facet of the complete knowledge encapsulated in a cumbersome network. To mine for richer dark knowledge, we need an auxiliary task apart from the original classification task, so as to extract richer information that is complementary to the classification knowledge.</p><p>In this study, we show that a seemingly different learning scheme -selfsupervised learning, when treated as an auxiliary task, can help gaining more rounded knowledge from a teacher network. The original goal of self-supervised learning is to learn representations with natural supervisions derived from data via a pretext task. Examples of pretext tasks include exemplar-based method <ref type="bibr" target="#b7">[9]</ref>, rotation prediction <ref type="bibr" target="#b9">[11]</ref>, jigsaw <ref type="bibr" target="#b28">[30]</ref>, and contrastive learning <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b25">27]</ref>. To use selfsupervised learning as an auxiliary task for knowledge distillation, one can apply the pretext task to a teacher by appending a lightweight auxiliary branch/module to the teacher's backbone, updating the auxiliary module with the backbone frozen, and then extract the corresponding self-supervised signals from the auxiliary module for distillation. An example of combining a contrastive learning pretext task <ref type="bibr" target="#b2">[4]</ref> with knowledge distillation is shown in <ref type="figure">Fig. 1</ref>.</p><p>The example in <ref type="figure">Fig. 1</ref> reveals several advantages of using self-supervised learning as an auxiliary task for knowledge distillation (we name the combination as SSKD). First, in conventional knowledge distillation, a student mimics a teacher from normal data based on a single classification task. SSKD extends the notion to a broader extent, i.e., mimicking on transformed data and on an additional selfsupervision pretext task. This enables the student to capture richer structured knowledge from the self-supervision predictions of teacher, which cannot be sufficiently captured by a single task. We show that such structured knowledge not only improves the overall distillation performance, but also regularizes the student to generalize better on few-shot and noisy-label scenarios.</p><p>Another advantage of SSKD is that it is model-agnostic. Previous knowledge distillation methods suffer from degraded performance under cross-architecture settings, for the knowledge they transfer is very architecture-specific. For example, when transfer the feature of ResNet50 <ref type="bibr" target="#b11">[13]</ref> to ShuffleNet <ref type="bibr" target="#b49">[51]</ref>, student may have trouble in mimicking due to the architecture difference. In contrast, SSKD transfers only the last layer's outputs, hence allowing a more flexible solution space for the student model to search for intermediate representations that best suit its own architecture. Contributions: We propose a novel framework called SSKD that leverages selfsupervised tasks to facilitate extraction of richer knowledge from teacher network to student network. To our knowledge, this is the first work that defines the knowledge through self-supervised tasks. We carefully investigate the influence of different self-supervised pretext tasks and the impact of noisy self-supervised predictions to the performance of knowledge distillation. We show that SSKD greatly boosts the generalizability of student networks and offers significant advantages under few-shot and noisy-label scenarios. Extensive experiments on two standard benchmarks, CIFAR100 <ref type="bibr" target="#b21">[23]</ref> and ImageNet <ref type="bibr" target="#b4">[6]</ref>, demonstrate the effectiveness of SSKD over other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Knowledge Distillation. Knowledge distillation trains a smaller network using the supervision signals from both ground truth labels and a larger network. Hinton et al. <ref type="bibr" target="#b14">[16]</ref> propose to match the outputs of classifiers of two models by minimizing the KL-divergence of the category distribution. Besides the final layer logits, teacher network also distills compact feature representations from its backbone. FitNets <ref type="bibr" target="#b35">[37]</ref> proposes to mimic the intermediate feature maps of teacher network using L2 loss. AT <ref type="bibr" target="#b44">[46]</ref> uses attention transfer to teach student which region is the key for classification. FSP <ref type="bibr" target="#b42">[44]</ref> distills the second order statistics (Gram matrix) between different layers. To alleviate information leak, FT <ref type="bibr" target="#b18">[20]</ref> introduces an autoencoder in teacher network to compress features into "factors" and then use a translator to extract "factors" in student network. AB <ref type="bibr" target="#b13">[15]</ref> forces student to learn the binarized values of pre-activation map in teacher network. IRG <ref type="bibr" target="#b23">[25]</ref> explores whether the similarity between samples transfer more knowledge. KDSVD <ref type="bibr" target="#b17">[19]</ref> calls its method as self-supervised knowledge distillation. Nevertheless, the study regards the teacher's correlation maps of feature singular vectors as self-supervised labels. The label is obtained from the teacher rather than a self-supervised pretext task. Thus, their notion of self-supervised learning differ from the conventional one. Our work, to our knowledge, is the first study that investigates defining the knowledge via self-supervised pretext tasks. CRD <ref type="bibr" target="#b40">[42]</ref> also combines selfsupervision (SS) with knowledge distillation. The difference is the purpose of SS and how contrastive task is performed. In CRD, contrastive learning is performed across teacher and student networks to maximize the mutual information between two networks. In SSKD, contrastive task serves as a way to define knowledge. It is performed separately in two networks and then matched together through KL-divergence, which is very different from CRD. Self-Supervised Learning. Self-supervision methods design various pretext tasks whose labels can be derived from the data itself. In the process of solving these tasks, the network learn useful representations. Based on pretext tasks, SS methods can be grouped into several categories, including constructionbased methods such as inpainting <ref type="bibr" target="#b33">[35]</ref> and colorization <ref type="bibr" target="#b48">[50]</ref>, prediction-based methods <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b47">49]</ref>, cluster-based methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b46">48]</ref>, generation-based methods <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b10">12]</ref> and contrastive-based methods <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b39">41]</ref>. Exemplar <ref type="bibr" target="#b7">[9]</ref> applies heavy transformation to each training image and treat all the images generated from the same image as a separate category. This pseudo label is used to updated the network. Jigsaw puzzle <ref type="bibr" target="#b28">[30]</ref> splits the image into several non-overlapping patches and forces the network to recognise the shuffled order. Jigsaw++ <ref type="bibr" target="#b29">[31]</ref> also involves SS and KD. But it utilizes knowledge transfer to boost the self-supervision performance, which solves an inverse problem of SSKD. Rotation <ref type="bibr" target="#b19">[21]</ref> feeds the network with rotated images and forces it to recognise the rotation angle. To finish this task, the network has to understand the semantic information containing in the image. SimCLR <ref type="bibr" target="#b2">[4]</ref> applies augmentation to training samples and requires the network to match original image and transformed image through contrastive loss. Considering the excellent performance obtained by SimCLR <ref type="bibr" target="#b2">[4]</ref>, we adopt it as our main pretext task in SSKD. However, SSKD is not limited to using only contrastive learning, many other pretext tasks <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b28">30]</ref> can also serve the purpose. We investigate their usefulness in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section is divided into three main sections. We start with a brief review of knowledge distillation and self-supervision in Sec. 3.1. For self-supervision, we discuss contrastive prediction as our desired pretext task, although SSKD is not limited to contrastive prediction. Sec. 3.2 specifies the training process of teacher and student model. Finally, we discuss the influence of noisy self-supervised predictions and ways to handle the noise in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Knowledge Distillation. Hinton et al. <ref type="bibr" target="#b14">[16]</ref> suggest that the soft targets predicted by a well-optimized teacher model can provide extra information, comparing to hard labels (one-hot vectors). The relatively high probabilities assigned to wrong categories encode semantic similarities between different categories. Forcing a student to mimic teacher's prediction causes the student to learn this secondary information that cannot be expressed by hard labels alone. To obtain the soft targets, temperature scaling is introduced in <ref type="bibr" target="#b14">[16]</ref> to soften the peaky softmax distribution:</p><formula xml:id="formula_0">p i (x; ? ) = Softmax(s(x); ? ) = e si(x)/? k e s k (x)/? ,<label>(1)</label></formula><p>where x is the data sample, i is the category index, s i (x) is the score logit that x obtains on category i, and ? is the temperature. The knowledge distillation loss L kd measured by KL-divergence is:</p><formula xml:id="formula_1">L kd = ?? 2 x?Dx C i=1 p i t (x; ? ) log(p i s (x; ? )),<label>(2)</label></formula><p>where t and s denote teacher and student models, respectively, C is the total number of classes, D x indicates the dataset. The complete loss function L of the student model is a linear combination of the standard cross-entropy loss L ce and knowledge distillation loss L kd :</p><formula xml:id="formula_2">L = ? 1 L ce + ? 2 L kd ,<label>(3)</label></formula><p>where ? 1 and ? 2 are balancing weights.</p><p>Contrastive Prediction as Self-Supervision Task. Motivated by the success of contrastive prediction methods <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b39">41]</ref> for self-supervised learning, we adopt contrastive prediction as the self-supervision task in our framework. The general goal of contrastive prediction is to maximize agreement between a data point and its transformed version via a contrastive loss in latent space. Given a mini-batch containing N data points {x i } i=1:N , we apply independent transformation t(?) (sampled from the same distribution T ) to each data point and obtain { x i } i=1:N . Both x i and x i are fed into the teacher or student networks to extract representations ? i = f (x i ), ? i = f ( x i ). We follow Chen et al. <ref type="bibr" target="#b2">[4]</ref> and add a projection head on the top of the network. The projection head is a 2-layer multilayer perceptron. It maps the representations into a latent space where the contrastive loss is applied, i.e., z i = MLP(? i ), z i = MLP( ? i ).</p><p>We take ( x i , x i ) as the positive pair and ( x i , x k ) k =i as the negative pair. Given some x i , the contrastive prediction task is to identify the corresponding x i from the set {x i } i=1:N . To meet the goal, the network should maximize the similarity between positive pairs and minimize the similarity between negative pairs. In this work, we use a cosine similarity. If we organize the similarities between { x i } and {x i } into matrix form A, then we have:</p><formula xml:id="formula_3">A i,j = cosine( z i , z j ) = dot( z i , z j ) || z i || 2 ||z j || 2 ,<label>(4)</label></formula><p>where A i,j represents the similarity between x i and x j . The loss of contrastive prediction is:</p><formula xml:id="formula_4">L = ? i log exp(cosine( z i , z i )/? ) k exp(cosine( z i , z k )/? ) = ? i log exp(A i,i /? ) k exp(A i,k /? ) ,<label>(5)</label></formula><p>where ? is another temperature parameter (can be different from ? in Eqn. (1)). The loss form is similar to softmax loss and can be understood as maximizing the probability that z i and z i come from a positive pair. In the process of matching { x i } and {x i }, the network learns transformation invariant representations. In SSKD, however, the main goal is not to learn representations invariant to transformations, but to exploit contrastive prediction as an auxiliary task for mining richer knowledge from the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning SSKD</head><p>The framework of SSKD is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Both teacher and student consist of three components: a backbone f (?) to extract representations, a classifier p(?) for the main task and a self-supervised (SS) module for specific self-supervision task. In this work, contrastive prediction is selected as the SS task, so the SS module c t (?, ?) and c s (?, ?) consist of a 2-layer MLP and a similarity computation module. More SS tasks will be compared in the experiments.</p><p>Training the Teacher Network. The inputs are normal data {x i } and transformed version { x i }. The transformation t(?) is sampled from a predefined transformation distribution T . In this study, we select four transformations, i.e., color dropping, rotation, cropping followed by resize and color distortion, as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. More transformations can be included. We feed x and x to the backbone and obtain their representations</p><formula xml:id="formula_5">? = f t (x), ? = f t ( x).</formula><p>The training of teacher network contains two stages. In the first stage, the network is trained with the classification loss. Only the backbone f t (?) and classifier p t (?) are updated. Note that the classification loss is not computed on transformed data x because the transformation T is much heavier than usual data augmentation. Its goal is not to enlarge the training set but to make the x visually less similar to x. It makes the contradistinction much harder, which is beneficial to representation learning <ref type="bibr" target="#b2">[4]</ref>. Forcing the network to classify x correctly can destroy the semantic information learned from x and hurt the performance. Training scheme of SSKD. Input images are transformed by designated transformations to prepare data for the self-supervision task. Teacher and student networks both contain three components, i.e., backbone f (?), classifier p(?) and SS module c(?, ?). Teacher's training are split into two stages. The first stage trains ft(?) and pt(?) with a classification task, and the second stage fine-tunes ct(?, ?) with a selfsupervision task. In student's training, we force the student to mimic teacher on both classification output and self-supervision output, besides the standard label loss</p><p>In the second stage, we fix f t (?) and p t (?), and only update parameters in SS module c t (?, ?) using the contrastive prediction loss in Eqn. <ref type="bibr" target="#b3">(5)</ref>.</p><p>The two stages of training have distinct roles. The first stage is simply the typical training of a network for classification. The second stage, aims at adapting the SS module to use the features from the existing backbone for contrastive prediction. This allows us to extract knowledge from the SS module for distillation. It is worth pointing out that the second-stage training is highly efficient given the small MLP head, thus it is easy to prepare a teacher network for SSKD. Training the Student Network. After training the teacher's SS module, we apply softmax (with temperature scale ? ) to the teacher's similarity matrix A (Eqn. (4)) along the row dimension leading to a probability matrix B t , with B t i,j representing the probability that x i and x j is a positive pair. Similar operations are applied to the student to obtain B s . With B t and B s , we can compute the KL-divergence loss between the SS module's output of both teacher and student:</p><formula xml:id="formula_6">L ss = ?? 2 i,j B t i,j log(B s i,j ).<label>(6)</label></formula><p>The transformed data point x is the side product of contrastive prediction task. Though we do not require the student to classify them correctly, we can encourage the student's classifier output p s (f s ( x)) to be close to that of teacher's. The loss function is:</p><formula xml:id="formula_7">L T = ?? 2 x?T (Dx) C i=1 p i t ( x; ? ) log(p i s ( x; ? )).<label>(7)</label></formula><p>The final loss for student network is the combination of aforementioned terms, i.e., cross entropy loss L ce , L kd in Eqn. <ref type="bibr" target="#b0">(2)</ref>, L ss in Eqn. <ref type="bibr" target="#b4">(6)</ref>, and L T in Eqn. <ref type="formula" target="#formula_7">(7)</ref>:</p><formula xml:id="formula_8">L = ? 1 L ce + ? 2 L kd + ? 3 L ss + ? 4 L T ,<label>(8)</label></formula><p>where the ? i is the balancing weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Imperfect Self-Supervised Predictions</head><p>When performing contrastive prediction, a teacher may produce inaccurate predictions, e.g., assigning x k to the x i , i = k. This is very likely since the backbone of the teacher is not fine-tuned together with the SS module for contrastive prediction. Similar to conventional knowledge distillation, those relative probabilities that the teacher assigns to incorrect answers contain rich knowledge of the teacher. Transferring this inaccurate but structured knowledge is the core of our SSKD. Nevertheless, we empirically found that an extremely incorrect prediction may still mislead the learning of the student. To ameliorate negative impacts of those outliers, we adopt an heuristic approach to perform selective transfer. Specifically, we define the error level of a prediction as the ranking of the corresponding ground truth label in the classification task. Given a transformed sample x i and corresponding positive pair index i, we sort the scores that the network assigns to each {x i } i=1:N in a descending order. The rank of x i represents the error level of the prediction about x i . The rank of 1 means the prediction is completely correct. A lower rank indicates a higher degree of error. During the training of student, we sort all the x in a mini-batch in an ascending order according to error levels of the teacher's prediction, and only transfer all the correct predictions and the top-k% ranked incorrect predictions. This strategy suppresses potential noise in teacher's predictions and transfer only beneficial knowledge. We show our experiments in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The experiments section consists of three parts. We first conduct ablation study to examine the effectiveness of several components of SSKD in Sec. 4.1. Comparison with state-of-the-art methods is conducted in Sec. 4.2. In Sec. 4.3, we further show SSKD's advantages under few-shot and noisy-label scenarios.</p><p>Evaluations are conducted on CIFAR100 <ref type="bibr" target="#b21">[23]</ref> and ImageNet <ref type="bibr" target="#b4">[6]</ref> datasets, both of which are widely used as the benchmarks for knowledge distillation. CIFAR100 consists of 60, 000 32 ? 32 colour images, with 50, 000 images for training and 10, 000 images for testing. There are 100 classes, each contains 600 images. ImageNet is a large-scale classification dataset, containing 1, 281, 167 images for training and 50, 000 images for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>Effectiveness of Self-Supervision Auxiliary Task. The motivation behind SSKD is that teacher's inaccurate self-supervision output encodes rich structured knowledge of teacher network and mimicking this output can benefit student's learning. To examine this hypothesis, we train a student network whose only training signals come from teacher's self-supervision output, i.e., set ? 1 ,? 2 , ? 3 in Eqn. (8) to be 0, and observe whether student can learn good representations.</p><p>We first demonstrate the utility by examining the student's feature distribution. We select vgg13 <ref type="bibr" target="#b36">[38]</ref> and vgg8 as the teacher and student networks, respectively. The CIFAR100 <ref type="bibr" target="#b21">[23]</ref> training split is selected as the training set. After the training, we use the student backbone to extract features (before logits) of CIFAR100 test set. We randomly select 9 categories out of 100 and visualize the features with t-SNE. The results are shown in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. Though the accuracy of teacher's contrastive prediction is only around 50%, mimicking this inaccurate output still makes student learn highly clustered patterns, showing that teacher's self-supervision output does transfer meaningful structured knowledge.</p><p>To test the effectiveness of designed L T and L ss , we compare three variants of SSKD with CIFAR100 on four teacher-student pairs. The three variants are: 1) conventional KD, 2) KD with additional loss L T (KD+L T ), 3) full SSKD (KD+L T +L ss ). The results are shown in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>. On all four different teacherstudent pairs, L T and L ss boost the accuracies by a large margin, showing the effectiveness of our designed components. Influence of Noisy Self-Supervision Predictions. As discussed in Sec. 3.3, removing some extreme outliers are beneficial for SSKD. Some transformed samples with large error levels may play a misleading role. To examine this conjecture, we compare several students that receive different proportions of incorrect predictions from teacher. Specifically, we sort all the transformed x in <ref type="table">Table 1</ref>. Influence of Noisy Self-Supervision Predictions to Student accuracies(%), when transferring the top-k% smallest error-level samples. As more samples with large error level are transferred, the performances go through a rise-and-fall process. The baseline with k = 0 is equivalent to transferring only correct predictions a mini-batch according to their error levels in an ascending order. We transfer all the correct predictions. For incorrect predictions, we only transfer top-k% samples with the smallest error levels. A higher k value indicates a higher number of predictions with larger error levels being transferred to student network. Experiments are conducted on CIFAR100 with three teacher-student pairs. The results are shown in <ref type="table">Table 1</ref>. The general trend shows that incorrect predictions are beneficial (k = 0 yields the lowest accuracies). Removing extreme outliers help to give a peak performance between k = 50 and k = 75 across different acchitectures. When comparing with other methods in Sec. 4.2 and 4.3, we fix k = 75 for all the teacher-student pairs.</p><p>Influence of Different Self-Supervision Tasks. Different pretext tasks in self-supervision would result in different qualities of extracted features. Similarly, distillation with different self-supervision tasks also lead to students with different performances. Here, we examine the influence of SS method's performance on SSKD. We employ the commonly used linear evaluation accuracy as our metric.</p><p>In particular, each method first trains a network with its own pretext task. A single layer classifier is then trained by using the representations extracted from the fixed backbone. In this way, the classification accuracies represent the quality of SS methods. In <ref type="table">Table 2</ref>, we compare four widely used self-supervision methods: Exemplar <ref type="bibr" target="#b7">[9]</ref>, Rotation <ref type="bibr" target="#b19">[21]</ref>, Jigsaw <ref type="bibr" target="#b28">[30]</ref> and Contrastive <ref type="bibr" target="#b2">[4]</ref>. We list the linear evaluation accuracies each method obtains on ImageNet with ResNet50 <ref type="bibr" target="#b11">[13]</ref> network and also student's accuracies when they are incorporated, respectively, into KD. We find that the performance of SSKD is positively correlated with the corresponding SS method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark</head><p>CIFAR100. We compare our method with representative knowledge distillation methods, including: KD <ref type="bibr" target="#b14">[16]</ref>, FitNet <ref type="bibr" target="#b35">[37]</ref>, AT <ref type="bibr" target="#b44">[46]</ref>, SP <ref type="bibr" target="#b41">[43]</ref>, VID <ref type="bibr" target="#b0">[2]</ref>, RKD <ref type="bibr" target="#b31">[33]</ref>, PKT <ref type="bibr" target="#b32">[34]</ref>, AB <ref type="bibr" target="#b13">[15]</ref>, FT <ref type="bibr" target="#b18">[20]</ref>, CRD <ref type="bibr" target="#b40">[42]</ref>. ResNet <ref type="bibr" target="#b11">[13]</ref>, wideResNet <ref type="bibr" target="#b43">[45]</ref>, vgg <ref type="bibr" target="#b36">[38]</ref>, ShuffleNet <ref type="bibr" target="#b49">[51]</ref> and MobileNet <ref type="bibr" target="#b16">[18]</ref> are selected as the network backbones. For all competing methods, we use the implementation of <ref type="bibr" target="#b40">[42]</ref>. For a fair comparison, we combine all competing methods with conventional KD <ref type="bibr" target="#b14">[16]</ref> (except KD itself). And we omit "+KD" notation in all the following tables (except for <ref type="table">Table 5</ref>) and figures for simplicity. <ref type="bibr" target="#b1">3</ref> We compare performances on 11 teacher-student pairs to investigate the generalization ability of each method. Following CRD <ref type="bibr" target="#b40">[42]</ref>, we split these pairs into 2 groups according to whether teacher and student have similar architecture styles. The results are shown in <ref type="table" target="#tab_1">Table 3</ref> and <ref type="table">Table 4</ref>. In each table, the second partition after the header show the accuracies of the teacher's and student's performance when they are trained individually, while the third partition show the student's performance after knowledge distillation.</p><p>For teacher-student pairs with a similar architecture, SSKD performs the best in four out of five pairs ( <ref type="table" target="#tab_1">Table 3</ref>). The gap between SSKD and the best-performing competing methods is 0.52% (averaged on five pairs). Notably, in all six teacherstudent pairs with different architectures, SSKD consistently achieves the best results <ref type="table">(Table 4)</ref>, surpassing the best competing methods by a large margin with an average absolute accuracy difference of 2.14%. Results on cross-architecture pairs clearly demonstrate that our method does not rely on architecture-specific cues. Instead, SSKD distills knowledge only from the outputs of the final layer of <ref type="table">Table 4</ref>. KD between Different Architectures. Top-1 accuracy (%) on CIFAR100. Bold and underline denote the best and the second best results, respectively. We denote by * methods that we re-run using author-provided code. SSKD consistently obtains the best results on all pairs  <ref type="table">Table 5</ref>. Top-1/Top-5 error (%) on ImageNet. Bold and underline denote the best and the second best results, respectively. The competing methods include CC <ref type="bibr" target="#b34">[36]</ref>, SP <ref type="bibr" target="#b41">[43]</ref>, Online-KD <ref type="bibr" target="#b22">[24]</ref>, KD <ref type="bibr" target="#b14">[16]</ref>, AT <ref type="bibr" target="#b44">[46]</ref>, and CRD <ref type="bibr" target="#b40">[42]</ref>. The results of competing methods are obtained from <ref type="bibr" target="#b40">[42]</ref> Teacher Student CC SP Online-KD KD AT CRD CRD+KD Ours ImageNet. Limited by computation resources, we only conduct one teacherstudent pair on ImageNet, i.e., ResNet34 as teacher and ResNet18 as student. As shown in <ref type="table">Table 5</ref>, for both Top-1 and Top-5 error rates, our SSKD obtains the best performances. The results on ImageNet demonstrate the scalability of SSKD to large-scale dataset.</p><p>Evaluation of Learned Representations. Following CRD <ref type="bibr" target="#b40">[42]</ref>, we also investigate the qualities of student representations. A good feature extractor should generate linear separable features. Hence, we use the fixed backbone of student (trained on CIFAR100) to extract features of STL10 <ref type="bibr" target="#b3">[5]</ref> and TinyImageNet [1], and then train a linear classifier. We select wrn40-2 and ShuffleNetV1 as teacher and student networks, respectively. As shown in <ref type="table">Table 6</ref>, SSKD achieves the highest accuracies on both STL10 and TinyImageNet. <ref type="table">Table 6</ref>. Linear Classification Accuracy (%) on STL10 and TinyImageNet. We use wrn40-2 and ShuffleNetV1 as teacher and student networks, respectively. The competing methods include KD <ref type="bibr" target="#b14">[16]</ref>, FitNet <ref type="bibr" target="#b35">[37]</ref>, AT <ref type="bibr" target="#b44">[46]</ref>, FT <ref type="bibr" target="#b18">[20]</ref>, and CRD <ref type="bibr" target="#b40">[42]</ref> Student Teacher-Student Similarity. SSKD can extract richer knowledge by mimicking self-supervision output and make student much more similar to teacher than other KD methods. To examine this claim, we analyze the similarity between student and teacher networks using two metrics, i.e., KL-divergence and CKA similarity <ref type="bibr" target="#b20">[22]</ref>. Small KL-divergence and large CKA similarity indicate that student is similar to teacher. We use vgg13 and vgg8 as teacher and student, respectively, and use CIFAR100 as the training set. We compute the KL-divergence and CKA similarity between teacher and student on three sets, i.e., test partitions of CIFAR100, STL10 <ref type="bibr" target="#b3">[5]</ref> and SVHN <ref type="bibr" target="#b27">[29]</ref>. As shown in <ref type="table" target="#tab_4">Table 7</ref>, our method achieves the smallest KL-divergence and the largest CKA similarity on CIFAR100 test set. Compared to CIFAR100, STL10 and SVHN have different distributions that have not been seen during training, therefore more difficult to mimic. However, the proposed SSKD still obtains the best results in all the metrics except KLdivergence in STL10. From this similarity analysis, we conclude that SSKD can help student mimic teacher better and get a larger similarity to teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Analysis</head><p>Few-Shot Scenario. In a real-world setting, the number of samples available for training is often limited <ref type="bibr" target="#b24">[26]</ref>. To investigate the performance of SSKD under fewshot scenarios, we conduct experiments on subsets of CIFAR100. We randomly The accuracies of FT and CRD drop dramatically as noisy labels increase, while SSKD is much more stable and maintains a high performance in all cases sample images of each class to form a new training set. We train student model using newly crafted training set, while maintaining the same test set. Vgg13 and vgg8 are chosen as teacher and student model, respectively. We compare our student's performance with KD <ref type="bibr" target="#b14">[16]</ref>, AT <ref type="bibr" target="#b44">[46]</ref> and CRD <ref type="bibr" target="#b40">[42]</ref>. The percentages of reserved samples are 25%, 50%, 75% and 100%. For a fair comparison, we employ the same data for different methods.</p><p>The results are shown in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>. In all data proportions, SSKD achieves the best result. As training samples decrease, the superiority of our method becomes more apparent, e.g., ? 7% absolute improvement in accuracy compared to all competing methods when the percentage of reserved samples are 25%. Previous methods mainly focus on learning various intermediate features of teacher or exploring the relations between samples. The excessive mimicking leads to overfitting on the training set. In SSKD, the transformed images and self-supervision task endow the student model with structured knowledge that provides strong regularization, hence making it generalizes better to test set. Noisy-Label Scenario. Our SSKD forces student to mimic teacher on both category classification task and self-supervision task. The student learns more well rounded knowledge from the teacher model than relying entirely on annotated labels. Such strategy strengthens the ability of student model to resist label noise. In this section, we investigate the performance of KD <ref type="bibr" target="#b14">[16]</ref>, FT <ref type="bibr" target="#b18">[20]</ref>, CRD <ref type="bibr" target="#b40">[42]</ref> and SSKD when trained with noisy label data. We choose vgg13 and vgg8 as the teacher and student models, respectively. We assume the teacher is trained with clean data and will be shared by all students. This assumption does not affect our evaluation on robustness of different knowledge distillation methods. When training student models, we randomly perturb the labels of certain portions of training data and use the original test data for evaluation. We introduce same disturbances to all methods. Since the loss weight of cross entropy on annotated labels affects how well a model resists label noise, we use the same loss weight for all methods for a fair comparison. We set the percentage of disturbed labels to be 0%, 10%, 30% and 50%. Results are shown in <ref type="figure" target="#fig_2">Fig. 4(b)</ref>. SSKD outperforms competing methods in all noise ratios. As noise data increase, the performance of FT and CRD drop dramatically. KD and SSKD are more stable. Specifically, accuracy of SSKD only drop by a marginal 0.45% when the percentage of noise data increases from 0% to 50%, demonstrating the robustness of SSKD against noisy data labels. We attribute the robustness to the structured knowledge offered by self-supervised tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a novel framework called SSKD, the first attempt that combines self-supervision with knowledge distillation. It employs contrastive prediction as an auxiliary task to help extracting richer knowledge from teacher network. A selective transfer strategy is designed to suppress the noise in teacher knowledge. We examined our method by conducting thorough experiments on CIFAR100 and ImageNet using various architectures. Our method achieves stateof-the-art performances, demonstrating the effectiveness of our approach. Further analysis showed that our SSKD can make student more similar to teacher and work well under few-shot and noisy-label scenarios.</p><p>the details when combining other three self-supervision tasks with knowledge distillation. Exemplar. Exemplar treats each instance of the dataset as a separate class. It applies heavy image transformations to the original image and forces the network to classify transformed image correctly. When combining it with KD, we adopt the same transformations as discussed in Sec 6.2. The SS module is a classifier with class number equalling to the dataset size. In student's training, we transfer the logits of this classifier from teacher to student. Jigsaw. Jigsaw splits the original image into several non-overlapping patches and permutes them. It re-organizes these patches into image format and forces the network to recognize the permutation pattern. When combining it with KD, we split each image into 2 ? 2 = 4 patches. Thus, the SS module is a classifier with 24 (4! = 24) classes. In student's training, we transfer the logits of this classifier. Rotation. Rotation first rotates the image with four angles, i.e., 0 ? , ?90 ? , 180 ? and then forces the network to classify the rotation angle correctly. When combining it with KD, the SS module is a 4-way classifier. In student's training, we transfer the logits of this classifier from teacher to student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Training Hyperparameters</head><p>Hyperparameters in Competing Methods. The losses of all competing methods can be uniformly written as:</p><formula xml:id="formula_9">L = ? 1 L ce + ? 2 L kd + ? 3 L distill ,<label>(9)</label></formula><p>where L ce , L kd and L distill are cross-entropy loss, conventional KD loss and specially designed loss in each method, respectively. In CIFAR100, we re-run all the competing methods using the implementation of CRD <ref type="bibr" target="#b40">[42]</ref>. We set ? 1 = 0.1, ? 2 = 0.9. For ? 3 of different methods, we use the values reported in CRD. Note that CRD sets the ? 2 in all methods to be 0, so the results of competing methods in our experiments are better than those in CRD. In ImageNet, we do not re-run the competing methods, but copy their results from CRD directly. Hyperparameters in SSKD. In CIFAR100, we set the temperature ? in L kd and L T to be 4 and ? in L ss to be 0.5. For student training, we set ? 1 = 0.1, ? 2 = 0.9, ? 3 = 2.7, ? 4 = 10.0 in Eq 8. We train all the models for 240 epochs. The initial learning rate is 0.05 and is decayed by a factor of 10, respectively, at 150, 180 and 210 epochs. We run experiments on a TITAN-X-Pascal GPU with a batch size of 64. An SGD optimizer with a 5 ? 10 ?4 weight decay and 0.9 momentum is adopted.</p><p>In ImageNet, we use the same temperatures and loss weights as those in CIFAR100 experiments, except that ? 1 is set to 1.0. We train all the models for 100 epochs. The initial learning rate is 0.1 and is decayed by 10, respectively, at 30, 60 and 90 epochs. We train models with eight parallel GPUs with a total batch size of 256. The optimizer parameters are the same as those in CIFAR100 experiments.</p><p>(a) Teacher: ResNet50, Student: MobileNetV2 (b) Teacher: resnet32?4, Student: ShuffleNetV2 <ref type="figure">Fig. 5</ref>. The difference between correlation matrices of teacher's and student's classifier weights (of CIFAR100). The correlation matrices are computed using normalized weights. We select two teacher-student pairs and four methods: Vanilla student, AB <ref type="bibr" target="#b13">[15]</ref>, CRD <ref type="bibr" target="#b40">[42]</ref> and our SSKD. SSKD achieves significant matching between student's and teacher's correlations, indicating that it captures the teacher's correlation structures best</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Visualization of Correlation Difference</head><p>We compute the difference (L1 error) between the correlation matrices of the teachers and students classifier weights and visualize the difference through heatmap <ref type="figure">(Fig. 5</ref>). We select four methods: vanilla student without distillation and students trained by AB <ref type="bibr" target="#b13">[15]</ref>, CRD <ref type="bibr" target="#b40">[42]</ref> and our SSKD. It is clear that SSKD obtains the smallest difference on both two teacher-student pairs, indicating that SSKD captures the teacher's correlation structure best.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Training scheme of SSKD. Input images are transformed by designated transformations to prepare data for the self-supervision task. Teacher and student networks both contain three components, i.e., backbone f (?), classifier p(?) and SS module c(?, ?). Teacher's training are split into two stages. The first stage trains ft(?) and pt(?) with a classification task, and the second stage fine-tunes ct(?, ?) with a selfsupervision task. In student's training, we force the student to mimic teacher on both classification output and self-supervision output, besides the standard label loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) t-SNE visualization (b) The effects of LT and Lss Effectiveness of Self-Supervision Auxiliary Task. Mimicking the selfsupervision output benefits the feature learning and final classification performance. (a) t-SNE visualization of learned features by mimicking teacher's self-supervision output. Each color represents one category. (b) The consistent improvement across all four tested teacher-student network pairs demonstrates the effectiveness of including self-supervision task as an auxiliary task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Accuracies on CIFAR100 test set under few-shot and noisy-label scenarios. (a) Students are trained with subsets of CIFAR100. SSKD achieves the best results in all cases. The superiority is especially striking when only 25% of the training data is available. (b) Students are trained with data with perturbed labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="6">Teacher wrn40-2 wrn40-2 resnet56 resnet32?4 vgg13</cell></row><row><cell cols="2">KD between</cell><cell cols="6">Student wrn16-2 wrn40-1 resnet20 resnet8?4 vgg8</cell></row><row><cell cols="2">Similar Architectures.</cell><cell>Teacher</cell><cell>76.46</cell><cell>76.46</cell><cell>73.44</cell><cell>79.63</cell><cell>75.38</cell></row><row><cell cols="2">Top-1 accuracy (%) on</cell><cell>Student</cell><cell>73.64</cell><cell>72.24</cell><cell>69.63</cell><cell>72.51</cell><cell>70.68</cell></row><row><cell cols="2">CIFAR100. Bold and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">underline denote the</cell><cell>KD [16]</cell><cell>74.92</cell><cell>73.54</cell><cell>70.66</cell><cell>73.33</cell><cell>72.98</cell></row><row><cell cols="2">best and the second best</cell><cell cols="2">FitNet [37] 75.75</cell><cell>74.12</cell><cell>71.60</cell><cell>74.31</cell><cell>73.54</cell></row><row><cell cols="2">results, respectively. We</cell><cell>AT [46]</cell><cell>75.28</cell><cell cols="2">74.45 71.78</cell><cell>74.26</cell><cell>73.62</cell></row><row><cell cols="2">denote by * methods</cell><cell>SP [43]</cell><cell>75.34</cell><cell>73.15</cell><cell>71.48</cell><cell>74.74</cell><cell>73.44</cell></row><row><cell cols="2">that we re-run using</cell><cell>VID [2]</cell><cell>74.79</cell><cell>74.20</cell><cell>71.71</cell><cell>74.82</cell><cell>73.96</cell></row><row><cell>author-provided</cell><cell>code.</cell><cell cols="2">RKD [33] 75.40</cell><cell>73.87</cell><cell>71.48</cell><cell>74.47</cell><cell>73.72</cell></row><row><cell cols="2">SSKD obtains the best</cell><cell cols="2">PKT [34] 76.01</cell><cell>74.40</cell><cell>71.44</cell><cell>74.17</cell><cell>73.37</cell></row><row><cell cols="2">results on four out of five</cell><cell>AB [15]</cell><cell>68.89</cell><cell>75.06</cell><cell>71.49</cell><cell>74.45</cell><cell>74.27</cell></row><row><cell cols="2">teacher-student pairs</cell><cell>FT [20]</cell><cell>75.15</cell><cell>74.37</cell><cell>71.52</cell><cell>75.02</cell><cell>73.42</cell></row><row><cell></cell><cell></cell><cell cols="3">CRD* [42] 76.04 75.52</cell><cell>71.68</cell><cell>75.90</cell><cell>74.06</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell cols="3">76.04 76.13 71.49</cell><cell cols="2">76.20 75.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Teacher KD FitNet AT FT CRD Ours CIFAR100?STL10 71.58 71.01 73.25 73.77 73.47 73.56 74.44 74.74 CIFAR100?TinyImageNet 32.43 27.74 32.05 33.28 33.75 33.69 34.30 34.54 Teacher-Student Similarity. KL-divergence and CKA-similarity<ref type="bibr" target="#b20">[22]</ref> between student and teacher networks. Bold and underline denote the best and the second best results, respectively. All the models are trained on CIFAR100 training set. ? (?) indicates the smaller (larger) the better. SSKD wins in five out of six comparisons</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR100 test set</cell><cell cols="2">STL10 test set</cell><cell cols="2">SVHN test set</cell></row><row><cell cols="7">Metric KL-div(?) CKA-simi(?) KL-div(?) CKA-simi(?) KL-div(?) CKA-simi(?)</cell></row><row><cell>KD [16]</cell><cell>6.91</cell><cell>0.7003</cell><cell>16.28</cell><cell>0.8234</cell><cell>15.21</cell><cell>0.6343</cell></row><row><cell>SP [43]</cell><cell>6.81</cell><cell>0.6816</cell><cell>16.07</cell><cell>0.8278</cell><cell>14.47</cell><cell>0.6331</cell></row><row><cell>VID [2]</cell><cell>6.76</cell><cell>0.6868</cell><cell>16.15</cell><cell>0.8298</cell><cell>12.60</cell><cell>0.6502</cell></row><row><cell>FT [20]</cell><cell>6.69</cell><cell>0.6830</cell><cell>15.95</cell><cell>0.8287</cell><cell>12.53</cell><cell>0.6734</cell></row><row><cell>RKD [33]</cell><cell>6.68</cell><cell>0.7010</cell><cell>16.14</cell><cell>0.8290</cell><cell>13.78</cell><cell>0.6503</cell></row><row><cell>FitNet [37]</cell><cell>6.63</cell><cell>0.6826</cell><cell>15.99</cell><cell>0.8214</cell><cell>16.34</cell><cell>0.6634</cell></row><row><cell>AB [15]</cell><cell>6.51</cell><cell>0.6931</cell><cell>15.34</cell><cell>0.8356</cell><cell>11.13</cell><cell>0.6532</cell></row><row><cell>AT [46]</cell><cell>6.61</cell><cell>0.6804</cell><cell>16.32</cell><cell>0.8204</cell><cell>15.49</cell><cell>0.6505</cell></row><row><cell>PKT [34]</cell><cell>6.73</cell><cell>0.6827</cell><cell>16.17</cell><cell>0.8232</cell><cell>14.08</cell><cell>0.6555</cell></row><row><cell>CRD [42]</cell><cell>6.34</cell><cell>0.6878</cell><cell>14.71</cell><cell>0.8315</cell><cell>10.85</cell><cell>0.6397</cell></row><row><cell>Ours</cell><cell>6.24</cell><cell>0.7419</cell><cell>14.91</cell><cell>0.8521</cell><cell>10.58</cell><cell>0.7382</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For experiments on CIFAR100, since we add the conventional KD with competing methods, the results are slightly better than those reported in CRD<ref type="bibr" target="#b40">[42]</ref>. More details on experimental setting are provided in the appendix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.6909</idno>
		<title level="m">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Dataefficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inter-region affinity distillation for road marking segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised knowledge distillation using singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge distillation via instance relationship graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01991</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representions by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Knowledge transfer with jacobian matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4730" to="4738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-supervised learning via conditional motion propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">51] as the network backbones. For resnet, We use resnet-d to represent CIFAR-style resnet with three groups of basic blocks, each with 16, 32 and 64 channels, respectively. resnet8?4 and resnet32?4 indicate a 4? wider network (with 64, 128 and 256 channels for each block). For ResNet, ResNet-d represents ImageNet-style ResNet with Bottleneck blocks and more channels. For WideResNet (wrn), wrn-d-w represents wide ResNet with depth d and width factor w. For MobileNet, following [42], we use a width multiplier of 0.5. For vgg, ShuffleNetV1 and ShuffleNetV2</title>
		<idno>Network Architectures We adopt cifar-style resnet [13], ResNet [13], WideResNet [45], MobileNet [18], vgg [38] and ShuffleNet [40</idno>
		<imprint/>
	</monogr>
	<note>we adapt their architectures to CIFAR100 [23] dataset from their original ImageNet [6] counterparts</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Data Processing Data Augmentation of Normal Images</title>
	</analytic>
	<monogr>
		<title level="m">CIFAR100, We pad the original</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">We select four kinds of transformations to construct the transformation pool, namely: 1) color dropping, 2) rotation (?90 ? , 180 ? ), 3) cropping + resizing, 4) color jitter</title>
	</analytic>
	<monogr>
		<title level="m">ImageNet, the images are randomly cropped, into sizes ranging from 0.08 in Self-Supervision</title>
		<imprint/>
	</monogr>
	<note>SSKD, the input images contain both normal images and transformed version. Color dropping converts an RGB image to a gray image through L = 0.229R +</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">where L is the gray value. Rotation is specifically performed at three angles, i.e., ?90 ? and 180 ? , because rotation at these angles do not introduce artifacts</title>
		<idno>587G + 0.114B</idno>
		<imprint/>
	</monogr>
	<note>black regions at the image edge</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Implementations of different self-supervision tasks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">In ablation study, we investigate the effects of different self-supervision tasks</title>
		<idno>Jigsaw [30] and Rotation [21</idno>
		<imprint/>
	</monogr>
	<note>i.e., Contrastive [4], Exemplar [9</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
