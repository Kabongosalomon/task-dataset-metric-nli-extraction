<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxiang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">XUAN WANG</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<region>Tencent AI</region>
									<country>China, China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">YICHUN SHI</orgName>
								<orgName type="institution" key="instit2">ByteDance Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">JUE WANG</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">LIZHEN WANG</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<region>Tencent AI</region>
									<country>China, China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">YEBIN LIU</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Image manipulation Additional Key Words and Phrases: 3D GAN, Image synthesis Authors&apos; addresses: Jingxiang Sun, Tsinghua University, China</term>
					<term>Xuan Wang, Tencent AI Lab, China</term>
					<term>Yichun Shi, ByteDance Inc, USA</term>
					<term>Lizhen Wang, Tsinghua University, China</term>
					<term>Jue Wang, Tencent AI Lab, China</term>
					<term>Yebin Liu, Tsinghua University, China</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Our 3D portrait image generator allows users to perform interactive global and local editing on shape and texture in a view-consistent way. First row: starting from the source image (a), we gradually change the person's glasses (b), haircut (c), expression (d) and the global texture (e) with view consistency. Second row: Our method allows for disentangled region-level texture adjustment including hair (f), eyes &amp; eye brows (g), cloth &amp; background (h), the whole face region (i), etc.. Moreover, our method learns high-quality geometry from a collection of 2D images without multi-view supervision (j).</p><p>Existing 3D-aware facial generation methods face a dilemma in quality versus editability: they either generate editable results in low resolution, or high quality ones with no editing flexibility. In this work, we propose a new approach that brings the best of both worlds together. Our system consists of three major components: (1) a 3D-semantics-aware generative model that produces view-consistent, disentangled face images and semantic masks;</p><p>(2) a hybrid GAN inversion approach that initialize the latent codes from the semantic and texture encoder, and further optimized them for faithful reconstruction; and (3) a canonical editor that enables efficient manipulation of semantic masks in canonical view and producs high quality editing results. Our approach is competent for many applications, e.g. free-view face drawing, editing and style control. Both quantitative and qualitative results show that our method reaches the state-of-the-art in terms of photorealism, faithfulness and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Portrait synthesis finds broad applications in computer graphics including virtual reality (VR), augmented reality (AR), avatars-based telecommunication and so on. In this context, a generator is expected to synthesize photo-realistic facial images with control over multiple factors (e.g. haircut, wearings, poses, pupil color). Existing StyleGAN-based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> achieve the editing ability by either learning attribute-specific directions in the latent space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>, or learning a more disentangled and controllable latent space through conditioning various priors <ref type="bibr">[8-10, 13, 24, 30, 35, 37, 38, 40, 47]</ref>. Such methods are effective on synthesizing 2D images, but suffer from view-inconsistency if directly applied for editing different views of a 3D face.</p><p>Exploiting implicit neural representations to build 3D-aware GANs has recently gained considerable attention. Early NeRF-based generators <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref> are proposed to generate view-consistent portraits relying on volumetric representation, which is memory-inefficient and can only synthesize images with limits resolution and fidelity. Several approaches are proposed to alleviate this problem by using the CNN-based up-sampler <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Recently, Sun et al. <ref type="bibr" target="#b35">[36]</ref> attempt to enable local face editing in the generated NeRF arXiv:2205.15517v1 [cs.CV] 31 May 2022 representations via GAN inversion, however the results exhibit inadequate photorealism. Furthermore, the optimization-based GAN inversion is very time-consuming, thus is not suitable for real-time interactive editing tasks.</p><p>In this work, we propose a high-resolution 3D-aware generative model that not only enables local control of the facial shape and texture, but also supports real-time, interactive editing. Our framework consists of a multi-head StyleGAN2 feature generator, a neural volume renderer and a 2D CNN-based up-sampler. To disentangle different facial attributes, shape and texture codes are injected separately into both the shallow and the deep layers of the StyleGAN-based feature generator. The output features are then used to construct the spatially aligned 3D volumes of shape (encoded in facial semantics) and texture in an efficient tri-plane representation. Given the generated 3D volumes, free-view portraits can be rendered via the volume rendering and a 2D CNN-based up-sampler with satisfactory view-consistency and photorealism.</p><p>To enable various face editing applications, it is necessary to map the input image and semantic mask to the latent space and edit the encoded face via GAN inversion techniques. One possible solution is to use optimization-based GAN inversion as done in FENeRF <ref type="bibr" target="#b35">[36]</ref>, whereas two obvious drawbacks exist. On the one hand, it is difficult to obtain a proper initialization that can produce optimal latent codes. On the other hand, optimization is usually too time-consuming for interactive editing. We thus adopt a hybrid GAN inversion approach. Given the input facial image and corresponding semantic masks, we use the texture and semantic encoders to obtain corresponding latent codes, which are used as the initialization for the optimization-based pivotal tuning <ref type="bibr" target="#b31">[32]</ref> to obtain high-fidelity reconstruction.</p><p>Given the optimized latent codes, editing can be performed by directly drawing on the inverted semantic masks. These modifications can then be mapped into the latent space via the semantic encoders. Unfortunately, the output latent code of the encoders cannot be used to faithfully reconstruct the input images and semantic masks. This is because it is too difficult for these encoders to learn the mapping from pose-entangled (semantic) image space to the pose-invariant latent space. It is possible to use the proposed hybrid GAN inversion again to improve faithfulness, however it makes the editing operation time-consuming. To overcome this limitation, we present a canonical editor: the additional encoders that always take as input the images in canonical view and map them into the latent space. Note that the optimization-based pivotal tuning runs only once per input image to get the inverted latent codes. We then perform all the following operations through the proposed canonical editor. Benefiting from view normalization, the editor then enables real-time editing without sacrificing faithfulness.</p><p>In summary, we propose a locally disentangled, semantics-aware 3D face generator which supports interactive 3D face synthesis and local editing. Our method supports various free-view portrait editing tasks with the state-of-the-art performance in photorealism and efficiency. Our main technical contributions are as followings:</p><p>? We present a high-resolution semantic-aware 3D generator which enables disentangled control over local shape and texture.</p><p>? We propose an hybrid GAN inversion method, which can faithfully invert the facial image and semantic mask into the latent spaces. ? We propose a canonical editor module that is capable of realtime editing on free-view portrait with high quality results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Generative 3D-aware Image Synthesis</head><p>Generative adversarial networks have recently achieved tremendous breakthroughs in terms of image quality and editability for 2D image synthesis. In recent years, neural scene representations have lifted image generation into 3D settings with explicit camera control. Early voxel-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">46]</ref> fail to synthesize the complex scenes and photo-realistic details due to the limited grid resolutions. The Implicit Neural Representations (INR), especially the Neural Radiance Fields (NeRF), which have proven to generate high-fidelity results in novel view synthesis, are introduced to 3D-aware generative models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>. However, the costly sampling and MLP queries in neural volume rendering make those unsuitable for training high-resolution GANs. To this end, the recently developed 3D-aware GANs make use of a two-stage rendering process <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> or efficient sampling strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44]</ref> to tackle this problem. Meanwhile, aimed to reduce view-inconsistent artifacts brought by the 2D renderers, they adopt different strategies including NeRF path regularization <ref type="bibr" target="#b16">[17]</ref>, dual discriminators <ref type="bibr" target="#b5">[6]</ref>, etc. Based on 3D-aware GANs, FENeRF <ref type="bibr" target="#b35">[36]</ref> has attempted to edit the local shape and texture in a facial volume via GAN inversion. Nonetheless, this method suffers from the low-resolution results and its optimization-based inversion cannot meet the demand for real-time and user-interactive applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Portrait Editing</head><p>GANs are widely leveraged to learn a mapping from a reference in source domain to the target domain. Specifically for portrait synthesis, the reference are often referred to semantic masks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">47]</ref>, or hand-written sketches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In the context of semanticguided facial image synthesis, SPADE <ref type="bibr" target="#b29">[30]</ref> leverages the semantic information to modulate the image decoder for better visual fidelity and texture-semantic alignment. <ref type="bibr">Zhu et al. [47]</ref> further enable the region-wise control over the texture style relying on its regionadaptive normalization. Recently, SofGAN <ref type="bibr" target="#b7">[8]</ref> has been proposed to use semantic volumes for 3D editable image synthesis. Nevertheless, it requires multi-view images and the ground-truth 3D geometry to learn the semantic volumes. In addition, it lacks any mechanism for preserving the view-consistency in the synthesized textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GAN Inversion</head><p>GAN inversion techniques play an important role for editable portrait synthesis. Existing methods can be roughly categorized into three classes: optimization-based, learning-based and hybrid GAN inversion approaches. Optimization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> directly optimize latent codes by minimizing the difference between the input image and the reconstructed ones, which are usually harmed by the improper initialization and low efficiency. By contrast, learningbased methods exploit an encoder network to directly map the input . Moreover, the deep layers are designed to three parallel branch corresponding to each feature plane to reduce the entanglement among them. Given the generated 3D volumes, RGB images and semantic masks can be rendered jointly via the volume rendering and a 2D CNN-based up-sampler. Encoders (lower) embeds the portrait images and corresponding semantic masks into the texture and semantic latent codes by two independent proposed encoders. With a predicted camera pose, Then the fixed generator reconstructs the portrait under the predicted camera pose. In order to eliminate pose effect, we jointly train a canonical editor which takes as input the portrait images and semantic masks under the canonical view, with the consistency enforcement.</p><p>images into the latent space. Richardson et al. <ref type="bibr" target="#b30">[31]</ref> first present to train an encoder to map the images into the + space. Alaluf et al. <ref type="bibr" target="#b3">[4]</ref> further propose iterative forward refinement to predict latent codes with better image preservation. Most recently, Wang et al. <ref type="bibr" target="#b38">[39]</ref> propose the distortion consultation branch to recover high-frequency image-specific details. In the case of hybrid GAN inversion, it leverages the encoder to predict a latent code which is used as the initialization in the optimization process <ref type="bibr" target="#b44">[45]</ref>. In addition to optimizing the latent codes, Pivotal Tuning Inversion <ref type="bibr" target="#b31">[32]</ref> also fine-tunes the generator parameters for recovering image details that cannot be encoded in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we provide a more detailed description of the presented approach, IDE-3D. The organization of this section is as follows: We introduce the 3D-semantic-aware generator in Sec. 3.1, including the network architecture and the training losses. In Sec. 3.2, we first describe the encoders employed in then hybrid GAN inversion. Then we present the design of the canonical editor, as well as explain the training loss and strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Photorealistic 3D-Semantic-Aware GAN</head><p>In order to synthesize the high-definition portrait image with flexible editability, as shown in 2, our generator consists of the Multi-head StyleGAN-based feature generator which generates the semanticdisentangled 3D representations and the neural renderer that is composed of the volume renderer and a 2D CNN-based up-sampler. Multi-head StyleGAN generator. We construct the generator with a StyleGAN-based backbone and dual Multi-Layer Perceptron (MLP) decoders. Given a randomly sampled latent code ? R 512 and camera pose ? R 25 , we first map to the W + space via the network M. Different from EG3D Chan et al. <ref type="bibr" target="#b5">[6]</ref>, we inject the latent codes and into the shallow (first eight layers) and the deep (last ten layers) layers of the StyleGAN generator g respectively to obtain the disentangled semantic and texture features in tri-plane formulation.</p><p>In particular, we introduce a multi-head architecture. The deep layers of StyleGAN-based backbone are split into three branches which are connected to the same shallow layers. In other words, the three axis-aligned orthogonal texture feature planes are generated from different branches individually. We empirically find that the benefit of the multi-head architecture is two-head. On one head, it facilitates training convergence. On the other hand, it helps to avoid the "hollow face illusions", especially for the illusion of the eyes that follow the viewing camera. Dual MLP decoders. Our Dual encoder ? has two independent modules ? , ? , for facial semantic and texture, both of which are formulated as a single hidden layer MLP of 64 units. For each point in the 3D space, we query its semantic and texture feature vectors , by projecting it to axis-aligned planes of and . The ? decodes the semantic features to the volume density and semantic labels s. And the multi-channel colour feature is decoded by ? from . The total function of our semantic-aware decoder ? can be written as below:</p><formula xml:id="formula_0">? : ( , , ) ? ? ( , , ).</formula><p>(1)</p><p>Neural renderer. We adopt a neural volume renderer and a 2D CNN-based up-sampler to render the portrait image from the decoded 3D volume. Given the all the sampled volume densities, semantic labels and color features of all the pixel rays , we exploit the volume renderer to obtain a colour feature map and a semantic mask in low resolution. In specific, the first three channels of store the ( , , ) values. It contains a low-resolution portrait in the color feature map.</p><p>In order to bring the image rendering into high resolution and photorealism, a 2D CNN-based up-sampler is introduced. The rendered intermediate color feature</p><p>and semantic map are upsampled into the final high-resolution RGB image ? and semantic map ? via a two StyleGAN2 synthesis blocks which are modulated by the corresponding latent codes and predict the residuals of the LR inputs. Dual discrimination and training objective. Aimed to model the joint distribution of aligned real RGB images and semantic masks ( , ), we present the dual discriminator D which takes as input both RGB images and semantic masks. As mentioned before, the first three channels of feature map exactly form a low-resolution RGB image. The dual discriminator thus takes as input the concatenation of resized LR rgb/semantics image and the synthesised HR rgb/semantic image ( ? , ? , ? , ? ). D is designed with has two independent convolution blocks as well as minibatch standard deviation layers for RGB images ( ? , ? ) and semantic m( ? , ? ), followed by a concatenation before the final output. The dual-branch discriminator has two advantages: 1) Decoupling RGB image and semantic mask reduces the unnatural contours of synthesized images effected by semantic mask; 2) It supports for regularizing the gradient penalty of image and semantic branches separately. We obtain the discrimination loss as follows:</p><formula xml:id="formula_1">L D ,G =E ?Z, ?P [ (D (G( , )))]+ E ?P I ,S?P S [ (?D ( , , , )+ ??D ( )? 2 ) + ??D ( )? 2 )]<label>(2)</label></formula><p>Density regularization As pointed in EG3D <ref type="bibr" target="#b5">[6]</ref>, we sometimes observe the "seam" artifacts along the edge of the faces in both synthesized image and the extracted mesh. A possible reason could be the imbalanced camera pose distribution of datasets. In particular, the distribution of head pose angles, in FFHQ dataset, has relatively small variance, thus it is challenging to learn a good shape of faces rendered from side views. In IDE-3D, we introduce a density regularization loss which regularize the density field such that nearby 3D points have similar density values, inspired by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. The density regularization loss is:</p><formula xml:id="formula_2">L = ?? ?S ? ( ) ? ( + )? 2<label>(3)</label></formula><p>Here, S denotes the set of fine points, ( ) is the predicted density of point . The total learning objective is:</p><formula xml:id="formula_3">L =L D ,G + L<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hybrid GAN Inversion and Canonical Editor</head><p>Given a pre-trained generator, the GAN inversion approach is necessary for interactive editing. In this context, photorealism, faithfulness and efficiency are the most important factors.</p><p>Encoder training and multi-view augmentation. A hybrid GAN inversion method, which can be regarded as a combination of the learning-based and optimization-based approaches, is introduced. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, we adopt two encoders that map the input image and semantic map to the latent spaces W + and W + . In our case, the camera pose is also needed to be extracted from the poseentangled 2D images. We achieve it by appending two dimensions in the output of the texture branch for predicting yaw and pitch. We adopt two types data during training. One is the real data from specific dataset. The other is synthetic which can be rendered by the pre-trained generator. Furthermore, we present a multi-view augmentation strategy. In particular, for a synthetic portrait image, we additionally render it from another random views and train them together with two-fold consistency constrains. On the one hand, as they are synthesized from the same latent code, their inverted codes should be the same. On the other hand we also add a high-level feature consistency loss on the reconstructed images. We observe that the multi-view augmentation benefits a better contentpose disentanglement and improve the stability of learning-based inversion across different views. During real portrait editing, we gain the inverted latent code as an initialization for pivotal tuning <ref type="bibr" target="#b31">[32]</ref> to faithfully reconstruct the inputs. Canonical editor. In conjunction with the optimization-based tuning, the presented hybrid GAN inversion can reconstruct the inputs faithfully. Unfortunately, the time-consuming tuning process is impractical for the real-time tasks. The possible reason that the encoders cannot yield the faithful latent codes is learning the mapping from pose-entangled images to the pose-invariant latent space could be too difficult. To tackle this problem, we propose a specific encoder, the canonical editor which only map canonical-viewed images and semantic masks into latent spaces as shown in bottom-right of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>takes as input canonical-view synthetic face images and semantic masks rendered from a pretrained generator in training. During real portrait editing, we gain the reconstructed latent code by exploit the aforementioned hybrid inversion only once, and all the following editing operations are performed thorough the . Experiments demonstrate that the canonical editor produces much more faithful results compared with the other encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATIONS AND EXPERIMENTS</head><p>In this section we conduct a series of qualitative and quantitative experiments to demonstrate the superiority of our framework from three dimensions: photorealism, faithfulness and editability <ref type="bibr" target="#b40">[41]</ref>.   Datasets. We train and evaluate our models on two popular datasets for face synthesis and editing: CelebAHQ-Mask <ref type="bibr" target="#b22">[23]</ref> and FFHQ <ref type="bibr" target="#b20">[21]</ref>. For both datasets, we predict approximate camera extrinsics for each image with a off-the-shelf head pose estimator <ref type="bibr" target="#b14">[15]</ref>, and all images share a set of constant camera intrinsics. For semantic masks, we adopt the provided ones of CelebAHQ-Mask and label the semantic classes for FFHQ with a pretrained face parser following SofGAN. For data augmentation, we augment both datasets by horizontal flip as well as rebalance which is aimed to increase the sampling probability of images photoed at steep angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons</head><p>Baselines. We select different baselines for various tasks. For image synthesis and geometry quality, we compare our method with the state-of-the-art high-resolution 3D generative models, StyleNeRF, StyleSDF and EG3D, as well as FENeRF which share some similarity with us on semantic awareness and editing ability on local shape. For the task of semantic-guided synthesis and editing, we compare our method with FENeRF and 2D GANs, including SPADE, SEAN and SofGAN. Qualitative comparisons. <ref type="figure" target="#fig_1">Fig. 3</ref> presents a qualitative comparison against baselines. FENeRF synthesizes view-consistent images, while the rendering resolution and image quality are limited. StyleN-eRF and StyleSDF show impressive image synthesis quality and StyleSDF also learns a good quality shape with the implicit SDF representation and geometry constraint during training, but it fails to capture shape details. our method achieves not only higher photorealism in image synthesis but also higher-fidelity geometry. Quantitative evaluations. We quantitatively evaluate the photorealism and the diversity of image synthesis using he Frechet Inception Distance (FID) <ref type="bibr" target="#b18">[19]</ref> and Kernel Inception Distance (KID) <ref type="bibr" target="#b4">[5]</ref>. Following <ref type="bibr" target="#b5">[6]</ref>, we evaluate view consistency assessed by multi-view facial identity consistency (ID). This metric calculates the mean Arcface <ref type="bibr" target="#b8">[9]</ref> cosine similarity score between pairs of the same synthetic face rendered from two random camera poses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Canonical editor. Recall that we propose two encoders, a general encoder E and a canonical editor E . In order to investigate the effectiveness of E , we conduct experiments to explore: 1) how the input view angles influence on the inversion quality of ; 2) Is it easier to learn the style mapping under the canonical view than random views? First we sample 4000 camera poses using a stratified sampling approach ranging from t -63 ? to +63 ? . Then we sample 4000 latent codes and render each in a sampled view for input , the front view for E input , and a random view for test . Given the predicted latent code from E and E , we utilize them to render the test view and compute the similarity with , respectively. Furthermore, in order to eliminate the data effect, we also process the canonical view image into E. <ref type="figure">Fig 4 plots</ref> the inversion performance of the three settings above assessed by 3 metrics against the yaw angles of our sampled views. We can see that the inversion faithfulness of E degenerates with the head pose getting far from the canonical view, which indicates the steep view angles truly affect the style mapping ability. Moreover, while the performance of E and E with the canonical input both remain stable along with the changing poses, E illustrates the superior performance on all metrics, indicating it is easier to learn a good style mapping in the canonical view. <ref type="figure">Fig 5 shows</ref> two examples of our experiment results. Density regularization. In this work we propose the density regularization to enforce the nearby points to the similar volume density, <ref type="figure">Fig. 4</ref>. The inversion ability of T/S encoder degenerates along with the increasing yaw angles. Input with the fixed canonical view (green "X" point) helps to stabilize the performance. Our proposed canonical editor (blue "star" point) shows the superior inversion capacity, indicating the effectiveness of learning style mapping from the canonical view. <ref type="figure">Fig. 5</ref>. Effectiveness of our canonical style encoder. We compare the inversion results of the standard encoder, the standard encoder with canonical input and our canonical encoder. Both the qualitative results and the quantitative errors prove the superiority of our canonical style encoder. <ref type="figure">Fig. 6</ref>. Effect of density regularization. We synthesize images and geometry for seeds 0-3 with truncation = 0.5, using two models w/ and w/o density regularization.</p><p>which effectively remove the "seam" shape artifacts of side faces. <ref type="figure">Fig. 6</ref> demonstrates the comparison between our model trained w/ and w/o density regularization. It is obviously that the renderings of the model w/o density regularization show "seam" artifacts in both images and the extracted meshes, with varying degrees. On the contrary, density regularization wipe off such artifacts without any degeneration of shape quality. Hybrid GAN inversion. We perform the experiments to illustrate the effectiveness of the proposed hybrid GAN inversion. <ref type="figure">Fig. 7</ref> demonstrates the inversion performance under three kinds of initialization approaches: the averaged random latent codes (Avg init), reconstructed latent code by the encoder with (without) multi-view augmentation (Encoder init w/ or w/o aug.). Note the first initialization manner is exactly the optimization-based inversion method. Recall the multi-view augmentation. During training encoders, we introduce multi-view augmentation, i.e. training with multi-view rendered synthetic data with consistency enforcement to encourage the coder to decouple style encoding from pose-entangled RGB (semantic) images. We can see that the encoder without multi-view augmentation provides a reasonable shape and texture prior thus obtains more consistent faces under novel views than average initialization. However, it still suffers from inconsistent artifacts in novel views with self occlusion (the 5th column in both cases). On the contrary, the encoder with multi-view augmentation significantly alleviate the inconsistent artifacts and also learns a more faithful geometry, indicating the augmentation helps to learns a more view-consistent style mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Applications</head><p>Interactive neural 3D face drawing and editing. IDE-3D supports interactive 3D face drawing and editing. Given a semantic mask taken from real portrait images or randomly sampled from the shape latent space, our method can generate a 3D face with the same layout as the input semantic mask. As demonstrated in <ref type="figure">Fig. 8</ref>, we interactively edit the semantic mask to obtain the user-desired shapes in a view consistent manner. Region-level texture editing. Our semantic-aware synthesis framework enables region-level texture style controlling. Benefiting from the semantic segmentation branch of neural decoder, we can predict the semantic labels of each 3D point. By selecting user-specific target regions( e.g. hair, eyes, cloth etc), we can construct a 3D style mask which only select 3D points belonging to these semantic classes according to their semantic labels. Then given a desired texture style, we utilize it to generate new texture triplane and replace the color of selected 3D points by querying from this triplane. <ref type="figure">Fig. 1</ref> demonstrates the region-level texture style adjustment in hair, lips, cloth and background. Our approach is capable of maintaining the texture style unchanged except the target regions. Please refer to our supplementary video for more visualization examples. Real Portrait local editing. Our IDE-3D enables high-fidelity 3D inversion and interactive editing of real portrait images. We take a comparison with SofGAN and FENeRF, two representative works in 2D and 3D semantic guided editing, as illustrated in <ref type="figure">Fig. 9</ref>. First, We take three real portrait images and project them into the latent space of each synthesis framework. For a fair comparison, we adopt the unified optimization-based inversion method in StyleGAN2 <ref type="bibr" target="#b21">[22]</ref> for all three frameworks. Turing to editing, we maintain their original editing methods for the best performance. Given an edited semantic <ref type="figure">Fig. 7</ref>. Different initialization strategies for hybrid GAN inversion. Multiview augmentation helps to alleviate inconsistent artifacts within self occluded regions and reconstruct more faithful shapes. <ref type="figure">Fig. 8</ref>. Interactive 3D face drawing. Our method supports interactively synthesize view-consistent photo-realistic portrait images by drawing a semantic mask. The figure shows some facial attribute editing examples, e.g. hairstyle, glass (the first raw), hat, eyes and expression (the second raw). We can observe that our method preserves the unedited local regions well when manipulating the shape and expressions. <ref type="figure">Fig. 9</ref>. Real Portrait local shape editing. IDE-3D supports high-quality interactive real face editing using the semantic mask in a view consistent manner, which can not be guaranteed in <ref type="bibr">SofGAN and FENeRF.</ref> mask under the input view, we combine it with the optimized latent code into a pretrained SofGAN model to obtain the edited portrait image. In order to obtain the free-viewed results, we need to project the semantic mask into its semantic field. Unfortunately, we fail to get a reasonable result. Therefore, we provide SofGAN with the free-viewed semantic masks rendered by our method instead. For FENeRF, we directly process the target semantic mask into it for the semantic inversion. <ref type="figure">Fig. 9</ref> demonstrates that our method outperforms SofGAN and FENeRF in editing fidelity, image quality and view-consistency. SofGAN and our IDE-3D both obtain high-fidelity inversion result in the input view. However, SofGAN changes the identity obviously when turning to other views. FENeRF achieves high view consistency due its 3D nature, however, struggles to reconstruct high-fidelity texture detail. FENeRF also fails to model glasses due to its limited diversity of its GAN latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose the IDE-3D consisting of a 3D-semanticaware generator, a hybrid GAN inversion approach and the canonical editor. As demonstrated in massive experiments, our method supports many the flexible and interactive face editing tasks in real time without sacrificing faithfulness and photorealism. Limitations and future works. As the free-view portrait is encoded in a 3D volume, using single image to reconstruct the 3D facial volume is an ill-posed problem. As a result, it could produce the inplausible facial geometry in some cases, which is a known limitation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A OVERVIEW</head><p>In the supplement, we first discuss the implementation details of our framework (Sec. B) including the network architecture and hyperparameters. We also provide the experiment details (Sec. C) such as datasets and baselines. Finally, we provide additional visual results (Sec. D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAILS</head><p>We implement our framework on the top of the official PyTorch implementation of StyleGAN2, with the updated version: StyleGAN3. We adopt the efficient synthesis structure and training designs of StyleGAN which incorporates equalized learning rate, label conditioned mapping network and discriminator, blur initialization tricks and GAN loss with R1 regularization.</p><p>Multi-head StyleGAN-based feature generator. In the main paper we introduce a multi-head StyleGAN-based feature generator to disentangle feature planes in the texture tri-planes. In specific, the multi-head architecture is designed to the three parallel branches with a shared input feature map from the last layer of the semantic branches with dimension 64 ? 64 ? 64. Each texture branch upsamples the shared feature map into 256 ? 256 ? 32. Semantic-aware neural renderer. Our decoder consists of two independent multi-layer perceptrons (MLPs), each with a single hidden layer of 64 units and softplus activation functions. We let one decoder to predict both semantic labels ? R 19 and density ? R 1 as we observe that the 3D points belonging to the same semantic class are more likely to share the similar geometry. The other decoder is to predict a feature vector ? R 32 . Following EG3D <ref type="bibr" target="#b5">[6]</ref>, we treat the first three channels of as low-resolution RGB image . Dual-branch discriminator. We introduce the dual branch discriminator to model the joint distribution of RGB images and semantic masks. Besides the naive concatenation mentioned in the main paper, we tried two dual branch approaches as illustrated in <ref type="figure">Fig. 11</ref>. The first method ( <ref type="figure">Fig. 11 a )</ref> first processes semantic masks and RGB images into two independent discriminator blocks and concatenate them together to calculate the standard deviation. The output of the first method is 1 ? 1 vector. Instead of concatenation, the second method shown in <ref type="figure">Fig. 11</ref> b directly outputs the two vector independently for the semantic masks and RGB images. Though the second method doesn't explicitly enforce the semantic alignment, we experimentally found that it leads to a better image quality without lost semantic-texture consistency.</p><p>Training. We train all models on 4 V100s with a batch size of 32. The learning rates of generator and discriminator are 0.0025 and 0.002, respectively. We set the gamma value of RGB image and semantic mask to 2 and 200. Such a high gamma for semantic masks prevents the semantic masks dominating the backward gradient which affects image quality. Following EG3D <ref type="bibr" target="#b5">[6]</ref>, we regularize the generator pose conditioning by randomly swapping the conditioning pose of the generator with another random pose. We fit a pose distribution from the extracted poses by an off-the-shelf pose detector <ref type="bibr" target="#b14">[15]</ref>, and randomly sample a camera pose for swapping. We adopt the blur initialization and style mixing trick in StyleGAN3 <ref type="bibr" target="#b19">[20]</ref>.</p><p>We train our models in two stages split by the neural rendering resolution: 64 ? 64 for 10M images and at 128 ? 128 for additional 2.5M images. Note that we obtain the comparable performance with EG3D with only about half training iterations (25M + 2.5M for EG3D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENT DETAILS. C.1 Baselines</head><p>We take a comparison with StyleNeRF <ref type="bibr" target="#b16">[17]</ref>, StyleSDF <ref type="bibr" target="#b28">[29]</ref>, FENeRF <ref type="bibr" target="#b35">[36]</ref>, EG3D <ref type="bibr" target="#b5">[6]</ref> and SofGAN <ref type="bibr" target="#b7">[8]</ref>. For StyleNeRF and StyleSDF, we utilize their official codes and train on different resolutions or datasets for comparison. For FENeRF, we re-implement it following the paper and obtain a reasonable performance quantitatively and qualitatively. For EG3D, we use the quantitative results in their paper for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL VISUAL RESULTS</head><p>In this section we provide additional visual results of experiments related to the our core technical contributions: the global style adjustment ( <ref type="figure" target="#fig_0">Fig. 12</ref>), interactive local shape editing ( <ref type="figure" target="#fig_1">Fig. 13</ref>), region-level texture editing ( <ref type="figure">Fig. 14)</ref>, real image editing ( <ref type="figure">Fig. 15</ref>), the effectiveness of our proposed canonical encoder ( <ref type="figure" target="#fig_5">Fig. 16</ref>) and the high-fidelity facial geometry <ref type="figure">(Fig. 17</ref>).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Pipeline of our 3D generator and encoders. The 3D generator (upper) consists of several parts. First, a StyleGAN feature generator G constructs the spatially aligned 3D volumes of semantic and texture in an efficient tri-plane representation. To decouple different facial attributes, shape and texture codes are injected separately into both the shallow and the deep layers of G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative comparison among FENeRF, StyleNeRF, StyleSDF and ours. Models are trained with FFHQ at 512 2 , except FENeRF with FFHQ at 128 2 . Note that we don't extract shape of StyleNeRF as there is no released scripts in their released codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FFHQ</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 .</head><label>10</label><figDesc>Region-level texture style adjustment. Our approach enables viewconsistent region-level texture style adjustment on 19 classes: skin, eye glasses, eye brows, hair, nose, lips, cloth, wearings, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 Fig. 12 IDE 11 Fig. 13 12?</head><label>11121113</label><figDesc>-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis ? Sun et al. Fig. 14 IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis ? 13 Fig. 15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 16</head><label>16</label><figDesc>Fig. 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table.1 reports the quantitative metrics comparing with baselines on the FFHQ and CelebAHQ-Mask datasets. Our method shows the state-of-the-art image synthesis quality and view consistency across all datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>[ 47 ]</head><label>47</label><figDesc>Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. 2020. Sean: Image synthesis with semantic region-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">? Sun et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">? Sun et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">? Sun et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8"> ? Sun et al.    to our method. To this end, we plan to study the multi-view/frame GAN inversion approaches in future works.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">? Sun et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">? Sun et al.Fig. 17</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image2stylegan++: How to edit the embedded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Restyle: A residual-based stylegan encoder via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Demystifying mmd gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<idno>arxiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sameh Khamis, Tero Karras, and Gordon Wetzstein. 2021. Efficient Geometry-aware 3D Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sofgan: A portrait image generator with dynamic styling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepFaceEditing: Deep Generation of Face Images from Sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep-FaceDrawing: Deep generation of face images from sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SNARF: Differentiable forward skinning for animating non-rigid neural implicit shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08867</idno>
		<title level="m">Gram: Generative radiance manifolds for 3d-aware image generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2017. 3d shape induction from 2d views of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stylenerf: A stylebased 3d-aware generator for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08985</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Escaping Plato&apos;s cave: 3D shape from adversarial rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>arxiv:2106.12423</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepFacePencil: Creating Face Images from Freehand Sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HoloGAN: Unsupervised learning of 3d representations from natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BlockGAN: Learning 3d object-aware scene representations from unlabelled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thu H Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Giraffe: Representing scenes as compositional generative neural feature fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11427</idno>
		<title level="m">Jeong Joon Park, and Ira Kemelmacher-Shlizerman. 2021. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pivotal Tuning for Latent-based Editing of Real Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Roich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05744</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graf: Generative radiance fields for 3d-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">InterFaceGAN: Interpreting the disentangled face representation learned by GANs. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyue</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FENeRF: Face Editing in Neural Radiance Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stylerig: Rigging stylegan for 3d control over portrait images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mingming He, Dongdong Chen, and Jing Liao. 2021. Cross-domain and disentangled face manipulation with 3d guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11228</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High-Fidelity GAN Inversion for Image Attribute Editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05278</idno>
		<title level="m">GAN inversion: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10759</idno>
		<title level="m">3D-aware Image Synthesis via Learning Structural and Textural Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GIRAFFE HD: A High-Resolution 3D-aware Generative Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09788</idno>
		<title level="m">Cips-3d: A 3d-aware generator of gans based on conditionally-independent pixel synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">In-domain GAN Inversion for Real Image Editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual object networks: Image generation with disentangled 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
