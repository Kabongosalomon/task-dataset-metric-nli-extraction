<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can We Generate Shellcodes via Natural Language? An Empirical Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liguori</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfan</forename><surname>Al-Hossami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenico</forename><surname>Cotroneo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Natella</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojan</forename><surname>Cukic</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
						</author>
						<title level="a" type="main">Can We Generate Shellcodes via Natural Language? An Empirical Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Automatic Exploit Generation ? Software Exploits ? Shellcode ? Neural Machine Translation ? Assembly</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Writing software exploits is an important practice for offensive security analysts to investigate and prevent attacks. In particular, shellcodes are especially timeconsuming and a technical challenge, as they are written in assembly language. In this work, we address the task of automatically generating shellcodes, starting purely from descriptions in natural language, by proposing an approach based on Neural Machine Translation (NMT). We then present an empirical study using a novel dataset (Shellcode IA32), which consists of 3, 200 assembly code snippets of real Linux/x86 shellcodes from public databases, annotated using natural language. Moreover, we propose novel metrics to evaluate the accuracy of NMT at generating shellcodes. The empirical analysis shows that NMT can generate assembly code snippets from the natural language with high accuracy and that in many cases can generate entire shellcodes with no errors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b58">62,</ref><ref type="bibr">12,</ref><ref type="bibr">32]</ref><p>. For example, in June 2021, GitHub updated its policy on malware and exploit research by allowing and even encouraging users to post proof-of-concept (PoC) exploits or vulnerabilities on the platform <ref type="bibr">[64]</ref>.</p><p>Among software exploits, code-injection attacks are considered the most dangerous ones, since they have the worst consequences on the victim organizations <ref type="bibr" target="#b57">[61]</ref>. Moreover, code-injection attacks have been drastically increasing with the growth of applications exposed to the Internet <ref type="bibr" target="#b69">[75]</ref>, as shown by statistics from the Common Vulnerabilities and Exposures (CVE) database <ref type="bibr" target="#b14">[16]</ref>. These attacks deliver and run malicious code (payload) on the victims' machine, in order to give attackers control of the target system. Since the payload is typically designed to launch a command shell, the hacking community generically refers to the payload portion of a code-injection attack as a shellcode. Other objectives of shellcodes include killing or restarting other processes, causing a denial-of-service (e.g., a fork bomb), leaking secret data, etc. Listing 1 shows an example of shellcode 1 in assembly for Linux OS running on the 32-bit Intel Architecture).</p><p>The development of software exploits is a technically difficult activity. Shellcodes are typically written in assembly language, in order to gain full control on the layout of code and data in stack and heap memory, to make the shellcode more compact, to obfuscate the code, and to perform low-level operations on data representation <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b59">63]</ref>. However, programming in assembly is time-consuming and has low productivity compared to high-level languages <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b67">72]</ref>.</p><p>In order to make assembly programming easier and more efficient, we investigate the use of Neural Machine Translation (NMT) for the generation of shellcodes. In general, NMT translates between different languages (including natural and programming languages), using Natural Language Processing (NLP) and Deep Learning (DL) techniques <ref type="bibr" target="#b28">[31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b75">83,</ref><ref type="bibr" target="#b8">9]</ref>, in order to learn the typical idioms of a target programming language from datasets of annotated programs. NMT is an emerging approach for code generation <ref type="bibr" target="#b80">[88,</ref><ref type="bibr" target="#b52">56]</ref> and other programming tasks, such as code completion <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b70">77]</ref>, the generation of UNIX commands <ref type="bibr" target="#b48">[52,</ref><ref type="bibr" target="#b49">53]</ref> or commit messages <ref type="bibr" target="#b40">[44,</ref><ref type="bibr" target="#b54">58,</ref><ref type="bibr" target="#b41">45]</ref>, etc. However, NMT techniques have not heretofore been applied in the field of software security to generate software exploits. In our case, developers would translate a description (intent) of a piece of code in English, into the corresponding code snippet in assembly language. For example, developers can use NMT to generate code snippets that they could not recall, or that are not yet confident to write themselves, similarly to querying a search engine, with the additional benefit of tailoring the code according to their query.</p><p>In this paper, we introduce a novel approach for generating shellcodes in assembly language, from their description in natural language. Differing from previous research, which adopts static and/or dynamic program analysis (e.g., fuzzing, program synthesis, etc.), we adopt a novel statistical, data-driven approach. Specifically, our approach leverages state-of-the-art NMT techniques. Since NMT has never been applied to low-level languages such as assembly, our approach extends NMT by introducing an Intent Parser specialized for the assembly language and adopts transfer learning to bootstrap an NMT model from a training set of shellcodes. Then, the pa-1 global _start;</p><p>Declare global _start. <ref type="bibr" target="#b1">2</ref> section .text;</p><p>Declare code section. 3 _start:;</p><p>Define the _start label. 4 cld;</p><p>Clear the direction flag. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xor ecx, ecx;</head><p>Zero out the EAX register 6 mul ecx; and the ECX register. 7 incpage:;</p><p>Declare incpage function. 8 or cx, 0xfff;</p><p>Perform logical or between the CX register and 0xfff. 9 IncAddr:;</p><p>Declare the IncAddr label. 10 inc ecx; Increment ECX. 11 push byte 0x43;</p><p>Put the syscall 0x43 into the EAX register. 12 pop eax 13 int 0x80;</p><p>Execute execve syscall. 14 cmp al, 0xf2;</p><p>Jump to the IncPage label if the contents 15 jz IncPage;</p><p>of the AL register is equal to the value 0xf2. <ref type="bibr" target="#b14">16</ref> mov eax, 0x50905090; Move 0x50905090 into EAX. <ref type="bibr" target="#b15">17</ref> mov edi, ecx;</p><p>Move ECX into EDI. <ref type="bibr" target="#b16">18</ref> scasd;</p><p>Jump to the IncAddr label if the value in the 19 jnz IncAddr;</p><p>EAX register is not equal to the doubleword addressed by EDI 20 scasd;</p><p>Jump to the IncAddr label if the value in 21 jnz IncAddr; the EAX register is not equal to the doubleword addressed by EDI 22 jmp edi; else jump to the EDI register. <ref type="bibr" target="#b21">23</ref> 24 xor ecx, ecx; Zero out the EAX register and the ECX register 25 mul ecx <ref type="bibr" target="#b24">26</ref> push eax;</p><p>Push EAX on the stack. 27 push 0x68732f2f;</p><p>Move ASCII /bin/sh into EBX. <ref type="bibr" target="#b25">28</ref> push 0x6e69622f 29 mov ebx, esp 30 mov al, 0xb;</p><p>Move 0xb into AL. <ref type="bibr" target="#b28">31</ref>  per presents an extensive evaluation of the NMT approach. As there is no unique metric able to comprehensively represent the quality of translations, we introduce new metrics for this purpose. Indeed, the generated assembly code can have high accuracy compared to the ground truth, yet it may not be a working shellcode. Or, the generated program can be compilable and executable, but it may not implement the intended shellcode. Or again, the generated program does not exactly match the ground truth, but it can still be a correct shellcode (e.g., by using alternate valid labels or addressing modes), and so on. Therefore, we evaluate NMT from several points of view.</p><p>In summary, this work provides the following key contributions: -We propose a novel approach for translating natural language into shellcode in assembly language, based on NMT. The approach improves the state-of-the-art by using a novel, specialized Intent Parser and transfer learning. To the best of our knowledge, this is the first effort towards applying NMT to automatically generate code for security purposes; -We release a curated, substantive corpus of real shellcodes from public databases, in order to support the training and evaluation of NMT systems for shellcode generation; -We propose novel metrics to evaluate the performance of NMT systems for shellcode generation. Different from the metrics commonly used in other code generation tasks, the metrics proposed in this work go beyond evaluating performance on single-line snippets of code and also encompass the ability to generate entire, compilable shellcodes. Moreover, we look at the semantic correctness of the generated shellcode; -We present an extensive empirical analysis of NMT techniques at generating shellcodes, supported by the proposed metrics and dataset. In the following, Section 2 discusses related work; Section 3 introduces background concepts; Section 4 presents the proposed approach; Section 5 describes the dataset; Sections 6 experimentally evaluates the approach; Section 7 describes the ethical considerations; Section 8 discusses the threats to validity of the work; Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is situated at the intersection of machine translation and code/exploit generation, by applying NLP techniques to software security. Accordingly, we review related work in these areas. Neural Machine Translation for Code Generation There are several recent works that focus on generating code from natural language <ref type="bibr" target="#b83">[91,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b68">74]</ref>. Ling et al. <ref type="bibr" target="#b52">[56]</ref> and Yin and Neubig <ref type="bibr" target="#b80">[88]</ref> proposed a novel neural architecture for code generation, while Xu et al. <ref type="bibr" target="#b76">[84]</ref> incorporated pre-training and fine-tuning of a model to generate Python snippets from natural language using the CoNaLa dataset <ref type="bibr" target="#b79">[87]</ref>. Furthermore, Gemmell et al. <ref type="bibr" target="#b27">[30]</ref> used a transformer architecture with relevance feedback for code generation, and reported improvements over state-of-the-art on several datasets. There also exist approaches that perform the reverse task, i.e., generating natural language from code. Oda et al. <ref type="bibr" target="#b62">[67]</ref> pioneered the task of translating python code to pseudo-code while others proposed an n-gram language model to generate comments from source code <ref type="bibr" target="#b60">[65]</ref>. Iyer et al. <ref type="bibr" target="#b38">[42]</ref> proposed an attention model that summarizes code. Code2Seq <ref type="bibr" target="#b1">[2]</ref> embeds abstract syntax tree paths to encode context and was used for code documentation generation (generating natural language from code) and code summarization. A notable example of applying code documentation generation in software engineering is generating git commit messages from git-tracked codebase changes <ref type="bibr" target="#b40">[44]</ref>. NMT has been widely adopted also for different programming tasks. For example, Lin et al. <ref type="bibr" target="#b49">[53]</ref> presented new data and semantic parsing methods to address the problem of mapping English sentences to bash commands, and Zhong et al. <ref type="bibr" target="#b85">[93]</ref> generated SQL queries from natural language. Tufano et al. <ref type="bibr" target="#b73">[80]</ref> investigated the ability of the NMT to learn how to automatically apply code changes implemented by developers during pull requests. The authors trained the model on a dataset containing pairs of code components before and after the implementation of the changes provided in the pull requests and showed that the NMT can accurately replicate the changes implemented by developers. Hata et al. <ref type="bibr" target="#b32">[36]</ref> presented Ratchet, an NMT-based technique that generates a fixed code for a given bug-prone code query. The technique uses a Seq2Seq model trained on pre-correction and post-correction code in past fixes. To prove the feasibility of the approach, the authors performed an empirical study on five open source projects, showing that Ratchet can generate syntactically valid statements with high accuracy.</p><p>Our empirical analysis investigates these recent advances in NMT in the context of the open problem of generating shellcodes in assembly language, from natural language intents.</p><p>Automated Exploit Generation. The task of exploit generation via automatic techniques has been addressed in several ways. ShellSwap [8] is a system that generates new exploits based on existing ones, by modifying the original shellcode with arbitrary replacement shellcode. Hu et al. <ref type="bibr" target="#b35">[39]</ref> developed a novel approach to construct data-oriented exploits through data flow stitching, by composing the benign data flows in an application via a memory error. They built a prototype attack generation tool that operates directly on Windows and Linux x86 binaries. Avgerinos et al. <ref type="bibr" target="#b4">[5]</ref> developed an end-to-end system for automatic exploit generation (AEG) on real programs by exploring execution paths. Given the potentially buggy program in source form, their proposal automatically looks for bugs, determines whether the bug is exploitable, and produces a working control-flow hijack exploit string. SemFuzz <ref type="bibr" target="#b84">[92]</ref> extracts necessary information from non-code text related to a vulnerability, using natural language processing and a semantics-based fuzzing process, in order to discover and trigger deep bugs. Chen et al. <ref type="bibr" target="#b11">[13]</ref> presented techniques to find out the gadgets, i.e., the basic building block in Jump Oriented Programming (JOP), and showed these gadgets are Turing complete. They implemented an automatic tool able to generate JOP shellcodes. Ding et al. <ref type="bibr" target="#b18">[20]</ref> proposed a reverse derivation of a transformation method driven by state machines indicating the status of data flows, in order to transform the original shellcode into printable Return Oriented Programming (ROP) payload. Chainsaw [1] is a tool for analyzing web applications and generating injection exploits. The tool performs static analysis and defines a model of the application behavior to generate injection exploits, by leveraging application workflow structures and database schemes. Brumley et al. <ref type="bibr" target="#b10">[11]</ref> proposed an approach for Automatic Patch-based Exploit Generation (APEG). Starting from a program and its patched version, the approach identifies the security checks added by the patch and automatically generates inputs to fail the checks. Huang et al. <ref type="bibr" target="#b36">[40]</ref> introduced a method to automatically generate exploits based on software crash analysis. This method analyzes software crashes using a symbolic failure model, to generate exploits from crash inputs and existing exploits for several types of applications. Xu et al. <ref type="bibr" target="#b78">[86]</ref> developed a tool to find buffer overflow vulnerabilities in binary programs and automatically generate exploits using a constraint solver. Vulnerability detection is achieved through symbolic execution and the exploit generated by this tool can bypass different types of protection.</p><p>Similar to our previous work <ref type="bibr" target="#b47">[51]</ref>, our approach uses natural language statements to generate exploits and adopts neither a static nor dynamic program analysis approach (e.g., fuzzing, program synthesis, etc.), but a statistical, data-driven approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>This section introduces background concepts on neural machine translation (NMT). We follow the notation defined by Eisenstein <ref type="bibr" target="#b24">[26]</ref>.</p><p>Machine translation refers to the translation of a language into another by the means of a computerized system <ref type="bibr" target="#b20">[22]</ref>. It is defined as an optimization problem, which maximizes the conditional probability that a sentence ? (t) in the target language is the likely translation of a sentence ? (s) in the source language, by using a scoring function ?:</p><formula xml:id="formula_0">? (t) = argmax ? (t) ?(? (s) , ? (t) )<label>(1)</label></formula><p>The resolution of the problem requires a decoding algorithm for computing? <ref type="bibr">(t)</ref> , and a learning algorithm for estimating the parameters of the scoring function ?. Neural network models for machine translation are based on the encoder-decoder architecture <ref type="bibr" target="#b12">[14]</ref>. The encoder network converts the source language sentence into a context vector or matrix representation z of fixed length. The decoder network then converts the encoding into a sentence in the target language by defining the conditional probability p(? (t) |? (s) ).</p><p>The decoder is typically a recurrent neural network, which generates the target language sentence one word at a time, while recurrently updating a hidden state. The encoder and decoder networks are trained end-to-end from parallel sentences. If the output layer of the decoder is a logistic function, then the entire architecture can be trained to maximize the conditional log-likelihood:</p><formula xml:id="formula_1">log p(? (t) |? (s) ) = M (t) m=1 p(? (t) m |? (t) 1:m?1 , z) (2) p(? (t) m |? (t) 1:m?1 , ? (s) ) ? exp(? ? (t) m ? h (t) m?1 )<label>(3)</label></formula><p>where the hidden state h (t) m?1 is a recurrent function of the previously generated text ? (t) 1:m?1 and the encoding z, while ? ? R (V (t)?K ) is the matrix of output word vectors for the V (t) words in the target language vocabulary, and K is the dimension of the hidden state. Seq2Seq. The simplest encoder-decoder architecture is the sequence-to-sequence model <ref type="bibr" target="#b71">[78]</ref>. In this model, the encoder is set to the final hidden state of a long short-term memory (LSTM) <ref type="bibr" target="#b34">[38]</ref> on the source sentence:</p><formula xml:id="formula_2">h (s) m = LST M (x (s) m , h (s) m?1 ) (4) z h (s) M (s) (5) where x (s)</formula><p>m is the embedding 2 of the target language word ? (s) m . The encoding then provides the initial hidden state for the decoder LSTM:</p><formula xml:id="formula_3">h (t) 0 = z (6) h (t) m = LST M (x (t) m , h (t) m?1 )<label>(7)</label></formula><p>where</p><formula xml:id="formula_4">x (t)</formula><p>m is the embedding of the target language word ? (t) m . Sequence-to-Sequence translation is nothing more than wiring together two LSTMs: one to read the source, and another to generate the target. Attention Mechanism. The weakness of using a fixed-length context vector is the difficulty to remember long sentences. Indeed, in the traditional Seq2Seq model, the intermediate states of the encoder are discarded, and only the final states (vector) are used to initialize the decoder. To overcome this limitation, Bahdanau et al. <ref type="bibr" target="#b6">[7]</ref> proposed the attention mechanism, i.e., a solution that uses a context vector to align the source sentence and target sentence. The context vector holds the information from all hidden states from the encoder and aligns them with the current target output. By using this mechanism, the model is able to look at a specific part of the source sentence and better understand the relationship between the source and target. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The keyvalue-query concepts come from retrieval systems. For example, when a user types a query to search for a resource (value) on a contents-sharing platform, the search engine maps the query against a set of keys associated with the resources in the database of the platform and will show to the user the best-matched resource. Formally speaking, for each key n, the attention mechanism assigns a score ? a (m, n) with respect to the query m, based on how much they match. In Bahdanau's paper, the score is parametrized by a feed-forward network with a single hidden layer. The output of this activation function is a vector of non-negative numbers [? m?1 , ? m?2 , . . . , ? m?N ] T , with length N equal to the size of the memory (i.e., the space of all the generated words). Each value in the memory v n is multiplied by the attention ? m?n ; the sum of these scaled values is the output. At each step m in decoding, the attentional state is computed by executing a query, which is equal to the state of the decoder, h (t) m . The resulting compatibility scores are:</p><formula xml:id="formula_5">? ? (m, n) = v ? ? tanh(? ? |h (t) m ; h (s) n )<label>(8)</label></formula><p>Transformer. In the encoder-decoder model, the keys and values used in the attention mechanism are the hidden state representations in the encoder network z, and the queries are state representations in the decoder network h (t) . Vaswani et al. <ref type="bibr" target="#b74">[82]</ref> proposed a new model architecture, the Transformer, that does not rely on the recurrent neural networks by applying self-attention <ref type="bibr" target="#b50">[54,</ref><ref type="bibr" target="#b42">46]</ref>) within the encoder and decoder. For level i, the basic equations of the encoder side of the transformer are:</p><formula xml:id="formula_6">z (i) m = M (s) n=1 ? (i) m?n (? v h (i?1) n ) (9) h (i) m = ? 2 ReLU (? 1 z (i) m + b 1 ) + b 2<label>(10)</label></formula><p>For each token m at level i, we compute self-attention over the entire source sentence. The keys, values, and queries are all projections of the vector h (i?1) . The attention scores ? (i) m?n are computed using a scaled form of softmax attention. This encourages the attention to be more evenly dispersed across the input. Self-attention is applied across multiple "heads", each using different projections of h (i?1) to form the keys, values, and queries. The output of the self-attentional layer is the representation z (i) m , which is then passed through a two-layer feed-forward network, yielding the input to the next layer h (i) .</p><p>The Transformer architecture first refines the input embedding of each token, by combining it with a positional encoding vector. The architecture has a different positional encoding vector for each position of the sentence, in order to enrich the input embedding with positional information. Then, the transformed input embeddings sequentially go through the stacked encoder layers, which all apply a self-attention process. The self-attention further refines an input embedding, by combining it with the other input embeddings for the sentence in a weighted way, in order to account for correlations among the words (e.g., to get information for a pronoun from the noun it refers to, the input embedding of the noun is given a large weight).</p><p>For more detailed information on NMT models, we refer the reader to the work of Eisenstein <ref type="bibr" target="#b24">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We leverage neural machine translation (NMT) to automatically generate shellcodes starting from their natural language description. Following prior work (e.g., <ref type="bibr" target="#b56">[60]</ref>), we build a neural network that directly models the conditional probability of translating an intent, in natural language into a code snippet in assembly language.</p><p>The main challenge towards the goal of automatically generating shellcodes is represented by the programming language, i.e., the assembly. This language is significantly different from other languages addressed so far by research on NMT, which focused so far on mainstream imperative languages such as Python and Java. Assembly is a low-level programming language with many syntactical differences from these languages. For example, assembly does not provide the concept of variable, which is instead replaced by registers, memory addresses, addressing modes, and labels. Moreover, some programming constructs in assembly require multiple statements, which instead could be expressed with only one statement of other programming languages. To address this new language for NMT, we opted to base our solution on existing deep neural network architectures: Seq2Seq with Attention, and CodeBERT. We refrained from proposing a new architecture, for several reasons: (i) using an existing, well-tested architecture can be used with more confidence in a comparative setting in which numerical issues (such as, the vanishing gradient) can be prevented; (ii) existing architectures were shown to perform well when translating from English descriptions, which is also the case of our problem; (iii) using an existing architecture enables us to reuse pre-trained models, which are costly to pre-train from scratch in terms of data size, computational time, and resources. Furthermore, assembly is a low-resource programming language and its codebases are scarce data compared to mainstream program languages and, therefore, it would be a challenge to pre-train a model from scratch on assembly-based shellcode bases. Since NMT for assembly code-based shellcodes is not investigated in prior works, there are limited resources for processing assembly codebases such as abstract syntax trees (AST), which are abundant for other programming languages and provide domain knowledge for some existing code generation architectures. Due to these reasons, we hence wanted to thoroughly investigate the strengths and weaknesses of current architectures. In the following, we briefly describe these architectures.</p><p>Seq2Seq is a common model used in a variety of neural machine translation tasks. Similar to the encoder-decoder architecture with Bahdanau's attention mechanism <ref type="bibr" target="#b6">[7]</ref>, we use a bi-directional LSTM as the encoder, to transform an embedded intent sequence into a vector of hidden states with equal length. Within the bidirectional LSTM encoder, each hidden state corresponds to an embedded token. The encoder LSTM is bidirectional, which means it reads the source sequence ordered from left to right and from right to left. To combine both directions, each hidden state for the bidirectional LSTM encoder is computed by concatenating the forward and backward hidden states in the encoder.</p><p>CodeBERT <ref type="bibr" target="#b25">[28]</ref> is a large multi-layer bidirectional Transformer architecture <ref type="bibr" target="#b74">[82]</ref>. Like Seq2Seq, the Transformer architecture is made up of encoders and decoders. CodeBERT has 12 stacked encoders and 6 stacked decoders. Compared to Seq2Seq, the Transformer architecture introduces mechanisms to address key issues in machine translation: (i) the translation of a word depends on its position within the sentence; (ii) in the target language, the order of the words (e.g., adjectives before a noun) can be different from the order of words in the source language (e.g., adjectives after a noun); (iii) several words in the same sentence can be correlated (e.g., pronouns). These problems are especially important when dealing with long sentences. Different from Seq2Seq, CodeBERT also comes with a pre-trained neural network model, learned from large amounts of code snippets and their descriptions in the English language, and covering six different programming languages, including Python, Java, Javascript, Go, PHP, and Ruby. The goal of pre-training is to bootstrap the training process, by establishing an initial version of the neural network, to be further trained for the specific task of interest <ref type="bibr" target="#b65">[70,</ref><ref type="bibr" target="#b53">57,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b9">10]</ref>. This approach is called transfer learning. In our case, we train the CodeBERT model to translate English intents to assembly code snippets using our dataset (see ? 5).</p><p>To better support such existing models at performing a new translation task, we extended the process with data processing. Data processing is an essential step to sup-</p><formula xml:id="formula_7">Post-Processing Destandardizer Assem: ['cmp', 'al', 'var0', '\n','jz', 'var1'] Slotmap: { 'var0': '0xf2','var1': '_start'} Assem: ['cmp'. 'al', '0xf2', '\n', 'jz', '_start']</formula><p>Assem: cmp al, 0xf2 \n jz _start</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standardizer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Std. Intents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Std. Snippets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intent Parser Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tagged Intents</head><p>Eng: if the contents of the al register is equal to the value 0xf2 then jump to the _start label Assem: cmp al, 0xf2 \n jz _start (0xf2, identifier) (_start, identifier) <ref type="figure">Fig. 1</ref>: Diagram showing the steps of the approach: 1) Pre-Processing of intent-code samples in both training and validation sets, 2) translation of unseen intent samples from the validation-set, and lastly, 3) Post-Processing applied to generated samples port the NMT models in the automatic code generation and refers to all the operations performed on the data used to train, validate and test the models. These operations strongly depend on the specific source and target languages to translate (in our case, English and assembly language). We process data through a pipeline of steps, which we tailored for the task of generating assembly code snippets. The data processing steps are performed both before translation (pre-processing), to train the NMT model and prepare the input data, and after translation (post-processing), to improve the quality and the readability of the code in output. <ref type="figure">Figure 1</ref> shows the architecture of our approach, along with an example of inputs and outputs at each step, further discussed in the following.</p><formula xml:id="formula_8">Eng: ['if', 'contents', 'al', 'register', 'equal', 'value', 'var0', 'then', 'jump', 'var1', 'label'] Assem: ['cmp'. 'al', 'var0', '\n', 'jz', 'var1'] Slotmap: { 'var0': '0xf2','var1': '_start'}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2Seq + Attention CodeBERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-Processing</head><p>The pre-processing starts with the stopwords filtering, i.e., by removing a set of custom compiled words (e.g., the, each, onto), in order to include only relevant data for machine translation. This phase also includes the identification of tokens, i.e., basic units which need not be decomposed in subsequent processing. Therefore, the input sequences of natural language tokens and assembly code are split in a process called tokenization. The tokenizer converts the input strings into their byte representations, and learns to break down a word into subword tokens (e.g., lower becomes [low,er]. We tokenize intents using the nltk word tokenizer <ref type="bibr" target="#b55">[59]</ref> and snippets using the Python tokenize package [73].</p><p>One task for code generation systems is to prevent non-English tokens (e.g., start) from getting transformed during the learning process. This process is known as object standardization. Abstracting important words for the assembly language can make it easier for the model to reuse existing structures learned from other imperative languages, such as moving data and changing the control flow. To perform the standardization, we adopt an intent parser, which takes in input a natural language intents and provides as output a dictionary of standardizable tokens (i.e., it identifies the correct names for the standardization process), such as the names of the registers, the actions (e.g., /bin/sh), the hexadecimal values, etc. We implement the intent parser using spaCy, an open-source, industrial-strength Natural Language Processing library written in Python and Cython. We also use custom rules defined with regular expressions to identify hexadecimal values (e.g., 0xbb), strings that fall between quotation marks, squared brackets, variable name notations (e.g., variableName, variable name), function and register names, mathematical expressions, and byte arrays (e.g., \xe3 \xa1). Hence, this component is tailored for the task of generating shellcodes in assembly language starting from their natural language description.</p><p>All tokens selected by the parser are therefore passed to the Standardizer. The standardization process simply replaces the selected token in both the intent and snippet with var#, with # denoting a number from 0 to |l|, and |l| is the number of tokens to standardize. In <ref type="figure">Fig. 1</ref>, the intent parser identifies 0xf2, and start as standardizable tokens and standardizes them to var0, and var1 respectively (based on order of appearance in the intent). To improve the process, we prevent the standardization of unimportant tokens, by compiling a dictionary of 45 assembly keywords (e.g., register, address, byte, etc.) as non-standardizable tokens. After the standardization process, both the original token and its standardized counterpart (var#) are stored in a dictionary (named Slotmap) to be used during post-processing to restore the original words.</p><p>Lastly, we create word embeddings, i.e., we map each token (in both the intent and code snippet sequences) into a numerical id representation in order to capture their semantic and syntactic information, where the semantic information correlates with the meaning of the tokens, while the syntactic one refers to their structural roles <ref type="bibr" target="#b45">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Post-Processing</head><p>Post-processing is an automatic post-editing process, applied during decoding in the translation process (i.e., after the generation of the code snippet). This phase include a Destandardizer, which uses the slot map dictionary generated by the parser to replace all keys in the standardized intent (i.e., var0 and var1) with the corresponding memorized values (i.e., 0xf2, and start).</p><p>The generated snippets are then further post-processed using regular expressions. This operation includes the removal of (any) extra-spaces in the output (e.g., between operations and operands), and the removal of (any) extra-backslashes in escaped characters (e.g., \\n). Also, during the post-processing, newline characters \n are replaced with new lines to generate multi-line snippets. As a final step, snippet tokens are joined to form a complete code snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset</head><p>We curated and released a dataset for, Shellcode IA32 <ref type="bibr" target="#b46">[50]</ref>, specific to shellcode generation. This dataset consists of 3, 200 examples of instructions in assembly language for IA-32 (the 32-bit version of the x86 Intel Architecture) collected from publicly available security exploits. The x86 is a complex instruction set computer (CISC), in which single instructions can perform several low-level operations (such as a load from memory, an arithmetic operation, and a memory store) or are capable of multistep operations or addressing modes within single instructions. The dataset is comparable in size to the popular CoNaLa dataset <ref type="bibr" target="#b81">[89]</ref> (2, 379 training and 500 test samples in the annotated version of the dataset), which is the basis for state-of-the-art studies in NMT for Python code generation <ref type="bibr" target="#b79">[87,</ref><ref type="bibr" target="#b83">91,</ref><ref type="bibr" target="#b27">30]</ref>.</p><p>We collected assembly programs used to generate shellcode from shell-storm [76] and from Exploit Database <ref type="bibr">[27]</ref>, in the period between August 2000 and July 2020. We focus on shellcode for Linux, the most common OS for security-critical network services. Accordingly, we gathered assembly instructions written for the Netwide Assembler (NASM) for Linux <ref type="bibr" target="#b22">[24]</ref>. NASM is a line-based assembler. <ref type="figure">Figure 2</ref> shows a simple example of a NASM source line. Every source line contains a combination of four fields: an optional label, to symbolically represent the address of an opcode or data location defined by the line; a mnemonic or instruction, which identifies the purpose of the statement and is optionally followed by operands specifying the data to be manipulated; an optional comment, i.e., free text ignored by the compiler. A mnemonic is not required if a line contains only a label or a comment. The assembly programs collected in the dataset implement a varied set of shellcode attacks. One of the most common and basic shellcodes is the execution of a system shell (e.g., the /bin/sh command). This shellcode is often used in combination with more sophisticated attacks. The main categories include: exfiltrating password, e.g., from /etc/passwd (a plain text-based database that contains information for all user accounts on the system); breaking a chroot jail (an additional layer of security to run untrusted programs, which can be evaded by invoking vulnerable system calls with malicious inputs); running executables with the file system permissions of the executable's owner; flushing firewall rules (e.g., IPtables). Another form of shellcodes is the egg hunter, i.e., a piece of code that when executed looks for other pieces of code (usually bigger) called the egg and passes the execution to the egg. This technique is usually used when the space of executing shellcode is limited (the available space is less than the egg size) and it is possible to inject the egg into another memory location. Shellcodes are also used to perform denial-of-service (DoS) attacks, such as for the fork-bomb attack, in which a process continually replicates itself to deplete system resources, slowing down or crashing the system due to resource exhaustion. Among the most complex shellcodes, we find the bind shell attacks. These attacks, which can easily reach hundreds of bytes, are used to open up a port on the victim system and connect to it from the remote attacking box. The complexity further increases when an attack redirects all inputs and outputs to a socket (reverse shell) in order to evade firewalls.</p><p>Each sample of the Shellcode IA32 dataset represents a snippet -intent pair. The snippet is a line or a combination of multiple lines of assembly code, following the NASM syntax. The intent is a comment in the English language (c.f. Listing 1). To take into account the variability of descriptions in natural language, multiple authors described independently different samples of the dataset in the English language. Where available, we used as natural language descriptions the comments written by developers of the collected programs. Moreover, in the preliminary phase of the dataset collection, we enriched the dataset with lines of assembly code and their relative English comments extracted from popular tutorials and books <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b44">48,</ref><ref type="bibr">81]</ref>. This helped us to learn the typical style for describing assembly code and to mitigate bias in our descriptions in English of assembly code. Once we reached confidence about the description style (i.e., the description style was recurring when adding more samples), we focused our efforts on real shellcodes, by writing ourselves the descriptions where no comment or documentation about the code snippet was available. Our dataset consists of 10% of instructions collected from books and guidelines, while the rest are from real shellcodes. However, there is no qualitative difference between both sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-line Snippets:</head><p>Since assembly is a low-level language, it is often necessary to use multiple instructions to perform a given task. Thus, we go beyond one-to-one mappings between a line of code and its comment/intent. For example, a common operation in shellcodes is to save the ASCII string "/bin/sh" into a register. This operation requires three distinct assembly instructions: push the hexadecimal values of the words "/bin" and "//sh" onto the stack register before moving the contents of the stack register into the destination register (lines 27-28-29 in Listing 1). It would be meaningless to consider these three instructions as separate. To address such situations, we include 510 lines (? 16% of the dataset) of intents that generate multiple lines of shellcodes (separated by the newline character \n). <ref type="table" target="#tab_1">Table 1</ref> shows two further examples of multi-line snippets with their natural language intent.</p><p>Statistics: <ref type="table" target="#tab_2">Table 2</ref> presents descriptive statistics of the Shellcode IA32 dataset. The dataset contains 52 distinct assembly mnemonics, excluding declarations of functions, sections, and labels. The two most frequent assembly instructions are mov (? 30% frequency), used to move data into/from registers/memory or to invoke a system call, and push (? 22% frequency), which is used to push a value onto the stack. The next most frequent instructions are the cmp (? 7% frequency), xor and jmp instructions (? 4% frequency). The low-frequency words (i.e., the words that appear only once or twice in the dataset) contribute to the 3.6% and 7.3% of the natural language and the assembly language, respectively. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the distribution of the number of tokens across the intents and snippets in the dataset. We publicly shared our entire Shellcode IA32 dataset on a GitHub repository. 3  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Analysis</head><p>This section presents an extensive evaluation of our approach to generating shellcodes from natural language descriptions. We conducted the experimental analysis to target the following experimental objectives. ? Feasibility in applying NMT for shellcode generation.</p><p>We first perform an initial assessment on the feasibility of using NMT for shellcode generation with reasonably good accuracy, by applying techniques commonly used for code generation (e.g., generating Python code from natural language). We evaluate a broad set of state-of-the-art models for code generation, in combination with different techniques for data processing. In this initial stage, we adopt automatic evaluation metrics. ? Accuracy of NMT at generating assembly code snippets.</p><p>In this experimental objective, we deepen the analysis of the accuracy of NMT models. This is a cumbersome task since automatic metrics do not catch the deeper linguistic features of generated code, such as its semantic correctness <ref type="bibr" target="#b30">[34]</ref>. Therefore, it is also advisable for NMT studies to perform an evaluation through manual analysis, by using additional metrics in order to have a more precise and complete evaluation.</p><p>The second experimental objective still focuses on the analysis of individual intents and their corresponding translations into code snippets. ? Accuracy of the NMT at generating whole shellcodes. We investigate if it is possible to apply NMT to generate full shellcodes, i.e., entire assembly programs from a set of intents. Ideally, the generated code is entirely or mostly correct, in order to reduce the human effort towards developing assembly programs. Therefore, in this experimental objective, we evaluate how many entire shellcodes are correctly generated by NMT (unlike the previous experimental objective, where we analyze individual code snippets regardless of which program they belong to). ? Types of errors incurred by NMT in the generation of shellcodes.</p><p>In this experimental objective, we are concerned with diagnosing the error predictions in the code generation task. We qualitatively analyze a representative sample of the most frequent mistakes, including both syntactic and semantic ones, to get more insight into the severity of the errors, and to understand potential areas of improvement for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model Implementation</head><p>We implement the Seq2Seq model using xnmt <ref type="bibr" target="#b61">[66]</ref>. We use an Adam optimizer <ref type="bibr" target="#b43">[47]</ref> with ? 1 = 0.9 and ? 2 = 0.999, while the learning rate ? is set to 0.001. We set all the remaining hyper-parameters in a basic configuration: layer dimension = 512, layers = 1, epochs (with early stopping enforced) = 200, beam size = 5.</p><p>Our CodeBERT implementation uses an encoder-decoder framework where the encoder is initialized to the pre-trained CodeBERT weights, and the decoder is a transformer decoder. The decoder is composed of 6 stacked layers. The encoder fol-lows the RoBERTa architecture <ref type="bibr" target="#b53">[57]</ref>, with 12 attention heads, hidden layer dimension of 768, 12 encoder layers, 514 for the size of position embeddings. We use the Adam optimizer <ref type="bibr" target="#b43">[47]</ref>. The total number of parameters is 125M. The max length of the input is 256 and the max length of inference is 128. The learning rate ? = 0.00005, batch size = 32, beam size = 10, and train steps = 2800.</p><p>We performed our experiments on a Linux machine. Seq2seq utilized 8 CPU cores and 8 GB RAM. CodeBERT utilized 8 CPU cores, 16 GB RAM, and 2 GTX1080Ti GPUs. The computational time needed to generate the output depends on the settings of the hyper-parameters and the size of the dataset. On average, the training time for the Seq2Seq model was ? 60 minutes, while CodeBERT required for the training on average ? 220 minutes. Once the models are trained, the time to translate intent into a code snippet is below 1 second and can be considered negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Test Set</head><p>To perform the experimental evaluation, we split our entire dataset into train/dev/test sets by using an 80/10/10 ratio. To divide the data between training, dev, and test set, we did not individually sample intent-snippet pairs from the dataset, but we took groups of intent-snippet pairs that belonged to the same shellcode, in order to be able to evaluate generate shellcodes in their entirety (see ? 6.5). The test set contains 30 complete shellcodes (e.g. the entire Listing 1).</p><p>We selected the 30 shellcodes of the test set in order to maximize the heterogeneity among the programs and mitigate bias. We anticipated that these biases could affect the evaluation: the type of attack (as they may entail different instructions and constructs); the authors of the shellcode (as it may also affect the programming style); and the complexity of the shellcode (as more complex shellcodes may also be more difficult to describe and to translate). We divided the shellcodes according to the type of the attack (shell spawning, break chroot, fork bomb, etc.), and sampled the shellcodes uniformly across these classes. When sampling within each class, we doublechecked that no programmer was over-represented. We used the shellcode length as a proxy for complexity, and we increased the sample size until the distribution of the shellcode length was comparable to the distribution of the whole population (min=12, max=61, mean=26.9, median=24.5). The histograms in <ref type="figure" target="#fig_4">Fig. 4</ref> summarize the statistic of the programs in the test set in terms of lines of code. Additional information on the test set is presented in the Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Feasibility in applying NMT for shellcode generation</head><p>We first analyze the feasibility of Seq2Seq with attention mechanism and CodeBERT for the generation of shellcodes and investigate the impact of the data processing described in ? 4. In this stage, we use automatic evaluation metrics. Automatic metrics are commonly used in the field of machine translation. They are reproducible, easy to be tuned, and time-saving. The BiLingual Evaluation Understudy (BLEU) <ref type="bibr" target="#b64">[69]</ref> score is one of the most popular automatic metric <ref type="bibr" target="#b62">[67,</ref><ref type="bibr" target="#b52">56,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b72">79]</ref>. This metric is based on   the concept of n-gram, i.e., the adjacent sequence of n items (e.g., syllables, letters, words, etc.) from a given example of text or speech. In particular, this metric measures the degree of n-gram overlapping between the strings of words produced by the model and the human translation references at the corpus level. BLEU measures translation quality by the accuracy of translating n-grams to n-grams, for n-gram of size 1 to 4 <ref type="bibr" target="#b29">[33]</ref>. The Exact match accuracy (ACC) is another automatic metric often used for evaluating neural machine translation <ref type="bibr" target="#b52">[56,</ref><ref type="bibr" target="#b81">89,</ref><ref type="bibr" target="#b82">90,</ref><ref type="bibr" target="#b83">91]</ref>. It measures the fraction of the exact match between the output predicted by the model and the reference.</p><p>To assess the influence of our tailoring to NMT for the assembly language (e.g., the intent parser), we compare three "variants" of NMT by varying the steps of the data processing pipeline (see ? 4):</p><p>w/o data processing: the model performs the translation task without applying any step of the data processing pipeline. w/o intent parser: in this case, the model is trained on processed data, but without adopting the intent parser. with intent parser: the data processing pipeline also includes the intent parser. <ref type="table" target="#tab_3">Table 3</ref> shows the results of this analysis. The table shows that the data processing aids the Seq2Seq model also without the use of the intent parser, while CodeBERT does not take benefit from the basic data processing steps. The performance of both models significantly increases when the data processing is used in combination with the intent parser. Indeed, the full data processing pipeline improves all the metrics by ? 31% on average for Seq2Seq and by ? 19% on average for CodeBERT when the results of the models are compared without using the data processing process. The table also highlights that CodeBERT outperforms the Seq2Seq model across all metrics. We conducted a paired t-test and found that the differences between the results obtained by CodeBERT with the intent parser and all the other model configurations are statistically significant for all metrics (at p &lt; 0.05).</p><p>To estimate the actual goodness of the results, we compared the best performance achieved on the Shellcode IA32 dataset with the state-of-the-art best performances on  <ref type="bibr" target="#b33">[37]</ref> and 80.20 <ref type="bibr" target="#b83">[91]</ref>, respectively, and are therefore lower than the best results in <ref type="table" target="#tab_3">Table 3</ref>. We attribute these differences to the nature of the assembly language, which is a low-level language. Indeed, even if this work targets the IA-32 processor, which is a CISC architecture, the instruction set of the assembly language is still limited if compared to high-level languages, such as Python, which include a wide number of libraries and functions and, therefore, are more complex to automatically generate.</p><p>We also investigate the performance of the code generation task on single-line snippets vs. multi-line snippets by performing a fine-grained evaluation. <ref type="table" target="#tab_4">Table 4</ref> shows the performance of CodeBERT (with data processing) for single vs. multi-line snippets. Unsurprisingly, we find that accuracy is negatively affected by the length of snippets, while BLEU scores are higher for multi-line snippets. This is because multi-line snippets are longer, there is more opportunity for BLEU scores to be higher (there can be more n-grams that are matched in longer snippets), in contrast to single line snippets. And likewise, since the accuracy metric is an exact match on the entire snippet, performance on multi-line snippets is lower than for single line snippets. This first analysis allows us to conclude that the state-of-the-art NMT models can be applied for the generation of code used to exploit the software, and provide high performance when used in combination with data processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Accuracy of NMT at generating assembly code snippets</head><p>In ? 6.3, we used the code written by the programmers (i.e., the authors of the shellcodes) as ground truth for the evaluation. Therefore, when the model predicts the assembly code snippets starting from their natural language description, the predicted output is compared to code composing the original shellcode attacks. However, since the same English intent can be translated into different but equivalent assembly snippets, automated metrics (such as BLEU scores) are not perfect in that they do not credit semantically correct code that fails to match the reference. For example, the snippets jz label and je label are semantically identical, even if they use different instructions (jz vs. je). Furthermore, these metrics do not indicate whether the generated code would compile or not. Accordingly, we define two new metrics: a generated output snippet (single or multi-line) is considered syntactically correct if it is correctly structured in assembly language and compiles correctly. The output is considered semantically correct if the snippet is an appropriate translation in assembly language given the intent description. Consider the intent transfer the contents of the ebx register into the eax register. If the approach generates the snippet mov ebx, eax, then the snippet is considered syntactically correct (it would compile), but not semantically correct because the order of the operands is inverted. These two metrics allow us to assess the deeper linguistic features of the code <ref type="bibr" target="#b31">[35]</ref>. The semantic correctness implies syntax correctness, while a snippet can be syntactically correct but semantically incorrect. When a snippet is syntactically incorrect it is also semantically incorrect. The evaluation of the semantic equivalence between the output predicted by the models and the code written by the authors of the shellcodes provides the best insights into the quality of the output since it allows us to assess the correctness of the predicted code even if its syntax differs from the ground truth. This is the reason why we did not limit the analysis to automatic metrics, and manually evaluated the semantic meaning of generated code.</p><p>To evaluate the syntactic correctness of the outputs, we used the NASM compiler in order to check whether the code is compilable, while we evaluated the semantic correctness by checking if the code generated by the models is a correct translation of the English intent. We performed this analysis manually, by checking every single line of generated code. This analysis could not be performed automatically, since an English intent can be translated into several forms that are different, but semantically equivalent. For the same reason, manual ("human") evaluation is a common practice in NMT studies. The manual evaluation also gives better insights into the quality of machine translation and allows us to analyze errors in the output. To reduce the possibility of errors in manual analysis, multiple authors performed this evaluation independently, obtaining a consensus for the semantic correctness of the output predicted by the models. <ref type="table" target="#tab_5">Table 5</ref> shows the percentage of syntactically and semantically correct snippets across all the examples of the test set. We evaluated the performance of Seq2Seq and CodeBERT, both using data processing. Both syntactic and semantic evaluations were performed by compiling the generated snippets under the NASM compiler. Table 5 shows that both approaches are able to generate &gt; 95% of syntactically correct snippets. Paired t-tests indicated that the differences between the models are not statistically significant for the syntactic correctness, but they are statistically significant for semantic correctness (at p &lt; 0.01). Again, we further investigated the results provided by CodeBERT, by evaluating the performance of the model on single vs. multi-line snippets. <ref type="table" target="#tab_6">Table 6</ref> highlights that the multi-line snippets affect model performance on syntactic correctness, although we find no statistically significant difference in model performance on the semantic correctness metric.  <ref type="table">Table 7</ref> show illustrative examples of code snippets that the model can successfully translate (i.e., the snippets generated by the approach are syntactically and semantically correct). Rows 3, 6, and 8 are examples of correct snippets that are penalized by automated metrics, even if they do not exactly match the ground truth. Despite some slight differences with the ground truth, the generated code is semantically correct, due to the ambiguity of the assembly language. Thus, these differences are still considered correct by our manual analysis. We note correctly generated examples of multi-line snippets in rows 2, 3, 4, and 6. Also, we observe in row 3, the ability to generate multi-line snippets from a relatively abstract intent.</p><p>We conclude that both Seq2Seq and CodeBERT provide syntactically and semantically correct code snippets with high accuracy. Moreover, CodeBERT provides the best performance in the task of generating shellcodes from natural language intents. <ref type="table">Table 7</ref>: Illustrative examples of successfully generated snippets using our approach. Differences between the output and ground truth are bolded. Such differences are penalized by automatic metrics even though they are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Row</head><p>Natural Language Intent Ground Truth Model Output Due to these findings, we consider CodeBERT (with data processing) as our reference NMT model for the following experimental objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Accuracy of the NMT at generating whole shellcodes</head><p>The ultimate goal of developers is to craft entire shellcodes. The previous evaluation showed that NMT can generate individual code snippets that are likely the correct ones. Thus, NMT can be queried by developers to translate specific parts of a program. Here, we raise the bar for the evaluation, by analyzing to which extent NMT can generate an entire shellcode. To this purpose, we consider groups of intents from the same exploit and compare the resulting code snippets with the original shellcode. We use two new metrics to evaluate the ability of the approach to generate semantically and syntactically correct code for entire shellcodes. Let n i t be the the number of total lines of the i-th program in the test set (i ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">30]</ref>). Let also consider n i syn as the number of automatically-generated snippets for the i-th program that are syntactically correct, and n i sem as the number of automaticallygenerated snippets that are semantically correct. For every program of the test set, we define the syntactic correctness of the program i as the ratio n i syn /n i t , and the semantic correctness of the program as the ratio n i sem /n i t . To perform a conservative evaluation on multi-line snippets, even if only one line of code of the generated snippets is syntactically (semantically) incorrect, we consider all the lines belonging to the multi-line block as syntactically (semantically) incorrect. Both metrics range between 0 and 1.</p><p>For each i ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">30]</ref>, we computed the values n i syn and n i sem for the assembly programs in the test set. We found that the average syntactic correctness over all the programs of the test set is ? 98% (standard deviation is ? 4%). Similarly, we estimated the average semantic correctness, which is equal to ? 96% (standard deviation is ? 6%). Out of 30 programs, we found that 21 are compilable with NASM and executable on the target system.</p><p>Since even one incorrect line of code suffices to thwart the effectiveness of a shellcode, we analyzed how many shellcodes could be generated with no errors. We consider a shellcode as fully correct if all the assembly instructions composing the shellcode are individually semantically correct (i.e., n i sem /n i t = 1). This evaluation metric is a demanding one. Even if one single line of the shellcode is not semantically correct, then the whole program is considered as not correctly generated. Despite this conservative evaluation, our approach is able to correctly generate 16 out of 30 whole shellcodes. <ref type="figure">Figure 5</ref> shows the summary statistics with a density and a box plot, differentiating the fully correct shellcodes from the incorrect ones. As expected, the complexity of the shellcode -in terms of lines of assembly code -impacts the ability of the approach to correctly generate the whole program. However, the average (and the median) length of the shellcodes incorrectly generated by the model is affected by the three assembly programs of lengths 55, 59, and 61. If we consider these shellcodes as outliers, then the group of fully correct shellcodes and the group of the incorrectly generated shellcodes are very similar in terms of size. We interpret these results as a promising indication towards our ultimate goal of generating entire shellcode programs automatically from short natural language intents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Types of errors incurred by NMT in the generation of shellcodes</head><p>In the last experiment objective, we performed a manual inspection of the model's mispredictions. We noticed that the failure outputs fall down in the following three failure types; -Failure Type A: translation failure in generating the correct label, instruction, operand(s), or delimiter(s).  <ref type="figure">Fig. 5</ref>: Plots visualizing the statistics, in terms of lines of assembly code, of the 30 shellcodes in the test set. The labels Fully Correct and Incorrect refer to the shellcodes that are generated by the approach as fully correct (n sem /n t = 1) and incorrect (n sem /n t &lt; 1), respectively.. C, instead, is attributed to the intent parser failure. Indeed, even if the performance of the translation task benefits from the work of the intent parser (see ? 6.3), it is not perfect and can lead to a failure prediction by wrongly identifying the variable or register names, labels, etc. Moreover, the error predictions can be further classified as syntactically incorrect and semantically incorrect. We remark that the syntactic incorrectness implies the semantic one. To better illustrate the problem, we present in <ref type="table" target="#tab_8">Table 8</ref> a qualitative evaluation using cherry-and lemon-picked examples of failure prediction from our test set.</p><p>The first row showcases an example of failing to model because of implicit knowledge. The intent does not mention the indirect addressing mode (specified by the bracket [] in NASM syntax). In the second row, we note that the model failed to generate the newline token properly to separate the snippets with lines. This causes a syntax issue, and since it does not compile we count it as syntactically incorrect. The third row shows an example in which a byte string is declared without defining the label, while the fourth row illustrates the model's failure to predict the right instruction (the definition of the function decoder instead of the execution of the function). Both outputs do not raise an exception when compiled, therefore they are syntactically but not semantically correct. In the fifth row, we note that the intent parser correctly identifies main push in the standardization process, but fails to recognize the cl register and misidentifies ecx instead. We also note that the model predicted a mov operation between two registers (register, esp) rather than a register and a value. The predicted register does not exist in the intent hence, the output is a var3. The sixth row shows an example with incorrect instruction and inverse operands order. The remaining examples include the intent parser failing to identify explicitly stated identifiers or letters in values sometimes in long intents such as in the case of the bh register (row 7) and occasionally in simple contexts such as in the case of Define the array of bytes encodedshellcode and initialize it to 0x32, 0x51, 0x30, 0x74, 0x69, 0x63, 0x6f, 0xe4, 0x8a, 0x54, 0xe2, 0x0c, 0x81, 0xc1, 0x69, 0x30, 0x69, 0x30, 0x6a, 0x8a, 0x51, 0xe3, 0x8a, 0xb1, 0xce encodedshellcode: db 0x32, 0x51,0x30, 0x74, 0x69, 0x63, 0x6f, 0xe4, 0x8a, 0x54, 0xe2, 0x0c, 0x81, 0xc1, 0x69, 0x30, 0x69, 0x30, 0x6a, 0x8a, 0x51, 0xe3, 0x8a, 0xb1, 0xce @ @ @ @ @ @ @ @ Push the word 0666q onto the stack push word 0666q push word 0644o C, Sem</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>Declare the read label read: section: A, C, Syn read (row 10). The last row is considered also syntactically incorrect since it is not possible to declare a label with the section assembly directive. This goes to show when there is a mistake in the standardization step, the translation may fail to work around it even if the intent seems simple. The failure outputs also provide indications on what it can be done to increase the performance of the code generation task. Most of the errors can be easily identified by the programmers: incorrect addressing modes (first row), wrong newline character (second row), missing labels (e.g., encodedshellcode in row number 3), wrong instructions (row 4, 6), undefined variables (e.g., var3 in row 5), wrong operand orders (row number 6), etc. The syntactically incorrect predictions, i.e., the predictions that do not follow the syntax, can be identified with a compiler and can be fixed through an "intelligent" post-processing phase, which should be trained to identify and fix the failure outputs. This is part of the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Discussion and Lessons Learned</head><p>The experimental analysis pointed out that NMT models can efficiently generate assembly code for real shellcodes, starting from their natural description. When used in combination with data processing, the accuracy of the code generation task is high enough to support developers in developing software exploits. Even if the size and the complexity of an English intent increase, the performance of the translation task is not negatively affected. CodeBERT achieves the best performance and further justifies its wide usage to address software engineering tasks. The model is able to generate whole software exploits with syntactic and semantic correctness greater than 95%. It is also able to generate programs that are fully correct, i.e., compilable and executable on the target system. However, the complexity of the software attacks (in terms of lines of code) reduces the accuracy of generating entire programs. The analysis also pointed out that the most common error predictions are easily identifiable and can be fixed during the post-processing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethical Considerations</head><p>Recognizing that attackers use exploit code as a weapon, it is important to specify that the goal of the proof-of-concept (POC) exploits is not to cause harm but to surface security weaknesses within the software. Identifying such security issues allows companies to patch vulnerabilities and protect themselves against attacks.</p><p>Offensive security is a sub-field of security research that tests security measures from an adversary or competitor's perspective. It can employ ethical hackers to probe a system for vulnerabilities <ref type="bibr">[12,</ref><ref type="bibr">32,</ref><ref type="bibr">64]</ref>. Automatic exploit generation (AEG), an offensive security technique, is a developing area of research that aims to automate the exploit generation process and to explore and test critical vulnerabilities before they are discovered by attackers <ref type="bibr" target="#b5">[6]</ref>. Indeed, work such as ours, which studies exploits on compromised systems can provide valuable information about the technical skills, degree of experience, and intent of the attackers. By using this information, it is possible to implement measures to detect and prevent attacks <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Threats to Validity</head><p>NMT models: Before the era of NMT, Statistical Machine Translation (SMT) <ref type="bibr" target="#b13">[15]</ref> was the most popular technique for software engineering (SE) problems, it still outperforms NMT in some SE problems <ref type="bibr" target="#b66">[71]</ref>. However, since we are interested in the specific problem of code generation, we focus on NMT that has shown superior performance on public benchmarks <ref type="bibr" target="#b8">[9]</ref>, and that it is widely recognized as the premier method for the translation of different languages <ref type="bibr" target="#b75">[83]</ref>. Our choice of the NMT models has been influenced by their popularity and the availability of mature open-source implementations. We acknowledge that using only two state-of-the-art models can be a limitation of this work. Nevertheless, we believe that these two models are valid representatives of the NMT research area, and can provide us with a realistic evaluation of NMT for code generation. Seq2Seq has been for several years the most used model for code generation tasks, and it is still widely employed in NMT studies as a baseline model. CodeBERT has pushed the boundaries in natural language processing and represents the state-of-the-art for generating code documentation given snippets, as well as retrieving code snippets given a natural language search query across six different programming languages <ref type="bibr" target="#b37">[41]</ref>. Moreover, it has also been applied in software engineering to perform different tasks <ref type="bibr" target="#b63">[68]</ref>.</p><p>Size of our dataset: Our dataset contains 3, 200 instances, which may seem relatively small compared to training data available for other NLP tasks. The data about shellcodes is much more difficult to obtain than other data for NMT. For example, before starting the collection of the dataset, we developed a script to collect assembly code for IA-32 from all of the repositories on GitHub (by far the source most used by empirical software engineering studies). We found that the amount of available data is very limited. The data is further restricted by the fact that we are specifically interested in security-oriented assembly codes (i.e., shellcodes). Therefore, we decided to collect all the shellcodes for Linux/IA-32 from exploit-db and shell-storm, the two public databases for shellcodes most popular among the security professionals, to achieve representativeness. We collected shellcodes written over a large period (from 2000 to 2020) from a variety of authors, in order to achieve diversity. To the best of our knowledge, the resulting dataset is the largest collection of shellcodes in assembly available to date. Despite the previous considerations, we note that our dataset is comparable in size to the popular CoNaLa dataset <ref type="bibr" target="#b81">[89]</ref> (2, 379 training and 500 test samples in the annotated version of the dataset), which is the basis for state-of-the-art studies in NMT for Python code generation <ref type="bibr" target="#b79">[87,</ref><ref type="bibr" target="#b83">91,</ref><ref type="bibr" target="#b27">30]</ref>. Further, Shellcode IA32 contains a higher percentage of multi-line snippets (? 16% vs. ? 4%). We also note here that existing code generation datasets do contain a larger, potentially noisy, subset of training examples (ranging in several thousand) obtained by mining the web. For example, the CoNaLa mined (as opposed to the CoNaLa annotated) dataset contains 598, 237 training examples mined directly from StackOverflow <ref type="bibr" target="#b79">[87]</ref>. We designed the proposed approach to leverage existing pre-trained models to compensate for the need for big data, by training the model using our assembly dataset.</p><p>Code description: To build the dataset, we described in the English language the shellcodes collected from publicly available exploit databases. Therefore, the description of the assembly code derives from our considerations and knowledge. However, the building process of the Shellcode IA32 dataset is not different from other corpus built from scratch. For example, Oda et al. <ref type="bibr" target="#b62">[67]</ref> hired an engineer to create pseudocode for the Django Web application framework and obtain the corpus. We avoided a single centralized version of the code description to take into account the variability of descriptions in natural language. Indeed, multiple authors described independently different samples of the dataset in the English language, and, where available, we kept untouched the comments written by developers of the collected programs to describe the assembly code snippets. To understand how different programmers and experts describe the assembly code for IA-32 and how to deal with the ambiguity of natural language in this specific context, we took inspiration from popular tutorials and books <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b44">48,</ref><ref type="bibr">81]</ref>.</p><p>Translation task: As assembly code is a low-level language, it often takes a long sequence of instructions to complete an atomic function. Therefore, some translations presented in the dataset are too "literal" and cumbersome. For example, instead of writing "Define the start label", a user might just as well write " start:", similarly, the intent "Push the contents of eax onto the stack" takes longer than writing the assembly instruction "push eax". However, this is a common situation in any translation task from English to programming language. For example, the Django dataset contains numerous Python code snippets that are relatively short (e.g., "chunk buffer = BytesIO(chunk)") described with with English statements that are definitely longer than the snippets ("evaluate the function BytesIO with argument chunk, substitute it for chunk buffer."). Similarly, in the CoNaLa dataset we can find shortcode snippets (e.g., "GRAVITY = 9.8") described with longer English intents ("assign float 9.8 to variable GRAVITY"). Nevertheless, we -and other datasets-still include such verbose intents to provide richer learning of NMT models. Moreover, we mitigated this problem by adding multi-line snippets, i.e., single intents described in natural language that generate more lines of assembly codes, that are closer to the intent that developers may want to use during development.</p><p>Scope of the approach: A shellcode is a piece of assembly code written specifically for exploitation purposes. From this perspective, all shellcodes are security-related programs and, therefore, the proposed approach is tailored for generating software exploits. It is an interesting question whether the proposed approach has applications beyond security. The approach is focused on assembly programs, which is the most used language for shellcodes. Thus, the processing pipeline has been designed to handle relevant elements of the assembly language, such as keywords and register names. This approach significantly contributes to generating more accurate code compared to generic NMT techniques but narrows the scope to assembly code. As future work, we are exploring the use of NMT for other programming languages, such as Python. In principle, a programmer can use the method to generate assembly code unrelated to security applications. However, the method might be less accurate in this case, since our solution is trained with a dataset of mostly security-related assembly code snippets. To be used outside security applications, the programmer would need to adopt a training dataset with more non-security assembly code (e.g., assembly code for device drivers or microcontrollers). Moreover, it may be necessary to tweak the processing pipeline to support special keywords that are not adopted for shellcodes (e.g., linking directives for embedded software). We opted to leave such extensions out of the scope of our work, as security applications are the ones that have by far the highest demand for increasing the productivity of assembly programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>We addressed the problem of automated exploit generation using natural language processing techniques. We use Neural Machine Translation to translate natural language intents into shellcode. We built and released the first dataset of shellcodes, Shellcode IA32, containing 3, 200 pairs of code snippets and intents. The dataset also contains 510 intents that generate multiple snippets. These assembly language snippets can be combined to generate shellcodes for the Intel 32-bit Architecture. Our empirical analysis demonstrated the feasibility of using NMT for this task, using both automated and manual metrics. We also propose the use of novel metrics for the task of code generation, that we anticipate would be useful to the community.</p><p>Our work enables further studies in the area, to make NMT more and more effective. We are currently working on a new engine for the post-processing phase, in order to identify and fix the assembly lines wrongly generated by the NMT model and to further improve accuracy. We are also analyzing the impact of "noisy inputs" or "perturbation" in the natural language, since human developers may provide inaccurate or incomplete descriptions of the shellcode to be generated. For example, perturbations can be introduced by replacing words with "unseen" synonyms, or by removing redundant information. In this direction, we are investigating a solution to make NMT more robust and usable, by helping the model to derive the missing information (i.e., information not explicitly stated in the English intent) from the context of the programs. Finally, as part of future research, we aim to evaluate our approach with actual humans instructing with comments, so that the evaluation could take into account how the humans perceive the actual usefulness of developing a shellcode that achieves the desired result.</p><p>Beyond our current work on extending the proposed approach, we expect that this work can support more researchers in the field. Indeed, in the era where deep learning is evolving at a quick pace and succeeding in more and more tasks with surprising accuracy, we expect in the near future the development of new deep learning architectures, which could potentially bring benefits for the automatic generation of exploits. In this light, the proposed approach and dataset represent valid means to pave the way for a new generation of offensive security methods. This work represents a first step towards the ambitious goal of automatically generating shellcodes from natural language, provides originally-collected data, enables replication, and describes successes and challenges through rigorous evaluation. <ref type="table">Table 9</ref> presents detailed information on the 30 shellcodes composing the test set. In particular, the table shows the URL where the shellcode is collected, the number of assembly lines of the program, the number of multi-line snippets, and the number of snippets generated incorrectly from our approach. We consider the whole shellcode generated correctly only if the approach produces 0 incorrect snippets. Our approach generated correctly 16 out of 30 whole shellcodes. <ref type="table">Table 9</ref>: The 30 shellcodes composing the test set. We consider a shellcode executed correctly if all the generated snippets composing the program are semantically correct. n t : number of total assembly lines of the program. Multi-line: number of multi-lines snippets in the program. n syn : number of syntactically correct lines generated by the approach. n sem : number of semantically correct lines generated by the approach. www.exploit-db.com/shellcodes/48703 <ref type="bibr">33 (16)</ref> 31 <ref type="bibr">29 3</ref> www.exploit-db.com/shellcodes/47877 40 (0) 40 <ref type="bibr">40 4</ref> www.exploit-db.com/shellcodes/13716 59 (0) 58 <ref type="bibr">52 5</ref> www.exploit-db.com/shellcodes/47513 14 (0) 14 <ref type="bibr">14 6</ref> www.exploit-db.com/shellcodes/47511 24 (0) 24 <ref type="bibr">24 7</ref> www.exploit-db.com/shellcodes/47481 <ref type="bibr">41 (2)</ref> 40 <ref type="bibr">38 8</ref> www.exploit-db.com/shellcodes/47396 <ref type="bibr">61 (15)</ref> 61 <ref type="bibr">60 9</ref> www.exploit-db.com/shellcodes/47200 29 (2) 29 <ref type="bibr">28 10</ref> www.exploit-db.com/shellcodes/47202 <ref type="bibr">29 (4)</ref> 29 <ref type="bibr">29 11</ref> www.exploit-db.com/shellcodes/47108 <ref type="bibr">26 (9)</ref> 26 <ref type="bibr">26 12</ref> www.exploit-db.com/shellcodes/47068 12 (0) 12 12 13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Test Set</head><p>www.exploit-db.com/shellcodes/46994 <ref type="bibr">28 (4)</ref> 27 <ref type="bibr">26 14</ref> www.exploit-db.com/shellcodes/46829 <ref type="bibr">20 (6)</ref> 20 <ref type="bibr">20 15</ref> www.exploit-db.com/shellcodes/46801 <ref type="bibr">34 (9)</ref> 34 <ref type="bibr">34 16</ref> www.exploit-db.com/shellcodes/46791 27 <ref type="bibr" target="#b7">(8)</ref> 27 <ref type="bibr">26 17</ref> www.exploit-db.com/shellcodes/46704 <ref type="bibr">29 (6)</ref> 29 <ref type="bibr">29 18</ref> www.exploit-db.com/shellcodes/46704 55 (4) 55 <ref type="bibr">54 19</ref> www.exploit-db.com/shellcodes/45669 <ref type="bibr">20 (6)</ref> 20 <ref type="bibr">20 20</ref> www.exploit-db.com/shellcodes/45940 <ref type="bibr">25 (4)</ref> 25 <ref type="bibr">25 21</ref> www.exploit-db.com/shellcodes/45529 14 <ref type="formula" target="#formula_3">(7)</ref> 14 <ref type="bibr">14 22</ref> www.exploit-db.com/shellcodes/45441 <ref type="bibr">20 (9)</ref> 17 <ref type="bibr">17 23</ref> www.exploit-db.com/shellcodes/44963 <ref type="bibr">17 (6)</ref> 17 <ref type="bibr">17 24</ref> www.exploit-db.com/shellcodes/44609 32 (0) 31 <ref type="bibr">30 25</ref> www.exploit-db.com/shellcodes/44509 <ref type="bibr">16 (2)</ref> 16 <ref type="bibr">16 26</ref> www.exploit-db.com/shellcodes/44594 <ref type="bibr">15 (2)</ref> 15 <ref type="bibr">15 27</ref> www.exploit-db.com/shellcodes/44510 23 (3) 23 <ref type="bibr">21 28</ref> www.exploit-db.com/shellcodes/43476 <ref type="bibr">15 (6)</ref> 15 <ref type="bibr">15 29</ref> www.exploit-db.com/shellcodes/43489 <ref type="bibr">18 (2)</ref> 17 <ref type="bibr">17 30</ref> www.exploit-db.com/shellcodes/43463 15 <ref type="table" target="#tab_1">(3)  15</ref> 14</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>wordvar: resw 1 ;Fig. 2 :</head><label>12</label><figDesc>reserve a word for wordvar label instruction operand comment Layout of a NASM source line</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Histogram of the Shellcode IA32 dataset showcasing the distribution of token counts across intents and snippets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Number of assembly lines of code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Number of multi-lines snippets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Histograms visualizing the statistics of the 30 shellcodes in the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>-</head><label></label><figDesc>Failure Type B: translation failure in identifying the correct order and/or the addressing mode of operands.-Failure Type C: intent parser's failure in identifying one or more of the explicitly stated identifiers.The failure types A and B are due to the lack of ability of the model to perform the correct translation of the English intent in the assembly code. The failure type</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Assembly code used to generate a shellcode on Linux OS running on 32 bit Intel Architecture. Lines 5-6, 11-12, 15-16, 19-20, 21-22-23, 24-25, 27-28-29 are multi-line snippets generated by seven different intents.</figDesc><table><row><cell>int 0x80;</cell><cell>call kernel</cell></row><row><cell>32</cell><cell></cell></row><row><cell>33 mov al, 0x01;</cell><cell>Move 0x01 into AL.</cell></row><row><cell>34 xor ebx, ebx;</cell><cell>Clear EBX.</cell></row><row><cell>35 int 0x80;</cell><cell>Call kernel.</cell></row><row><cell>Listing 1:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Examples of multi-line snippets English Intent Multi-line Snippets jump short to the decode label if the contents of the al register is not equal to the contents of the cl register else jump to the shellcode label cmp al, cl \n jne short decode \n jmp shellcode jump to the label recv http request if the contents of the eax register is not zero else subtract he value 0x6 from the contents of the ecx register test eax, eax \n jnz recv http request \n sub ecx, 0x6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Shellcode IA32 statistics</figDesc><table><row><cell>Language</cell><cell>Unique statements</cell><cell>Unique tokens</cell><cell>Avg. tokens per statement</cell><cell>Min tokens per statement</cell><cell>Max tokens per statement</cell></row><row><cell>Natural Language</cell><cell>3,184</cell><cell>1639</cell><cell>9.15</cell><cell>1</cell><cell>46</cell></row><row><cell>Assembly Language</cell><cell>2,248</cell><cell>1401</cell><cell>4.17</cell><cell>2</cell><cell>30</cell></row></table><note>? (a) Number of tokens in the intents.? (b) Number of tokens in the snippets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Automated evaluation of the translation task. Bolded values are the best performance. IP: Intent Parser. ( * = p&lt;0.05)</figDesc><table><row><cell>Automated Metrics (%)</cell><cell>w/o data processing</cell><cell>Seq2Seq w/o IP</cell><cell>with IP</cell><cell>w/o data processing</cell><cell>CodeBERT w/o IP</cell><cell>with IP</cell></row><row><cell>BLEU-1</cell><cell>69.99</cell><cell>74.57</cell><cell>93.46</cell><cell>78.42</cell><cell>80.11</cell><cell>94.95*</cell></row><row><cell>BLEU-2</cell><cell>64.18</cell><cell>69.82</cell><cell>91.98</cell><cell>75.11</cell><cell>75.89</cell><cell>93.61*</cell></row><row><cell>BLEU-3</cell><cell>60.09</cell><cell>66.35</cell><cell>90.87</cell><cell>72.75</cell><cell>73.15</cell><cell>92.68*</cell></row><row><cell>BLEU-4</cell><cell>56.43</cell><cell>62.97</cell><cell>90.03</cell><cell>70.54</cell><cell>70.11</cell><cell>91.70*</cell></row><row><cell>ACC</cell><cell>39.44</cell><cell>51.55</cell><cell>82.92</cell><cell>69.57</cell><cell>67.39</cell><cell>89.75*</cell></row><row><cell cols="7">the Django dataset [67], a corpus widely used for code generation tasks [55, 89, 90,</cell></row><row><cell cols="7">91, 37, 21, 30, 85] and consisting of 18, 805 pairs of Python statements for the Django</cell></row><row><cell cols="7">Web application framework alongside the corresponding English pseudo-code. The</cell></row></table><note>state-of-the-art best performances on this dataset provide BLEU-4 score and accu- racy equal to 84.70</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Automatic evaluation of the translation task comparing single-line and multiline snippets from the test set. Bolded values are the best performance.</figDesc><table><row><cell>Automated Metrics</cell><cell>Single-line</cell><cell>Multi-line</cell></row><row><cell>(%)</cell><cell>snippets</cell><cell>snippets</cell></row><row><cell>BLEU-1</cell><cell>93.64</cell><cell>98.14</cell></row><row><cell>BLEU-2</cell><cell>92.24</cell><cell>96.86</cell></row><row><cell>BLEU-3</cell><cell>91.29</cell><cell>95.84</cell></row><row><cell>BLEU-4</cell><cell>90.21</cell><cell>94.91</cell></row><row><cell>ACC</cell><cell>90.51</cell><cell>85.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Code correctness evaluation of the translation task given the whole test set. Bolded values are the best performance. ( * = p&lt;0.01)</figDesc><table><row><cell>Code Correctness Metrics (%)</cell><cell>Seq2Seq with data processing</cell><cell>CodeBERT with data processing</cell></row><row><cell>Syntactically Correct</cell><cell>96.58</cell><cell>97.20</cell></row><row><cell>Semantically Correct</cell><cell>85.40</cell><cell>93.16*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Code correctness evaluation of the translation task comparing single-line and multi-line snippets from the test set. Bolded values are the best performance.</figDesc><table><row><cell>Code Correctness</cell><cell>Single-line</cell><cell>Multi-line</cell></row><row><cell>Metrics (%)</cell><cell>snippets</cell><cell>snippets</cell></row><row><cell>Syntactically Correct</cell><cell>97.81</cell><cell>93.75</cell></row><row><cell>Semantically Correct</cell><cell>93.06</cell><cell>93.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Illustrative examples of incorrect outputs. The prediction errors are red/bold. Slashed text refers to omitted predictions. Syn indicates a syntactically and semantically incorrect snippet, while Sem indicates a semantically incorrectness output.</figDesc><table><row><cell cols="2">$ $ $ $</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Row</cell><cell>Natural Language Intent</cell><cell>Ground Truth</cell><cell>Model Output</cell><cell>Failure Type</cell></row><row><cell></cell><cell>Perform the xor operation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>between the location pointed</cell><cell>xor [ecx], dh</cell><cell>xor ecx, [dh]</cell><cell>B, Syn</cell></row><row><cell></cell><cell>by ecx and dh</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Jump to the start label if the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>value in the eax register is not equal to the doubleword addressed by edi else jump to</cell><cell>scasd \n jnz start \n jmp edi</cell><cell>scasd \\ jnz start \n jmp edi</cell><cell>A, Syn</cell></row><row><cell></cell><cell>the edi register</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Shellcode collected from https://www.exploit-db.com/shellcodes/48703</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The name is due to the fact that each word is embedded in a continuous vector space.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The dataset can be found here: https://github.com/dessertlab/Shellcode_IA32</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Pietro Liguori et al.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work has been partially supported by the University of Naples Federico II in the frame of the Programme F.R.A., project id OSTAGE.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chainsaw: Chained automated workflow-based exploit generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alhuzali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eshete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gjomemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="641" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">code2seq: Generating sequences from structured representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Shellcoder&apos;s Handbook: Discovering and Exploiting Security Holes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heasman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richarte</surname></persName>
		</author>
		<ptr target="https://books.google.it/books?id=8PLYwAEACAAJ" />
		<imprint>
			<date type="published" when="2007" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The shellcode generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE security &amp; privacy</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="72" to="76" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Aeg: Automatic exploit generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avgerinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L T</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brumley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>NDSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic exploit generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avgerinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brumley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2560217.2560219</idno>
		<ptr target="https://doi.org/10.1145/2560217.2560219" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Your exploit is mine: Automatic shellcode transplant for remote exploits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shoshitaishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brumley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="824" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<title level="m">Findings of the 2016 conference on machine translation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
	<note>Conf. on Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic patch-based exploit generation is possible: Techniques and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brumley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poosankam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2008.1712</idno>
		<ptr target="https://www.bugcrowd.com/products/how-it-works/" />
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2021" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic construction of jump-oriented programming shellcode (on the x86)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ACM Symp. on Information</title>
		<imprint>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Computer and Communications Security</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical machine translation enhancements through linguistic levels: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farr?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cve: Cve Details</surname></persName>
		</author>
		<ptr target="https://www.cvedetails.com/vulnerabilities-by-types.php" />
		<imprint>
			<biblScope unit="page" from="2021" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Guide to Assembly Language Programming in Linux. ITPro collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dandamudi</surname></persName>
		</author>
		<ptr target="https://books.google.it/books?id=HeorH2cE7WkC" />
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<pubPlace>US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Buffer Overflow Attacks: Detect, Exploit, Prevent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deckard</surname></persName>
		</author>
		<ptr target="https://books.google.it/books?id=NYyKhOqOCF8C" />
		<imprint>
			<date type="published" when="2005" />
			<publisher>Elsevier Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic construction of printable return-oriented programming payload</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.1109/MALWARE.2014.6999408</idno>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Malicious and Unwanted Software: The Americas (MAL-WARE)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of current paradigms in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Benoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in computers</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wrex: A unified programming-by-example interaction for synthesizing readable code for data scientists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drosos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI conference on human factors in computing systems</title>
		<meeting>the 2020 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Assembly language step-by-step: programming with DOS and Linux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duntemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Assembly language step-by-step: Programming with Linux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duntemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploitdb: Exploit Database Shellcodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<ptr target="https://www.exploit-db.com/shellcodes?platform=linux_x86" />
	</analytic>
	<monogr>
		<title level="m">Natural language processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2021" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Codebert: A pre-trained model for programming and natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sockets, Shellcode, Porting, and Coding: Reverse Engineering Exploits and Tool Coding for Security Professionals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<ptr target="https://books.google.it/books?id=ZNI5dvBSfZoC" />
		<imprint>
			<date type="published" when="2005" />
			<publisher>Elsevier Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relevance transformer: Generating concise code snippets with relevance feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. ACM Conf. on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2005" to="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hackerone: Hackerone Bounty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="https://www.hackerone.com/product/bug-bounty-program" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
			<biblScope unit="page" from="2021" to="2027" />
		</imprint>
	</monogr>
	<note>Deep learning</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04515</idno>
		<title level="m">Machine translation evaluation resources and methods: A survey</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03311</idno>
		<title level="m">Translation quality assessment: A brief survey on manual and automatic methods</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Translation quality assessment: A brief survey on manual and automatic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.motra-1.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age</title>
		<meeting>for the First Workshop on Modelling Translation: Translatology in the Digital Age</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to generate corrective patches using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shihab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07170</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Avvaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10025</idno>
		<title level="m">Retrieval-based neural code generation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic generation of data-oriented exploits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="177" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Software crash analysis for automatic exploit generation on binary programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TR.2014.2299198</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Reliability</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="270" to="289" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<ptr target="http://arxiv.org/abs/1909.09436" />
		<title level="m">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jamwal</surname></persName>
		</author>
		<ptr target="https://books.google.it/books?id=pZWKBAAAQBAJ" />
		<editor>C Programming. Pearson India</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatically generating commit messages from diffs using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Armaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM Intl. Conf. on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Commitbert: Commit message generation using pre-trained programming language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14242</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00887</idno>
		<title level="m">Structured attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kusswurm</surname></persName>
		</author>
		<title level="m">Modern X86 Assembly Language Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Word embedding for understanding natural language: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Guide to big data applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="83" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shellcode IA32: A dataset for automatic shellcode generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liguori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Al-Hossami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cotroneo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cukic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shaikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nlp4prog-1.7</idno>
		<ptr target="https://aclanthology.org/2021.nlp4prog-1.7" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Processing for Programming</title>
		<meeting>the 1st Workshop on Natural Language Processing for Programming<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">EVIL: exploiting software via natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liguori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Al-Hossami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Orbinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cotroneo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cukic</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSRE52982.2021.00042</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Program synthesis from natural language using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<idno>UW-CSE-17-03-01</idno>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Seattle, WA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Washington Department of Computer Science and Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06744</idno>
		<title level="m">Latent predictor networks for code generation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06744</idno>
		<title level="m">Latent predictor networks for code generation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural-machine-translation-based commit message generation: how far are we?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Intl. Conf. on Automated Software Engineering (ASE)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="373" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno type="arXiv">arXiv:cs/0205028</idno>
		<title level="m">Nltk: the natural language toolkit</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">English shellcode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monrose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Macmanus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Software security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mcgraw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Penetration Testing with Shellcode: Detect, exploit, and secure network-level and operating system vulnerabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Megahed</surname></persName>
		</author>
		<ptr target="https://github.blog/2021-06-04-updates-to-our-policies-regarding-exploits-malware-and-vulnerability-research/" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>Packt Publishing</publisher>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="2021" to="2027" />
		</imprint>
	</monogr>
	<note>Mike Hanley: Updates to our policies regarding exploits, malware, and vulnerability research</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Natural language models for predicting programming comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Godard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00188</idno>
		<title level="m">Xnmt: The extensible neural machine translation toolkit</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to generate pseudo-code from source code using statistical machine translation (t)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fudaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Intl. Conf. on Automated Software Engineering (ASE)</title>
		<imprint>
			<biblScope unit="page" from="574" to="584" />
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An empirical study on software defect prediction using codebert model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4793</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Statistical machine translation outperforms neural machine translation in software engineering: why and how</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jannesari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages</title>
		<meeting>the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Modern Assembly Language Programming with the ARM Processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pyeatt</surname></persName>
		</author>
		<idno>id=gks1CgAAQBAJ 73. Python: tokenize (Accessed: 2020-05-20</idno>
		<ptr target="https://docs.python.org/3/library/tokenize.html" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>Elsevier Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Shellstorm: Shellcodes database for study cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ligatti</surname></persName>
		</author>
		<ptr target="http://shell-storm.org/shellcode/" />
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2021" to="2025" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Defining code-injection attacks</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09040</idno>
		<title level="m">TF-Coder: Program Synthesis for Tensor Manipulations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Does BLEU score work for code migration?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM Intl. Conf. on Program Comprehension (ICPC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On learning meaningful code changes via neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pantiuchina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poshyvanyk</surname></persName>
		</author>
		<ptr target="https://www.tutorialspoint.com/assembly_programming/index.htm" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM 41st Intl. Conf. on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Incorporating external knowledge through pretraining for natural language to code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno>ArXiv abs/2004.09015</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Incorporating external knowledge through pretraining for natural language to code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09015</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Automatic exploit generation for buffer overflow vulnerabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Software Quality, Reliability and Security</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="463" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning to mine aligned code and natural language pairs from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3196398.3196408</idno>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Mining Software Repositories, MSR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="476" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno>abs/1704.01696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Tranx: A transition-based neural abstract syntax parser for semantic parsing and code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02720</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Reranking for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4553" to="4559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Semfuzz: Semantics-based automatic generation of proof-of-concept exploits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2139" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>ArXiv abs/1709.00103</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
