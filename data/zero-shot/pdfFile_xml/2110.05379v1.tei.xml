<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point Cloud Augmentation with Weighted Local Transformations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihyeon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyeok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasol</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Seong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
							<email>hyunwoojkim@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point Cloud Augmentation with Weighted Local Transformations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the extensive usage of point clouds in 3D vision, relatively limited data are available for training deep neural networks. Although data augmentation is a standard approach to compensate for the scarcity of data, it has been less explored in the point cloud literature. In this paper, we propose a simple and effective augmentation method called PointWOLF for point cloud augmentation. The proposed method produces smoothly varying non-rigid deformations by locally weighted transformations centered at multiple anchor points. The smooth deformations allow diverse and realistic augmentations. Furthermore, in order to minimize the manual efforts to search the optimal hyperparameters for augmentation, we present AugTune, which generates augmented samples of desired difficulties producing targeted confidence scores. Our experiments show our framework consistently improves the performance for both shape classification and part segmentation tasks. Particularly, with PointNet++, PointWOLF achieves the stateof-the-art 89.7 accuracy on shape classification with the real-world ScanObjectNN dataset. The code is available at https://github.com/mlvlab/PointWOLF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern deep learning techniques, which established their popularity on structured data, began showing success on point clouds. Unlike images with clear lattice structures, each point cloud is an unordered set of points with no inherent structures that globally represent various 3D objects. Recent deep learning efforts have focused on enabling neural networks to operate on point clouds. While several point cloud datasets appeared, a particular dataset of scanned realworld objects <ref type="bibr" target="#b0">[1]</ref> required a much greater understanding of the point cloud structures to identify highly complex realworld objects. In response, the approaches have evolved from extracting point-wise information with no structural * Equal contribution. ? is the corresponding author. In each row, the left most sample is the original, and the remaining samples are its locally transformed results (brighter regions indicate stronger local transformations). PointWOLF can locally transform objects while preserving the original shape identity. information <ref type="bibr" target="#b1">[2]</ref> to explicitly encoding the local structure <ref type="bibr" target="#b2">[3]</ref>. These works on network development have been making steady progress despite the scarcity of point cloud data.</p><p>Our interest lies in data augmentation, which is extensively utilized in other machine learning pipelines for solving the data scarcity issue. Interestingly, despite its prevalence in other data modalities, data augmentation (DA) on point clouds is relatively less explored. For instance, conventional data augmentation (CDA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, which consists of global rotation, scaling, translation, and small point-wise noise, is commonly applied to point cloud datasets. Recently, PointAugment <ref type="bibr" target="#b3">[4]</ref> proposes to learn the transformation matrix with an augmentor network to produce augmentations. PointMixup <ref type="bibr" target="#b4">[5]</ref> finds linearly interpolated point cloud samples and their classes ( e.g ., Mixup <ref type="bibr" target="#b5">[6]</ref>). Despite their efforts to elevate the previous CDA, they still have apparent limitations. Specifically, PointAugment relies on a single (thus global) transformation matrix/vector for each sample, and PointMixup mixes up the samples globally without explicitly considering each sample's local structures. Thus, the need for a point cloud augmentation approach capable of producing diverse samples that accurately depict real-world local variability ( e.g ., airplanes with varying lengths of wings and body) still remains.</p><p>In this work, we propose a novel point cloud augmentation PointWOLF that satisfies such needs. PointWOLF generates diverse and realistic local deformations such as a person with varying postures (see <ref type="figure" target="#fig_0">Figure 1</ref>). Our approach systematically enables local deformation by first considering multiple local transformations with respect to anchor points and carefully combining them in smoothly varying manners. Furthermore, we present AugTune to adaptively control DA strength in a sample-wise manner. AugTune produces consistent and beneficial samples during training with a single hyperparameter which alleviates the known dependence on hyperparameter selection of augmentation. We believe our method can further resolve this common dependence issue in general data augmentation.</p><p>Our contributions is fourfold: ( i ) We propose a powerful point cloud transformation approach capable of generating diverse and realistic augmented samples by deforming local structures. ( ii ) Our framework adaptively adjusts the strength of augmentation with only a single hyperparameter.</p><p>( iii ) We demonstrate that our framework brings consistent improvements over existing state-of-the-art augmentation methods on both synthetic and real-world datasets in point cloud shape classification and part segmentation tasks. ( iv ) Our framework improves the robustness of models against various local and global corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Learning on Point Clouds. Early deep learning works on point cloud have focused on enabling existing CNNs to operate on point clouds. These include multi-view based methods like <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> where they project the 3D point cloud to 2D space through bird's-eye view or multiview where 2D convolution becomes feasible. Similarly, other works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> voxelize the point cloud to directly apply 3D convolution on the voxelized point cloud. To preserve the original structure of point clouds, point-based methods have emerged. PointNet <ref type="bibr" target="#b1">[2]</ref> considers each point cloud as an unordered set and derives pointwise features with multi-layer perceptron and max pool-ing. However, a symmetric function such as pooling cannot characterize the local structure of point clouds, thus, PointNet++ <ref type="bibr" target="#b2">[3]</ref> appeared which utilizes local information through hierarchical sampling and grouping. Other related studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> also rely on grouping to identify the relationship between points and extract local structure. DGCNN <ref type="bibr" target="#b19">[20]</ref> explicitly leverages the graph-like structure of the point clouds in the feature space rather than the 3D space. Interestingly, despite these network-wise efforts to exploit the local structure, only a few works have looked for solutions outside the networks, e.g ., data augmentation. Data Augmentation. Data augmentation (DA) has become a necessity for modern machine learning model training to improve the generalization power. For point clouds, global similarity transformations such as rotation, scaling, and translation with point-wise jittering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> are conventionally used. However, such conventional DAs (CDA) do not augment local structures which many successful works mentioned above find beneficial and explicitly leverage. In light of this, only a few recent studies proposed more advanced DAs on point cloud. PointAugment <ref type="bibr" target="#b3">[4]</ref> learns an augmentor network to augment samples to be more difficult to classify than the original sample. PointMixup <ref type="bibr" target="#b4">[5]</ref>, enables Mixup <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> technique to point cloud, specifically by interpolating between two point cloud samples and predicting the ratio of the mixed classes with a soft label. While these works enable augmentations beyond simple similarity transformations, the transformations are fundamentally global : PointAugment learns a sample-wise global transformations matrix and PointMixup globally interpolates between samples. Thus, they often do not produce augmentations that are truly local and realistic. In response to this need, we propose a novel augmentation method Point-WOLF which locally transforms samples as in <ref type="figure" target="#fig_0">Figure 1</ref>. Searching for Optimal DA. In practice, identifying strong candidate transformations and optimal parameters for DA lacks intuitive conventions and heuristics thus requires extensive searching process. Several works address this, for instance, AutoAugment <ref type="bibr" target="#b22">[23]</ref> and Fast AutoAugment <ref type="bibr" target="#b23">[24]</ref> dynamically search for the best transformation policy via costly solvers such as reinforcement learning or bayesian optimization. RandAug <ref type="bibr" target="#b24">[25]</ref> has drastically reduced the search space by binding multiple augmentation parameters as a single hyperparameter. In this paper, we present Aug-Tune that efficiently controls the sample-wise DA strength with a single parameter using the target confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We first briefly describe the conventional DA for point clouds. Then, we describe PointWOLF , which aims to generate augmented point clouds. Unlike previous works that perform a global transformation and point-wise jittering, our framework augments the point clouds by locally weighted multiple transformations. We generate diverse and realistic augmented samples by deforming local structures. Also, to reduce the dependence on optimal hyperparameters of DA frameworks, we present AugTune , adaptively modulates the augmentation strength with a single parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>A point cloud P ? R (3+ C ) ? N in 3D is a set of points { p 1 , ? ? ? , p N } . Each point is represented as a vector p i ? R 3+ C which is a concatenation of its coordinates ( i.e ., [x,y,z]) and C dimensional input features such as color and a surface normal. Since the problem of our interest only focuses on the point cloud structure, we assume that no additional input features are given, i.e ., P ? R 3 ? N . A conventional data augmentation (CDA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> for point clouds applies a global similarity transformation ( e.g ., scaling, rotation and translation) and point-wise jittering. A resulting augmented point cloud P ? ? R 3 ? N is given as follows:</p><formula xml:id="formula_0">P ? = s R P + B ,<label>(1)</label></formula><p>where s &gt; 0 is a scaling factor, R is a 3D rotation matrix, and B ? R 3 ? N is a translation matrix with global translation and point-wise jittering. Typically, R is an extrinsic rotation parameterized by a uniformly drawn Euler angle for up-axis orientation. Scaling and translation factors are uniformly drawn from an interval, and point-wise jittering vectors are sampled from a truncated Gaussian distribution. Thus, CDA is simply a similarity transformation with small jittering that cannot simulate diverse shapes and deformable objects. Unlike synthetic datasets like Mod-elNet <ref type="bibr" target="#b13">[14]</ref> and ShapeNet <ref type="bibr" target="#b25">[26]</ref>, a real-world dataset like ScanObjectNN <ref type="bibr" target="#b0">[1]</ref> further necessitates the generation of sophisticated deformations such as a mixture of local transformations. These are exemplified in <ref type="figure" target="#fig_0">Figure 1</ref>: airplanes with varying lengths and directions of wings and body, guitars in varying sizes and aspect ratios, and people with different heights and postures (e.g., crossing legs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">PointWOLF</head><p>We present a simple yet effective point cloud augmentation with w eighted l o ca l trans f ormations (PointWOLF). Our method generates deformation for point clouds by a convex combination of multiple transformations with smoothly varying weights. PointWOLF first selects several anchor points and locates random local transformations ( e.g ., similarity transformations) at the anchor points. Based on the distance from a point in the input to the anchor points, our method differentially applies the local transformations. The smoothly varying weights based on the distance to the anchor points allow spatially continuous augmentation and generate realistic samples. Our framework can be viewed as a kernel regression with transformations. Sampling anchor points is the first step of our framework to locate multiple local transformations. To minimize the redundancy between local transformations, the anchor points P A ? P are selected by the Farthest Point Sampling (FPS) algorithm. FPS randomly chooses the first point and then sequentially chooses the farthest points from previous points. This maximizes the coverage of anchor points and allows diverse transformations. Local transformations in our framework are centered at the anchor points. At each anchor point, we randomly sam-ple a local transformation that includes scaling from the anchor point, changing aspect ratios, translation, and rotation around the anchor point. This subsumes the global transformation in <ref type="bibr" target="#b0">(1)</ref>. Given an anchor point p A j in P A , the local transformation for an input point p i can be written as:</p><formula xml:id="formula_1">p j i = SjRj( pi ? p A j ) + bj + p A j ,<label>(2)</label></formula><p>where R j , S j and b j are rotation matrix, scaling matrix and translation vector b j respectively which specifically correspond to p A j . S is a diagonal matrix with three positive real values, i.e ., S = diag ( s x , s y , s z ) to allow different scaling factors for different axes. In order to change the aspect ratios along arbitrary directions, a randomly rotated scaling matrix, e.g ., S ? j = R ? 1 S j R , can be used. Since many commonly used datasets are pre-aligned in a standard space ( e.g ., airplanes facing the same direction), we may assume sensible object orientations. In practice, we see that scaling with reasonable rotations as in <ref type="formula" target="#formula_1">(2)</ref> is sufficient.</p><p>Smooth deformations are key to generate realistic and locally transformed samples. A na?ve application of a random local transformation within its finite neighborhood may result in a discontinuous shape and an overlap of different parts. It has a high chance to lose discriminative structures. Instead, we employ the Nadaraya-Watson kernel regression <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> to smoothly interpolate the local transformations in the 3D space. Given M local transformations { T j } M j =1 , our smoothly varying transformation at an arbitrary point p i is given as:</p><formula xml:id="formula_2">T ( pi) = M j =1 K h ( pi , p A j ) Tj M k =1 K h ( pi , p A k ) ,<label>(3)</label></formula><p>where K h ( ? , ? ) is a kernel function with bandwidth h , and T j is the local transformation in (2) centered at p A j . To define T ( p i ) at any point in the 3D space, we use a kernel function that has a strictly positive value for any pair of points, i.e .,</p><formula xml:id="formula_3">K h ( p i , p j ) &gt; 0 for ? p i , ? p j .</formula><p>The following proposition theoretically guarantees that our augmentation is a smooth transformation under mild conditions. The proof is in the supplement.</p><formula xml:id="formula_4">Proposition 1 If a kernel function K h ( ? , ? ) and all lo- cal transformations { T j } M j =1 are smooth, then the locally weighted transformation T ( ? ) in (3) is a smooth transfor- mation.</formula><p>In our experiments, we use the Gaussian kernel with Euclidean distance after projection. Our kernel function is</p><formula xml:id="formula_5">K h ( pi , p A j ; ?j) = exp ?? ?j( pi ? p A j ) ? 2 2 2 h 2 ,<label>(4)</label></formula><p>where h is the bandwidth and ? j ? R 3 ? 3 is a projection matrix. ? = diag ( ? x , ? y , ? z ) is constructed with </p><formula xml:id="formula_6">? R 3 ? N 1: P A ? FPS ( P , M ) ? P A ? R 3 ? M 2: for j = 1 to M do 3: S j ? diag ( sx , sy , sz) ? s ? U [1 ,?s] 4: R j ? RotationMatrix ( ?x , ?y , ?z) ? ? ? U [ ? ?r ,?r ] 5: b j ? ( bx , by , bz) ? b ? U [ ? ? t ,? t ] 6:</formula><p>? j ? ( ?x , ?y , ?z) ? ? ? Bernoulli ( ? ) 7: end for 8: for i = 1 to N do 9:</p><p>for j = 1 to M do 10:</p><formula xml:id="formula_7">p j i ? S j R j ( p i ? p A j ) + b j + p A j ? Eq. (2) 11: w j i ? K h ( p i , p A j ; ? j ) ? Eq. (4) 12: end for 13: p ? i ? M j =1 w j i p j i M k =1 w k i ? Eq. (3) 14: end for 15: P ? ? { p ? i } N i =1 ? x , ? y , ? z ? Bernoulli ( ? )</formula><p>for ? ? (0 , 1) 1 , which acts as a "binary mask" to measure distances with respect to a random subset of the coordinates. For instance, a kernel function with ? j = diag (0 , 0 , 1) attenuates the influence of local transformation T j based on the distance from p A j along the z -axis, and this allows more diverse and realistic transformations such as shearing and torsion (Section 4.4) by a combination of local similarity transformations. Similar to the scaling matrix S in (2), in our experiments we use the projections onto the canonical axes/planes instead of an arbitrary subspace. Our preliminary experiments show that they are sufficient for pre-aligned point clouds.</p><p>We have introduced our framework from a kernel regression perspective. <ref type="figure" target="#fig_1">Figure 2</ref> shows a pipeline of our framework where the Augmented Sample is obtained by combining local transformations as a smooth transformation T and applying it on the Original Sample. Interestingly, at a high-level, we may also view our framework as an adaptive interpolation of multiple globally transformed point clouds resulting from applying different (local) transformations ( e.g ., T 1 , T 2 , T 3 in <ref type="figure" target="#fig_1">Figure 2</ref>) on the Original Sample. Thus, our framework can be implemented in two ways: (1) Transforming each point once by a smoothly varying transformation T in Eq. <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_1">(2)</ref> Transforming each point M times by the local transformations { T j } M j =1 and interpolate these M augmented points by the adaptive weights</p><formula xml:id="formula_8">K ( p , p A j ) / k K ( p , p A k )</formula><p>. Although both approaches require O ( M N ) complexity if we mainly consider M anchor points and N points, the second approach is slightly more efficient in practice since this only involves operations on points (vector) while the first approach involves operations Algorithm 2 AugTune</p><formula xml:id="formula_9">Input: original point cloud P ? R 3 ? N , ground truth y Input: classifier f ( ? ; w ) , difficulty coefficient ? ? (0 , 1] Output: Final augmented point cloud P * ? R 3 ? N 1: P ? ? PointWOLF ( P ) ? Algorithm 1 2: ? P ? f ( P ; w ) , ? P ? ? f ( P ? ; w ) 3: c P ? ? ? P y , c P ? ? ? ? P ? y ? confidence scores 4: c ? max( c P ? , (1 ? ? ) c P ) ? target confidence score 5: ? ? ? c ? c P ? c P ? c P ? ? approximate ? * by Eq. (7) 6: P * ? ? ? P + (1 ? ? ? ) P ? ? interpolate P and P ?</formula><p>on transformation matrices. Thus, we show the pseudocode of the second approach in Algorithm 1 and show the first approach's pseudocode in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">AugTune: Effective DA Tuning Method</head><p>The keys to effective data augmentation are strong candidate transformations and the optimal strength of the augmentation. We introduced PointWOLF that generates more diverse and smooth nonlinear transformations. Now, we present an efficient scheme to adaptively adjust the strength of data augmentation during training with a single hyperparameter. We believe that our scheme benefits not only our framework but also any classical data augmentation methods that heavily rely on an exhaustive grid search with a huge number of hyperparameters.</p><p>AugTune. We present AugTune described in Algorithm 2 to control the strength of augmentation. AugTune adjusts the strength of data augmentation by mixing the augmentation proposal P ? from PointWOLF and the original sample P . Given a classifier f ( ? ; w ) and a sample P , let ? P and c P denote its prediction and confidence score, i.e ., ? P = f ( P , w ) and c P = ? ? P y , where y is the ground truth label represented in one-hot encoding. ? P ? and c P ? are similarly defined for P ? . Note that all the confidence scores c P , c P ? are obtained on the fly while training the model, i.e ., an extra pretrained classifier is not required. To adjust the strength of augmentation, given a difficulty coefficient ? ? (0 , 1] , AugTune first computes a target confidence score c for each sample by</p><formula xml:id="formula_10">c = max( c P ? , (1 ? ? ) c P ) .<label>(5)</label></formula><p>Assuming the augmented P ? is difficult than the original P , i.e ., c P ? &lt; c P , as ? gets close to 0, it implies that Aug-Tune generates samples similar to the original sample P . Conversely, when ? = 1 , c = c P ? , AugTune uses the augmentation proposal P ? without any adjustment. To generate an augmented sample with the target confidence score, we use the linear interpolation of two samples P and P ? . Then, the problem is reduced to finding ? * defined by</p><formula xml:id="formula_11">? * = argmin ? ? c ? f ( ? P + (1 ? ? ) P ? ) ? 2 .<label>(6)</label></formula><p>However, solving (6) directly by optimization algorithms or grid search is still computationally expensive. Thus, we ap-</p><formula xml:id="formula_12">proximate ? * by ? ? = c ? c P ? c P ? c P ? which is the solution to ? c P + (1 ? ? ) c P ? = c.<label>(7)</label></formula><p>Our experiments show this approximation does not cause degradation in the target tasks (see the supplement). The final augmented sample P * is a convex combination of P and</p><formula xml:id="formula_13">P ? with ? ? , i.e ., P * = ? ? P + (1 ? ? ? ) P ? , then the model pa- rameter w is updated as w ? w ? ? ? w L ( f ( P * , w ) , y ) ,</formula><p>where ? is a learning rate. Note that since the correspondence between P and P ? are known by construction, the interpolation of two point clouds can be obtained by a simple point-wise interpolation given as p * = ? ? p + (1 ? ? ? ) p ? . Moreover, AugTune works as a safeguard to preserve the shape identity for the final P * . So, we rarely observed unrealistic augmented samples with reasonable hyperparameters (see the supplement for visualizations). Remarks. As we viewed our framework as the kernel regression on local transformations, AugTune is directly applicable to the transformation (parameter) space. In other words, instead of the point-wise interpolation, we may simply interpolate the local transformation parameters: scaling</p><formula xml:id="formula_14">matrix S ? j = ? I + (1 ? ? ) S j , rotation ? ? j = (1 ? ? ) ? j , and translation b ? j = (1 ? ? ) b j .</formula><p>However, due to its slightly higher computational cost and inferior performance, we applied AugTune on the input data space, see Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we demonstrate the effectiveness of our PointWOLF on both synthetic and real-world datasets. We begin by describing the datasets, baselines, and experimental setup. Then, we evaluate our framework for shape classification and part segmentation (Section 4.1), followed by ablation studies and analyses (Section 4.2). We conduct the experiments to show whether our method improves the robustness of models against both local and global corruptions leveraging diverse locally-augmented samples (Section 4.3). Lastly, we provide qualitative analysis of the augmented samples by PointWOLF <ref type="figure" target="#fig_5">(Section 4.4)</ref>. Datasets. We use both synthetic and real-world datasets for shape classification to evaluate our framework. Model-Net40 ( MN40 ) <ref type="bibr" target="#b13">[14]</ref> is a widely used synthetic benchmark dataset containing 9,840 CAD models in the training set and 2,468 CAD models in the validation set with total 40 classes of common object categories. As in <ref type="bibr" target="#b4">[5]</ref>, we also use the reduced version of MN40 ( ReducedMN40 ) to simulate data scarcity. ScanObjectNN (SONN) <ref type="bibr" target="#b0">[1]</ref> is a recent point cloud object dataset constructed from the real-world indoor datasets such as SceneNN <ref type="bibr" target="#b28">[29]</ref> and ScanNet <ref type="bibr" target="#b29">[30]</ref>. We use the following versions of SONN: (1) OBJ_ONLY which has 2,309 and 581 scanned objects for the training and validation sets respectively and (2) PB_T50_RS which With DGCNN, we use the official PyTorch code by the authors. We train each model with a batch size of 32 for 250 epochs. Note that for maximal fairness and consistency, we reproduced the numbers for every baselines except for PointMixup <ref type="bibr" target="#b4">[5]</ref> and followed the evaluation protocol of <ref type="bibr" target="#b4">[5]</ref> for every case. For our framework, the augmentation strength of PointWOLF was controlled by AugTune. Indeed in our framework, the difficulty coefficient ? is the only hyperparameter to tune. We used ? = 0 . 1 for synthetic datasets and ? = 0 . 3 for all real-world datasets. For more details, see the supplement. Baselines. We compare our framework (PointWOLF with AugTune) with the following data augmentation methods:</p><p>(1) A conventional DA ( CDA ) that performs the global similarity transformation ( e.g ., rotation along the up-axis, scaling, and translation) with point-wise jittering as <ref type="bibr" target="#b2">[3]</ref>.</p><p>(2) PointAugment [4] performs shape-wise transformation and point-wise displacement by learning an augmentor network.</p><p>For datasets on which the models have not been evaluated in the literature, we use the authors' official implementation of <ref type="bibr" target="#b3">[4]</ref>.</p><p>(3) PointMixup <ref type="bibr" target="#b4">[5]</ref> uses the interpolated sample between two point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shape Classification and Part Segmentation</head><p>We evaluate our methods on shape classification using a synthetic dataset (MN40) and a real-world dataset (SONN). Also we conduct experiments on a synthetic dataset (ShapeNetPart) for part segmentation. Shape Classification. <ref type="table">Table 1</ref> shows that our PointWOLF achieves consistent improvements in overall accuracy on both MN40 and ReducedMN40 with all three models compared to other augmentation methods (CDA, PointAugment, and PointMixup). Observe that MN40 and Re-ducedMN40 are pre-aligned synthetic datasets and interestingly CDA without rotation denoted by CDA (w/o R) outperforms CDA. Despite the saturated datasets, PointWOLF improves overall accuracy by 1.6 % compared to the best performing baseline on ReducedMN40 with PointNet.</p><p>Next, <ref type="table">Table 2</ref> shows the experimental results on SONN that is a more challenging and diverse real-world dataset. As expected, diverse and realistic augmented samples from PointWOLF significantly improve the performance on both OBJ_ONLY and PB_T50_RS with all three models . Specifically, on PB_T50_RS with PointNet++, the performance gains are 4.7%, 6.2%, and 3.5% compared to CDA, PointAugment, and PointMixup, respectively. Our Point-WOLF benefits the models more on the challenging cases with real-world data. Part Segmentation. Given a point cloud P , part segmentation is a point-wise classification where a model predicts a label for each point p i . In part segmentation, to derive the mixing ratio ? ? in <ref type="bibr" target="#b6">(7)</ref> at the object-level, we simply used the average of the pixel-wise confidence scores for our Aug-Tune, i.e ., c P = i c pi / |P | . Our experiments in <ref type="table">Table 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analyses on PointWOLF and AugTune</head><p>We conduct ablation studies and analyses on SONN (OBJ_ONLY) dataset with PointNet++ to analyze the significance of each component of PointWOLF and AugTune. Local Transformation Ablations. <ref type="table">Table 4</ref> reports the ablations on three types of local transformations in PointWOLF: rotation (R), scaling (S), and translation (T). PointWOLF with no local transformations is equivalent to PointNet++ <ref type="bibr" target="#b2">[3]</ref> with CDA. All three types of local transformations contribute to the accuracy gain. The best performance is obtained by +RST which utilizes all three local transformations, providing 3.5% improvement over the baseline with no local transformations denoted by 'None'. AugTune Ablations. We evaluate how effectively Aug-Tune controls the augmentation strengths given suboptimal augmentation ranges. We set the augmentation ranges S = ( ? r = 15 ? , ? s = 2 , ? t = 1 ) and use the multiples of the augmentation ranges: k S =( k ? r , k ? s , k ? t ). <ref type="table">Table 5</ref> shows that Point-WOLF w/ AugTune outperforms PointWOLF w/o Aug-Tune by 0.4 % ? 1.9 %. Our AugTune simplifies and accelerates the augmentation strength tuning with one difficulty coefficient ? . Our AugTune also benefits other augmentation methods, e.g., CDA, (see the supplement). Interpolation Space for AugTune. Two interpolation spaces can be considered for AugTune: the input data space and the transformation (parameter) space. Although directly tuning the transformation parameters seems natural, we have experimentally shown that AugTune in the input data space is a sensible choice. <ref type="table">Table 6</ref> shows the superiority of AugTune in the input data space regarding both performance and computational efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness to Corruption</head><p>Additional studies demonstrate our PointWOLF improves the robustness of models against various corruptions as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. First, we consider two local corruptions: (1) LocalDrop drops C local clusters and (2) Lo-calAdd adds C local clusters where a cluster consists of K nearest points from a randomly selected cluster center point. We used K = 50 in both cases. Second, to examine the general robustness to global corruption, we perform random point-wise (3) Dropout with a dropout rate r ? { 0 . 25 , 0 . 5 , 0 . 75 } and (4) Noise perturbation by offsets drawn from a Gaussian distribution with standard deviation ? ? { 0 . 01 , 0 . 03 , 0 . 05 } .</p><p>We trained PointNet++ with CDA (baseline) and Point-WOLF and evaluated them on corrupted samples by the local and global corruptions above. Experimental results on MN40 in <ref type="table" target="#tab_4">Table 7</ref> show that compared to CDA, Point-WOLF consistently and significantly improves the robustness against various corruptions. Importantly, the gain over the baseline significantly increases as the amount of corrup-  tions increases: 7.2% for LocalDrop ( C = 7 ), 13.1% for Lo-calAdd ( C = 7 ), 31.3% for Dropout ( r = 0 . 75 ), and 22.2% for Noise ( ? = 0 . 05 ). We believe that the diverse samples augmented by locally weighted transformations in Point-WOLF help models to learn more robust features against both 'local' and 'global' corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Analysis</head><p>Although PointWOLF essentially makes use of simple transformations such as rotation, scaling, and translation, we interestingly find that PointWOLF often mimics highly advanced yet realistic global deformations like torsion and shearing which cannot trivially be applied to point clouds. We achieve this by (1) projecting the transformations to random subsets of the axes and (2) allowing AugTune to identify "beneficial" cases which interestingly turn out to be a set of realistic deformations. <ref type="figure" target="#fig_5">Figure 4</ref> displays several such examples. For instance, when two anchor points are located at the top and bottom of the stool in <ref type="figure" target="#fig_5">Figure 4</ref>(a), a torsion occurs when it rotates only along the up-axis while preserving the near-anchor shapes of bright regions.</p><p>Similarly, a combination of local scaling and translation produce shearing or partial scaling . In fact, many advanced deformations that naturally preserve the shape identity are commonly defined by combinations of simpler transforma-  tions. In this sense, PointWOLF can adaptively allow a set of local transformations that often mimic advanced deformations. Importantly, seeing how these visually explainable augmentations from local transformations also bring empirical benefits, understanding and exploiting local structures are crucial for successful DA on point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel point cloud augmentation method, PointWOLF, which augments point clouds by weighted local transformations. Our method generates diverse and realistic augmented samples with smoothly varying deformations formulated as a kernel regression and brings significant improvements on point cloud tasks across several datasets. Moreover, to find an optimal augmentation in an expansive search space, our AugTune adaptively controls the strength of augmentation during training with a single hyperparameter. Our findings show that the augmentations we produce are not only visually realistic but also beneficial to the models, further validating the importance of understanding the local structure of point clouds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Locally augmented point clouds using PointWOLF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>PointWOLF Framework Illustration. Given an original sample, PointWOLF has multiple local transformations at each anchor point (red). PointWOLF produces smoothly varying non-rigid deformations based on the weighted local transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For N points and M anchor points, AugTune in the transformation (parameter) space requires computing a new transformation for each point and each anchor point in O ( M N ) . Contrarily, Aug-Tune in the input data space simply interpolates the points ( i.e ., ? p + (1 ? ? ) p ? for each p ) in O ( N ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of Local and Global Corruption. (a) and (b) are local corruptions while (c) and (d) are global corruptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Advanced Deformations by PointWOLF. In each transformations, the locally transformed samples (right) are generated from original samples (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 PointWOLFInput: original point cloud P ? R 3 ? N Input: # points N , # anchor points M , kernel bandwidth h Input: range for scaling ?s, range for rotation ?r, range for translation ?t, axis dropout probability ? Output: augmented point cloud P ?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Overall accuracy on ModelNet40. Overall accuracy on ScanObjectNN.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>CDA</cell><cell cols="2">CDA (w/o R)</cell><cell>PointAugment [4]</cell><cell>PointMixup [5]</cell><cell>PointWOLF</cell></row><row><cell></cell><cell>PointNet</cell><cell>89.2</cell><cell>89.7</cell><cell></cell><cell>90.8</cell><cell>89.9</cell><cell>91.1</cell></row><row><cell>MN40</cell><cell>PointNet++</cell><cell>91.3</cell><cell>92.5</cell><cell></cell><cell>92.4</cell><cell>92.7</cell><cell>93.2</cell></row><row><cell></cell><cell>DGCNN</cell><cell>91.7</cell><cell>92.7</cell><cell></cell><cell>92.9</cell><cell>93.1</cell><cell>93.2</cell></row><row><cell></cell><cell>PointNet</cell><cell>81.9</cell><cell>82.7</cell><cell></cell><cell>84.1</cell><cell>83.4</cell><cell>85.7</cell></row><row><cell>ReducedMN40</cell><cell>PointNet++</cell><cell>85.9</cell><cell>87.8</cell><cell></cell><cell>87.0</cell><cell>88.6</cell><cell>88.7</cell></row><row><cell></cell><cell>DGCNN</cell><cell>87.5</cell><cell>88.8</cell><cell></cell><cell>88.3</cell><cell>89.0</cell><cell>89.3</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell></cell><cell>CDA</cell><cell cols="2">PointAugment [4]</cell><cell>PointMixup [5]</cell><cell>PointWOLF</cell></row><row><cell></cell><cell>PointNet</cell><cell></cell><cell>76.1</cell><cell></cell><cell>74.4</cell><cell>-</cell><cell>78.7</cell></row><row><cell>OBJ_ONLY</cell><cell>PointNet++</cell><cell></cell><cell>86.6</cell><cell></cell><cell>85.4</cell><cell>88.5</cell><cell>89.7</cell></row><row><cell></cell><cell>DGCNN</cell><cell></cell><cell>85.7</cell><cell></cell><cell>83.1</cell><cell>-</cell><cell>88.8</cell></row><row><cell></cell><cell>PointNet</cell><cell></cell><cell>64.0</cell><cell></cell><cell>57.0</cell><cell>-</cell><cell>67.1</cell></row><row><cell>PB_T50_RS</cell><cell>PointNet++</cell><cell></cell><cell>79.4</cell><cell></cell><cell>77.9</cell><cell>80.6</cell><cell>84.1</cell></row><row><cell></cell><cell>DGCNN</cell><cell></cell><cell>77.3</cell><cell></cell><cell>76.8</cell><cell>-</cell><cell>81.6</cell></row><row><cell cols="4">is a perturbed version with 11,416 and 2,882 scanned ob-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">jects for the training and validation sets respectively. Both</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">have 15 classes. For part segmentation we adopt ShapeNet-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Part [26] which is a synthetic dataset contains 14,007 and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2,874 samples for training and validation sets. ShapeNet-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Part consists of 16 classes with 50 part labels. Each class</cell><cell></cell><cell></cell><cell></cell></row><row><cell>has 2 to 6 parts.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Implementation Details. All models and experiments are</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">implemented in PyTorch. For PointNet and PointNet++, the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">PyTorch implementation by [31] was used with a minimum</cell><cell></cell><cell></cell><cell></cell></row><row><cell>modification.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Overall mean IoU ( mI oU ) on ShapeNetPart. 74.7 80.2 71.9 89.6 71.5 90.3 84.9 79.5 95.2 65.2 91.1 81.1 55.1 72.8 82.2 83.5 +PointWOLF 82.5 73.3 78.8 73.2 89.6 72.2 91.2 86.2 79.7 95.2 64.6 92.5 80.2 56.6 73.1 82.2 83.8 PointNet++ 81.9 83.4 86.4 78.6 90.5 64.7 91.4 83.1 83.4 95.1 69.6 94.7 82.8 56.9 76.0 82.3 84.8 +PointWOLF 82.0 83.9 87.3 77.6 90.6 78.4 91.1 87.6 84.7 95.2 62.0 94.5 81.3 62.5 75.7 83.2 85.2 DGCNN 82.2 75.1 81.3 78.2 90.6 73.6 90.8 87.8 84.4 95.6 57.8 92.8 80.6 51.5 73.9 82.8 84.8 +PointWOLF 82.9 73.3 83.5 76.7 90.8 76.7 91.4 89.2 85.2 95.8 53.7 94.0 80.1 54.9 74.3 83.4 85.2 PointWOLF Ablation. R: rotate, S: scale, T: translate.</figDesc><table><row><cell>Method</cell><cell cols="2">air plane bag cap car chair</cell><cell>ear phone guitar knife lamp labtop</cell><cell>motor bike mug pistol rocket</cell><cell>skate board table mIoU</cell></row><row><cell cols="4">PointNet 81.8 Local Transformation R S T Accuracy</cell><cell></cell></row><row><cell></cell><cell>None</cell><cell></cell><cell>86.6</cell><cell></cell></row><row><cell></cell><cell>+R</cell><cell>?</cell><cell>88.1</cell><cell></cell></row><row><cell></cell><cell>+S</cell><cell>?</cell><cell>88.6</cell><cell></cell></row><row><cell></cell><cell>+T</cell><cell>?</cell><cell>89.5</cell><cell></cell></row><row><cell cols="2">+R, S, T</cell><cell>? ? ?</cell><cell>89.7</cell><cell></cell></row><row><cell cols="4">show that on ShapeNetPart [26], PointWOLF consistently</cell><cell></cell></row><row><cell cols="4">improves mean IoU (mIoU) over baselines (0.3% over</cell><cell></cell></row><row><cell cols="4">PointNet, 0.4% over PointNet++ and DGCNN), demon-</cell><cell></cell></row><row><cell cols="4">strating the applicability of PointWOLF to point-wise tasks.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Search Space Robustness Comparison. Interpolation space for AugTune.</figDesc><table><row><cell cols="3">Search Space w/o AugTune w/ AugTune</cell></row><row><cell>S</cell><cell>88.8</cell><cell>89.2</cell></row><row><cell>2 S</cell><cell>87.6</cell><cell>88.6</cell></row><row><cell>3 S</cell><cell>86.1</cell><cell>88.0</cell></row><row><cell>Space</cell><cell>Accuracy</cell><cell>Complexity</cell></row><row><cell>Transformation Space</cell><cell>88.1</cell><cell>O ( M N )</cell></row><row><cell>Input Data Space</cell><cell>89.7</cell><cell>O ( N )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Robustness to Corruption.</figDesc><table><row><cell cols="2">Corruption</cell><cell cols="2">CDA PointWOLF</cell></row><row><cell></cell><cell>C =3</cell><cell>67.0</cell><cell>68.8 (1.8 ? )</cell></row><row><cell>LocalDrop</cell><cell>C =5</cell><cell>63.2</cell><cell>66.5 (3.3 ? )</cell></row><row><cell></cell><cell>C =7</cell><cell>52.8</cell><cell>60.0 (7.2 ? )</cell></row><row><cell></cell><cell>C =3</cell><cell>73.9</cell><cell>77.2 (3.3 ? )</cell></row><row><cell>LocalAdd</cell><cell>C =5</cell><cell>63.5</cell><cell>69.4 (5.9 ? )</cell></row><row><cell></cell><cell>C =7</cell><cell>51.5</cell><cell>64.6 (13.1 ? )</cell></row><row><cell></cell><cell>r =0.25</cell><cell>91.2</cell><cell>92.2 (1.0 ? )</cell></row><row><cell>Dropout</cell><cell>r =0.5</cell><cell>84.0</cell><cell>90.4 (6.4 ? )</cell></row><row><cell></cell><cell>r =0.75</cell><cell>29.5</cell><cell>60.8 (31.3 ? )</cell></row><row><cell></cell><cell>? =0.01</cell><cell>91.5</cell><cell>93.0 (1.5 ? )</cell></row><row><cell>Noise</cell><cell>? =0.03</cell><cell>78.8</cell><cell>87.6 (8.8 ? )</cell></row><row><cell></cell><cell>? =0.05</cell><cell>22.9</cell><cell>45.1 (22.2 ? )</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To prevent the projection matrix from zero-matrix, we resample ? if (0,0,0) is selected.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointaugment: An auto-augmentation framework for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR , 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointmixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV , 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view harmonized bilinear network for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointvoxel CNN for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bin Fan, Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states. In ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On estimating regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elizbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadaraya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smooth regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhy?: The Indian Journal of Statistics, Series A</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scenenn: A scene meshes dataset with annotations</title>
		<editor>Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pytorch implementation of pointnet and point-net++</title>
		<idno>31] yanx27</idno>
		<ptr target="https://github.com/yanx27/Pointnet_Pointnet2_pytorch" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
