<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015">2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kitti</forename><surname>Sintel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kitti</forename></persName>
						</author>
						<title level="a" type="main">UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning</title>
					</analytic>
					<monogr>
						<title level="m">particu-lar, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI</title>
						<imprint>
							<date type="published" when="2015">2015. 2015</date>
						</imprint>
					</monogr>
					<note>which outperform the previous state-of-the-art methods by 22.2% and 15.7%, respectively.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an unsupervised learning approach for optical flow estimation by improving the upsampling and learning of pyramid network. We design a self-guided upsample module to tackle the interpolation blur problem caused by bilinear upsampling between pyramid levels. Moreover, we propose a pyramid distillation loss to add supervision for intermediate levels via distilling the finest flow as pseudo labels. By integrating these two components together, our method achieves the best performance for unsupervised optical flow learning on multiple leading benchmarks, including MPI-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow estimation has been a fundamental computer vision task for decades, which has been widely used in various applications such as video editing <ref type="bibr" target="#b13">[14]</ref>, behavior recognition <ref type="bibr" target="#b31">[31]</ref> and object tracking <ref type="bibr" target="#b2">[3]</ref>. The early solutions focus on minimizing a pre-defined energy function with optimization tools <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b30">30]</ref>. Nowadays deep learning based approaches become popular, which can be classified into two categories, the supervised <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref> and unsupervised ones <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b37">37]</ref>. The former one uses synthetic or humanlabelled dense optical flow as ground-truth to guide the motion regression. The supervised methods have achieved leading performance on the benchmark evaluations. However, the acquisition of ground-truth labels are expensive. In addition, the generalization is another challenge when trained on synthetic datasets. As a result, the latter category, i.e. the unsupervised approaches attracts more attentions recently, which does not require the ground-truth labels. In unsupervised methods, the photometric loss between two images is commonly used to train the optical flow estima- <ref type="figure">Figure 1</ref>. An example from Sintel Final benchmark. Compared with previous unsupervised methods including SelFlow <ref type="bibr" target="#b21">[22]</ref>, Epi-Flow <ref type="bibr" target="#b44">[44]</ref>, ARFlow <ref type="bibr" target="#b19">[20]</ref>, SimFlow <ref type="bibr" target="#b11">[12]</ref> and UFlow <ref type="bibr" target="#b14">[15]</ref>, our approach produces sharper and more accurate results in object edges. tion network. To facilitate the training, the pyramid network structure <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b9">10]</ref> is often adopted, such that both global and local motions can be captured in a coarse-to-fine manner. However, there are two main issues with respect to the pyramid learning, which are often ignored previously. We refer the two issues as bottom-up and top-down problems. The bottom-up problem refers to the upsampling module in the pyramid. Existing methods often adopt simple bilinear or bicubic upsampling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref>, which interpolates cross edges, resulting in blur artifacts in the predicted optical flow. Such errors will be propagated and aggregated when the scale becomes finer. <ref type="figure">Fig. 1</ref> shows an example. The top-down problem refers to the pyramid supervision. The previous leading unsupervised methods typically add guidance losses only on the final output of the network, while the intermediate pyramid levels have no guidance. In this condition, the estimation errors in coarser levels will accumulate and damage the estimation at finer levels due to the lack of training guidance.</p><p>To this end, we propose an enhanced pyramid learning framework of unsupervised optical flow estimation. First, we introduce a self-guided upsampling module that supports blur-free optical flow upsampling by using a selflearned interpolation flow instead of the straightforward in-terpolations. Second, we design a new loss named pyramid distillation loss that supports explicitly learning of the intermediate pyramid levels by taking the finest output flow as pseudo labels. To sum up, our main contributions include:</p><p>? We propose a self-guided upsampling module to tackle the interpolation problem in the pyramid network, which can generate the sharp motion edges.</p><p>? We propose a pyramid distillation loss to enable robust supervision for unsupervised learning of coarse pyramid levels.</p><p>? We achieve superior performance over the state-of-theart unsupervised methods with a relatively large margin, validated on multiple leading benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised Deep Optical Flow</head><p>Supervised methods require annotated flow ground-truth to train the network <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref>. FlowNet <ref type="bibr" target="#b5">[6]</ref> was the first work that proposed to learn optical flow estimation by training fully convolutional networks on synthetic dataset FlyingChairs. Then, FlowNet2 <ref type="bibr" target="#b10">[11]</ref> proposed to iteratively stack multiple networks for the improvement. To cover the challenging scene with large displacements, SpyNet <ref type="bibr" target="#b25">[26]</ref> built a spatial pyramid network to estimate optical flow in a coarse-to-fine manner. PWC-Net <ref type="bibr" target="#b34">[34]</ref> and LiteFlowNet <ref type="bibr" target="#b8">[9]</ref> proposed to build efficient and lightweight networks by warping feature and calculating cost volume at each pyramid level. IRR-PWC <ref type="bibr" target="#b9">[10]</ref> proposed to design pyramid network by an iterative residual refinement scheme. Recently, RAFT <ref type="bibr" target="#b35">[35]</ref> proposed to estimate flow fileds by 4D correlation volume and recurrent network, yielding state-of-the-art performance. In this paper, we work in unsupervised setting where no ground-truth labels are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Unsupervised Deep Optical Flow</head><p>Unsupervised methods do not need annotations for training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">42]</ref>, which can be divided into two categories: the occlusion handling methods and the alignment learning methods. The occlusion handling methods mainly focus on excluding the impact of the occlusion regions that cannot be aligned inherently. For this purpose, many methods are proposed, including the occlusion-aware losses by forwardbackward occlusion checking <ref type="bibr" target="#b23">[24]</ref> and range-map occlusion checking <ref type="bibr" target="#b37">[37]</ref>, the data distillation methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">28]</ref>, and augmentation regularization loss <ref type="bibr" target="#b19">[20]</ref>. On the other hand, the alignment learning methods are mainly developed to improve optical flow learning under multiple image alignment constrains, including the census transform constrain <ref type="bibr" target="#b29">[29]</ref>, multi-frame formulation <ref type="bibr" target="#b12">[13]</ref>, epipolar constrain <ref type="bibr" target="#b44">[44]</ref>, depth constrains <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b18">19]</ref> and feature similarity constrain <ref type="bibr" target="#b11">[12]</ref>. Recently, UFlow <ref type="bibr" target="#b14">[15]</ref> achieved the state-of-the-art performance on multiple benchmarks by systematically analyzing and integrating multiple unsupervised components into a unified framework. In this paper, we propose to improve optical flow learning with our improved pyramid structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image Guided Optical Flow Upsampling</head><p>A series of methods have been developed to upsample images, depths or optical flows by using the guidance information extracted from high resolution images. The early works such as joint bilateral upsampling <ref type="bibr" target="#b15">[16]</ref> and guided image filtering <ref type="bibr" target="#b7">[8]</ref> proposed to produce upsampled results by filters extracted from the guidance images. Recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b32">32]</ref> proposed to use deep trainable CNNs to extract guidance feature or guidance filter for upsampling. In this paper, we build an efficient and lightweight selfguided upsampling module to extract interpolation flow and interpolation mask for optical flow upsampling. By inserting this module into a deep pyramid network, high quality results can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pyramid Structure in Optical Flow Estimation</head><p>Optical flow estimation can be formulated as:</p><formula xml:id="formula_0">V f = H(?, I t , I t+1 ),<label>(1)</label></formula><p>where I t and I t+1 denote the input images, H is the estimation model with parameter ?, and V f is the forward flow field that represents the movement of each pixel in I t towards its corresponding pixel in I t+1 . The flow estimation model H is commonly designed as a pyramid structure, such as the classical PWC-Net <ref type="bibr" target="#b34">[34]</ref>. The pipeline of our network is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The network can be divided into two stages: pyramid encoding and pyramid decoding. In the first stage, we extract feature pairs in different scales from the input images by convolutional layers. In the second stage, we use a decoder module D and an upsample module S ? to estimate optical flows in a coarse-to-fine manner. The structure of the decoder module D is the same as in UFlow <ref type="bibr" target="#b14">[15]</ref>, which contains feature warping, cost volume construction by correlation layer, cost volume normalization, and flow decoding by fully convolutional layers. Similar to recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>, we also make the parameters of D and S ? shared across all the pyramid levels. In summary, the pyramid decoding stage can be formulated as follows:</p><formula xml:id="formula_1">V i?1 f = S ? (F i t , F i t+1 , V i?1 f ),<label>(2)</label></formula><formula xml:id="formula_2">V i f = D(F i t , F i t+1 ,V i?1 f ),<label>(3)</label></formula><p>where i ? {0, 1, ..., N } is the index of each pyramid level and the smaller number represents the coarser level,    is the upsampled flow of the i ? 1 level. In practice, considering the accuracy and efficiency, N is usually set to 4 <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">34]</ref>. The final optical flow result is obtained by directly upsampling the output of the last pyramid level. Particularly, in Eq. 2, the bilinear interpolation is commonly used to upsample flow fields in previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>, which may yield noisy or ambiguity results at object boundaries. In this paper, we present a self-guided upsample module to tackle this problem as detailed in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-guided Upsample Module</head><p>In <ref type="figure" target="#fig_2">Fig. 3</ref> left, the case of bilinear interpolation is illustrated. We show 4 dots which represent 4 flow vectors, belonging to two motion sources, marked as red and blue, respectively. The missing regions are then bilinear interpolated with no semantic guidance. Thus, a mixed interpolation result is generated at the red motion area, resulting in cross-edge interpolation. In order to alleviate this problem, we propose a self-guided upsample module (SGU) to change the interpolation source points by an interpolation flow. The main idea of our SGU is shown in <ref type="figure" target="#fig_2">Fig. 3</ref> right. We first interpolate a point by its enclosing red motions and then bring the result to the target place with the learned interpolation flow ( <ref type="figure" target="#fig_2">Fig. 3</ref>, green arrow). As a result, the mixed interpolation problem can be avoided.</p><p>In our design, to keep the interpolation in plat areas from being changed and make the interpolation flow only applied on motion boundary areas, we learn a per-pixel weight map to indicate where the interpolation flow should be disabled. Thus, the upsampling process of our SGU is a weighted </p><formula xml:id="formula_3">V i?1 f (p) = k?N (p/s) w(p/s, k)V i?1 f (k),<label>(4)</label></formula><p>where p is a pixel coordinate in higher resolution, s is the scale magnification, N denotes the four neighbour pixels, and w(p/s, k) is the bilinear interpolation weights. Then, we compute an interpolation flow U i f from features F i t and F i t+1 to change the interpolation of V i?1 f by warping:</p><formula xml:id="formula_4">V i?1 f (p) = k?N (d) w(d, k)V i?1 f (k),<label>(5)</label></formula><formula xml:id="formula_5">d = p + U i f (p),<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">V i?1 f is the result of warping V i?1 f</formula><p>by the interpolation flow U i f . Since the interpolation blur only occurs in object edge regions, it is unnecessary to learn interpolation flow in flat regions. We thus use an interpolation map B i f to explicitly force the model to learn interpolation flow only in motion boundary regions. The final upsample result is the</p><formula xml:id="formula_7">fusion of V i?1 f and V i?1 f : V i?1 f = B i f V i?1 f + (1 ? B i f ) V i f ,<label>(7)</label></formula><p>whereV i?1 f is the output of our self-guided upsample module and is the element-wise multiplier.</p><p>To produce the interpolation flow U i f and the interpolation map B i f , we use a dense block with 5 convolutional layers. Specifically, we concatenate the feature map F i t and the warped feature map F i t+1 as the input of the dense block. The kernel number of each convolutional layer in the dense block is 32, 32, 32, 16, 8 respectively. The output of the dense block is a tensor map with 3 channels. We use the first two channels of the tensor map as the interpolation flow and use the last channel to form the interpolation map through a sigmoid layer. Note that, no supervision is introduced for the learning of interpolation flow and interpolation map. <ref type="figure" target="#fig_5">Fig. 5</ref> shows an example from MPI-Sintel Final dataset, where our SGU produces cleaner and sharper results at object boundaries compared with the bilinear method. Interestingly, the self-learned interpolation map is nearly to be an edge map and the interpolation flow is also focused on object edge regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Guidance at Pyramid Levels</head><p>In our framework, we use several losses to train the pyramid network: the unsupervised optical flow losses for the final output flow and the pyramid distillation loss for the intermediate flows at different pyramid levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Unsupervised Optical Flow Loss</head><p>To learn the flow estimation model H in unsupervised setting, we use the photometric loss L m based on the brightness constancy assumption that the same objects in I t and I t+1 must have similar intensities. However, some regions may be occluded by moving objects, so that their corresponding regions do not exist in another image. Since the photometric loss can not work in these regions, we only add L m on the non-occluded regions. The photometric loss can be formulated as follows:</p><formula xml:id="formula_8">L m = p ? I t (p) ? I t+1 p + V f (p) ? M t (p) p M 1 (p) ,<label>(8)</label></formula><p>where M t is the occlusion mask and ? is the robust penalty function <ref type="bibr" target="#b20">[21]</ref>: ?(x) = (|x| + ) q with q, being 0.4 and 0.01. In the occlusion mask M t , which is estimated by forward-backward checking <ref type="bibr" target="#b23">[24]</ref>, 1 represents the nonoccluded pixels in I t and 0 for those are occluded.</p><p>To improve the performance, some previously effective unsupervised components are also added, including smooth loss <ref type="bibr" target="#b37">[37]</ref> L s , census loss <ref type="bibr" target="#b23">[24]</ref> L c , augmentation regularization loss <ref type="bibr" target="#b19">[20]</ref> L a , and boundary dilated warping loss <ref type="bibr" target="#b22">[23]</ref> (a) Visual comparison on KITTI 2012 (first two rows) and KITTI 2015 (last two rows).  L b . For simplicity, we omit these components. Please refer to previous works for details. The capability of these components will be discussed in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Pyramid Distillation Loss</head><p>To learn intermediate flow for each pyramid level, we propose to distillate the finest output flow to the intermediate ones by our pyramid distillation loss L d . Intuitively, this is equivalent to calculating all the unsupervised losses on each of the intermediate outputs. However, the photometric consistency measurement is not accurate enough for optical flow learning at low resolutions <ref type="bibr" target="#b14">[15]</ref>. As a result, it is inappropriate to enforce unsupervised losses at intermediate levels, especially at the lower pyramid levels. Therefore, we propose to use the finest output flow as pseudo labels and add supervised losses instead of unsupervised losses for intermediate outputs.</p><p>To calculate L d , we directly downsample the final output flow and evaluate its difference with the intermediate flows.</p><p>Since occlusion regions are excluded from L m , flow estimation in occlusion regions is noisy. In order to eliminate the influence of these noise regions in the pseudo label, we also downsample the occlusion mask M t and exclude occlusion regions from L d . Thus, our pyramid distillation loss can be formulated as follow:</p><formula xml:id="formula_9">L d = N i=0 p ? V i f ? S ? (s i , V f ) ? S ? (s i , M t ),<label>(9)</label></formula><p>where s i is the scale magnification of pyramid level i and S ? is the downsampling function. Eventually, our training loss L is formulated as follows:</p><formula xml:id="formula_10">L = L m + ? d L d + ? s L s + ? c L c + ? a L a + ? b L b ,<label>(10)</label></formula><p>where ? d , ? s , ? c , ? a and ? b are hyper-parameters and we set ? d = 0.01, ? s = 0.05, ? c = 1, ? a = 0.5 and ? b = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Implementation Details</head><p>We conduct experiments on three datasets: MPI-Sintel <ref type="bibr" target="#b4">[5]</ref>, KITTI 2012 <ref type="bibr" target="#b6">[7]</ref> and KITTI 2015 <ref type="bibr" target="#b24">[25]</ref>  <ref type="table">Table 1</ref>. Comparison with previous methods. We use the average EPE error (the lower the better) as evaluation metric for all the datasets except on KITTI 2015 benchmark test, where the F1 measurement (the lower the better) is used. Missing entries '?' indicates that the result is not reported in the compared paper, and (?) indicates that the testing images are used during unsupervised training. The best unsupervised results are marked in red and the second best are in blue. Note that, for results of the supervised methods, '+ft' means the model is trained on the target domain, otherwise, the model is trained on synthetic datasets such as Flying Chairs <ref type="bibr" target="#b5">[6]</ref> and Flying Chairs occ <ref type="bibr" target="#b9">[10]</ref>. For unsupervised methods, we report the performance of the model trained using images from target domain.</p><p>the same dataset setting as previous unsupervised methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref>. For MPI-Sintel dataset, which contains 1, 041 training image pairs rendered in two different passes ('Clean' and 'Final'), we use all the training images from both 'Clean' and 'Final' to train our model. For KITTI 2012 and 2015 datasets, we pretrain our model using 28, 058 image pairs from the KITTI raw dataset and then finetune our model on the multi-view extension dataset. The flow ground-truth is only used for validation.</p><p>We implement our method with PyTorch, and complete the training in 1000k iterations with batch size of 4. The total number of parameters of our model is 3.49M, in which the proposed self-guided upsample module has 0.14M trainable parameters. Moreover, the running time of our full model is 0.05s for a Sintel image pair with resolution 436 ? 1024. The standard average endpoint error (EPE) and the percentage of erroneous pixels (F1) are used as the evaluation metric of optical flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Existing Methods</head><p>We compare our method with existing supervised and unsupervised methods on leading optical flow benchmarks. Quantitative results are shown in <ref type="table">Table 1</ref>, where our method outperforms all the previous unsupervised methods on all the datasets. In <ref type="table">Table 1</ref>, we mark the best results by red and the second best by blue in unsupervised methods.</p><p>Comparison with Unsupervised Methods. On KITTI 2012 online evaluation, our method achieves EPE=1.4, which improves the EPE=1.8 of the previous best method ARFlow <ref type="bibr" target="#b19">[20]</ref> by 22.2%. Moreover, on KITTI 2015 online evaluation, our method reduces the F1-all value of 11.13% in UFlow <ref type="bibr" target="#b14">[15]</ref>   <ref type="table">Table 2</ref>. Ablation study of the unsupervised components. CL: census loss <ref type="bibr" target="#b23">[24]</ref>, BDWL: boundary dilated warping loss <ref type="bibr" target="#b22">[23]</ref>, ARL: augmentation regularization loss <ref type="bibr" target="#b19">[20]</ref>, SGU: self-guided upsampling, PDL: pyramid distillation loss. The best results are marked in bold.</p><p>On the test benchmark of MPI-Sintel dataset, we achieve EPE=4.68 on the 'Clean' pass and EPE=5.32 on the 'Final' pass, both outperforming all the previous methods. Some qualitative comparison results are shown in <ref type="figure" target="#fig_7">Fig. 6</ref>, where our method produces more accurate results than the stateof-the-art method UFlow <ref type="bibr" target="#b14">[15]</ref>.</p><p>Comparison with Supervised Methods. <ref type="table">Table 1</ref>, representative supervised methods are also reported for comparison. In practical applications where flow ground-truth is not available, the supervised methods can only train models using synthetic datasets. In contrast, unsupervised methods can be directly implemented using images from the target domain. As a result, on KITTI and Sintel Final datasets, our method outperforms all the supervised methods trained on synthetic datasets, especially in real scenarios such as the KITTI 2015 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>As for the in-domain ability, our method is also comparable with supervised methods. Interestingly, on KITTI 2012 and 2015 datasets, our method achieve EPE=1.4 and F1=9.38%, which outperforms classical supervised methods such as PWC-Net <ref type="bibr" target="#b34">[34]</ref> and LiteFlowNet <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To analyze the capability and design of each individual component, we conduct extensive ablation experiments on the train set of KITTI and MPI-Sintel datasets following the setting in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12]</ref>. The EPE error over all pixels (ALL), non-occluded pixels (NOC) and occluded pixels (OCC) are reported for quantitative comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Components.</head><p>Several unsupervised components are used in our framework including census loss <ref type="bibr" target="#b23">[24]</ref> (CL), boundary dilated warping loss <ref type="bibr" target="#b22">[23]</ref> (BDWL), augmentation regularization loss <ref type="bibr" target="#b19">[20]</ref> (ARL), our proposed self-guided upsampling (SGU) and pyramid distillation loss (PDL). We assess the effect of these components in <ref type="table">Table.</ref> 2. In the first row of <ref type="table">Table.</ref> 2, we only use photometric loss and smooth loss to train the pyramid network with our SGU disabled. Comparing the first four rows in  <ref type="table">Table 3</ref>. Comparison of our SGU with different upsampling methods: the basic bilinear upsampling, image guided upsampling methods including JBU <ref type="bibr" target="#b15">[16]</ref>, GF <ref type="bibr" target="#b7">[8]</ref>, DJF <ref type="bibr" target="#b16">[17]</ref>, DGF <ref type="bibr" target="#b38">[38]</ref> and PAC <ref type="bibr" target="#b32">[32]</ref>, and the variants of SGU such as SGU-FM, where the interpolation flow and weight map are both removed, and SGU-M, where the only the interpolation map is removed.</p><p>ARL, the performance of optical flow estimation can be improved, which is equivalent to the current best performance reported in UFlow <ref type="bibr" target="#b14">[15]</ref>. Comparing the last four rows in <ref type="table">Table.</ref> 2, we can see that: (1) the EPE error can be reduced by using our SGU to solve the bottom-up interpolation problem; (2) the top-down supervision information by our PDL can also improve the performance; (3) the performance can be further improved by combining the SGU and PDL. Some qualitative comparison results are shown in <ref type="figure" target="#fig_8">Fig. 7</ref>, where 'Full' represents our full method, 'W/O SGU' means the SGU module of our network is disabled and 'W/O PDL' means the PDL is not considered during training. Comparing with our full method, the boundary of the predicted flow becomes blurry when SGU is removed while the error increases when PDL is removed.</p><p>Self-guided Upsample Module. There is a set of methods that use image information to guide the upsampling process, e.g., JBU <ref type="bibr" target="#b15">[16]</ref>, GF <ref type="bibr" target="#b7">[8]</ref>, DJF <ref type="bibr" target="#b16">[17]</ref>, DGF <ref type="bibr" target="#b38">[38]</ref> and PAC <ref type="bibr" target="#b32">[32]</ref>. We implement them into our pyramid network and train with the same loss function for comparisons. The average EPE errors of the validation sets are reported in <ref type="table">Table.</ref> 3. As a result, our SGU is superior to the image guided upsampling methods. The reason lies in two folds: (1) the guidance information that directly extracted from images </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sintel may not be favorable to the unsupervised learning of optical flow especially for the error-prone occlusion regions; (2) our SGU can capture detail matching information by learning from the alignment features which are used to compute optical flow by the decoder. In the last three rows of <ref type="table">Table.</ref> 3, we also compare our SGU with its variants: (1) SGU-FM, where the interpolation flow and interpolation map are both removed so that the upsampled flow is directly produced by the dense block without warping and fusion in <ref type="figure" target="#fig_4">Fig. 4;</ref> (2) SGU-M, where the interpolation map is disabled. Although the performance of SGU-FM is slightly better than the baseline bilinear method, it is poor than that of SGU-M, which demonstrates that using an interpolation flow to solve the interpolation blur is more effective than directly learning to predict a new optical flow. Moreover, the performance reduced as the interpolation map is removed from SGU, which demonstrates the effectiveness of the interpolation map.</p><p>Pyramid Distillation Loss. We compare our PDL with different pyramid losses in <ref type="table">Table.</ref> 4, where 'w/o PL' means no pyramid loss is calculated, 'PUL-up' and 'PUL-down' represent the pyramid unsupervised loss by upsampling intermediate flows to the image resolution and by downsamling the images to the intermediate resolutions accordingly. 'PDL w/o occ' means the occlusion masks on pyramid levels are disabled in our PDL. In 'PUL-up' and 'PUL-down', the photometric loss, smooth loss, census loss and boundary dilated warping loss are used for each pyramid level and their weights are tuned to our best in the experiments. To eliminate variables, the occlusion masks used in 'PUL-up' and 'PUL-down' are calculated by the same method as in our PDL. As a result, model trained by our pyramid distillation loss can generate better results on each pyramid level than by pyramid unsupervised losses. This is because our pseudo labels can provide better supervision on low resolutions than unsupervised losses. Moreover, the error increased when the occlusion mask is disabled in our PDL, indicating that excluding the noisy occlusion regions can improve the quality of the pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel framework for unsupervised learning of optical flow estimation by bottom-up and topdown optimize of the pyramid levels. For the interpolation problem in the bottom-up upsampling process of pyramid network, we proposed a self-guided upsample module to change the interpolation mechanism. For the top-down guidance of the pyramid network, we proposed a pyramid distillation loss to improve the optical flow learning on intermediate levels of the network. Extensive experiments have shown that our method can produce high-quality optical flow results, which outperform all the previous unsupervised methods on multiple leading benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the pipeline of our network, which contains two stage: pyramid encoding to extract feature pairs in different scales and pyramid decoding to estimate optical flow in each scale. Note that the parameters of the decoder module and the upsample module are shared across all the pyramid levels.Bilinear InterpolationInterpolation flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of bilinear upsampling (left) and the idea of our self-guided upsampling (right). Red and blue dots are motion vectors from different objects. Bilinear upsampling often produces cross-edge interpolation. We propose to first interpolate a flow vector in other position without crossing edge and then bring it to the desired position by our learned interpolation flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc>i t+1 are features extracted from I t and I t+1 at the i-th level, andV i?1 f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of our self-guided upsample module. We first upscale the input low resolution flow V i?1 f by bilinear upsampling and use a dense block to compute an interpolation flow U i f and an interpolation map B i f . Then we generate the high resolution flow by warping and fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visual example of our self-guided upsample module (SGU) on MPI-Sintel Final dataset. Results of bilinear method and our SGU are shown. The zoom-in patches are also shown on the right of each sample for better comparison. combination of the bilinear upsampled flow and a modified flow obtained by warping the upsampled flow with the interpolation flow. The detailed structure of our SGU module is shown in Fig. 4. Given a low resolution flow V i?1 f from the i ? 1-th level, we first generate an initial flow V i?1 f in higher resolution by bilinear interpolation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) Visual comparison on Sintel Clean (first two rows) and Sintel Final (last two rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Visual comparison of our method with the state-of-the-art method UFlow<ref type="bibr" target="#b14">[15]</ref> on KITTI (a) and Sintel (b) benchmarks. The error maps visualized by the benchmark websites are shown in the last two columns with obvious difference regions marked by yellow boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Visual results of removing the SGU or PDL from our full method on Sintel dataset. The room in flows and error maps are shown in the right corner of each sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>to 9.38% with 15.7% improvement.</figDesc><table><row><cell>CL</cell><cell>BDWL ARL SGU</cell><cell>PDL</cell><cell>KITTI 2012</cell><cell></cell><cell></cell><cell>KITTI 2015</cell><cell>Sintel Clean</cell><cell>Sintel Final</cell></row><row><cell></cell><cell></cell><cell cols="6">ALL NOC OCC ALL NOC OCC ALL NOC OCC ALL NOC OCC</cell></row><row><cell></cell><cell></cell><cell>4.52</cell><cell cols="3">1.76 19.63 7.58</cell><cell cols="2">2.46 30.43 (3.52) (1.87) (12.93) (4.19) (2.59) (13.64)</cell></row><row><cell></cell><cell></cell><cell>3.39</cell><cell cols="3">1.09 16.58 6.89</cell><cell cols="2">2.20 28.12 (3.41) (1.62) (13.51) (3.85) (2.17) (13.71)</cell></row><row><cell></cell><cell></cell><cell>1.42</cell><cell>0.91</cell><cell>4.39</cell><cell>3.00</cell><cell>2.12</cell><cell>6.89 (2.84) (1.50) (10.63) (3.60) (2.28) (11.52)</cell></row><row><cell></cell><cell></cell><cell>1.37</cell><cell>0.93</cell><cell>3.98</cell><cell>2.64</cell><cell>1.96</cell><cell>6.01 (2.61) (1.33) (10.14) (3.17) (1.92) (10.70)</cell></row><row><cell></cell><cell></cell><cell>1.33</cell><cell>0.88</cell><cell>4.00</cell><cell>2.56</cell><cell>1.91</cell><cell>5.35 (2.46) (1.17) (9.89) (2.79) (1.53) (10.28)</cell></row><row><cell></cell><cell></cell><cell>1.36</cell><cell>0.91</cell><cell>4.03</cell><cell>2.61</cell><cell>1.96</cell><cell>5.52 (2.53) (1.23) (10.12) (2.93) (1.67) (10.38)</cell></row><row><cell></cell><cell></cell><cell>1.27</cell><cell>0.85</cell><cell>3.77</cell><cell>2.45</cell><cell>1.87</cell><cell>5.32 (2.33) (1.07) (9.66) (2.63) (1.39) (9.91)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table.2, we can see that by combining CL, BDWL and</figDesc><table><row><cell>Method</cell><cell cols="3">KITTI 2012 KITTI 2015 Sintel Clean</cell><cell>Sintel Final</cell></row><row><cell>Bilinear</cell><cell>1.36</cell><cell>2.61</cell><cell>(2.53)</cell><cell>(2.93)</cell></row><row><cell>JBU [16]</cell><cell>1.51</cell><cell>3.00</cell><cell>(2.66)</cell><cell>(2.98)</cell></row><row><cell>GF [8]</cell><cell>1.40</cell><cell>2.90</cell><cell>(2.72)</cell><cell>(2.92)</cell></row><row><cell>DJF [17]</cell><cell>1.36</cell><cell>2.79</cell><cell>(2.75)</cell><cell>(3.20)</cell></row><row><cell>DGF [38]</cell><cell>1.41</cell><cell>3.14</cell><cell>(2.69)</cell><cell>(3.05)</cell></row><row><cell>PAC [32]</cell><cell>1.42</cell><cell>2.65</cell><cell>(2.58)</cell><cell>(2.95)</cell></row><row><cell>SGU-FM</cell><cell>1.35</cell><cell>2.60</cell><cell>(2.52)</cell><cell>(2.91)</cell></row><row><cell>SGU-M</cell><cell>1.33</cell><cell>2.59</cell><cell>(2.41)</cell><cell>(2.86)</cell></row><row><cell>SGU</cell><cell>1.27</cell><cell>2.45</cell><cell>(2.33)</cell><cell>(2.63)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of different pyramid losses: no pyramid loss (w/o PL), pyramid unsupervised loss by upsampling intermediate flows to image resolution to compute unsupervised objective functions (PUL-up) and by downsampling images to the intermediate resolution (PUL-down), our pyramid distillation loss without masking out occlusion regions (PDL w/o occ) and our pyramid distillation loss (PDL). All the intermediate output flows are evaluated on the train set of Sintel Clean and Final.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Clean train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sintel Final train</cell><cell></cell></row><row><cell></cell><cell>?1</cell><cell>?4</cell><cell>?8</cell><cell>?16</cell><cell>?32</cell><cell>?64</cell><cell>?1</cell><cell>?4</cell><cell>?8</cell><cell>?16</cell><cell>?32</cell><cell>?64</cell></row><row><cell>w/o PL</cell><cell cols="12">(2.46) (2.53) (2.78) (3.38) (4.70) (7.39) (2.79) (2.89) (3.11) (3.73) (5.07) (7.59)</cell></row><row><cell>PUL-up</cell><cell cols="12">(2.45) (2.52) (2.75) (3.35) (4.61) (7.32) (2.77) (2.86) (3.09) (3.68) (5.00) (7.52)</cell></row><row><cell>PUL-down</cell><cell cols="12">(2.49) (2.56) (2.82) (3.43) (4.84) (7.69) (2.80) (2.89) (3.12) (3.74) (5.18) (8.26)</cell></row><row><cell>PDL w/o occ</cell><cell cols="12">(2.37) (2.42) (2.61) (3.15) (4.17) (6.31) (2.73) (2.81) (3.00) (3.59) (4.82) (7.16)</cell></row><row><cell>PDL</cell><cell cols="12">(2.33) (2.37) (2.56) (3.03) (3.88) (5.58) (2.63) (2.69) (2.87) (3.38) (4.43) (6.39)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised convolutional neural networks for motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnn-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2710" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3d scene flow estimation in autonomous driving scenarios?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">Abu</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2593" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High accuracy optical flow etimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5747" to="5756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of optical flow with deep feature similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woobin</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Eui</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kurt Konolige, and Anelia Angelova</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04902</idno>
	</analytic>
	<monogr>
		<title level="m">What matters in unsupervised optical flow</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Joint bilateral upsampling. ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep joint image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast guided global interpolation for depth and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of scene flow estimation fusing with local rigidity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6489" to="6498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ddflow:learning optical flow with unlabeled data distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8770" to="8777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selflow:self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4571" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Occinpflow: Occlusion-inpainting optical flow estimation by unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16637</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2720" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stflow: Self-taught optical flow estimation using pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9113" to="9124" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1495" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Epicflow: Edgepreserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixel-adaptive convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11166" to="11175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of optical flow with cnn-based non-local filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8429" to="8442" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast end-to-end trainable guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1838" to="1847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5807" to="5815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="794" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Back to basics:unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantions</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Maskflownet: Asymmetric feature matching with learnable occlusion mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised deep epipolar flow for stationary or dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12095" to="12104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="38" to="55" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
