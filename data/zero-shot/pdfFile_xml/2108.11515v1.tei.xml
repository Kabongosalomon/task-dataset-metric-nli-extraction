<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust High-Resolution Video Matting with Temporal Guidance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
							<email>linjie.yang@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
							<email>imran.saleemi@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust High-Resolution Video Matting with Temporal Guidance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Our method produces robust and coherent results on all videos without requirements for trimaps or pre-captured backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We introduce a robust, real-time, high-resolution human video matting method that achieves new state-of-theart performance. Our method is much lighter than previous approaches and can process 4K at 76 FPS and HD at 104 FPS on an Nvidia GTX 1080Ti GPU. Unlike most existing methods that perform video matting frame-by-frame as independent images, our method uses a recurrent architecture to exploit temporal information in videos and achieves significant improvements in temporal coherence and matting quality. Furthermore, we propose a novel training strategy that enforces our network on both matting and segmentation objectives. This significantly improves our model's robustness. Our method does not require any auxiliary inputs such as a trimap or a pre-captured background image, so it can be widely applied to existing human matting applications. Our code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Matting is the process of predicting the alpha matte and foreground color from an input frame. Formally, a frame I can be viewed as the linear combination of a foreground F and a background B through an ? coefficient:</p><formula xml:id="formula_0">I = ?F + (1 ? ?)B</formula><p>(1) By extracting ? and F , we can composite the foreground object to a new background, achieving the background replacement effect.</p><p>Background replacement has many practical applications. Many rising use cases, e.g. video conferencing and entertainment video creation, need real-time background replacement on human subjects without green-screen props. * Work performed during an internship at ByteDance. Neural models are used for this challenging problem but the current solutions are not always robust and often generate artifacts. Our research focuses on improving the matting quality and robustness for such applications.</p><p>Most existing methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">34]</ref>, despite being designed for video applications, process individual frames as independent images. Those approaches neglect the most widely available feature in videos: temporal information. Temporal information can improve video matting performance for many reasons. First, it allows the prediction of more coherent results, as the model can see multiple frames and its own predictions. This significantly reduces flicker and improves perceptual quality. Second, temporal information can improve matting robustness. In the cases where an individual frame might be ambiguous, e.g. the foreground color becomes similar to a passing object in the background, the model can better guess the boundary by referring to the previous frames. Third, temporal information allows the model to learn more about the background over time. When the camera moves, the background behind the subjects is revealed due to the perspective change. Even if the camera is fixed, the occluded background still often reveals due to the subject's movements. Having a better understanding of the background simplifies the matting task. Therefore, we propose a recurrent architecture to exploit the temporal information. Our method significantly improves the matting quality and temporal coherence. It can be applied to all videos without any requirements for auxiliary inputs, such as a manually annotated trimap or a pre-captured background image.</p><p>Furthermore, we propose a new training strategy to enforce our model on both matting and semantic segmentation objectives simultaneously. Most existing methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">34]</ref> are trained on synthetic matting datasets. The samples often look fake and prevent the network to generalize to real images. Previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> have attempted to initialize the model with weights trained on segmentation tasks, but the model still overfits to the synthetic distribution during the matting training. Others have attempted adversarial training <ref type="bibr" target="#b34">[34]</ref> or semi-supervised learning <ref type="bibr" target="#b17">[18]</ref> on unlabeled real images as an additional adaptation step. We argue that human matting tasks are closely related to human segmentation tasks. Simultaneously training with a segmentation objective can effectively regulate our model without additional adaptation steps.</p><p>Our method outperforms the previous state-of-the-art method while being much lighter and faster. Our model uses only 58% parameters and can process real-time highresolution videos at 4K 76 FPS and HD 104 FPS on an Nvidia GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Trimap-based matting. Classical (non-learning) algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">38]</ref> require a manual trimap annotation to solve for the unknown regions of the trimap. Such methods are reviewed in the survey by Wang and Cohen <ref type="bibr" target="#b42">[43]</ref>. Xu et al. <ref type="bibr" target="#b44">[45]</ref> first used a deep network for trimapbased matting and many recent research continued this approach. FBA <ref type="bibr" target="#b8">[9]</ref> is one of the latest. Trimap-based methods are often object agnostic (not limited to human). They are suitable for interactive photo-editing applications where the user can select target objects and provide manual guidance. To extend it to video, Sun et al. proposed DVM <ref type="bibr" target="#b39">[39]</ref>, which only requires a trimap on the first frame and can propagate it to the rest of the video.</p><p>Background-based matting. Soumyadip et al. proposed background matting (BGM) <ref type="bibr" target="#b34">[34]</ref>, which requires an additional pre-captured background image as input. This information acts as an implicit way for foreground selection and increases matting accuracy. Lin and Ryabtsev et al. further proposed BGMv2 <ref type="bibr" target="#b21">[22]</ref> with improved performance and a focus on real-time high-resolution. However, background matting cannot handle dynamic backgrounds and large camera movements.</p><p>Segmentation. Semantic segmentation is to predict a class label for every pixel, often without auxiliary inputs. Its binary segmentation mask can be used to locate the human subjects, but using it directly for background replacement will result in strong artifacts. Nonetheless, segmentation tasks are similar to matting tasks in an auxiliary-free setting, and the research in segmentation inspires our network design. DeepLabV3 <ref type="bibr" target="#b2">[3]</ref> proposed ASPP (Atrous Spatial Pyramid Pooling) module and used dilated convolution in its encoder to improve performance. This design has been adopted by many following works, including MobileNetV3 <ref type="bibr" target="#b14">[15]</ref>, which simplified ASPP to LR-ASPP.</p><p>Auxiliary-free matting. Fully automatic matting without any auxiliary inputs has also been studied. Methods like <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">46]</ref> work on any foreground objects but not as robust, while others like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b46">47]</ref> are trained specifically for human portraits. MODNet <ref type="bibr" target="#b17">[18]</ref> is the latest portrait matting method. In contrast, our method is trained to work well on the full human body.</p><p>Video matting. Very few neural matting methods are designed for videos natively. MODNet <ref type="bibr" target="#b17">[18]</ref> proposed a postprocessing trick that compares the prediction of neighboring frames to suppress flicker, but it cannot handle fastmoving body-parts and the model itself still operates on frames as independent images. BGM <ref type="bibr" target="#b34">[34]</ref> explored taking a few neighboring frames as additional input channels, but this only provides short-term temporal cues and its effects were not the focus of the study. DVM <ref type="bibr" target="#b44">[45]</ref> is video-native but focused on utilizing temporal information to propagate trimap annotations. On contrary, our method focuses on using temporal information to improve matting quality in an auxiliary-free setting.</p><p>Recurrent architecture. Recurrent neural network has been widely used for sequence tasks. Two of the most popular architectures are LSTM (Long Short-Term Memory) <ref type="bibr" target="#b12">[13]</ref> and GRU (Gated Recurrent Unit) <ref type="bibr" target="#b5">[6]</ref>, which have also being adopted to vision tasks as ConvLSTM <ref type="bibr" target="#b36">[36]</ref> and Con-vGRU <ref type="bibr" target="#b1">[2]</ref>. Previous works have explored using recurrent architectures for various video vision tasks and showed improved performance compared to the image-based counterparts <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>. Our work adopts recurrent architectures to the matting task.</p><p>High-resolution matting. Patch-based refinement has been explored by PointRend <ref type="bibr" target="#b18">[19]</ref> for segmentation and BGMv2 <ref type="bibr" target="#b21">[22]</ref> for matting. It only performs convolution on selective patches. Another approach is using Guided Filter <ref type="bibr" target="#b10">[11]</ref>, a post-processing filter that jointly upsamples the low-resolution prediction given the high-resolution frame as guidance. Deep Guided Filter (DGF) <ref type="bibr" target="#b43">[44]</ref> was proposed as a learnable module that can be trained with the network end-to-end without manual hyperparameters. Despite filterbased upsampling being less powerful, we choose it because it is faster and well supported by all inference frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Architecture</head><p>Our architecture consists of an encoder that extracts individual frame's features, a recurrent decoder that aggregates temporal information, and a Deep Guided Filter module for high-resolution upsampling. <ref type="figure">Figure 2</ref> shows our model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature-Extraction Encoder</head><p>Our encoder module follows the design of state-of-theart semantic segmentation networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>   <ref type="figure">Figure 2</ref>: Our network consists of a feature-extraction encoder, a recurrent decoder, and Deep Guided Filter (DGF) module. To process high-resolution videos, the input is first downsampled for the encoder-decoder network, then DGF is used to upsample the result.</p><p>to the matting task. We adopt MobileNetV3-Large <ref type="bibr" target="#b14">[15]</ref> as our efficient backbone followed by the LR-ASPP module as proposed by MobileNetV3 for semantic segmentation tasks. Noticeably, the last block of MobileNetV3 uses dilated convolutions without downsampling stride. The encoder module operates on individual frames and extracts features at 1 2 , </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Decoder</head><p>We decide to use a recurrent architecture instead of attention or simply feedforwarding multiple frames as additional input channels for several reasons. Recurrent mechanisms can learn what information to keep and forget by itself on a continuous stream of video, while the other two methods must rely on a fixed rule to remove old and insert new information to the limited memory pool on every set interval. The ability to adaptively keep both long-term and short-term temporal information makes recurrent mechanisms more suitable for our task.</p><p>Our decoder adopts ConvGRU at multiple scales to aggregate temporal information. We choose ConvGRU because it is more parameter efficient than ConvLSTM by having fewer gates. Formally, ConvGRU is defined as:</p><formula xml:id="formula_1">z t = ?(w zx * x t + w zh * h t?1 + b z ) r t = ?(w rx * x t + w rh * h t?1 + b r ) o t = tanh(w ox * x t + w oh * (r t h t?1 ) + b o ) h t = z t h t?1 + (1 ? z t ) o t (2)</formula><p>where operators * and represent convolution and element-wise product respectively; tanh and ? represent hyperbolic tangent and sigmoid function respectively. w and b are the convolution kernel and the bias term. The hidden state h t is used as both the output and the recurrent state to the next time step as h t?1 . The initial recurrent state h 0 is an all zero tensor. <ref type="figure">Figure 2</ref>, our decoder consists of a bottleneck block, upsampling blocks, and an output block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Bottleneck block operates at the <ref type="bibr">1 16</ref> feature scale after the LR-ASPP module. A ConvGRU layer is operated on only half of the channels by split and concatenation. This significantly reduces parameters and computation since ConvGRU is computationally expansive.</p><p>Upsampling block is repeated at 1 8 , <ref type="bibr">1 4</ref> , and 1 2 scale. First, it concatenates the bilinearly upsampled output from the previous block, the feature map of the corresponding scale from the encoder, and the input image downsampled by repeated 2 ? 2 average pooling. Then, a convolution followed by Batch Normalization <ref type="bibr" target="#b15">[16]</ref> and ReLU <ref type="bibr" target="#b25">[26]</ref> activation is applied to perform feature merging and channel reduction. Finally, a ConvGRU is applied to half of the channels by split and concatenation.</p><p>Output block does not use ConvGRU because we find it expansive and not impactful at this scale. The block only uses regular convolutions to refine the results. It first concatenates the input image and the bilinearly upsampled output from the previous block. Then it employs 2 repeated convolution, Batch Normalization, and ReLU stacks to produce the final hidden features. Finally, the features are projected to outputs, including 1-channel alpha prediction, 3channel foreground prediction, and 1-channel segmentation prediction. The segmentation output is used for the segmentation training objective as later described in Section 4.</p><p>We find applying ConvGRU on half of the channels by split and concatenation effective and efficient. This design helps ConvGRU to focus on aggregating temporal information, while the other split branch forwards the spatial features specific to the current frame. All convolutions use 3 ? 3 kernels, except the last projection uses a 1 ? 1 kernel.</p><p>We modify our network such that it can be given T frames at once as input and each layer processes all T frames before passing to the next layer. During training, this allows Batch Normalization to compute statistics across both batch and time to ensure the normalization is consistent. During inference, T = 1 can be used to process live videos and T &gt; 1 can be used to exploit more GPU parallelism from the non-recurrent layers as a form of batching if the frames are allowed to be buffered. Our recurrent decoder is uni-directional so it can be used for both live streaming and post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Guided Filter Module</head><p>We adopt Deep Guided Filter (DGF) as proposed in <ref type="bibr" target="#b43">[44]</ref> for high-resolution prediction. When processing highresolution videos such as 4K and HD, we downsample the input frame by a factor s before passing through the encoder-decoder network. Then the low-resolution alpha, foreground, final hidden features, as well as the highresolution input frame are provided to the DGF module to produce high-resolution alpha and foreground. The entire network is trained end-to-end as described in Section 4. Note that the DGF module is optional and the encoderdecoder network can operate standalone if the video to be processed is low in resolution.</p><p>Our entire network does not use any special operators and can be deployed to most existing inference frameworks. More architectural details are in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training</head><p>We propose to train our network with both matting and semantic segmentation objectives simultaneously for several reasons:</p><p>First, human matting tasks are closely related to human segmentation tasks. Unlike trimap-based and backgroundbased matting methods that are given additional cues as inputs, our network must learn to semantically understand the scene and be robust in locating the human subjects.</p><p>Second, most existing matting datasets only provide ground-truth alpha and foreground that must be synthetically composited to background images. The compositions sometimes look fake due to the foreground and the background having different lighting. On the other hand, semantic segmentation datasets feature real images where the human subjects are included in all types of complex scenes. Training with semantic segmentation datasets prevents our model from overfitting to the synthetic distribution.</p><p>Third, there is a lot more training data available for semantic segmentation tasks. We harvest a variety of publically available datasets, both video-based and image-based, to train a robust model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Matting Datasets</head><p>Our model is trained on VideoMatte240K (VM) <ref type="bibr" target="#b21">[22]</ref>, Distinctions-646 (D646) <ref type="bibr" target="#b29">[30]</ref>, and Adobe Image Matting (AIM) <ref type="bibr" target="#b44">[45]</ref> datasets. VM provides 484 4K/HD video clips.</p><p>We divide the dataset into 475/4/5 clips for train/val/test splits. D646 and AIM are image matting datasets. We use only images of humans and combine them to form 420/15 train/val splits for training. For evaluation, D646 and AIM each provide 11 and 10 test images respectively.</p><p>For backgrounds, the dataset by <ref type="bibr" target="#b39">[39]</ref> provides HD background videos that are suitable for matting composition. The videos include a variety of motions, such as cars passing, leaves shaking, and camera movements. We select 3118 clips that does not contain humans and extract the first 100 frames from every clip. We also crawl 8000 image backgrounds following the approach of <ref type="bibr" target="#b21">[22]</ref>. The images have more indoor scenes such as offices and living rooms.</p><p>We apply motion and temporal augmentations on both foreground and background to increase data variety. Motion augmentations include affine translation, scale, rotation, sheer, brightness, saturation, contrast, hue, noise and blur that change continuously over time. The motion is applied with different easing functions such that the changes are not always linear. The augmentation also adds artificial motions to the image datasets. Additionally, we apply temporal augmentation on videos, including clip reversal, speed changes, random pausing, and frame skipping. Other discrete augmentations i.e. horizontal flip, grayscale, and sharpening, are applied consistently to all frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Segmentation Datasets</head><p>We use video segmentation dataset YouTubeVIS and select 2985 clips containing humans. We also use image segmentation datasets COCO <ref type="bibr" target="#b22">[23]</ref> and SPD <ref type="bibr">[40]</ref>. COCO provides 64,111 images containing humans while SPD provides additional 5711 samples. We apply similar augmentations but without motion, since YouTubeVIS already contains large camera movements and the image segmentation datasets do not require motion augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Procedures</head><p>Our matting training is pipelined into four stages. They are designed to let our network progressively see longer sequences and higher resolutions to save training time. We use Adam optimizer for training. All stages use batch size B = 4 split across 4 Nvidia V100 32G GPUs.</p><p>Stage 1: We first train on VM at low-resolution without the DGF module for 15 epochs. We set a short sequence length T = 15 frames so that the network can get updated quicker. The MobileNetV3 backbone is initialized with pretrained ImageNet <ref type="bibr" target="#b31">[32]</ref> weights and uses 1e ?4 learning rate, while the rest of the network uses 2e ?4 . We sample the height and width of the input resolution h, w independently between 256 and 512 pixels. This makes our network robust to different resolutions and aspect ratios.</p><p>Stage 2: We increase T to 50 frames, reduce the learning rate by half, and keep other settings from stage 1 to train our model for 2 more epochs. This allows our network to see longer sequences and learn long-term dependencies. T = 50 is the longest we can fit on our GPUs.</p><p>Stage 3: We attach the DGF module and train on VM with high-resolution samples for 1 epoch. Since high resolution consumes more GPU memory, the sequence length must be set to very short. To avoid our recurrent network overfitting to very short sequences, we train our network on both low-resolution long sequences and high-resolution short sequences. Specifically, the low-resolution pass does not employ DGF and has T = 40 and h, w ? (256, 512). The high-resolution pass entails the low-resolution pass and employs DGF with downsample factor s = 0.25,T = 6 and?,? ? (1024, 2048). We set the learning rate of DGF to 2e ?4 and the rest of the network to 1e ?5 .</p><p>Stage 4: We train on the combined dataset of D646 and AIM for 5 epochs. We increase the decoder learning rate to 5e ?5 to let our network adapt and keep other settings from stage 3.</p><p>Segmentation: Our segmentation training is interleaved between every matting training iteration. We train the network on image segmentation data after every odd iteration, and on video segmentation data after every even ones. Segmentation training is applied to all stages. For video segmentation data, we use the same B, T , h, w settings following every matting stage. For image segmentation data, we treat them as video sequences of only 1 frame, so T = 1. This gives us room to apply a larger batch size B = B ? T . Since the images are feedforwarded as the first frame, it forces the segmentation to be robust even in the absence of recurrent information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Losses</head><p>We apply losses on all t ? [1, T ] frames. To learn alpha ? t w.r.t. ground-truth ? * t , we use L1 loss L ? l1 and pyramid Laplacian loss L ? lap , as reported by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> to produce the best result. We also apply a temporal coherence loss L ? tc , as used by <ref type="bibr" target="#b39">[39]</ref>, to reduce flicker:</p><formula xml:id="formula_2">L ? l1 = ||? t ? ? * t || 1<label>(3)</label></formula><formula xml:id="formula_3">L ? lap = 5 s=1 2 s?1 5 ||L s pyr (? t ) ? L s pyr (? * t )|| 1 (4) L ? tc = || d? t dt ? d? * t dt || 2<label>(5)</label></formula><p>To learn foreground F t w.r.t. ground-truth F * t , We compute L1 loss L F l1 and temporal coherence loss L F tc on pixels where ? * t &gt; 0 following the approach of <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_4">L F l1 = ||(a * t &gt; 0) * (F t ? F * t )|| 1<label>(6)</label></formula><formula xml:id="formula_5">L F tc = ||(a * t &gt; 0) * ( dF t dt ? dF * t dt )|| 2<label>(7)</label></formula><p>The total matting loss L M is:</p><formula xml:id="formula_6">L M = L ? l1 + L ? lap + 5L ? tc + L F l1 + 5L F tc<label>(8)</label></formula><p>For semantic segmentation, our network is only trained on the human category. To learn the segmentation probability S t w.r.t. the ground-truth binary label S * t , we compute binary cross entropy loss:</p><formula xml:id="formula_7">L S = S * t (? log(S t )) + (1 ? S * t )(? log(1 ? S t )) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation on Composition Datasets</head><p>We construct our benchmark by compositing each test sample from VM, D646, and AIM datasets onto 5 video and 5 image backgrounds. Every test clip has 100 frames. Image samples are applied with motion augmentation.</p><p>We compare our approach against state-of-the-art trimap-based method (FBA <ref type="bibr" target="#b8">[9]</ref>), background-based method (BGMv2 <ref type="bibr" target="#b21">[22]</ref> with MobileNetV2 <ref type="bibr" target="#b33">[33]</ref> backbone), and auxiliary-free method (MODNet <ref type="bibr" target="#b17">[18]</ref>). To fairly compare them for fully automatic matting, FBA uses synthetic trimaps generated by dilation and erosion of semantic segmentation method DeepLabV3 <ref type="bibr" target="#b2">[3]</ref> with ResNet101 <ref type="bibr" target="#b11">[12]</ref> backbone; BGMv2 only sees the first frame's ground-truth background; MODNet applies its neighbor frame smoothing trick. We attempt to re-train MODNet on our data but get worse results, potentially due to issues during the training, so MODNet uses its official weights; BGMv2 is already trained on all three datasets; FBA has not released the training code at the time of writing.</p><p>We evaluate ? w.r.t. ground-truth ? * using MAD (mean absolute difference), MSE (mean squared error), Grad (spatial gradient) <ref type="bibr" target="#b30">[31]</ref>, and Conn (connectivity) <ref type="bibr" target="#b30">[31]</ref> for quality, and adopt dtSSD <ref type="bibr" target="#b7">[8]</ref> for temporal coherence. For F , we only measure pixels where ? * &gt; 0 by MSE. MAD and MSE are scaled by 1e 3 and dtSSD is scaled by 1e 2 for better readability. F is not measured on VM since it contains noisy ground-truth. MODNet does not predict F , so we evaluate on the input frame as its foreground prediction. This simulates directly applying the alpha matte on the input. <ref type="table">Table 1</ref> compares methods using low-resolution input. Our method does not use DGF in this scenario. Ours predicts more accurate and consistent alpha across all datasets. In particular, FBA is limited by the inaccurate synthetic trimap. BGMv2 performs poorly for dynamic backgrounds. MODNet produces less accurate and coherent results than ours. For foreground prediction, ours is behind BGMv2 but outperforms FBA and MODNet.  expansive to compute at high-resolution. Our method outperforms MODNet on all metrics. <ref type="figure">Figure 3</ref> shows qualitative comparisons on real-videos. In <ref type="figure">Figure 3a</ref>, we compare alpha predictions across all methods and find ours predicts fine-grained details like hair strands more accurately. In <ref type="figure">Figure 3b</ref>, we experiment on random YouTube videos. We remove BGMv2 from the comparison since these videos do not have pre-captured backgrounds. We find our method is much more robust to semantic errors. In <ref type="figure">Figures 3c and 3d</ref>, we further compare real-time matting against MODNet on cellphone and webcam videos. Our method can handle fast-moving body parts better than MODNet. <ref type="table" target="#tab_5">Tables 3 and 4</ref> show that our method is significantly lighter, with only 58% parameters compared to MODNet. Ours is the fastest on HD (1920 ? 1080), but a little slower than BGMv2 on 512 ? 288 and MODNet with FGF on 4K (3840 ? 2160). Our inspection finds that DGF and FGF incur very minor differences in performance. Our method is slower than MODNet in 4K because ours predicts foreground in addition to alpha, so it is slower to process 3 extra channels in high resolution. We use <ref type="bibr" target="#b37">[37]</ref> to measure GMACs (multiply-accumulate operations), but it only measures convolutions and misses out resize and many tensor operations which are used most in DGF and FGF, so GMACs is only a rough approximation. Our method achieves HD 104 FPS and 4K 76 FPS, which is considered real-time for many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on Real Videos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Size and Speed Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Parameters <ref type="formula">(</ref>    smoothing trick, has large fluctuations in the metric. We also experiment with disabling recurrence in our network by passing zero tensors as the recurrent states. The quality and consistency worsen as expected. This proves that temporal information improves quality and consistency. <ref type="figure">Figure 5</ref> compares temporal coherence with MODNet on a video sample. Our method produces consistent results on the handrail region while MODNet produces flicker, which significantly degrade perceptual quality. Please see our supplementary for more results.</p><p>We further examine the recurrent hidden state. In <ref type="figure">Figure 6</ref>, we find that our network has automatically learned to reconstruct the background as it is revealed over time and keep this information in its recurrent channels to help future predictions. It also uses other recurrent channels to keep track of motion history. Our method even attempts to reconstruct the background when the videos contain camera movements and is capable of forgetting useless memory on shot cuts. More examples are in the supplementary. <ref type="table" target="#tab_8">Table 5</ref> shows that our method is as robust as the semantic segmentation methods when evaluated on the subset of COCO validation images that contain humans and only on the human category. Our method achieves 61.50 mIOU, which is reasonably in between the performance of MobileNetV3 and DeepLabV3 trained on COCO considering the difference in model size. We also try to evaluate the robustness of our alpha output by thresholding ? &gt; 0.5 as the binary mask and our method still achieves 60.88 mIOU, showing that the alpha prediction is also robust. For comparison, we train a separate model by initializing our    <ref type="table" target="#tab_10">Table 6</ref> shows that DGF has only a small overhead in size and speed compared to FGF. DGF has a better Grad metric, indicating its high-resolution details are more accurate. DGF also produces more coherent results indicated by the dtSSD metric, likely because it takes hidden features from the recurrent decoder into consideration. The MAD and MSE metrics are inconclusive because they are dominated by segmentation-level errors, which are not corrected by either DGF or FGF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Role of Segmentation Training Objective</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Role of Deep Guided Filter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Params FPS MAD MSE Grad dtSSD   <ref type="table" target="#tab_12">Table 7</ref> compares the performance on static and dynamic backgrounds. Dynamic backgrounds include both background object movements and camera movements. Our method can handle both cases and performs slightly better on static backgrounds, likely because it is easier to reconstruct pixel-aligned backgrounds as shown in <ref type="figure">Figure 6</ref>. On the other hand, BGMv2 performs badly on dynamic backgrounds and MODNet does not exhibit any preference. In metric, BGMv2 outperforms ours on static backgrounds, but it is expected to do worse in reality when the precaptured background has misalignment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Static vs. Dynamic Backgrounds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Larger Model for Extra Performance</head><p>We experiment with switching the backbone to ResNet50 <ref type="bibr" target="#b11">[12]</ref> and increasing the decoder channels. <ref type="table" target="#tab_14">Table  8</ref> shows the performance improvement. The large model is more suitable for server-side applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Params  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Limitations</head><p>Our method prefers videos with clear target subjects. When there are people in the background, the subjects of interest become ambiguous. It also favors simpler backgrounds to produce more accurate matting. <ref type="figure" target="#fig_2">Figure 7</ref> shows examples of challenging cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have proposed a recurrent architecture for robust human video matting. Our method achieves new state-of-theart while being lighter and faster. Our analysis shows that temporal information plays an important role in improving the quality and consistency. We also introduce a new training strategy to train our model on both matting and semantic segmentation objectives. This approach effectively enforces our model to be robust on various types of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>We provide additional details in this supplementary. In Section B, we describe the details of our network architecture. In Section C, we explain the details on training. In Section D, we show examples of our composited matting data samples. In Section E, we show additional results from our method. We also attach video results in the supplementary. Please see our videos for better visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network</head><p>Backbone E  <ref type="table" target="#tab_16">Table 9</ref>: Feature channels at different scale. E k and D k denote encoder and decoder channels at k feature scale respectively. AS denotes LR-ASPP channels.  <ref type="bibr" target="#b0">1</ref> 32 scale, we modify the last block to use convolutions with a dilation rate of 2 and a stride of 1 following the design of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>. The last feature map E 1 16 is given to the LR-ASPP module, which compresses it to AS channels.</p><p>Decoder: All ConvGRU layers operate on half of the channels by split and concatenation, so the recurrent hidden state has D k 2 channels at scale k. For the upsampling blocks, the convolution, Batch Normalization, and ReLU stack compresses the concatenated features to D k channels before splitting to ConvGRU. For the output block, the first two convolutions have 16 filters and the final hidden features has 16 channels. The final projection convolution outputs 5 channels, including 3-channel foreground, 1-channel alpha, and 1-channel segmentation predictions. All convolutions uses 3 ? 3 kernels except the last projection uses a 1 ? 1 kernel. The average poolings use 2 ? 2 kernels with a stride of 2.</p><p>Deep Guided Filter: DGF contains a few 1 ? 1 convolutions internally. We modify it to take the predicted foreground, alpha, and the final hidden features as inputs. All internal convolutions use 16 filters. Please refer to <ref type="bibr" target="#b43">[44]</ref> for more specifications.</p><p>Our entire network is built and trained in PyTorch <ref type="bibr" target="#b26">[27]</ref>. We clamp the alpha and foreground prediction outputs to [0, 1] range without activation functions following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. The clamp is done during both training and inference. The segmentation prediction output is sigmoid logits. Our network is trained using 4 Nvidia V100 32G GPUs. We use mixed precision training <ref type="bibr" target="#b24">[25]</ref> to reduce the GPU memory consumption. The training takes approximately 18, 2, 8, and 14 hours in each stage respectively.  <ref type="figure">Figure 9</ref> shows examples of the composited testing samples. The testing samples only apply motion augmentation on image foreground and backgrounds. The motion augmentation only consists of affine transforms. The strength of the augmentation is also weaker compared to the training augmentation to make testing samples as realistic looking as possible. <ref type="figure">Figure 9</ref>: Example testing samples. The augmentation is only applied on image foreground and background. The augmentation strength is weaker to make samples look more realistic. <ref type="figure" target="#fig_3">Figure 10</ref> shows additional qualitative comparisons with MODNet. Our method is consistently more robust. <ref type="figure" target="#fig_3">Figure  11</ref> compares temporal coherence with MODNet. MODNet has flicker on low-confidence regions whereas our results are coherent. <ref type="figure" target="#fig_3">Figure 12</ref> shows additional examples of our model's recurrent hidden state. It shows that our model has learned to store useful temporal information in its recurrent state and is capable of forgetting useless information upon shot cuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>MODNet Input Ours MODNet <ref type="figure" target="#fig_3">Figure 10</ref>: More qualitative comparison with MODNet.  <ref type="figure" target="#fig_3">Figure 11</ref>: Temporal coherence comparison. Our result is temporally coherent, whereas MODNet produces flicker around the handrail. This is because MODNet processes every frame as indepdendent images, so its matting decision is not consistent.  <ref type="figure" target="#fig_3">Figure 12</ref>: More examples of the recurrent hidden states. The first example with the static background clearly shows our model reconstructs the occluded background region over time. The second example with a handheld camera shows that our model still attempts to reconstruct the background, and it has learned to forget useless recurrent states on shot cuts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 Figure 3 :</head><label>43</label><figDesc>shows the change of average alpha MAD metric across all VM test clips over time. The error of our model drops significantly in the first 15 frames then the metric stays stable. MODNet, even with its neighbor frame Qualitative comparison. Our method produces more detailed alpha compared to others. When evaluating on YouTube, cellphone, and webcam videos, ours is consistently more robust than others. See supplementary for more results. YouTube videos are crawled from the Internet; cellphone videos are from a public dataset<ref type="bibr" target="#b16">[17]</ref>; Some webcam examples are recorded while others are taken from<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Average alpha MAD over time on VM without DGF. Our metric improves over time and is stable, showing that temporal information improves quality and consistency. Temporal coherence comparison. MODNet's result has flicker on the handrail while ours is consistent.MobileNetV3 encoder and LR-ASPP module with the pretrained weights on COCO and removing the segmentation objective. The model overfits to the synthetic matting data and regresses significantly on COCO performance, achieving only 38.24 mIOU. Two example channels in the recurrent hidden state. Our network learns to to reconstruct the background over time and keeps track of the motion history in its recurrent state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Challenging cases. People in the background make matting target ambiguous. Complex scenes make matting harder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Training Procedures for stage ? [1, 2, 3, 4] do for epoch do for iteration do LowResMattingPass(B, T, h, w) if stage ? [3, 4] then HighResMattingPass(B,T ,?,?) if iteration % 2 = 0 then VideoSegmentationPass(B, T, h, w) else ImageSegmentationPass(B , 1, h, w) Algorithm 1 shows the training loop of our proposed training strategy. The sequence length parameters T ,T are set according to the stages, which is specified in our main text; the batch size parameters are set to B = 4, and B = B ? T ; The input resolutions are randomly sampled as h, w ? U nif orm(256, 512) and?,? ? U nif orm(1024, 2048).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 Figure 8 :</head><label>88</label><figDesc>shows examples of composited training samples from the matting datasets. The clips contain natural movements when compositing with videos as well as artificial movements generated by the motion augmentation.Composited FramesStd Dev Composited training samples. Last column shows the standard deviation of each pixel across time to visualize motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Video with a handheld camera and cut shots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>further compares our method with MODNet on high-resolution. Since DGF must be trained end-to-end with the network, we modify MODNet to use non-learned Fast Guided Filter (FGF) to upsample the prediction. Both methods use downsample scale s = 0.25 for the encoderdecoder network. We remove Conn metric because it is too</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Alpha</cell><cell>FG</cell></row><row><cell cols="2">Dataset Method</cell><cell cols="2">MAD MSE Grad Conn dtSSD MSE</cell></row><row><cell></cell><cell cols="3">DeepLabV3 14.47 9.67 8.55 1.69 5.18</cell></row><row><cell></cell><cell>FBA</cell><cell cols="2">8.36 3.37 2.09 0.75 2.09</cell></row><row><cell>VM</cell><cell>BGMv2</cell><cell cols="2">25.19 19.63 2.28 3.26 2.74</cell></row><row><cell>512?288</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MODNet</cell><cell cols="2">9.41 4.30 1.89 0.81 2.23</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">6.08 1.47 0.88 0.41 1.36</cell></row><row><cell></cell><cell cols="3">DeepLabV3 24.50 20.1 20.30 6.41 4.51</cell></row><row><cell></cell><cell>FBA</cell><cell cols="2">17.98 13.40 7.74 4.65 2.36 5.84</cell></row><row><cell>D646</cell><cell>BGMv2</cell><cell cols="2">43.62 38.84 5.41 11.32 3.08 2.60</cell></row><row><cell>512?512</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">MODNet 10.62 5.71 3.35 2.45 1.57 6.31</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">7.28 3.01 2.81 1.83 1.01 2.93</cell></row><row><cell></cell><cell cols="3">DeepLabV3 29.64 23.78 20.17 7.71 4.32</cell></row><row><cell></cell><cell>FBA</cell><cell cols="2">23.45 17.66 9.05 6.05 2.29 6.32</cell></row><row><cell>AIM</cell><cell>BGMv2</cell><cell cols="2">44.61 39.08 5.54 11.60 2.69 3.31</cell></row><row><cell>512?512</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">MODNet 21.66 14.27 5.37 5.23 1.76 9.51</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">14.84 8.93 4.35 3.83 1.01 5.01</cell></row><row><cell cols="2">Dataset Method</cell><cell></cell><cell>SAD MSE Grad dtSSD</cell></row><row><cell>VM</cell><cell cols="2">MODNet + FGF</cell><cell>11.13 5,54 15.30 3.08</cell></row><row><cell>1920?1080</cell><cell>Ours</cell><cell></cell><cell>6.57 1.93 10.55 1.90</cell></row><row><cell>D646</cell><cell cols="2">MODNet + FGF</cell><cell>11.27 6.13 30.78 2.19</cell></row><row><cell>2048?2048</cell><cell>Ours</cell><cell></cell><cell>8.67 4.28 30.06 1.64</cell></row><row><cell>AIM</cell><cell cols="2">MODNet + FGF</cell><cell>17.29 10.10 35.52 2.60</cell></row><row><cell>2048?2048</cell><cell>Ours</cell><cell></cell><cell>14.89 9.01 34.97 1.71</cell></row></table><note>Table 1: Low-resolution comparison. Our alpha prediction is bet- ter than all others. Our foreground prediction is behind BGMv2 but outperforms FBA and MODNet. Note that FBA uses synthetic trimap from DeepLabV3; BGMv2 only sees ground-truth back- ground from the first frame; MODNet does not predict foreground so it is evaluated on the input image.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>High-resolution alpha comparison. Ours is better than MODNet with Fast Guided Filter (FGF).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ours is lighter than all compared methods. Size is measured on FP32 weights.</figDesc><table><row><cell>Resolution</cell><cell cols="2">s Method</cell><cell cols="2">FPS GMACs  *</cell></row><row><cell></cell><cell></cell><cell>DeepLabV3 + FBA</cell><cell cols="2">12.3 205.77</cell></row><row><cell>512?288</cell><cell>1</cell><cell>BGMv2 MODNet</cell><cell>152.5 104.9</cell><cell>8.46 8.80</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>131.9</cell><cell>4.57</cell></row><row><cell></cell><cell></cell><cell>BGMv2</cell><cell>70.6</cell><cell>9.86</cell></row><row><cell cols="2">1920?1080 0.25</cell><cell>MODNet + FGF</cell><cell>100.3</cell><cell>7.78</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>104.2</cell><cell>4.15</cell></row><row><cell></cell><cell></cell><cell>BGMv2</cell><cell>26.5</cell><cell>17.04</cell></row><row><cell cols="2">3840?2160 0.125</cell><cell>MODNet + FGF</cell><cell>88.6</cell><cell>7.78</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>76.5</cell><cell>4.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Model performance comparison. s denotes the downsample scale. Models are converted to TorchScript and optimized before testing (BatchNorm fusion etc.). FPS is measured as FP32 tensor throughput on an Nvidia GTX 1080Ti GPU. GMACs is a rough approximation.</figDesc><table><row><cell>6. Ablation Studies</cell></row><row><cell>6.1. Role of Temporal Information</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Segmentation performance on COCO validation set.Training with segmentation objective makes our method robust while training only with pre-trained weights regresses.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Comparing switching DGF to FGF on D646. Parameters are measured in millions. FPS is measured in HD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Comparing VM samples on static and dynamic back-</cell></row><row><cell>grounds. Ours does better on static backgrounds but can handle</cell></row><row><cell>both cases. Note that BGMv2 receives ground-truth static back-</cell></row><row><cell>grounds, but in reality the backgrounds have misalignment.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Large model uses ResNet50 backbone and has more decoder channels. Evaluated on VM in HD. Size is measured in MB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>describes our network and its variants with fea-</cell></row><row><cell>ture channels. Our default network uses MobileNetV3-</cell></row><row><cell>Large [15] backbone while the large variant uses ResNet50</cell></row><row><cell>[12] backbone.</cell></row><row><cell>Encoder: The encoder backbone operates on individ-</cell></row><row><cell>ual frames and extracts feature maps of E k channels at</cell></row><row><cell>k ? [ 1 2 , 1 4 , 1 8 , 1 16 ] scales. Unlike regular MobileNetV3 and</cell></row><row><cell>ResNet backbones that continue to operate at</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tunc Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1511.06432</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knn matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptually motivated benchmark for video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vatolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Piti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">F</forename><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning high fidelity depths of dressed humans by watching social media dance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Jafarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="12753" to="12762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Is a green screen really necessary for real-time portrait matting? ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9796" to="9805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<title level="m">Spectral matting. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time high-resolution background matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ryabtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d corpus of spontaneous complex mental states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwa</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurel</forename><surname>Riek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Affective Computing and Intelligent Interaction</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic segmentation of video sequences with convolutional lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karina</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13676" to="13685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Background matting: The world is your green screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Vladislav Sovrasov. flops-counter</title>
		<imprint/>
	</monogr>
	<note>pytorch</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poisson matting. In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep video matting via spatio-temporal alignment and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alahari</forename><surname>Karteek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marqu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gir? I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5272" to="5281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image and video matting: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="175" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fast end-to-end trainable guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7469" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast deep matting for portrait animation on mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
