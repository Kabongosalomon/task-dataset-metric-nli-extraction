<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dialogue State Tracking with a Language Model using Schema-Driven Prompting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Hsuan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
							<email>ostendor@uw.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dialogue State Tracking with a Language Model using Schema-Driven Prompting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Task-oriented conversational systems often use dialogue state tracking to represent the user's intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of indomain knowledge. Our purely generative system achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks: Multi-WOZ 2.1 and M2M. The data and code will be available at https://github.com/ chiahsuan156/DST-as-Prompting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In task-oriented dialogues, systems communicate with users through natural language to accomplish a wide range of tasks, such as food ordering, tech support, restaurant/hotel/travel booking, etc. The backbone module of a typical system is dialogue state tracking (DST), where the user goal is inferred from the dialogue history <ref type="bibr" target="#b13">(Henderson et al., 2014;</ref><ref type="bibr" target="#b32">Shah et al., 2018;</ref><ref type="bibr" target="#b4">Budzianowski et al., 2018)</ref>. User goals are represented in terms of values of pre-defined slots associated with a schema determined by the information needed to execute task-specific queries to the backend. In other words, user goals are extracted progressively via slot filling based on the schema throughout the conversation. In this paper, we focus on multi-domain DST where the dialogue state is encoded as a list of triplets in the form of (domain, slot, value), e.g. <ref type="bibr">("restaurant", "area", "centre")</ref>.</p><p>There are two broad paradigms of DST models, classification-based and generation-based models, where the major difference is how the slot value is inferred. In classification-based models <ref type="bibr" target="#b36">(Ye et al., 2021;</ref>, the prediction of a slot value is restricted to a fixed set for each slot, and non-categorical slots are constrained to values observed in the training data. In contrast, generationbased models  decode slot values sequentially (token by token) based on the dialogue context, with the potential of recovering unseen values. Recently, generationbased DST built on large-scale pretrained neural language models (LM) achieve strong results without relying on domain-specific modules. Among them, the autoregressive model <ref type="bibr">(Peng et al., 2020a;</ref><ref type="bibr">Hosseini-Asl et al., 2020)</ref> uses a uni-directional encoder whereas the sequence-to-sequence model <ref type="bibr" target="#b18">(Lin et al., 2020a;</ref><ref type="bibr" target="#b11">Heck et al., 2020)</ref> represents the dialogue context using a bi-directional encoder.</p><p>In this study, we follow a generation-based DST approach using a pre-trained sequence-to-sequence model, but with the new strategy of adding taskspecific prompts as input for sequence-to-sequence DST models, inspired by prompt-based fine-tuning <ref type="bibr" target="#b25">(Radford et al., 2019;</ref><ref type="bibr">Brown et al., 2020a)</ref>. Specifically, instead of generating domain and slot symbols in the decoder, we concatenate the dialogue context with domain and slot prompts as input to the encoder, where prompts are taken directly from the schema. We hypothesize that jointly encoding dialogue context and schema-specific textual information can further benefit a sequence-to-sequence DST model. This allows task-aware contextualization for more effectively guiding the decoder to generate slot values.</p><p>Although the domain and slot names typically have interpretable components, they often do not reflect standard written English, e.g. "arriveby" and "ref ". Those custom meaning representations are typically abbreviated and/or under-specified, which creates a barrier for effectively utilizing the pretrained LMs. To address this issue, we further incorporate natural language schema descriptions into prompting for DST, which include useful information to guide the decoder. For example, the description of "ref " is "reference number of the hotel booking"; the values of "has_internet" are "yes", "no", "free", and "don't care".</p><p>In short, this work advances generation-based DST in two ways. First, candidate schema labels are jointly encoded with the dialogue context, providing a task-aware contextualization for initializing the decoder. Second, natural language descriptions of schema categories associated with database documentation are incorporated in encoding as prompts to the language model, allowing uniform handling of categorical and non-categorical slots. When implemented using a strong pretrained text-to-text model, this simple approach achieves state-of-theart (SOTA) results on MultiWOZ 2.2, and performance is on par with SOTA on MultiWOZ 2.1 and M2M. In addition, our analyses provide empirical results that contribute towards understanding how schema description augmentation can effectively constrain the model prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Domain Dialogue State Tracking</head><p>Task-oriented dialogue datasets <ref type="bibr" target="#b32">(Shah et al., 2018;</ref><ref type="bibr" target="#b13">Henderson et al., 2014)</ref>, have spurred the development of dialogue systems <ref type="bibr" target="#b41">(Zhong et al., 2018;</ref><ref type="bibr" target="#b5">Chao and Lane, 2019)</ref>. Recently, to further examine the generalization abilities, large scale cross-domain datasets have been proposed <ref type="bibr" target="#b4">(Budzianowski et al., 2018;</ref><ref type="bibr" target="#b38">Zang et al., 2020;</ref><ref type="bibr" target="#b8">Eric et al., 2019;</ref><ref type="bibr" target="#b30">Rastogi et al., 2020b)</ref>. Classification-based models <ref type="bibr" target="#b36">(Ye et al., 2021;</ref> pick the candidate from the oracle list of possible slot values. The assumption of the full access of the schema makes them have limited generalization abilities. On the other hand, generation-based models <ref type="bibr" target="#b18">Lin et al., 2020a)</ref> directly generate slot values token by token, making it possible to handle unseen domains and values. Most of these models require task-specific modular designs.</p><p>Recently, generation-based models that are built on large-scale autoregressive pretrained language models <ref type="bibr" target="#b10">(Ham et al., 2020;</ref><ref type="bibr">Hosseini-Asl et al., 2020;</ref><ref type="bibr">Peng et al., 2020a)</ref> achieve promising state tracking results on MultiWOZ 2.0 and 2.1 when trained on additional supervision signals or dialogue corpus. Both <ref type="bibr" target="#b10">Ham et al. (2020)</ref> and <ref type="bibr">Hosseini-Asl et al. (2020)</ref> require dialogue acts as inputs. Both <ref type="bibr">Hosseini-Asl et al. (2020)</ref> and <ref type="bibr">Peng et al. (2020a)</ref> require DB search results as inputs. <ref type="bibr">Peng et al. (2020a)</ref> also leverages other dialogue corpora to finetune the language model. Our work requires only the dialogue state labels and does not utilize any external dialogue datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Models</head><p>Large-scale pretrained language models have obtained state-of-the-art performance on diverse generation and understanding tasks including bidirectional encoder style language models <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2019)</ref>, auto-regressive language models <ref type="bibr" target="#b25">(Radford et al., 2019;</ref><ref type="bibr">Brown et al., 2020b)</ref> and more flexible sequence-to-sequence language models <ref type="bibr" target="#b26">(Raffel et al., 2020)</ref>. To adapt to dialogue tasks, variants of systems are finetuned on different dialogue corpora including chit-chat systems <ref type="bibr" target="#b0">Adiwardana et al., 2020;</ref><ref type="bibr" target="#b31">Roller et al., 2020)</ref> and task-oriented dialogue systems <ref type="bibr" target="#b22">(Mehri et al., 2019;</ref><ref type="bibr" target="#b34">Wu et al., 2020;</ref><ref type="bibr" target="#b12">Henderson et al., 2020;</ref><ref type="bibr" target="#b24">Peng et al., 2020b)</ref>. We leave it as future work to leverage domain-adapted language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prompting Language Models</head><p>Extending a language model's knowledge via prompts is an active line of research. <ref type="bibr" target="#b25">Radford et al. (2019)</ref> obtain empirical success by using prompts to guide zero shot generation without finetuning on any prompts. <ref type="bibr" target="#b26">Raffel et al. (2020)</ref> uses task-specific prompts in both finetuning and testing phase. Recent studies have also tried to automatically discover prompts rather than writing them by humans <ref type="bibr" target="#b15">(Jiang et al., 2020)</ref>. Our proposed prompting method is largely inspired by this body of work. Instead of prompt engineering/generation, we focus on using available natural language descriptions of schema categories associated with database documentation as task-specific promptings for DST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prompting Language Model for Dialogue State Tracking</head><p>In this section, we first set up the notations that are used throughout paper, and then review the generative DST with the sequence-to-sequence framework.</p><p>Based on that, we formally introduce our promptbased DST model and the corresponding backbone  pretrained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation.</head><p>For task-oriented dialogues considered in this paper, a dialogue consists of a sequence of utterances alternating between two parties, 1 , 1 , ..., , , where and represent the user utterance and the system response, respectively. In a turn , the user provides a new utterance and the system agent responds with utterance . As shown in the bottom of <ref type="figure">Figure</ref> 1, at turn , we denote the dialogue context as = { 1 , 1 , ? , ?1 , }, which excludes the latest system response . In this work, we assume a multi-domain scenario, in which case the schema contains domains ? = { 1 , ? , } and slots ? = { 1 , ? , } to track as examples illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. , the dialogue state at turn , is then defined as a mapping from a pair ( , ) into values . Here, we define ( , ) = , if ( , ) is not in the current dialogue state. In the given example of <ref type="figure" target="#fig_0">Figure 1</ref>, the pair (domain=hotel, slot=ref ) is not in the dialogue state, and the value "none" is assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generation-based DST with the Sequence-to-sequence Model</head><p>There are primarily two decoding strategies for generation-based DST in the literature for inferring the dialogue state at a particular turn -sequential (a) and independent (b)(c) -both of which are explored in the paper as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In the first case (top system (a) in <ref type="figure" target="#fig_0">Figure 1</ref>), the dialogue history is taken as input to the encoder, and domain-slot-value triplets ( , , ) are generated sequentially, where ( , ) ? . This approach is adopted in many systems that leverage autoregressive LMs <ref type="bibr">(Peng et al., 2020a;</ref><ref type="bibr">Hosseini-Asl et al., 2020)</ref>. Despite being simple, this kind of sequential generation of multiple values is more likely to suffer from optimization issues with decoding long sequences resulting in lower performance. However, given its wide adoption in the literature, we still include this type of generative DST with the same backbone pretrained encoderdecoder Transformer model in our experiments. To partially address this issue, <ref type="bibr" target="#b19">Lin et al. (2020b)</ref> propose a domain independent decoding where the decoder only have to generate a sequence of slot and value pairs within a specific given domain. Although their model leverages the same backbone model as ours, we empirically find that this form of strategy is still of limited effectiveness.</p><p>In the second case (middle two systems (b)(c) in <ref type="figure" target="#fig_0">Figure 1)</ref>, the values for each domain-slot pair are generated independently, potentially in parallel. The domain and slot names (embedded as continuous representations) are either the initial hidden state of the decoder  or the first input of the decoder . Values are either generated for all possible domain-slot ( , ) pairs with a possible value of "none" and/or there is a separate gating mechanism for domain-slot combinations not currently active. Since we are interested in enriching the input with task-specific information, we focus on extending the independent decoding modeling for our prompt-based DST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prompt-based DST</head><p>In this section, we formally present the flow of our prompt-based DST with an encoder-decoder architecture. Here, we are interested in an encoderdecoder model with a bi-directional encoder <ref type="bibr" target="#b26">(Raffel et al., 2020;</ref><ref type="bibr" target="#b17">Lewis et al., 2020)</ref>, in contrast with the uni-directional encoder used in autoregressive LMs <ref type="bibr" target="#b25">(Radford et al., 2019;</ref><ref type="bibr">Brown et al., 2020a)</ref>.</p><p>The input of the prompt-based DST is made up of a dialogue context and a task-specific prompt. Here, we use two types of task-specific prompts, the domain-related prompt ( ), and slot-related prompt ( ), both of which are de-rived based on the given schema. We leave the discussion of two specific realizations of taskspecific prompts to the later part of this section. Specifically, all sub-sequences are concatenated with special segment tokens, i.e., "[user] <ref type="bibr">[slot]</ref> are special segment tokens for indicating the start of a specific user utterance, system utterance, domain-related prompt, and slotrelated prompt, respectively.</p><formula xml:id="formula_0">1 [system] 1 . . . [system] ?1 [user] [domain] ( ) [slot] ( )", as in- put to the encoder, where [user], [system], [domain],</formula><p>Given this prompt-augmented input, the bidirectional encoder then outputs</p><formula xml:id="formula_1">= Encoder( , ( ), ( )),<label>(1)</label></formula><p>where ? ? ? is the hidden states of the encoder, is the input sequence length, and is the encoder hidden size. Then, the decoder attends to the encoder hidden states and decodes the corresponding slot value ( , ):</p><p>( , ) = Decoder( ).</p><p>( <ref type="formula">2)</ref> The overall learning objective of this generation processing is maximizing the log-likelihood of ( , ) given , ( ) and ( ), that is</p><formula xml:id="formula_2">? ( , ) log ( ( , )| , ( ), ( )). (3)</formula><p>During inference, a greedy decoding procedure is directly applied, i.e., only the most likely token in the given model vocabulary is predicted at each decoding step. Schema-Based Prompt. The first realization of task-specific prompt considered in this paper is based on the domain and slot names as defined in the task-dependent schema. As shown in (b) of <ref type="figure" target="#fig_0">Figure 1</ref>, given the domain name train and the slot name day, the specific prompt is in the form of "[domain] train [slot] day". Different from <ref type="bibr" target="#b18">(Lin et al., 2020a;</ref> where the taskspecific information is used in the decoder side, our symbol-based prompt as additional input to the bi-directional encoder can potentially achieve taskaware contextualizations. Observing that users often revise/repair their earlier requests in dialogues, we posit that the resulting encoded representations can be more effectively used by the decoder for generating corresponding slot values. Natural Language Augmented Prompt. One main drawback of symbol-based prompt is that  those domain/slot names contain limited information that can be utilized by pretrained LMs. In other words, those symbols from the custom schema are typically under-specified and unlikely to appear in corpus for LM pretraining. Fortunately, documentation is commonly available for real-world databases, and it is a rich resource for domain knowledge that allows dialogue systems to better understand the meanings of the abbreviated domain and slot names. The documentation includes but is not limited to domain/slot descriptions and the list of possible values for categorical slots. In this work, we experiment with a simple approach that augments the input by incorporating the domain description after the domain name and the slot description (with the sequence of values, if any) following the slot name, as illustrated in the system (c) in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Backbone Sequence-to-sequence Model</head><p>Our prompt-based DST model is initialized with weights from a pretrained LM in an encoderdecoder fashion. In this paper, we use the Text-to-Text Transformer (T5) <ref type="bibr" target="#b26">(Raffel et al., 2020</ref>) as our backbone model. T5 is an encoder-decoder Transformer with relative position encodings <ref type="bibr" target="#b33">(Shaw et al., 2018)</ref>. We refer interested readers to the original paper for more details. <ref type="table" target="#tab_2">Table 1</ref> summarizes the statistics of the datasets used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>MultiWOZ <ref type="bibr" target="#b4">(Budzianowski et al., 2018</ref>) is a multidomain task-oriented dialogue dataset that contains over 10K dialogues across 8 domains. It is a collection of human-human written conversations and has been one of the most popular benchmarks in the DST literature. Since its initial release, many erroneous annotations and user utterances have been identified and fixed in subsequent versions, i.e., MultiWOZ 2.1 <ref type="bibr" target="#b8">(Eric et al., 2019)</ref> and MultiWOZ 2.2 <ref type="bibr" target="#b38">(Zang et al., 2020)</ref>. In addition, MultiWOZ 2.1 provides 2-3 descriptions for every slot in the dataset. We randomly sample one of them and use the same descriptions for every experiment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MultiWOZ 2.2: Fully Annotated Natural Language Augmented Prompt</head><p>We present the evaluation results on MultiWOZ 2.2 in <ref type="table" target="#tab_4">Table 2</ref>. The following baseline models are considered: TRADE , DS-DST <ref type="bibr" target="#b39">(Zhang et al., 2019)</ref> and Seq2Seq-DU <ref type="bibr" target="#b9">(Feng et al., 2020)</ref>. Similar to ours, the decoding strategy of TRADE is independent. However, the sum of domain and slot embeddings are the first input of the decoder, which makes their dialogue history representation not task-aware contextualized. The sequential decoding strategy is worse than the independent decoding strategy by over 5% with both T5-small and T5-base. Even with T5-small (almost half the model size of BERT-base which is used in most previous benchmark models), our system achieves the SOTA performance using the independent decoding. As expected, T5-base systems outperform T5-small systems. With the augmentation of descriptions, we improve the overall JGA by over 1% in both T5-small and T5-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MultiWOZ 2.1: Partially Annotated Natural Language Augmented Prompt</head><p>Different from MultiWOZ 2.2 studied in the previous section, MultiWOZ 2.1 only contains natural language descriptions for slots but not domains. In addition, there is no possible slot value information.</p><p>The evaluation results on MultiWOZ 2.1 are shown in <ref type="table" target="#tab_6">Table 3</ref>, where we compare with TRADE , MinTL <ref type="bibr" target="#b18">(Lin et al., 2020a)</ref>, SST , TripPy <ref type="bibr" target="#b11">(Heck et al., 2020)</ref>, Simple-TOD (Hosseini-Asl et al., 2020), SOLOIST <ref type="bibr">(Peng et al., 2020a)</ref> and TripPy+SCORE . Note that both SOLOIST and TripPY+SCORE use external dialogue datasets to  finetune their models.</p><p>As expected, we observe that T5-base models perform consistently better than T5-small models. Moreover, using descriptions consistently improves the performance of both models. All our models outperform baselines that do not use extra dialogue data. It is worth noting that comparing with MinTL (T5-small), our model is better by over 4% even without descriptions. Further, our T5-small system is even better than MinTL built on BART-LARGE <ref type="bibr" target="#b17">(Lewis et al., 2020)</ref> which has substantially more parameters. Similar to ours, MinTL leverages a sequence-to-sequence LM. One difference is that their domain information is fed only to the decoder while our approaches enables task-aware contextualization by prompting the LMs with domain and slot information on the encoder side. Another difference is that they jointly learn DST together with dialogue response generation, which provides more supervision signals. Therefore, the better performance of our systems implies that schema-driven prompting is effective.</p><p>Lastly, compared with MultiWOZ 2.2, the performance gain brought by augmenting natural language descriptions is less pronounced which is likely caused by the reduced information available in MultiWOZ 2.1 descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">M2M: Borrowed Natural Language</head><p>Augmented Prompt  <ref type="table" target="#tab_7">Table 4</ref>: Results on M2M. All numbers are reported in joint goal accuracy(JGA)(%). <ref type="bibr" target="#b28">(Rastogi et al., 2017)</ref> should be considered as a kind of oracle upper bound performance because the target slot value is guaranteed to be in the candidate list and consider by the model. notated in a different manner. We achieve the SOTA performance on Sim-R and Sim-M+R while being comparable on Sim-M. The improvements of descriptions are only evident on the restaurant domain. The lack of improvement from slot descriptions for the movie domain may be because the slot descriptions do not add much beyond the slot name (compared to "category" for the restaurant domain) or that it has slots that generalize better across domains (e.g. date, time, number of people).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Breakdown Evaluation for MultiWOZ</head><p>In <ref type="table" target="#tab_9">Table 5</ref>, we follow the categorization provided in <ref type="bibr" target="#b38">(Zang et al., 2020)</ref> and show the breakdown evaluation of categorical and non-categorical slots on MultiWOZ 2.2. As we can see, the breakdown accuracy scores for both categorical and non-categorical slots are pretty consistent with the overall JGA. For both T5-small and T5-base models, models with sequential decoding perform worse than the corresponding models with independent decoding for both categorical and non-categorical slots. In particular, the independent decoding models achieve more pronounced improvement in categorical slots indicating that the task-specific prompt is very helpful for guiding the decoder to predict valid values. When comparing models using natural language description with those not, we observe performance gains for both types of slots for T-base but only non-categorical slots for T5-small. It is likely that the smaller size of T5 has limited representation capability to effectively utilize the additional textual description information regarding types and possible values.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study on Schema Descriptions</head><p>To understand what parts of the schema descriptions are most important, we experiment with three kinds of description combinations on MultiWOZ 2.2 using the T5-small configuration: (i) excludes the list of possible values for categorical slots (ii) excludes slot descriptions (iii) excludes domain descriptions. For (i), there is an 0.4% point drop in JGA, validating that value sets can successfully constrain the model output, as we illustrate in <ref type="table" target="#tab_11">Table 6</ref>. For (ii), there is a 0.8% point drop in JGA. And for (iii), there is a 0.1% point drop in JGA. This shows that slot descriptions are the most important part of the schema prompts and domain descriptions are relatively less effective. This is probably due to the fact that there are 61 slots in MultiWOZ 2.2 but only 8 domains. Also, the domain names are all self-contained single words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Effectiveness of Natural Language Augmented Prompt</head><p>In order to understand the benefit of natural   the dialogue history are partially matched to the slot description of arriveby making it easier for the description-augmented system to detect the mention of the correct slot. For the second example, the type information in the description implicitly guides the model to focus on time-related information leading the correct output of the normalized time expression, 16:45. In contrast, the model without descriptions only generates the partial answer 4:45, ignoring PM. Lastly, "London Kings Street" is a typographical error in this case. By utilizing the provided possible values included in the slot descriptions, the model is able to generate the correct slot value without spelling error, demonstrating that the natural language augmented prompt can successfully constrain the model output and potentially provides robustness to the dialogue state tracking system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Error Analysis of Natural Language Augmented Prompt-based DST</head><p>Here, we further carry out error analyses into the natural language augmented prompt-based T5-base model on MultiWOZ 2.2. As shown in <ref type="table" target="#tab_12">Table 7</ref>, we randomly sample 50 turns and categorize them into different types. In summary, there are four types of errors: (i) The most common error type is annotation error in which the model prediction is actually correct, which is similar to the findings of <ref type="bibr" target="#b42">(Zhou and Small, 2019)</ref>. (ii) 20% of the errors come from model failing to capture information provided by the system. 3 (iii) 16.66% of the errors are caused by the model misses of at least one gold slot. (iv) 10% of the errors are correct slot predictions with the wrong corresponding values. In general, most errors are likely caused by the lack of explicit modeling of user-system interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a simple but effective task-oriented dialogue system based on large-scale pretrained LM. We show that, by reformulating the dialogue state tracking task as prompting knowledge from LM, our model can benefit from the knowledge-rich sequence to sequence T5 model. Based on our experiments, the proposed natural language augmented prompt-based DST model achieve SOTA on MultiWOZ 2.2 and comparable performance on MultiWOZ 2.1 and M2M to recent SOTA models. Moreover, our analyses provide evidence that the natural language prompt is effectively utilized to constrain the model prediction.  <ref type="bibr" target="#b29">(Rastogi et al., 2020a)</ref> and the descriptions of the restaurant domain is taken from <ref type="bibr" target="#b38">(Zang et al., 2020)</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of generative DST approaches for multi-domain scenario. The top three figures illustrate three different generative approaches considered in this paper and the bottom figure includes specific examples for dialogue history, domain names, slot names, natural language descriptions (types, set of valid values, etc.) for slots. Sub-figure (b)(c) demonstrate two prompt-based DST models proposed, where method in (c) includes additional natural language description of slots considered for tracking. Domain descriptions are omitted for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Sequential Decoding train day Monday Dialogue History train day T5 Monday London Kings Cross none</head><label></label><figDesc></figDesc><table><row><cell>Dialogue History</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T5</cell><cell>?</cell></row><row><cell>Dialogue History</cell><cell>? train</cell><cell cols="5">(a) Generation-based DST w/ ? destination T5 ?</cell></row><row><cell>Dialogue History</cell><cell>hotel</cell><cell>ref</cell><cell></cell><cell></cell><cell cols="2">T5</cell></row><row><cell></cell><cell cols="6">(b) Schema-Based Prompt DST w/ Independent Decoding</cell></row><row><cell>Dialogue History</cell><cell>train</cell><cell>day</cell><cell cols="3">day of the departure ?</cell><cell>T5</cell><cell>Monday</cell></row><row><cell>Dialogue History</cell><cell cols="3">train destination</cell><cell cols="2">destination location?</cell><cell>T5</cell><cell>London Kings Cross</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell>Dialogue History</cell><cell>hotel</cell><cell>ref</cell><cell cols="3">reference number?</cell><cell>T5</cell><cell>none</cell></row><row><cell></cell><cell cols="6">(c) Natural Language Augmented Prompt DST w/ Independent Decoding</cell></row><row><cell cols="2">Dialogue History</cell><cell></cell><cell>Domain</cell><cell></cell><cell>Slot</cell><cell>NL Description</cell><cell>Value</cell></row><row><cell>[User] ?</cell><cell></cell><cell></cell><cell cols="2">[Domain] train</cell><cell>[Slot] destination</cell><cell>destination location of the train, [Possible Values] London Kings Cross, ?</cell><cell>London Kings Cross</cell></row><row><cell cols="3">[System] ? [User] Can you help me find a train for Sunday. ?</cell><cell cols="2">[Domain] train ?</cell><cell>[Slot] day ?</cell><cell>day of the departure, [Possible Values] Monday, ?, Sunday ?</cell><cell>Monday ?</cell></row><row><cell cols="2">I would like to visit</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">London Kings Street.</cell><cell></cell><cell cols="2">[Domain] hotel</cell><cell>[Slot] ref</cell><cell>reference number of the hotel booking</cell><cell>none</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experiment data summary. The numbers are computed on all splits of the datasets. MWOZ stands for MultiWOZ. Cat. Slots and Non-Cat. Slots stand for categorical slots and non-categorical slots, respectively. The rows Domain Desc. and Slot Desc. indicate whether the corresponding dataset</figDesc><table /><note>has natural language description for domains and slots, respectively. The row Value Set incates whether the corresponding dataset provides possible value set for categorical slots.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Results on MultiWOZ 2.2. All numbers are re-</cell></row><row><cell>ported in joint goal accuracy (JGA)(%). w. desc means</cell></row><row><cell>the model is trained with the description. # Para. stands</cell></row><row><cell>for the number of model parameters.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on MultiWOZ 2.1. All numbers are reported in joint goal accuracy (JGA)(%). w. desc means the model is trained with the description. * means extra dialogue data is used to finetune the language model. # Para. stands for the number of model parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Models</cell><cell cols="3">Sim-M Sim-R Sim-M+R</cell></row><row><cell>(Rastogi et al., 2017)</cell><cell>96.8</cell><cell>94.4</cell><cell>-</cell></row><row><cell>(Rastogi et al., 2018)</cell><cell>50.4</cell><cell>87.1</cell><cell>73.8</cell></row><row><cell>(Chao and Lane, 2019)</cell><cell>80.1</cell><cell>89.6</cell><cell>-</cell></row><row><cell>(Heck et al., 2020)</cell><cell>83.5</cell><cell>90.0</cell><cell>-</cell></row><row><cell>Independent</cell><cell>83.3</cell><cell>89.6</cell><cell>88.0</cell></row><row><cell>w. desc</cell><cell>81.0</cell><cell>90.6</cell><cell>86.4</cell></row></table><note>shows the evaluation results on M2M. In this case, all natural language descriptions are di- rectly borrowed from dialogue datasets that are an-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Slot type breakdown results on the test set of</cell></row><row><cell>MultiWOZ 2.2. All numbers are reported in joint goal</cell></row><row><cell>accuracy(JGA) (%). CAT and NON-CAT correspond</cell></row><row><cell>to categorical slots JGA and non-categorical slots JGA,</cell></row><row><cell>respectively. w. desc indicates that the model is trained</cell></row><row><cell>with the full description.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Database Train Slot Descriptions || Possible Values arriveby arrival time of the train</head><label></label><figDesc>destination destination of the train || Birmingham New Street, London Kings Cross, ..., Stevenage Dialogue History ... [SYS] The earliest being 19:09 and arriving by 20:54. Would that work for you? [USR] Yes, I think the 20:54 arrival time should work.</figDesc><table><row><cell>no desc.</cell><cell>(train, day, friday) (train, departure, leicester) (train, destination, cambridge) (train, leaveat,</cell></row><row><cell></cell><cell>19:00)</cell></row><row><cell>desc.</cell><cell>(train, arriveby, 20:54) (train, day, friday) (train, departure, leicester) (train, destination, cam-</cell></row><row><cell></cell><cell>bridge) (train, leaveat, 19:00)</cell></row><row><cell cols="2">Dialogue History [USER] I need to find a train going to Leicester that arrives by 4:45 PM. Do you know of one?</cell></row><row><cell>no desc.</cell><cell>(train, arriveby, 04:45) (train, destination, leicester)</cell></row><row><cell>desc.</cell><cell>(train, arriveby, 16:45) (train, destination, leicester)</cell></row><row><cell cols="2">Dialogue History [USER] Can you help me find a train for Sunday. I would like to visit London Kings Street.</cell></row><row><cell>no desc.</cell><cell>(train, destination, London Kings Street) (train, day, Sunday)</cell></row><row><cell>desc.</cell><cell>(train, destination, London Kings Cross) (train, day, Sunday)</cell></row><row><cell></cell><cell>lan-</cell></row><row><cell></cell><cell>guage augmented prompt, we focus on analyzing</cell></row><row><cell></cell><cell>the examples where the description augmented</cell></row><row><cell></cell><cell>model correctly tracks the dialogue state while the</cell></row><row><cell></cell><cell>unaugmented one fails. Based on our analysis of</cell></row><row><cell></cell><cell>T5-base model on MultiWOZ 2.2, the most com-</cell></row><row><cell></cell><cell>mon errors are either misses of gold slots or over-</cell></row><row><cell></cell><cell>predictions of irrelevant slots (82.8% of all errors).</cell></row><row><cell></cell><cell>The remaining error cases are correct slot predic-</cell></row><row><cell></cell><cell>tions with wrong slot values (17.2%).</cell></row><row><cell></cell><cell>We provide representative examples for which</cell></row><row><cell></cell><cell>the description augmented system correctly tracks</cell></row><row><cell></cell><cell>the dialogue states but not the unaugmented one</cell></row><row><cell></cell><cell>in Table 6. In the first example, the phrases in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :Unable to Capture System Information Dialogue HistoryIt leaves at 19:35 and arrives at 19:52. IsUnable to Mention Slot Provided by User Dialogue History: Incorrect Value Reference Dialogue History [</head><label>6</label><figDesc>Examples for train domain dialogues where the description-augmented ("desc.") model make the correct state predictions but the unaugmented models ("no desc.") fails. The correctly predicted triplets are in bold. ...[SYSTEM] There is TR6679. ... [USER] Do you happen to know if there is a nightclub in the centre?[SYSTEM] Yes, we have FIVE nightclubs in the centre of town. USER] Hi can you help me find a very nice Italian restaurant near the centre of cambridge?[SYSTEM] Please specify your price range.</figDesc><table><row><cell></cell><cell>53.33%: Annotation Errors</cell></row><row><cell cols="2">Dialogue History ...[SYSTEM] Out of the 21 restaurant choices, one is the Yippee Noodle Bar which is moderately</cell></row><row><cell></cell><cell>priced in the centre of town. Would you like to make a reservation?</cell></row><row><cell></cell><cell>[USER] That sounds great, what is the postcode?</cell></row><row><cell>Gold</cell><cell>()</cell></row><row><cell>desc. Prediction</cell><cell>(restaurant, area, centre) (restaurant, pricerange, moderate) (restaurant, name, yippee noodle</cell></row><row><cell></cell><cell>bar</cell></row><row><cell></cell><cell>20.00%: that good for you?</cell></row><row><cell></cell><cell>[USER] Sounds good. May I have the travel time and ticket price, please?</cell></row><row><cell>Gold</cell><cell>(train, arriveby, 19:52) (train, leaveat, 19:35)</cell></row><row><cell>desc. Prediction</cell><cell>()</cell></row><row><cell></cell><cell>16.66%: Is there a particular one you're</cell></row><row><cell></cell><cell>looking for?</cell></row><row><cell></cell><cell>[USER] I don't care which one you recommend, but can you tell me the entrance fee and</cell></row><row><cell></cell><cell>address?</cell></row><row><cell>Gold</cell><cell>(attraction, area, centre) (attraction, type, nightclub) (attraction, name, dontcare)</cell></row><row><cell>desc. Prediction</cell><cell>(attraction, area, centre) (attraction, type, nightclub)</cell></row><row><cell></cell><cell>10.00%[USER] It does not matter.</cell></row><row><cell>Gold</cell><cell>(restaurant, area, centre) (restaurant, food, italian) (restaurant, pricerange, dontcare)</cell></row><row><cell>desc. Prediction</cell><cell>(restaurant, area, centre) (restaurant, food, italian) (restaurant, pricerange, expensive)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>The most common error types of our best model(t5-base w/ desc.) and corresponding examples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Domain and slot descriptions of M2M used in our experiments. The descriptions of the movie domain is taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>The randomly sampled descriptions of MultiWOZ 2.1 used in all our experiments.</figDesc><table><row><cell></cell><cell></cell><cell>MultiWOZ 2.1</cell></row><row><cell>Domain</cell><cell>Slot</cell><cell>Slot Description</cell></row><row><cell>taxi</cell><cell>leaveat</cell><cell>what time you want the taxi to leave your departure location by</cell></row><row><cell>taxi</cell><cell>destination</cell><cell>destination of taxi</cell></row><row><cell>taxi</cell><cell>departure</cell><cell>what place do you want to meet the taxi</cell></row><row><cell>taxi</cell><cell>arriveby</cell><cell>when you want the taxi to drop you off at your destination</cell></row><row><cell cols="2">restaurant book people</cell><cell>number of people booking the restaurant</cell></row><row><cell>restaurant</cell><cell>book day</cell><cell>what day of the week to book the table at the restaurant</cell></row><row><cell>restaurant</cell><cell>book time</cell><cell>time of the restaurant booking</cell></row><row><cell>restaurant</cell><cell>food</cell><cell>food type for the restaurant</cell></row><row><cell>restaurant</cell><cell>pricerange</cell><cell>price budget for the restaurant</cell></row><row><cell>restaurant</cell><cell>name</cell><cell>name of the restaurant</cell></row><row><cell>restaurant</cell><cell>area</cell><cell>preferred location of restaurant</cell></row><row><cell>train</cell><cell>destination</cell><cell>destination of the train</cell></row><row><cell>train</cell><cell>day</cell><cell>what day you want to take the train</cell></row><row><cell>train</cell><cell>departure</cell><cell>departure location of the train</cell></row><row><cell>train</cell><cell>arriveby</cell><cell>what time you want the train to arrive at your destination station by</cell></row><row><cell>train</cell><cell>book people</cell><cell>number of people booking for train</cell></row><row><cell>train</cell><cell>leaveat</cell><cell>when you want to arrive at your destination by train</cell></row><row><cell>hotel</cell><cell>pricerange</cell><cell>preferred cost of the hotel</cell></row><row><cell>hotel</cell><cell>type</cell><cell>type of hotel building</cell></row><row><cell>hotel</cell><cell>parking</cell><cell>parking facility at the hotel</cell></row><row><cell>hotel</cell><cell>book stay</cell><cell>length of stay at the hotel</cell></row><row><cell>hotel</cell><cell>book day</cell><cell>day of the hotel booking</cell></row><row><cell>hotel</cell><cell>book people</cell><cell>how many people are staying at the hotel</cell></row><row><cell>hotel</cell><cell>area</cell><cell>rough location of the hotel</cell></row><row><cell>hotel</cell><cell>stars</cell><cell>rating of the hotel out of five stars</cell></row><row><cell>hotel</cell><cell>internet</cell><cell>whether the hotel has internet</cell></row><row><cell>hotel</cell><cell>name</cell><cell>which hotel are you looking for</cell></row><row><cell>attraction</cell><cell>type</cell><cell>type of attraction or point of interest</cell></row><row><cell>attraction</cell><cell>area</cell><cell>area or place of the attraction</cell></row><row><cell>attraction</cell><cell>name</cell><cell>name of the attraction</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/google-research/ google-research/tree/master/schema_ guided_dst#evaluation-on-multiwoz-21</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://huggingface.co/t5-small ,https: //huggingface.co/t5-base</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by a grant from Allstate. We would like to thank the reviewers for their constructive feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Implementation Details</head><p>The backbone models we use for finetuning are T5-small( 60M parameters) and T5-base( 220M parameters). We use the pretrained checkpoint from transformers library 4 . For T5-small, we train the model with a batch size 4, a learning rate of 5e-5 for 3 epochs. For T4-base, we train the model with a batch size of 64, a learning rate of 5e?4 for 2 epochs. Both models are trained using Adam <ref type="bibr" target="#b21">(Loshchilov and Hutter, 2018)</ref>. We don't use any text or label normalization scripts like <ref type="bibr">Hosseini-Asl et al., 2020)</ref>.</p><p>For MultiWOZ 2.1 and 2.2, following many previous works , since police and hospital domains only appear in the training set, we exclude them in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Descriptions</head><p>We show the descriptions of M2M and MultiWOZ 2.1in Table8 and Table9</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">If the user confirms the booking or gives a positive response, then the dialogue states in the previous system utterance should be grounded. However, this rule is not always followed in the dataset construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Zhou and Small</publisher>
		</imprint>
	</monogr>
	<note>Rewon Child, 3 There is label inconsistency in the MultiWoZ as pointed out by. So to some extent, this type of error is inevitable</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020a. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2020b. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe?</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert-dst: Scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Lin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Schema-guided multi-domain dialogue state tracking with graph attention neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boer</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7521" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyag</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01669</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A sequenceto-sequence approach to dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09553</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end neural pipeline for goal-oriented dialogue systems using gpt-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghoon</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Gwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngsoo</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kee-Eung</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Trippy: A triple copy strategy for value independent neural dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nurul</forename><surname>Carel Van Niekerk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Lubis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsien-Chin</forename><surname>Geishauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Moresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convert: Efficient and accurate conversational representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrk?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2161" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL)</title>
		<meeting>the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00796</idno>
		<title level="m">Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient dialogue state tracking by selectively overwriting memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyuwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="567" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MinTL: Minimalist transfer learning for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3391" to="3405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mintl: Minimalist transfer learning for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3391" to="3405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pretraining methods for dialog context representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniia</forename><surname>Razumovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3836" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020a. Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Few-shot natural language generation for task-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-task learning for joint language understanding and dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 19th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="376" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalable multi-domain dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Khaitan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01359</idno>
		<title level="m">Schemaguided dialogue state tracking task at dstc8. arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Khaitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8689" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13637</idno>
		<title level="m">Recipes for building an open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Building a conversational agent overnight with dialogue self-play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04871</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tod-bert: Pre-trained natural language understanding for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="917" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="808" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanghua</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarana</forename><surname>Manotumruksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09374</idno>
		<title level="m">Slot selfattentive dialogue state tracking</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Score: Pretraining for context representation in conversational semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">Hassan</forename><surname>Awadallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 2nd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03544</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dialogpt: Largescale generative pre-training for conversational response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Global-locally self-attentive dialogue state tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09655</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-domain dialogue state tracking as dynamic knowledge graph enhanced question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
