<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mahdi</roleName><forename type="first">Pourmirzaei</forename><surname>Gholam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tarbiat Modares University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Montazer</surname></persName>
							<email>montazer@modares.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="institution">Tarbiat Modares University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Esmaili</surname></persName>
							<email>f.esmaili@modares.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="institution">Tarbiat Modares University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, at first, the impact of ImageNet pre-training on fine-grained Facial Emotion Recognition (FER) is investigated which shows that when enough augmentations on images are applied, training from scratch provides better result than fine-tuning on ImageNet pre-training. Next, we propose a method to improve fine-grained and in-the-wild FER, called Hybrid Multi-Task Learning (HMTL). HMTL uses Self-Supervised Learning (SSL) as an auxiliary task during classical Supervised Learning (SL) in the form of Multi-Task Learning (MTL). Leveraging SSL during training can gain additional information from images for the primary fine-grained SL task. We investigate how proposed HMTL can be used in the FER domain by designing two customized version of common pre-text task techniques, puzzling and in-painting. We achieve stateof-the-art results on the AffectNet benchmark via two types of HMTL, without utilizing pre-training on additional data. Experimental results on the common SSL pre-training and proposed HMTL demonstrate the difference and superiority of our work. However, HMTL is not only limited to FER domain. Experiments on two types of fine-grained facial tasks, i.e., head pose estimation and gender recognition, reveals the potential of using HMTL to improve finegrained facial representation.</p><p>Recently, with progress in Self-Supervised Learning (SSL), results show that in an end-to-end learning manner, SSL methods can help us to overcome needing lots of labels <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. While recent contrastive SSL methods have demonstrated capability of them, they still have some drawbacks which did not solve completely, such as requiring large batch sizes to train [6], need for accessing huge amount of computation power and data to work well <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>, and placing far behind of SL methods on fine-grained visual tasks <ref type="bibr" target="#b9">[10]</ref>. The last two ones are considered as the main hurdle to improve FER performance by SSL. In fact, as far as we know, there is no work which seriously attempt to use SSL on fine-grained FER. Furthermore, FER is one of the most challenging fine-grained problems because of its uncertainty. For example, in collecting and labeling AffectNet dataset, their collectors found that even with human training, there were about 60% to 70% agreement in their labeling among different emotions <ref type="bibr" target="#b10">[11]</ref>. In other words, it shows the uncertainty and ambiguity of emotion information through visual channel.</p><p>In some studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, by the another type of SSL methods named pre-text task SSL, they could extract valuable features for many downstream tasks, even by doing pre-training on one image <ref type="bibr" target="#b13">[14]</ref>. In fact, in this type of pre-training, middle layers show better features concerning final layers. Likewise, by moving from low level layer to high level layer, increase and then decrease could be seen via linear evaluation of feature map <ref type="bibr" target="#b13">[14]</ref>.</p><p>Since 1990s, different studies have shown the ability of Multi-Task Learning (MTL) in Supervised settings <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Yet, it is not quite obvious how to pick</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial emotions are important factors in human communication to help people convey their emotional states and intentions. One-third of these communications are verbal components and two-third of residues are nonverbal. Among the non-verbal components, facial emotions have a key role in communications between people <ref type="bibr" target="#b0">[1]</ref>.</p><p>Recently, various systems have been developed for recognizing facial emotions in the field of computer vision and deep learning <ref type="bibr" target="#b0">[1]</ref>. To detect emotion of a person in an image, a lot of visual information can be found such as appearance, gesture, behavior, and context of the scene. But facial emotions are dramatically the most important visual cue for analyzing basic human emotions <ref type="bibr" target="#b1">[2]</ref>.</p><p>There are two models to explain facial emotions in computer vision: categorical and circumplex (dimensional) <ref type="bibr" target="#b2">[3]</ref>:</p><p>In categorical model, Ekman <ref type="bibr" target="#b3">[4]</ref> has defined a list of affective-related categories (Anger, Happy, Sad, Surprise, Disgust, Neutral, Fear, and Contempt), which emotions have been chosen from this list. The facial action coding system (FACS) depends on action units (AUs) or a set of facial muscle movements being determined to display the emotions of participants. Ekman used FACS to recognize emotions.</p><p>The circumplex model of emotion was developed by James Russell <ref type="bibr" target="#b2">[3]</ref>. This model suggests that emotions are distributed in a two-dimensional circular space, containing arousal and valence dimensions. To be precise, the vertical and the horizontal axis determine valence and arousal respectively and the center of this circle represents the normal state of valence and arousal <ref type="bibr" target="#b4">[5]</ref>. This model is shown in figure 1.</p><p>According to Russel and Ekman model, Data labeling requires experts and it is not as simple as annotating like the other computer vision tasks such as object detection. The inherent uncertainty in facial emotion recognition is the reason for the difficulty and the cost of labeling. This uncertainty is even more for the Russell's model. Thus, unlike other computer vision issues, collecting large amounts of labeled data in the field of Facial Emotion Recognition (FER) is challenging and may adversely impact on quality of annotating. This makes the utilize of deep learning or specifically Supervised Learning (SL), become less effective in FER. the proper tasks due to the fact that, before a certain point, two tasks enhance shareable features for each other and after that point, they start hurting each other which is described as cooperation and competition between tasks <ref type="bibr" target="#b14">[15]</ref>. Therefore, from another view, using SSL tasks can be looked through the MTL lens. So, why do not we use SL with auxiliary tasks of SSL concurrently in the MTL setting?</p><p>In this study, we used SSL besides SL in the MTL setting called Hybrid Multi-Task Learning (HMTL). The proposed HMTL was considered during the training procedures. It means that the SSL tasks were placed on top of the backbone and they could be removed from it during testing or inference. The purpose of appending SSL tasks was to help the backbone to build better features for the fine-grained FER. One important thing in the proposed HMTL was that it basically is not a pre-training technique like other studies in this major. Mainly, there are two ways to use SSL: <ref type="bibr" target="#b0">(1)</ref> Using it to pre-train the weights, (2) leveraging SSL as an auxiliary task besides of SL which we showed the advantage of the second one. Need to mention that in this study, "HMTL", "SL+SSL" and "SL with auxiliary SSL task" are used interchangeably and refer to a same thing.</p><p>The contributions of this work are summarized as follows:</p><p>1. We showed the effect of augmentation intensity on FER using pre-trained ImageNet weights and random weights. It showed that the combination of random weights and strong augmentation is superior to the fine-tuning on ImageNet weights procedure with any sorts of augmentation. 2. We did pre-training with some SSLs pre-text tasks. Results showed that two of our proposed SSL methods can extract better and useful features for the FER problem. They both were also good to use as the pre-training step. 3. We proposed a hypothesis which says: "leveraging proper auxiliary tasks of SSL alongside with SL in MTL manner can improve performance of the downstream supervised task". 4. Results showed that by utilizing our HMTL approach on AffectNet dataset, the performance of both types of emotion recognition (i.e., dimensional and categorical) on all the augment levels remarkably increased, even when 20% of the heavily imbalanced training set was used. 5. The impact of using auxiliary SSL tasks on two fine-grained head pose estimation and gender recognition were scrutinized which concluded to substantial average error reduction in head pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Methods of emotion recognition from faces can be divided into two general categories: conventional methods and end-to-end learning methods, in which end-to-end based models have been able to achieve better performance in many computer vision problems <ref type="bibr" target="#b0">[1]</ref>.</p><p>In both approaches deep learning has been a crucial part. And also, among several deep learning models, convolutional neural networks (CNNs) have played important roles in FER for many years <ref type="bibr" target="#b0">[1]</ref>. CNNs completely reduce preprocessing techniques by providing end-to-end learning from inputs <ref type="bibr" target="#b16">[17]</ref>. Even though, some studies did not go only for CNN features. For example, in <ref type="bibr" target="#b17">[18]</ref>, they combined two types of feature extraction modes. First, they used features learned by CNN models through pre-trainings. Second, they used handcrafted features subtracted by a bag of visual words.</p><p>But, as we said, end-to-end learning methods have been superior than traditional methods. End-to-end methods mostly focused on designing different deep architecture <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b21">[21]</ref>. In a work <ref type="bibr" target="#b18">[19]</ref>, they used manifold networks in connection with CNN and showed that manifold networks of covariance pooling could get better performance than CNN networks with Softmax layers. Another work <ref type="bibr" target="#b19">[20]</ref>, introduced two CNN-based models for FER using different kernel sizes and numbers of filters.</p><p>Videos can also be used to recognize emotions from faces as well <ref type="bibr" target="#b22">[22]</ref>. In videos, frames are classified to various emotions such as happy, sad, etc. Although Inception and ResNet networks have had significant results in the field of FER, these two architectures did not use temporal aggregation. To solve this problem, a three-dimensional (3D) Inception-ResNet architecture was introduced <ref type="bibr" target="#b23">[23]</ref>. In this type of architecture, geometric and temporal features were extracted in a sequence of frames with the three-dimensional model.</p><p>Another study <ref type="bibr" target="#b24">[24]</ref>, used two different types of CNN networks on videos that improve the FER performance. First model extracted the temporal characteristics features of the images and the second one extracted the temporal geometric characteristics of the Facial Landmarks (FLs) points over time.</p><p>In addition to CNN, combining CNNs with RNN based models (GRU, LSTM) also could improve the performance of the networks on videos. In a study <ref type="bibr" target="#b25">[25]</ref>, three architectures that consist of a combination of CNN and RNN to recognize dimensional emotions in a MTL manner were used. To join CNN and RNN, each video frame was given to the CNN model, then several levels of features were extracted from it to feed each of them to multi RNNs. Combining end-toend approaches like LSTMs with CNNs and supporting the variable-length inputs and outputs are the most important advantages of using LSTM <ref type="bibr" target="#b0">[1]</ref>. In a study <ref type="bibr" target="#b26">[26]</ref>, a hybrid model in the form of LSTM-CNN was proposed which showed that this hybrid architecture could outperform previous 3D-CNN over time.</p><p>Just like CNNs, using attentional models have achieved significant improvements over other methods in recent years <ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref>. A study <ref type="bibr" target="#b27">[27]</ref>, used CNN with visual attention for feature extraction and detection of important regions. Another study <ref type="bibr" target="#b31">[31]</ref>, proposed a CNN with attention mechanisms that could understand occlusion regions in the face. Also, they created an end-toend trainable Patch-Gated CNNs to recognize facial expressions from occluded faces, and the model could automatically focus on unoccluded regions.</p><p>However, there have been a few methods which have done novel experiment such as using graph convolutional neural networks <ref type="bibr" target="#b32">[32]</ref>. In fact, undirected graphs were constructed from faces. Likewise, another study <ref type="bibr" target="#b33">[33]</ref> used an identity-free conditional Generative Adversarial Network (IF-GAN) to detect facial expressions by distinguishing two types of features in faces: Information related to personal identity and information related to facial expressions. This work tried to reduce the effect of identity features in facial images as much as possible, and then the features related to facial expressions were used to categorize emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised learning</head><p>SSL methods are used to learn features from unlabeled data without using annotated labels. This method of learning is used as a subset of unsupervised learning methods to avoid the large cost of gathering and labeling huge scales of datasets. SL methods need data pairs of (x, y) as the input and output which usually are annotated by a human. SSL methods are also learned from the input x, but unlike SL, the labels y are automatically generated from the inputs, without participating in any human annotation <ref type="bibr" target="#b34">[34]</ref>. In the variety of scales including the robustness of adversarial examples, label corruption, Semi-Supervised learning, etc. SSL methods are useful.</p><p>Many SSL methods have been proposed in the computer vision field <ref type="bibr" target="#b34">[34]</ref> which in general, three types of approaches exist:</p><p>1. Contrastive learning 2. Non-contrastive learning 3. Pre-text task learning</p><p>In the mid-2010s, pre-text task learning, including colorization, in-painting, and self-supervised jigsaw puzzle became popular in computer vision field <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref>. However, recently, contrastive and non-contrastive SSL methods have achieved better results on challenging datasets such as ImageNet <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Although these methods have been successful to achieve better results, they need more data and more computation cost to work well. Also, the training process of them are rather hard and tricky. However, these methods could not work well on finegrain problems such as FER which shows that there is a room for improvement <ref type="bibr" target="#b9">[10]</ref>.</p><p>One of the most well-known pre-text task techniques is random rotation. In <ref type="bibr" target="#b11">[12]</ref>, they used 2d image rotation, which was applied to input images to learn direction with CNN training. In addition to random rotation self-supervision, several papers used jigsaw puzzles in a SSL manner. For instance, <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref> used a jigsaw puzzle that shredded each image, then shuffled it and fed it to a Siamese network. Also, <ref type="bibr" target="#b12">[13]</ref> used SSL to solve jigsaw puzzles, in-painting, and colorization together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Imbalance dataset</head><p>In classification problems, when the number of samples is unevenly distributed in classes, the network learns classes with more samples better than other classes, and performance decreases for the classes with fewer samples. If the imbalance is high, it can affect the performance of the classifier and cause the network to bias towards the larger class <ref type="bibr" target="#b39">[39]</ref>. Therefore, there are several ways to deal with this problem:</p><p>1. Up sampling 2. Down sampling 3. Customize loss function</p><p>It has been shown that the customized loss function performs better in FER <ref type="bibr" target="#b10">[11]</ref>. In this work, we consider weighted loss and focal loss as two popular ones. In weighted loss method, a higher loss is assigned to the samples belonging to the classes with fewer samples. Focal loss <ref type="bibr" target="#b40">[40]</ref> reduces the effect of error calculation when the probability of predicting output p increased with adjustable alpha and gamma coefficients. If the network has more confidence in predicting samples the loss function will get lower and smaller coefficients and consequently, difficult samples will get bigger coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Before starting our work, three augmentation levels are defined (appendix A). The purpose of selecting these three levels is to investigate the impact of augmentation on the FER problem. At first step, a neural network is considered to compare fine-tuning and training from scratch on augmentation levels. Then, we evaluate features from SSL pre-trained methods. At the end, we propose our Hybrid Learning architecture for FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Supervised learning approach</head><p>To compare different augmentation settings, we test them under ImageNet weights and random weights (figure 2). We use EfficientNet B0 and B2 <ref type="bibr" target="#b41">[41]</ref> as our neural network backbones. Finally, there are six training modes: No augment w/ random weights -No augment w/ ImageNet weights -Weak augment w/ random weights -Weak augment w/ ImageNet weights -Strong augment w/ random weights -Strong augment w/ ImageNet weights </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-supervised learning approach</head><p>In this section, three SSL methods are selected: Rotating, Puzzling and Inpainting.</p><p>In rotation method, images multiply at N?45 degrees which N is chosen randomly from zero to seven. It means images rotate at one of eight directions and this direction is considered as the output label. Therefore, this is a classification problem with eight categories.</p><p>Our customized proposed puzzling pre-text task is somehow different from the original jigsaw puzzling method. Unlike general jigsaw puzzling which different parts of the images are fed to a Siamese network separately, in our method (figure 3) at first, an image is sliced to N identical size square regions, where N must have a second root <ref type="bibr">(4, 9, etc.)</ref>. Then, all pieces are randomly shuffled and for each region, a label is created to show the original location of it. In the end, regions are merge into a single image and a neural network tries to learn the correct location of each region in the image. For this purpose, we need to create classification heads according to the number of regions. For example, as we illustrate in <ref type="figure">figure 3</ref>, the image is divided into four regions which get shuffled and then, they merge into a single image. The neural network (EfficientNet) gets the puzzled images and tries to find the correct location of each region based on its head.</p><p>In contrast to previous methods, in-painting is not a classification problem. In this study, we consider two types of in-painting. The first one has one stage and the second one has two stages. The first one is a common SSL pre-text task which has been used a lot. We use it for both pre-training and HMTL that only has Pixel Wise Loss (PWL) function like mean square error (MSE). This is different from the two-stage type. We consider two-stage type due to the fact that we want to add a FER perceptual loss (PL) to it. The PL uses features of a pre-trained model which is trained on the same dataset in an ordinary supervised manner. In other words, PL function tries to reconstruct the missing part of an image with respect to the representation of a supervised model we already trained ( <ref type="figure" target="#fig_2">figure 4</ref>). In this type of in-painting, we only use the PL which is defined as one of the HMTL methods.</p><p>For the erasing procedure of in-painting, we determine a fixed region in the image consisting of face attributes (figure 4.a). Then, we cut a square with fix side randomly inside of the region (figure 4.b). In other words, the input images are cut out partially. Then, a SSH is built in the form of a deconvolutional decoder. The decoder tries to reconstruct the original image. Here the difference of two methods is shown up. The first one uses MSE loss function to fill the cutout image with respect to the original image, but, in the second type of inpainting (e.g., PL), at first, a pre-trained EfficientNet model as an offline teacher is considered to create representation for the loss function. Then, the SSH's output is trained only by representation of the offline teacher ( <ref type="figure">figure 6</ref>).</p><p>In HMTL, the decoder head tries to close the distances of representation for generated image with the original one (the input image w/o cutout). Equation 3</p><p>shows the total loss function. The important point is that the FER model for creating representation is trained with SL only on AffectNet under the same augmentation setting (w/o cutout). Therefore, this means no additional information from different augmenting levels or different datasets are going through the error signal. <ref type="bibr">Fig 3</ref> In our SSL puzzling procedure, at first, an image split into many pieces. Next, the pieces shuffle randomly, and then, the pieces are merged together into a new single image with a new order of pieces. Subsequently, in training, the puzzled image is considering as the input and the correct label of each part are considering as labels. In this example, part one belongs to region one, part two belongs to region four, part three belongs to region two and part four belongs to region three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hybrid multi-task learning approach (adding auxiliary SSL)</head><p>The impact of multitask learning in neural networks is palpable <ref type="bibr" target="#b15">[16]</ref>. As we look furthermore into this issue, there are a lot of information in images that can increase the performance of deep networks in computer vision indirectly. This information cannot correctly be recognized in SL because of their indirect effects on the loss function. For example, simultaneously recognition of facial features along with emotion could improve the final performance of emotion recognition <ref type="bibr" target="#b42">[42]</ref> <ref type="bibr" target="#b43">[43]</ref>. We believe that these features are not clear enough to be accessible from emotion labels. With consideration that gathering and annotating multi labels in FER (e.g., AU, landmark, etc.) are expensive and hard to access with hand labeling, we propose a hypothesis that says: "leveraging proper tasks of SSL as auxiliary tasks beside the SL in the MTL setting can improve supervised task representation". This statement means that if SSHs are put alongside SH, it helps the networks to create a better representation of images for the task. To choose the best HMTL approach, each method of self-supervision can be separately performed and the effectiveness of extracted features on solving the main problem can be examined as well. A SSL method that find valuable features for a downstream task (here FER), can be used besides of SL head (figure 5, 6).</p><formula xml:id="formula_0">= + ? = ? ? (?) ? ? ? , (?, ),<label>(1)</label></formula><p>Where:</p><p>? : weighted categorical cross entropy for supervised head ? : categorical cross entropy for puzzling self-supervised heads</p><formula xml:id="formula_1">= + = ? ? (?) ? ? ? , (?, ) ? ? (?) ,<label>(2)</label></formula><p>Where:</p><p>? : weighted categorical cross entropy for supervised head ? : categorical cross entropy for puzzling self-supervised heads and rotation selfsupervised head</p><formula xml:id="formula_2">= + = ? ? (?) ? ? 1 ? ( (I ) ? (I ) =1 ? (I ) ? ( ) =1 ) 2 =1 ,<label>(3)</label></formula><p>Where:</p><p>? : weighted categorical cross entropy for supervised head ? : After each representation are given to a softmax layer, the RMSE loss function of two representations will be calculate ? : weight for the decoder head </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Categorical and dimensional models</head><p>In categorical model, we only have one SH. That is to say, eight Ekman's emotions are added to the network as the supervised classification head with eight categories. In addition to correctly learning the SL task, the network is forced to solve SSL tasks as well (Equations 1 to 3). Unlike the training step, in the evaluation step, images are given to the network without SSL pre-task such as puzzling or cutout. Only SH is going to consider during the evaluation step.</p><formula xml:id="formula_3">? = * + = ? ? (?) + ? 1 ? ( ? (?) ) 2 =1 + ? 1 ? ( ? (?) ) 2 =1 ,<label>(4)</label></formula><p>Where:</p><p>? : categorical cross entropy for categorical head ? : RMSE loss function for arousal and valence heads ? ?: output of softmax layer ? : weight for the categorical head ? : expectation of softmax layer after categorical head's output which calculates a regression value <ref type="figure">Figure 7</ref> shows the procedure of calculating the loss function (equation 4) in the training process of continuous mode. The alpha coefficient is considered as a regulator for the attention of the network to the classification part and its value can be changed. This method is similar to HopeNet <ref type="bibr" target="#b44">[44]</ref> method which tries to transform continues labels to both categorical and continues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this paper, we used two versions of EfficientNet architectures, B0 and B2 among eight versions. we have considered EfficientNet architecture due to being better than other CNN architectures in terms of performance, number of parameters and flops in various domains and benchmarks <ref type="bibr" target="#b41">[41]</ref>. The resolution sizes have been set to 224?224 and 260?260 for B0 and B2 respectively. In this study, a 1080 Ti GPU card has been used to train neural network models. For training all models in this section, an Adabelief optimizer has been used <ref type="bibr" target="#b45">[45]</ref>. Also, the batch size has been set to 64 for all models except for the in-painting which is 32. For all the experiments, the TensorFlow framework have been used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AffectNet dataset</head><p>The AffectNet is one of the largest FER databases with more than 1 million images gathered from three search engines with querying emotion-related tags in different languages. 450,000 images are labeled in two modes: categorical and dimensional. In the categorical mode there are 11 labels in which eight of them are the basic emotions suggested by Ekman. Although labels in the training set are highly imbalanced (figure 8), in the validation set for each category 500 images are prepared. The test set has not been published yet <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Supervised learning</head><p>In this part, the EfficientNet B0 has been used as the backbone of SL to extract features for a linear classifier to predict eight emotions which is placed on top of it. In the training part, we set a learning rate of 0.0001 when using ImageNet weights and 0.001 when using random weights mode. The number of epochs based on the augmentation level and the weights were considered between 20 and 100. Weight decay was not used and we only used the step decay method for learning rate schedule. We considered the AffectNet training set for the training and validation set for the evaluation step. In the training set, there are approximately 287,000 images based on the eight emotions of Ekman model <ref type="figure" target="#fig_6">(Figure 8</ref>). In the validation set, there are 500 images for each category. Since the training set is very imbalanced, we used the weighted cross entropy. Also, the dropout and label smoothing methods with different intensities have been used. Each training mode has been repeated three times on three fixed random seeds and the model with the highest accuracy has been selected as the reported result. The results of each training mode are shown in table 1 and figure 9. As the results show, increasing augmentation intensity on the fine-tuning mode has not have remarkable impact on the results. In contrast, the consequence of that on the random weights mode has been more recognizable. As a matter of fact, when the strong level was used, it achieved higher accuracy compared to the ImageNet fine-tuning. In the learning procedure, before feeding inputs to the networks, images are divided into nine regions and these regions are shuffled and then merged. In the validation step, augmentation and SSL tasks do not perform on images. The puzzling pre-text block can be replaced with other pretext methods like rotation.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Self-supervised learning</head><p>We have used four pretext task learning approaches in this part:</p><p>1. Puzzling 2. Random rotation 3. Puzzling + random rotation 4. In-painting-pwl</p><p>In the first one, heads have been added on top of the EfficientNet backbone with the number of puzzle pieces (e.g., 4 or 9 heads). In the second approach, only one head has been placed on top of the backbone which the number of classes has been equal to the number of rotation directions. In the third one, all heads of the first and second approaches have been joined together (e.g., 5 or 10 heads). Lastly, the pre-training procedure of the fourth approach is based on a CNN decoder which has been placed on the backbone and tried to only reconstruct the original image with respect to the RMSE pixel-wise loss function. As the first three methods are relatively easy for a deep network to learn, the strong augmentation level has been considered for all three of them. Needless to say, that in the second and third approaches, random rotation augmentation has been removed. Moreover, in the last approach, no augment level has been used. Like previous part, each method has been trained three times on three pre-defined random seeds. Considering methods in this section do not require labels, all the AffectNet images (more than 1 million) can be suitable for self-supervised training. But all images in AffectNet are not face images. In other words, in 450,000 images with labels, 89,000 of them are faceless. This means that in 550,000 unlabeled images; there are over 100,000 faceless images which can be counted as noises and they are worthless for emotion recognition task as well as self-supervised pre-training. Thus, the 361,000 labeled images which include faces have been used for the training.</p><p>We used the focal loss function for puzzle classification losses due to the fact that some face images are more different from the others. After training based on all the approaches and reaching convergence, the SSHs have been removed from top of the backbone, then its output has been evaluated in a nonlinear way on the eight AffectNet emotions. That is to say, we have trained a neural network with two layers to classify the AffectNet training set on top of the frozen pre-trained backbone of for all four SSL approaches. Results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Additionally, we have fine-tuned all layers with SSL pre-trained backbones on AffectNet (table 3). In both nonlinear evaluation and fine-tuning settings, the no augment level has been selected. On one hand, as table 2 shows, features which have been trained by the SSL methods have been effective compared to the random classification. The puzzling approach and in-painting-pwl have gained more accuracy thought.</p><p>On the other hand, based on the fine-tuning results <ref type="table" target="#tab_3">(table 3)</ref>, we could see a backbone which has been pre-trained by a SSL task like in-painting-pwl could act differently on a downstream task whether doing fine-tuning or feature evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Hybrid multi-task learning</head><p>HMTL of this section has been done in recognizing emotions from single images in categorical and dimensional model. In this paper, the focus of HMTL has been on the categorical model. To show the effectiveness of our proposed HMTL, it has been tested on the circumplex model using 3?3 puzzling auxiliary task, nonetheless.</p><p>The categorical model is considered with four SSL methods:</p><formula xml:id="formula_4">? Puzzling (Equation 1) ? Puzzling-Rotation (Equation 2) ? In-painting-pl (Equation 3) ?</formula><p>In-painting-pwl</p><p>The dimensional mode consists of three methods:</p><formula xml:id="formula_5">? SL regression ? SL regression-categorical (Equation 4) ? SL regression-categorical + SSL puzzling</formula><p>Like previous parts, each method has been trained three times on three fixed random seeds and the best results have been reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Categorical</head><p>In this section, as <ref type="figure">figure 6 and figure 7</ref> are illustrating, the SSHs are put alongside the SH in the training procedures of the categorical model. Here, in all the puzzling SSHs, categorical cross-entropy has been used (Equation 1). We did not use focal loss due to the lower value compare to the cross-entropy loss function. It means that the SSHs loss values would decrease significantly after few epochs and the network could be forced towards the SH. Moreover, in the decoder loss function of the in-painting methods, the error rate has been enlarged with ? coefficient to make it be equal SH in the initial epochs (Equation 3).</p><p>In HMTL approach, due to hardware limitation we have considered random weights with the no and weak augment levels. Despite we have to recognize the location of regions in permuted images and the emotion label at the same time in the training step, we only give the original images to the network without doing permutation or cutout in the validation step, nevertheless. Accordingly, the validation step is only considered for the SH. Results are shown in <ref type="figure" target="#fig_0">figure  10</ref> and <ref type="table">table 5</ref>.</p><p>We need to emphasize that the number of emotions is an important factor in the final result (Accuracy). When the contempt emotion is added to the labels and the number of emotion categories increases from seven to eight, the accuracy decreases 3% to 4%. In this paper, due to more complexity and generalization of eight labels, only the eight categories have been considered.</p><p>Another important point in the use of SSHs is the less tendency of the network to reach overfitting. Because deep networks with a small amount of data tend to overfit, this may indicate that SSHs improving performance on low data. To test this claim, we have trained by the proposed HMTL with a small percentage of the AffectNet training set images. That is to say, eight Ekman labels in the AffectNet training set have been considered and 20% of them have been randomly selected with a fixed random seed for training process. Need to mention that the distribution of emotion labels in the new training set is also similar to the original training set which means the subset is heavily imbalanced. Similarly, the weighted cross entropy loss function has been utilized for the SH. We have compared the results based on the SL and SL+SSL approaches. Due to the effect of augmentation on overfitting, the training process was performed with the no and weak augment levels. It is important to separate our work on low data regimes from semi-supervised learning where the number of labels and images for training varies. In fact, in the semisupervised learning method, a model is first pre-trained on usually large amounts of data and then fine-tuned on a limited number of labels. That is, the number of tags and images in semi-supervised learning is different while it is the same in our work.</p><p>We saw that if the Softmax layer removed from the Equation 3, which acts as a normalizer for the representation, the training process becomes very unstable and sometimes collapses. In particular, this happens in low data regimes.</p><p>The results in <ref type="table" target="#tab_4">table 4</ref> show that when we are using hybrid architecture, the performance increases significantly. To illustrate the important regions in the images, several examples have been selected and the GradCam <ref type="bibr" target="#b46">[46]</ref> method has been performed on them ( <ref type="figure" target="#fig_0">figure 11</ref>). As <ref type="figure" target="#fig_0">figure 11</ref> displays, by using HMTL, more attention is paid to the different parts of the images and consequently, the network has a better power in selecting the relevant features from faces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Dimensional</head><p>In this section, all the images are included in the AffectNet training set with eight Ekman emotions and the "None" label. We have set the alpha coefficient equal to one and used 20 bins to construct categorical labels. <ref type="table" target="#tab_4">Table 4</ref> shows the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Feeding puzzled images or cutout images to the network without adding SSH may have the same effect on the SH performance. To look further more into that, at first, the effect of puzzling sizes has been examined. Next, we have looked at the impact of SSHs on the SH. <ref type="table">Table 5</ref> Comparison between different methods on the AffectNet validation set. All of the puzzling methods are 3?3. All rotations are in eight directions. The pl refers to perceptual loss and the pwl refers to pixel-wise loss. SL+SSL refers to HMTL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difference (%) Accuracy (%) Backbone pre-training weights Augment level</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Effect of puzzle sizes</head><p>In this part, we have showed the effect of puzzle sizes on the SH performance. For this purpose, a no puzzling mode as well as three different puzzle sizes of 2?2, 3?3, and 4?4 have been investigated. In the no puzzling mode, only SH and for the rest, HMTL of puzzling have been considered. In both, the no augment level has been selected and all networks have been trained for 20 epochs. When the puzzle size was set to 4?4, the performance of the emotion recognition head showed a great decline. We think it may be due to the presence of unrelated puzzle pieces in the images to learn emotion labels which distracts the focus of learning procedure to different information. As shown in <ref type="figure" target="#fig_0">figure 12</ref>, in the 4?4 puzzle size, some regions could contain unrelated information for emotion recognition. To look furthermore at this issue, we have given different weights to the SSHs of the puzzled images (Equation 5). </p><p>Where:</p><p>? : weight for the supervised head ? : weight for each self-supervised head</p><p>In fact, in the learning process, these weights are assigned to the SSHs with respect to the shuffling of puzzle pieces. We find these values experimentally.</p><p>The results are shown in <ref type="figure" target="#fig_0">figure 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Effect of self-supervised heads on FER</head><p>Here, the SSHs are removed, but puzzled images are given to the network during the training process. As shown in <ref type="figure" target="#fig_0">figure 14</ref>, during the training, the error rate decreases slower than when SSHs are next to SH, making it harder to recognize emotions. In other words, when we only have the SH head, it takes almost twice as many steps to achieve an accuracy of 50% in the validation set. We observed that as the number of puzzles and augmentation intensity is increased, this difference becomes greater. Similarly, when the average accuracy in SSHs reaches above 80%, the emotion loss function decreases more rapidly. It shows the important role of SSHs in distinguishing different parts of faces before recognizing emotions (table 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Barlow Twins self-supervised learning</head><p>The new wave of self-supervised learning methods (i.e., non-contrastive selfsupervised learning methods) is closing the gap between self-supervised and supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53]</ref>. Barlow twins <ref type="bibr" target="#b7">[8]</ref> is a great example of them which makes the self-supervised pre-training stage to be simpler and easier than other techniques, i.e., BYOL, DINO. In fact, Barlow Twins essentially is a siamese network which tries to find mutual information between two views of an image based on the covariance-correlation matrix via redundancy reduction.</p><p>To demonstrate a comparison, we pre-trained a B0 network using Barlow Twins SSL on the AffectNet training set in similar configuration of section 4.2. In addition, we used the strong augment level twice to create two different views for every sample in the training set. Despite Barlow Twins extracted good features in pre-training stage, it did not perform better than SL when doing finetuning on AffectNet labels. In <ref type="table">table 7</ref> we have compared Barlow Twins features using linear evaluation and fine-tuning on the AffectNet training set and then evaluated on the AffectNet validation set.  <ref type="table">Table 6</ref> The difference between using SSHs. The backbone of all approaches is B0 and All of them are trained from the scratch with the no augment level except the in-painting method which uses cutout. Need to mention that the "SL + in-painting w/o SSL" is identical to the "SL + cutout".  <ref type="table">Table 7</ref> The backbone of all approaches is B0. All of the puzzling methods are 3?3. The In-painting pre-training method is a CNN decoder on top of the backbone which is trained based on the PWL function. In SSL Barlow twins, feature evaluation is a linear classifier on top of the backbone and the rest is based on section 4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Macro F1 Accuracy (%) Augment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Facial emotion recognition benchmarks</head><p>AffectNet is a kind of challengeable dataset due to its different kinds of faces within, as well as its diversity. A network that creates a general representation in the field of FER should be able to identify important features in different benchmarks. Our view for AffectNet is that a network that can extract main features in this dataset, also would act as a domain generalization in emotion recognition and potentially, dealing better with out-of-distribution samples. Therefore, the pre-train network trained by the HMTL 3?3 puzzling approach (weak augmentation and random weights) is selected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1.">AffWild</head><p>AffWild <ref type="bibr" target="#b54">[54]</ref> is an in-the-wild database for analysis of continuous emotion dimensions (e.g., arousal and valence) in which more than 15 hours of data in 300 videos are annotated with regards to arousal and valence values. Also, 252 of the videos are provided as the training set that contains the videos and their corresponding annotation. Since, the competition has been ended, we have selected 16 videos from the training set to create a validation set (appendix part D). During the preprocessing step, we encountered with a problem in annotations which is described in the appendix part D.</p><p>To build our model, similar to the baseline paper <ref type="bibr" target="#b55">[55]</ref>, faces in a video are cropped and then, the sequence of frames is split into window sizes of 32 to train by a bidirectional GRU with two layers of 64 units. At first, the model encodes faces with the B0 backbone to vectors (representation) and next, gives the representation of faces to the GRU network to estimate arousal and valence values for each frame. We initialized the backbone with different weights to compare each method together. To prevent the model from overfitting, we set a dropout layer (0.3) after the encoder model. The results are placed in table 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2.">AFEW-VA</head><p>AFEW-VA <ref type="bibr" target="#b56">[56]</ref> is based on video sequences. First, faces region in the frames are cropped based on the bounding boxes and converted to fix vectors by the frozen pre-trained HMTL 3?3 puzzling and SL models. In this dataset, we consider the task of estimating the values of valence and arousal in each frame according to the Circumplex model. We used a bidirectional LSTM with a single layer with the window size of 32. <ref type="table">Table 9</ref> shows the results. <ref type="table">Table 9</ref> Evaluation of the frozen HMTL 3?3 puzzling and SL features which pre-trained on AffectNet and then evaluated on AFEW-VA dataset using 10-fold cross validation method. The compared method is trained directly on AFEW-VA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Valence (RMSE) Arousal (RMSE) <ref type="bibr" target="#b56">[56]</ref> 0.26 0.22 SL-eval (B0) 0.269 0.252 SL+ SSL puzzling-eval (B0) 0.261 0.243 4.7.3. CK+ <ref type="bibr">CK+ [57]</ref> is based on video sequences for 10 subjects. First, faces region in the frames are cropped based on the bounding boxes and converted to vectors by the frozen pre-trained HMTL 3?3 puzzling and SL models. In this dataset, we consider the task of classifying of frame sequences into seven emotion categories. We used bidirectional LSTM with a single layer. Since there are 10 subjects in CK+, 10-Fold cross validation method has been used which each subject is placed in each Fold. <ref type="table" target="#tab_1">Table 10</ref> shows the results. <ref type="table" target="#tab_1">Table 10</ref> Evaluation of the frozen HMTL 3?3 puzzling and SL features which pre-trained on AffectNet and then evaluated on CK + dataset based on 10-fold cross validation. The two compared methods are trained directly on CK+ dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%) <ref type="bibr" target="#b29">[29]</ref> 98 <ref type="bibr" target="#b58">[58]</ref> 98.06 SL-eval (B0) 97.87 SL+ SSL puzzling (B0)-eval 98.23</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.4.">JAFFE</head><p>For JAFEE samples, each image converted to a vector by the frozen pre-trained HMTL 3?3 puzzling and SL models. Vectors are trained by linear classification into seven categories of emotion. In order to evaluate, like CK+, a 10-Fold cross validation method is used that each subject is placed in each fold. <ref type="table" target="#tab_1">Table  11shows</ref> the result. <ref type="table" target="#tab_1">Table 11</ref> Linear evaluation of the frozen HMTL 3?3 puzzling and SL features which pretrained on AffectNet and then evaluated on JAFFE using 10-fold cross validation evaluation. The compared method is trained directly on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%) <ref type="bibr" target="#b29">[29]</ref> 92.8 SL-eval (B0) 77.6 SL+ SSL puzzling-eval (B0) 79.88</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Hybrid multi-task learning on other facial tasks</head><p>In this section, we have investigated our HMTL approach for two face related tasks: head pose estimation and gender recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1.">Hybrid multi-task learning on fine-grained head pose estimation</head><p>We have investigated our hybrid approach on the head pose estimation problem. The HopeNet method <ref type="bibr" target="#b44">[44]</ref> is considered as a baseline for our comparison. The 300W-LP dataset <ref type="bibr" target="#b59">[59]</ref> is chosen to train all methods. 300W-LP synthesizes faces to generate 61,225 samples across various poses. We have used random zoom, down sampling, image blurring, and cutout augmentation during training. After training, the mean average error of Euler angles on the AFLW2000 dataset <ref type="bibr" target="#b59">[59]</ref> has been reported. Also, just like the baseline, we have removed the images with labels of larger than the absolute value of 99 degrees. In table 12 and figure 15 methods on three SHs (yaw, roll, and pitch) are reported. Furthermore, to show the impact of HMTL, the SSHs are removed from the backbone at the same puzzling configuration. <ref type="bibr">Fig 15</ref> The effect of adding SSHs on the SH based on the average error rate in different head pose estimation methods. We have smoothed the average errors by a Gaussian filter to compare them easier. The raw data is placed in the appendix part E.</p><p>(a (b <ref type="table" target="#tab_1">Table 12</ref> The effect of HMTL on the mean average error of head pose estimation. All methods are trained on the 300W-LP images and evaluated on the AFLW2000. To have a fair comparison, the backbone of all approaches is set ResNet50 like HopeNet. *: The reproduced results of the HopeNet method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2.">Hybrid multi-task learning on gender recognition</head><p>The FairFace <ref type="bibr" target="#b60">[60]</ref> is a face image dataset which is race balanced. It contains 108,501 images, gathered from 7 different race groups, e.g., White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. FairFace includes gender, race and, age labels for each image. We have used our hybrid puzzling approach on to recognize gender labels with the no augment level. The results are placed in table 13. To show the impact of our HMTL approach, like the previous parts, the SSHs are removed from the backbone at the same settings. The results on the AffectNet validation set with different epsilons for FGSM manipulation. All the models are selected based on the lowest loss value on the validation set. All the above methods are trained on the no augment setting. Need to be mentioned that the "SL + in-painting w/o SSL" is identical to the "SL + cutout".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Adversarial robustness</head><p>An adversarial attack consists of subtly modifying an original image which changes are almost undetectable to the human eye. The modified image is called an adversarial image, and when submitted to a classifier, the network is going to misclassify it, while the original one is correctly classified. One of the wellknown and fast methods to create adversarial examples is the Fast gradient sign method (FGSM) which first has been introduced in [61] (Equation 6).</p><formula xml:id="formula_7">= + ? ( ( , )),<label>(6)</label></formula><p>Where:</p><formula xml:id="formula_8">? : adversarial image ? : original image ?</formula><p>: scale of the perturbations, by multiplying them a small float value ? ( , ): the mathematical representation of the loss of the model, where X is the input to the model and y is the true label of the image We have used this method to create adversarial examples when evaluating the models on the AffectNet validation set. For this purpose, we have taken trained models and made adversarial examples with different epsilons and then, evaluated the networks on them. The evaluation is performed on the AffectNet validation set ( <ref type="figure" target="#fig_0">figure 16</ref>). B0 model is chosen for all backbones. All models are trained with the no augment level and dropout using Adabelief optimizer. Although the SL+SSL 3?3 puzzling got better results than the SL, as the puzzle size increased, it has shown more performance loss when using the FGSM attack. In contrast to puzzling, via the in-painting auxiliary head, adversarial robustness is hugely improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this article, three subjects are investigated:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The effect of ImageNet transfer learning and random weights on the FER with different levels of augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The effectiveness of the puzzling, rotating, and in-painting selfsupervision pre-training features on the FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The impact of adding auxiliary self-supervised tasks on the FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ImageNet transfer learning vs training from scratch</head><p>With the advent of deep learning, transfer learning has become extremely popular and has shown good performance in many datasets <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63]</ref>. In this paper, besides the random weights, ImageNet weights are chosen as the first transfer learning option due to the challenging of ImageNet dataset. Also, the effect of different levels of augmentation is examined. The results of this section are summarized as follows:</p><p>? When the data size is small and the augmentation intensity is weak, finetuning on the ImageNet weights helps to increase the accuracy compared to the random weights. ? By intensify the augmentation, training from the scratch have better results than fine-tuning on the ImageNet weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Self-Supervised pre-training</head><p>SSL has shown significant results in recent years and its distance from the SL has been reduced in the computer vision benchmarks. In this paper, two types of SSL methods are investigated on AffectNet, pre-text task and nonecontrastive SSL. Each type is evaluated on the AffectNet benchmark by a classifier (i.e., evaluating the fixed image representations of the frozen backbones). The results of this evaluation showed that the puzzling and inpainting with pixel-wise loss methods provide a better representation for emotion recognition from faces compared to the rotation and puzzling-rotation methods. In the following, the effect of fine-tuning on the pre-train methods has been investigated which has exposed that fine-tuning on puzzling pre-training weights and puzzling-rotation pre-training weights can gain better results versus training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Hybrid multi-task learning</head><p>According to our proposed hypothesis, using HMTL can improve the results for the SHs. This hypothesis is tested on the FER, as well as head pose estimation and gender recognition benchmarks. By adding the auxiliary puzzling or inpainting with perceptual loss heads in the training, the accuracy and error rate are increased which concludes to the state-of-the-arts result on AffectNet.</p><p>The important points in this section are as follows:</p><p>? As puzzle size is increased, the error rate for three tasks of FER, head pose estimation and gender recognition is going to decrease. ? On all the augment levels, reaching to overfitting is significantly reduced. In our opinion, this can at least have two reasons in the puzzling methods. First, when using the multi-tasking learning method, the sensitivity to overfitting is decreased. Second, the puzzling method creates a lot of perturbation. For example, in the 3?3 puzzling approach, each image can be puzzled in 9! ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>As the input images get puzzled, by removing SSHs from the backbone, the performance slightly decreases compared to the classical SL. Additionally, it takes almost twice as many steps to achieve its best result. As the number of puzzle slices gets bigger and the intensity of the augmentation increases, the amounts of steps to achieve convergence increase. At the same configuration, on average, when it switches from the SL to the HMTL, the steps for reaching the best result increased by 30%. Yet, on AffectNet, when 3?3 puzzle size is selected, the error rate is reduced to its best result. Due to the two-stage training of the in-painting method, we cannot compare it directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>In the low data regime, using selected 20% of training data randomly (i.e., with the same distribution of the original), the effect of the hybrid learning is more tangible, especially when least augmentation is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Limitations and future works</head><p>We believe that using auxiliary self-supervised tasks is going to improve the performance on the other domains as well, but we could not test it except three types of fine-grained facial benchmarks. In this article, we encountered problems such as selecting appropriate SSL tasks, adjusting tasks weights in multi-task learning and instability in HMTL training. Items that can impact on overall HMTL performance and need further investigation are listed down below:</p><p>1. How to select best architecture when using self-supervised auxiliary tasks?</p><p>Selecting appropriate SSL tasks alongside SL requires reviewing and testing each of them precisely. As different tasks have different impact on SHs, finding the best backbone architecture is crucial and may show different result <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">How to assign weights for different tasks in MTL setting?</head><p>Tuning weights of self-surprise heads have had an impact on the results. Finding the appropriate weights for each task is a problem we encountered with. For example, in head pose estimation, the decoder error value is very small compared to the SHs based on the HopeNet loss function and when we assign a bigger coefficient for the SSHs, training become very unstable to converge. This issue also happens more often at low data regime for the in-painting with perceptual loss HMTL. One interesting solution for this can be using different losses like weighting by uncertainty <ref type="bibr" target="#b64">[64]</ref> or dynamic weight assigning <ref type="bibr" target="#b65">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">How SSL can be helpful for downstream tasks?</head><p>We have seen some interesting points from our study which shows the difference between using SSL pre-training and adding SSL auxiliary tasks for a downstream supervised task (here emotion recognition). In contrast to our initial thought, when we are pre-training based on a SSL task which can extract good features for the downstream task, that is not necessarily the best option to be used as the fine-tuning on that task as we see the inpainting-pwl results in <ref type="table" target="#tab_2">table 2 and 3</ref>. Similarly, it is observed that if finetuning on a SSL pre-training can improve the performance, it does not mean it is always good to be used as the auxiliary task besides SL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Which SSL task should be used besides SL?</head><p>In AffectNet, the puzzling and the in-painting with perceptual loss help to reduce the error rate, and on the flip side, the in-painting method with pixel wise loss, rotating and rotating-puzzle neither change the results and sometimes make it worse than using classical SL. Fortunately, there are bunch of SSL methods which have been created in recent years. Finding ways of adding those to SL is a good way to show the effectiveness of using HMTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this article, we examined the effect of transfer learning and random weights on AffectNet, and we observed that using random weights could be better than transfer learning when enough augmentation is applied. Moreover, we suggested that using HMTL (i.e., adding auxiliary self-supervised learning tasks to a supervised task) could improve the supervised task representation. SSHs only are used in the training stage and in the testing stage or inference time they are removed from the trained model. To do this, we chose proposed puzzling and in-painting SSL pre-text tasks to add them as the auxiliary heads to the supervised emotion recognition head. Results showed that utilizing proper self-supervised tasks could increase the accuracy of emotion recognition in different benchmarks both at different levels of augmentation and low amounts of data. With two proposed HMTL methods, we reached two new state-of-the-art result on AffectNet in the eight-emotion mode without using additional training data. We also evaluated our method on the head pose estimation and gender recognition datasets which concluded to decreasing in error rate.</p><p>We believe utilizing auxiliary self-supervised task besides a supervised task would impact many fields even beyond face-related problems. This article just scratched the surface and we hope it sets new directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Augmentation details</head><p>Three augmentation levels were defined from nine transformations. <ref type="table" target="#tab_1">Table 1</ref> shows all the three-level settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Label smoothing</head><p>In order to find the effect of label smoothing we did an experiment on AffectNet. We used the down sampling technique to balance the amount samples in the training set classes. In other words, at the end 10 percent of the training set remained. The results are placed in table 2. From the results we could observe that label smoothing does not improve the accuracy in emotion recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture details of self-supervised heads</head><p>In all HMTL methods, the linear classifier of supervised emotion head is placed on top of the backbone except in the in-painting methods. For all HL methods, the emotion head is placed with a linear classifier on top of the backbone's global average pooling output except for in-painting methods.</p><p>Puzzling. For all puzzling heads, we considered a linear classifier on the outputs of global average pooling in the EfficientNet backbone. All SSHs loss weights were set to one.</p><p>Rotation. Like puzzling, the rotation head is considered as a linear classifier on the outputs of global average pooling in EfficientNet backbone and the loss weights to one.</p><p>Puzzling-Rotation. When we have used a linear classifier for each head, we saw a large degradation in emotion head accuracy. So, to prevent it, we add two DNNs with one hidden layer inside with 512 nodes on top of the global average pooling's output, one for puzzling and one for rotation. Then for the puzzling branch, add linear classifiers for each head.</p><p>In-painting. This method includes a deconvolutional decoder head. The decoder is five-block deconvolutional with skip connections. Each block consists of Conv2DTranspose, batch normalization, Conv2DTranspose, batch normalization, and 2 times upsampling layers. In five blocks 256, 128, 64, 32, 16 filters consider and at the end of the last block add 1?1 convolution to reduce channel size to three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. AffWild</head><p>AffWild dataset consists of 300 videos which 252 of them are provided as the training set and the rest as the testing set. However, since the competition is over, we could not access the test annotations which made us to split the training set into training and validation sets. In this paper, we used 16 videos (subjects) as the validation videos: <ref type="bibr">110, 179, 189, 203, 221, 249, 260, 306, 330, 332, 345, 402, 415, 433, 448, 449.</ref> Despite the bounding boxes and landmarks are provided by the dataset, we found them quite inaccurate and noisy, nevertheless. Additionally, in some frames which include more than one face, bounding boxes and landmarks switch wrongly between faces. Since correcting those annotations is going to take a proportion amount of time, engineering and manual works, we guess many researchers leave the issue and use the original labels as they are which by the way, substantially impacts on the final results. In other words, due to the resolution of 640?360, feeding frames in deep neural networks without cropping faces would take huge amounts of computation resources to be feasible. This leads to use provided bounding boxes even they are inaccurate. <ref type="figure" target="#fig_0">Figure 1</ref> AffWild original face annotations. In many frames, bounding boxes and landmarks are wrong.</p><p>Taking into account the above issue, we left the original labels and run one of the best face detection algorithms, RetinaFace, to produce much accurate bounding boxes. In spite of better labels, the problem of more than one face was still existed in the produced labels. We partially solved it by choosing the larger face as the target bounding box. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Head pose estimation validation error rate through training epochs</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1</head><label>1</label><figDesc>Circumplex model with arousal and valence axis<ref type="bibr" target="#b2">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2</head><label>2</label><figDesc>Supervised Learning (SL) procedure with different augmentation levels and weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 4</head><label>4</label><figDesc>Our proposed two stage in-painting with PL pre-task consists of two stages. (a) An image is considered as an "original image". This image is used in the decoder loss function. In the original image, a fixed rectangular area determines as an important part of facial expression. (b) From the original image, a partially cutout image is created to use in training. The decoder head tries to reduce the distance of offline encoder representation between in-painted image and the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>? ( ): offline model which gets an image, and then outputs a feature representation ? I : reconstructed image by the decoder head ? I : original image without cutout</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 5</head><label>5</label><figDesc>Training SL by combining auxiliary SSL task to it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 6 Fig 7</head><label>67</label><figDesc>Training SL by combining auxiliary SSL task to it. Training consists of two stages. First, we train an offline backbone from the scratch under a pre-defined augmentation level. Then, in the second stage, another backbone is going to train under the same settings of the first stage but with an additional decoder head. The decoder creates a reconstructed image which gives to the offline backbone. finally, the representations for both images should be as similar as possible. From another view the pre-trained EfficientNet network can assume as a teacher and the EfficientNet as a student. Converting regression to the categorical-regression of valence and arousal values in SHs. First, the SHs predicts the categorical label and then converts it to the regression output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 8</head><label>8</label><figDesc>AffectNet labels distribution in the training set. The training set is heavily imbalanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Augment</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 9</head><label>9</label><figDesc>Comparing the fine-tuning and the training from scratch modes in different levels of augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig 10</head><label>10</label><figDesc>Comparing all settings of SL and HMTL methods with each other. The in-paintingpl method includes cutout augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig 11 Using</head><label>11</label><figDesc>GradCam method to show important regions on emotion classification. The self-supervised method is the 3?3 puzzling. All samples are randomly selected from the validation set. From top to down the true labels are fear, happy, neutral, sad, disgust,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig 12</head><label>12</label><figDesc>In the 4?4 puzzling setting (a) Every piece has a different emotion information signal. It means that some pieces are less important than others, like corners. (b) So, we adjusted different weights to different parts of the puzzled image before puzzlingFig 13 Effect of increasing of puzzle sizes on emotion recognition in the AffectNet validation set. In the 4?4 puzzling setting, Results show that adding weights to different regions could increase accuracy. All deep models have been trained for 20 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig 14</head><label>14</label><figDesc>Effect of the SSHs on training with 2?2 and 3?3 puzzling images. (a) is loss curve and (b) is the accuracy curve for the AffectNet validation set. Training w/o SSHs are two times slower.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2</head><label>2</label><figDesc>Impact of SSH on training with different puzzling sizes for average error of head pose estimation without smoothing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Results of SL methods with the difference in weights and augmentation level. The backbone of all approaches is B0. The results are reported on the AffectNet validation set.</figDesc><table><row><cell>Approach</cell><cell>Augment level</cell><cell>Pre-training weights</cell><cell>Accuracy (%)</cell></row><row><cell>SL</cell><cell>No</cell><cell>-</cell><cell>57.03</cell></row><row><cell>SL</cell><cell>Weak</cell><cell>-</cell><cell>60.09</cell></row><row><cell>SL</cell><cell>Strong</cell><cell>-</cell><cell>60.34</cell></row><row><cell>SL</cell><cell>No</cell><cell>ImageNet</cell><cell>59.3</cell></row><row><cell>SL</cell><cell>Weak</cell><cell>ImageNet</cell><cell>59.57</cell></row><row><cell>SL</cell><cell>Strong</cell><cell>ImageNet</cell><cell>60.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Nonlinear evaluation on AffectNet. The backbone of all approaches is B0. All of the puzzling methods are 3?3 and all the rotation methods are in eight directions. The Inpainting pre-training method is a CNN decoder on top of the backbone which is trained based on the PWL function.</figDesc><table><row><cell>Backbone pre-training weights</cell><cell>Accuracy (%)</cell><cell>Macro F1</cell></row><row><cell>SSL in-painting-pwl</cell><cell>34.41</cell><cell>0. 3372</cell></row><row><cell>SSL puzzling</cell><cell>32.98</cell><cell>0.3227</cell></row><row><cell>SSL rotation</cell><cell>15.48</cell><cell>0.1070</cell></row><row><cell>SSL puzzling-rotation</cell><cell>30.08</cell><cell>0.2754</cell></row><row><cell>Random initialization</cell><cell>12.5</cell><cell>0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Fine-tuning all layers on the AffectNet training set. The backbone of all approaches is B0. All of the Puzzling methods are 3?3 and all rotation methods are in eight directions.</figDesc><table><row><cell>Methods</cell><cell>Augment level</cell><cell>Pre-training weights</cell><cell>Accuracy (%)</cell></row><row><cell>SL</cell><cell>No</cell><cell>Random initialization</cell><cell>57.03</cell></row><row><cell>SL</cell><cell>No</cell><cell>AffectNet (SSL puzzling)</cell><cell>57.56</cell></row><row><cell>SL</cell><cell>No</cell><cell>AffectNet (SSL rotation)</cell><cell>54.26</cell></row><row><cell>SL</cell><cell>No</cell><cell>AffectNet (SSL puzzling-rotation)</cell><cell>58.86</cell></row><row><cell>SL</cell><cell>No</cell><cell>AffectNet (SSL inpainting-pwl)</cell><cell>51.84</cell></row><row><cell>SL</cell><cell>No</cell><cell>ImageNet (SL)</cell><cell>59.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 Results</head><label>4</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="2">Valence CCC RMSE</cell><cell cols="2">Arousal CCC RMSE</cell><cell cols="2">Mean CCC RMSE</cell></row><row><cell>ResNet50 [11]</cell><cell>0.60</cell><cell>0.37</cell><cell>0.34</cell><cell>0.41</cell><cell>0.47</cell><cell>0.39</cell></row><row><cell>SL (B0)</cell><cell>0.55</cell><cell>0.39</cell><cell>0.33</cell><cell>0.41</cell><cell>0.44</cell><cell>0.43</cell></row><row><cell>SL reg-cat (B0)</cell><cell>0.57</cell><cell>0.38</cell><cell>0.42</cell><cell>0.37</cell><cell>0.49</cell><cell>0.38</cell></row><row><cell>SL reg-cat + SSL puzzling (B0)</cell><cell>0.58</cell><cell>0.38</cell><cell>0.45</cell><cell>0.36</cell><cell>0.51</cell><cell>0.37</cell></row></table><note>of training based on the Russell model on the AffectNet validation set. Here, reg-cat refers to converting regression tasks into the regression-categorical. All methods are trained on random weights.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>The backbone of all methods is B0. HMTL pre-training is based on the 3?3 puzzling method. Both HMTL and SL backbone pre-training are done with the weak augment level on the AffectNet training images. The weak augment level w/o horizontal flip transformation is used to train on AffWild videos.</figDesc><table><row><cell>Backbone weights</cell><cell>Resolution</cell><cell cols="2">Valence MSE CCC</cell><cell cols="2">Arousal MSE CCC</cell><cell cols="2">Mean MSE CCC</cell></row><row><cell>Random</cell><cell>112?112 224?224</cell><cell>0.158 0.268</cell><cell>0.148 0.093</cell><cell>0.199 0.245</cell><cell>0.199 0.102</cell><cell>0.179 0.256</cell><cell>0.118 0.097</cell></row><row><cell>SL pre-training</cell><cell>112?112 224?224</cell><cell>0.165 0.281</cell><cell>0.137 0.11</cell><cell>0.205 0.292</cell><cell>0.091 0.102</cell><cell>0.185 0.286</cell><cell>0.114 0.106</cell></row><row><cell>HMTL pre-training</cell><cell>112?112 224?224</cell><cell>0.194 0.342</cell><cell>0.124 0.09</cell><cell>0.204 0.312</cell><cell>0.094 0.103</cell><cell>0.199 0.327</cell><cell>0.109 0.097</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="2">Gender classification with SL and SL+SSL methods. All methods are trained on</cell></row><row><cell cols="2">the FairFace training set and evaluated on the FairFace validation set. The backbone of all</cell></row><row><cell cols="2">approaches is B0. Need to mention that the "SL + in-painting w/o SSL" is identical to the</cell></row><row><cell>"SL + cutout".</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>SL (35 epochs)</cell><cell>91.51 (?0.02)</cell></row><row><cell>SL + in-painting w/o SSL (40 epoch)</cell><cell>91.59 (?0.02)</cell></row><row><cell>SL + SSL in-painting-pl</cell><cell>92.12 (?0.01)</cell></row><row><cell>SL + 2?2 puzzling w/o SSL (35 epochs)</cell><cell>91.33 (?0.03)</cell></row><row><cell>SL + SSL 2?2 puzzling (25 epochs)</cell><cell>91.98 (?0.01)</cell></row><row><cell>SL + 3?3 puzzling w/o SSL (45 epochs)</cell><cell>91.58 (?0.04)</cell></row><row><cell>SL + SSL 3?3 puzzling (35 epochs)</cell><cell>92.41 (?0.01)</cell></row><row><cell>Fig 16</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1</head><label>1</label><figDesc>Augmentation settings. All transformation's magnitude selected randomly.</figDesc><table><row><cell>Transformation</cell><cell>Level 1 (No augment)</cell><cell>Level 2 (Weak augment)</cell><cell>Level 3 (Strong augment)</cell></row><row><cell>Horizontal flip</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Central Zoom</cell><cell>?</cell><cell>? (0.69 to 100)</cell><cell>? (0.69 to 100)</cell></row><row><cell>Contrast</cell><cell>?</cell><cell>? (0.6 to 1.4)</cell><cell>? (0.6 to 1.4)</cell></row><row><cell>Rotation</cell><cell>?</cell><cell>? (-15? to 15?)</cell><cell>? (-20? to 20?)</cell></row><row><cell>Brightness</cell><cell>?</cell><cell>?</cell><cell>? (-0.05 to 0.05)</cell></row><row><cell>RGB channel swap</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Blurring</cell><cell>?</cell><cell>?</cell><cell>? (1, 3, 5 filter size)</cell></row><row><cell>Gaussian noise</cell><cell>?</cell><cell>?</cell><cell>? (mean=0, var=0.05)</cell></row><row><cell>Cutout</cell><cell>?</cell><cell>?</cell><cell>? (60?60)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2</head><label>2</label><figDesc>Different values of label smoothing on the AffectNet validation set. We used the weak augment level and B0 backbone as well as 0.2 value of dropout.</figDesc><table><row><cell cols="2">Label smoothing Accuracy (%)</cell></row><row><cell>0.0</cell><cell>53.11</cell></row><row><cell>0.1</cell><cell>52.74</cell></row><row><cell>0.2</cell><cell>51.65</cell></row><row><cell>0.3</cell><cell>51.62</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge Hadi Pourmirzaei for designing all pipelines pictures. Moreover, thanks to Cyrus Kazemirad for partially supporting us for hardware resources and also for proofreading and helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A brief review of facial emotion recognition based on visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">401</biblScope>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI 2015 -Proceedings of the 2015 ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<date type="published" when="1980-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reexamining the circumplex model of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Remington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Fabrigar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Visser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">286</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Big Self-Supervised Models are Strong Semi-Supervised Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="84" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deny</forename><forename type="middle">S</forename><surname>Barlow Twins</surname></persName>
		</author>
		<idno>arXiv210303230. 2021</idno>
		<title level="m">Self-Supervised Learning via Redundancy Reduction</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="9650" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<title level="m">When does contrastive visual representation learning work? arXiv Prepr arXiv210505837</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A critical analysis of selfsupervision, or what we can learn from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>arXiv190413132</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Which tasks should be learned together in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR; 2020</title>
		<imprint>
			<biblScope unit="page" from="9120" to="9152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crawshaw</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09796.2020</idno>
		<title level="m">Multi-task learning with deep neural networks: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep structured learning for facial action unit intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3405" to="3419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local learning with deep and handcrafted features for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="64827" to="64863" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Covariance pooling for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pani</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="367" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Using CNN for facial expression recognition: a study of the effects of kernel size and number of filters on accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mittal</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Vis Comput</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="417" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained facial expression analysis using dimensional emotion model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">392</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A deep learning perspective on the origin of facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno>arXiv170501842</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facial expression recognition using enhanced deep 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="30" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="606" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for emotion recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on international conference on multimodal interaction</title>
		<meeting>the 2015 ACM on international conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="467" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A visual attention based ROI detection method for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lossless attention in convolutional networks for facial expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<idno>arXiv200111869. 2020</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep-emotion: Facial expression recognition using attentional convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolrashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3046</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frame attention networks for facial expression recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE international conference on image processing (ICIP). IEEE; 2019</title>
		<imprint>
			<biblScope unit="page" from="3866" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occlusion aware facial expression recognition using CNN with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2439" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition using Convolutional Neural Network on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Control Conference (CCC). IEEE; 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7572" to="7578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Identity-free facial expression recognition using conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing (ICIP). IEEE; 2021</title>
		<imprint>
			<biblScope unit="page" from="1344" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4037" to="58" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A framework for contrastive self-supervised learning and designing a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>arXiv200900104. 2020</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1910" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A systematic review on imbalanced data challenges in machine learning: Applications and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Pannu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Malhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">Q</forename><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR; 2019</title>
		<imprint>
			<biblScope unit="page" from="6105" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep multi-task learning to recognise subtle facial expressions of mental states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Increasingly packing multiple facial-informatics modules in a unified deep-learning model via lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scy</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tst</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 on International Conference on Multimedia Retrieval</title>
		<meeting>the 2019 on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="339" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2074" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">AdaBelief optimizer: Adapting stepsizes by the belief in observed gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Papademetris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18795" to="18806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient facial feature learning with wide ensemble-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5800" to="5809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4057" to="69" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image superresolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid with super resolution for in-the-wild facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="131988" to="2001" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Emerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE/CVF Int Conf Comput Vis</title>
		<meeting>IEEE/CVF Int Conf Comput Vis</meeting>
		<imprint>
			<date type="published" when="2021-04" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>arXiv220203555. 2022</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="907" to="936" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aff-wild: valence and arousal&apos;In-the-Wild&apos;challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">AFEW-VA database for valence and arousal estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="23" to="36" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ieee computer society conference on computer vision and pattern recognitionworkshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Facial motion prior networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Visual Communications and Image Processing</title>
		<imprint>
			<publisher>VCIP). IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>K?rkk?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><forename type="middle">J</forename><surname>Fairface</surname></persName>
		</author>
		<title level="m">Face attribute dataset for balanced race, gender, and age. arXiv Prepr arXiv190804913</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arXiv14126572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A survey on deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition on small datasets using transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on international conference on multimodal interaction</title>
		<meeting>the 2015 ACM on international conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Dynamic multi-task learning for face recognition with facial expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Luqman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-C</forename><surname>Burie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<idno>arXiv191103281</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
