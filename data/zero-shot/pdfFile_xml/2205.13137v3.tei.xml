<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuhk</forename><surname>Mmlab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we propose Mixed and Masked Image Modeling (MixMIM), a simple but efficient MIM method that is applicable to various hierarchical Vision Transformers. Existing MIM methods replace a random subset of input tokens with a special [MASK] symbol and aim at reconstructing original image tokens from the corrupted image. However, we find that using the [MASK] symbol greatly slows down the training and causes training-finetuning inconsistency, due to the large masking ratio (e.g., 40% in BEiT). In contrast, we replace the masked tokens of one image with visible tokens of another image, i.e., creating a mixed image. We then conduct dual reconstruction to reconstruct the original two images from the mixed input, which significantly improves efficiency. While MixMIM can be applied to various architectures, this paper explores a simpler but stronger hierarchical Transformer, and scales with MixMIM-B, -L, and -H. Empirical results demonstrate that MixMIM can learn high-quality visual representations efficiently. Notably, MixMIM-B with 88M parameters achieves 85.1% top-1 accuracy on ImageNet-1K by pretraining for 600 epochs, setting a new record for neural networks with comparable model sizes (e.g., ViT-B) among MIM methods. Besides, its transferring performances on the other 6 datasets show MixMIM has better FLOPs / performance tradeoff than previous MIM methods. Code is available at https://github.com/Sense-X/MixMIM. Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Utilizing unlabeled visual data in self-supervised manners to learn representations is intriguing but challenging. Following BERT <ref type="bibr" target="#b11">[12]</ref> in natural language processing, pre-training with masked image modeling (MIM) shows great success on pretraining visual representations for various downstream vision tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b38">39]</ref>, including image classification <ref type="bibr" target="#b10">[11]</ref>, object detection <ref type="bibr" target="#b26">[27]</ref>, semantic segmentation <ref type="bibr" target="#b50">[51]</ref>, video classification <ref type="bibr" target="#b17">[18]</ref>, and motor control <ref type="bibr" target="#b44">[45]</ref>.</p><p>Existing MIM approaches generally replace a portion of input tokens with a special symbol <ref type="bibr">[MASK]</ref> and aim at recovering the original image patches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref>. However, the use of [MASK] symbol leads to two problems. On the one hand, the [MASK] symbol used in pretraining never appears in the finetuning stage, resulting in pretraining-finetuning inconsistency. On the other hand, the pretrained networks waste much computation on processing the uninformative [MASK] symbols, making the pretraining inefficient. Those problems become severer when a large masking ratio is used <ref type="bibr" target="#b18">[19]</ref>. For example, in BEiT <ref type="bibr" target="#b3">[4]</ref>, a mask ratio of 40% is used in pretraining, i.e., 40% of the input tokens are replaced by the [MASK] symbols. As a result, BEiT needs relatively more epochs (i.e., 800) for pretraining. Besides, as the high mask ratio causes much pretraining-finetuning inconsistency, the performances of BEiT on downstream tasks are limited.  <ref type="figure">Figure 1</ref>: Overview of MixMIM. For pretraining, two images are mixed with a random mixing mask to create a mixed image. MixMIM takes the mixed image as input and reconstructs the two original images. The mixing mask is randomly sampled at the last stage of the encoder and upsampled to match the resolutions of previous stages. Right before decoding, the token embeddings are unmixed and filled with mask tokens for dual reconstruction of the two original images. BEiT <ref type="bibr" target="#b3">[4]</ref> SimMIM <ref type="bibr" target="#b45">[46]</ref> MAE <ref type="bibr" target="#b18">[19]</ref> MixMIM MAE <ref type="bibr" target="#b18">[19]</ref> does not suffer from the above problems by discarding the masked tokens and uses the [MASK] symbols only in the lightweight decoder. However, MAE also breaks the 2D structure of the input image and is therefore not applicable to high-performing visual backbones, such as ConvNets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref> and hierarchical ViT architectures <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref>. How to efficiently pretrain hierarchical ViT architectures, e.g., Swin Transformer <ref type="bibr" target="#b28">[29]</ref> and Pyramid Vision Transformer <ref type="bibr" target="#b40">[41]</ref>, with MIM is still an open question.</p><p>In this work, we propose MixMIM, a generalized MIM method that takes advantage of both BEiT and MAE while avoiding their limitations. Given two random images from the training set, MixMIM creates a mixed image with random mixing masks as input and trains a hierarchical ViT to reconstruct the two original images to learn visual representations. From one image's perspective, instead of replacing the masked tokens of the image with the special [MASK] symbols, the masked tokens are replaced by visible tokens of the other image. MixMIM adopts an encoder-decoder design. The encoder processes the mixed image to obtain hidden representations of the two partially masked images, while the decoder reconstructs the two original images.</p><p>The self-attention operation has a global receptive field and each token can easily interact with others. Fully fusing the two partial images' tokens would also cause pretraining-finetuning discrepancy, as the two groups of tokens might be from images of significantly different appearances at pretraining. To avoid introducing the new discrepancy, MixMIM utilizes a masked attention mechanism to explicitly prevent the interactions of the two groups of tokens. Note that we upsample the mixing mask by nearest interpolation at different stages of the hierarchical encoder to match the resolution of the attention map.</p><p>MixMIM can be widely applied to pretrain different hierarchical ViTs, such as Swin Transformer, PVT, etc. Our MixMIM also explores a lightweight architecture by modifying the Swin Transformer as the encoder for pretraining and knowledge transfer. Thanks to the hierarchical architecture, MixMIM can naturally be applicable to object detection and semantic segmentation. Empirically, with similar model sizes and FLOPs, MixMIM consistently outperforms BEiT <ref type="bibr" target="#b3">[4]</ref> and MAE <ref type="bibr" target="#b18">[19]</ref> on a wide spectrum of downstream tasks, including image classification on iNaturalist <ref type="bibr" target="#b20">[21]</ref> and Places <ref type="bibr" target="#b49">[50]</ref>, object detection and instance segmentation on COCO <ref type="bibr" target="#b26">[27]</ref>, and semantic segmentation on ADE20K <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MixMIM for Masked Image Modeling</head><p>In this section, we introduce the proposed MixMIM for learning visual representations via Masked Image Modeling. We start by briefly revisiting MIM, and then introduce how MixMIM creates training inputs and performs image reconstruction, as well as our proposed architecture. Finally, we present how to reduce the difficulty of the pretext task to improve the pretraining efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Revisit of Masked Image Modeling</head><p>Following BERT <ref type="bibr" target="#b11">[12]</ref>, recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46]</ref> proposed MIM for learning visual representations. Given an input image x, MIM firstly divides the image into non-overlapping image patches x p , following ViT <ref type="bibr" target="#b13">[14]</ref>. It then samples a random mask M to mask a portion of the image patches, and fills the masked place with a special symbol [MASK],x p = x p M + [MASK] (1 ? M ), where denotes element-wise multiplication. The masked imagex p is processed by an image encoder to produce the latent representations, and a lightweight decoder (head) is utilized to reconstruct the original image based on the latent representations. The reconstruction target can be chosen as the normalized raw pixel <ref type="bibr" target="#b18">[19]</ref> or visual tokens <ref type="bibr" target="#b3">[4]</ref>. MIM computes the mean squared error (MSE) between the reconstructed image patches y p and the original image patches x p as the reconstruction loss, L rec = (y p ? x p ) (1 ? M) <ref type="bibr">2 2</ref> , which is only calculated on masked patches <ref type="bibr" target="#b18">[19]</ref>. After pretraining, the decoder is discarded and the encoder is used for further finetuning on downstream visual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixed and Masked Image Modeling (MixMIM)</head><p>While previous MIM works achieved great progress in self-supervised visual representation pretraining, they usually require a large number of epochs for pretraining. One reason is that they waste much computation on processing the uninformative [MASK] symbols. Besides, using the [MASK] symbol also causes pretraining-finetuning inconsistency as those symbols never appear during finetuning. To tackle the issues, we create mixed images as training inputs from pairs of unlabelled training images, which are generated by mixing two groups of visible tokens from two images, for pretraining. The mixed input is processed by MixMIM to reconstruct original images simultaneously. For better transferring the learned multi-scale representations to downstream tasks, we introduce a simple hierarchical vision Transformer as the encoder of the proposed MixMIM. <ref type="figure">Figure 1</ref> illustrates the proposed framework.</p><p>Mixed Training Inputs. Given two sets of image patches {x p 1 , x p 2 } of two random training images, we create a mixed image by filling each spatial location with the corresponding visual token from either x p 1 or x p 2 . The mask notation M is slightly abused and we denote M = 1 as choosing a token from x p 1 and vice versa. The mixed training imagex p m is therefore formulated as:</p><formula xml:id="formula_0">x p m = x p 1 M + x p 2 (1 ? M).<label>(1)</label></formula><p>MixMIM then takes the mixed image as input for reconstruction during pretraining. The mixed image no longer consists of the extra [MASK] symbol and only actual visual tokens, leading to better performances on downstream tasks. The design shares the same principle of MAE <ref type="bibr" target="#b18">[19]</ref>, but our approach does not disassemble the structure of the 2D image, making it more flexible for adapting to various visual backbones, such as PVT <ref type="bibr" target="#b40">[41]</ref> and Swin Transformer <ref type="bibr" target="#b28">[29]</ref>. We can conduct better pretraining based on various hierarchical vision architectures. We follow common practices to use random masking <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46]</ref>, and use masking ratio of 0.5 and masking patch size of 32 ? 32.</p><p>Hierarchical Vision Transformer. For better encoding multi-scale representations, we build the encoder of MixMIM based on Swin Transformer and introduce minor modifications to make the MixMIM encoder simpler but stronger.</p><p>Similar to a regular Swin Transformer, the input is split into non-overlapping image patches and processed by a linear projection layer. Then the image patches added with positional embeddings are processed by 4 stages of Transformer blocks to produce hierarchical representations, with a downsampling layer between every two successive stages. However, we do not use the complicated shifted window for information propagation across non-overlapping windows. Instead, we use a relatively large window size (i.e., 14?14), and only conducts global self-attention in stage-3 and -4 1 . The larger window size brings negligible computation overhead, but can better integrate the global context. As we usually use a large masking ratio in MIM, the global context is important for better reconstruction. We find our simpler architecture has better performance on downstream tasks compared to Swin Transformer.</p><p>We scale the encoder of MixMIM with configuration parameters listed bellow: where C, H, and B denote the channel numbers, numbers of the attention heads, and the numbers of blocks for each stage. The window size is set to 14 ? 14 / 7 ? 7 for stage-1, -2, and -3 / -4 during pretraining. A linear layer is added between the encoder and the decoder to convert the embedding dimension of the encoder's output to 512.</p><formula xml:id="formula_1">?</formula><p>Dual Reconstruction. After encoding the mixed input, we unmix the token embeddings into two groups according to the binary mask M . We then add the [MASK] tokens to reconstruct the original two images from the two groups with the decoder, which has 8 Transformer blocks with an embedding dimension of 512. The loss is therefore set as</p><formula xml:id="formula_2">L rec = (y p 1 ? x p 1 ) (1 ? M) 2 2 + (y p 2 ? x p 2 ) M 2 2 ,<label>(2)</label></formula><p>where y p 1 and y p 2 are the reconstructed images corresponding to x p 1 and x p 2 , respectively. The intuition behind is that as the mixed input contains tokens from two images, we can fully utilize them by reconstructing both images to pretrain the neural network. The computation overhead of reconstructing both images is negligible as the decoder is lightweight. Our approach demonstrates much higher efficiency than previous works, as to be introduced in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reducing the Difficulty of the Pretext Task</head><p>Although the dual reconstruction (Eq. (2)) enjoys several benefits, it is a much more challenging optimization problem due to the mixing of image tokens, which causes slow convergence in our preliminary experiments. To reduce the optimization difficulty, we facilitate the dual reconstruction by exploring the following approaches.</p><p>? Mix embedding: Besides the positional embeddings, we add two mix embeddings to the visual tokens to implicitly differentiate the two mixing groups. Each mix embedding is a vector and is shared for tokens from the same image. In practice, we use different mix embeddings for the 4 stages of the encoder and add the embedding at the beginning of each stage.</p><p>? Masked self-attention: Thanks to the flexibility of the self-attention mechanism, we can also differentiate two mixing images explicitly by masking the self-attention fields. Specifically, for each token, it is only allowed to aggregate information from the tokens belonging to the same image (group). We implement the masked self-attention by reusing the mixing mask M described in section 2.2. Note that we upsample the mask M by nearest interpolation at different stages to match the resolution of the feature map.</p><p>Both approaches do not introduce much computation overhead or extra parameters. The empirical results show that both approaches can help obtain better results. However, the second approach also leads to a faster convergence speed, which is crucial for large-scale pretraining. We use the second approach by default and ablate the design in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>We validate our proposed MixMIM by conducting experiments with pretraining-then-finetuning strategy, following previous practices <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. In particular, we use ImageNet-1K <ref type="bibr" target="#b10">[11]</ref> as the training set for self-supervised pretraining. We then finetune the encoder of MixMIM to downstreams tasks, including image classification on ImageNet-1K <ref type="bibr" target="#b10">[11]</ref>, iNaturalist <ref type="bibr" target="#b20">[21]</ref>, and Places <ref type="bibr" target="#b49">[50]</ref>, object detection and instance segmentation on COCO <ref type="bibr" target="#b26">[27]</ref>, and semantic segmentation on ADE20K <ref type="bibr" target="#b50">[51]</ref>.</p><p>Pretraining on ImageNet-1K. We conduct self-supervised pretraining on ImageNet-1K <ref type="bibr" target="#b10">[11]</ref>. By default, we pretrain for 600 epochs with the input size of 224 ? 224. The window size is set as </p><p>and mIoU on ADE20K. All results are from various self-supervised pretraining methods followed by supervised finetuning. All entries on COCO <ref type="bibr" target="#b26">[27]</ref> use Mask RCNN <ref type="bibr" target="#b19">[20]</ref> framework. All entries on ADE20K <ref type="bibr" target="#b50">[51]</ref> use UperNet <ref type="bibr" target="#b43">[44]</ref> framework. Note that this is a system-level comparison.</p><p>14 ? 14 for the first 3 stages and 7 ? 7 for stage-4. Note that when using Swin Transformer <ref type="bibr" target="#b28">[29]</ref> as the encoder, we set the window size following its original design. The patch size of the mask is set to 32 ? 32 as our hierarchical encoder eventually downsamples the input to <ref type="bibr">1 32</ref> of the input resolution. We follow all other pretraining hyperparameters of those in MAE <ref type="bibr" target="#b18">[19]</ref> for a fair comparison.</p><p>Finetuning for image classification. We conduct supervised finetuning with the pretrained encoder of our MixMIM on image classification tasks, including ImageNet-1K <ref type="bibr" target="#b10">[11]</ref>, Places <ref type="bibr" target="#b49">[50]</ref>, and iNaturalist <ref type="bibr" target="#b20">[21]</ref>. We follow previous practices <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> and use a layer-wise learning-rate decay strategy <ref type="bibr" target="#b8">[9]</ref> for finetuning. We sweep the decay rate in {0.7, 0.75, 0.8}, and report the best performing results. We use drop path regularization <ref type="bibr" target="#b21">[22]</ref>, and set the drop rate to 0.1/0.2/0.3 for MixMIM-B/L/H, respectively. We finetune MixMIM-B/L/H for 100/50/50 epochs following MAE <ref type="bibr" target="#b18">[19]</ref>.</p><p>Finetuning on COCO. We perform supervised finetuning on COCO <ref type="bibr" target="#b26">[27]</ref> for object detection and instance segmentation using the Mask RCNN framework <ref type="bibr" target="#b19">[20]</ref> with our pretrained encoder as the backbone. We reuse the training setup in MAE <ref type="bibr" target="#b18">[19]</ref> for a fair comparison. We change the window size to 16 ? 16 for being divisible by the input 1024 ? 1024 resolution. Besides, we change the window sizes of the 6th-, 12th-, and 18th-block in stage-3 to 32?32 for cross-window interactions following the previous practice <ref type="bibr" target="#b24">[25]</ref>.</p><p>Finetuning on ADE20K. We perform supervised finetuning on ADE20K <ref type="bibr" target="#b50">[51]</ref> for semantic segmentation. We use the UperNet <ref type="bibr" target="#b43">[44]</ref> framework with our pretrained encoder as its backbone. We also change the window size as mentioned above. We reuse the training setup in BEiT <ref type="bibr" target="#b3">[4]</ref> for a fair comparison.</p><p>We include more details about pretraining and finetuning in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Main Results</head><p>In this section, we compare our MixMIM to prior arts on various visual benchmarks. We present the results on ImageNet-1K in section 4.1, and then show the results on the other 6 benchmarks in section 4.2. Note that all the results of MixMIM are obtained by conducting supervised finetuning of the encoder with self-supervised pretraining, without extra intermediate finetuning <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on ImageNet-1K</head><p>Comparisons with other MIM approaches. <ref type="table" target="#tab_2">Table 2</ref> presents the comparison between MixMIM and state-of-the-art Masked Image Modeling (MIM) works. Our MixMIM can obtain higher accuracy while requiring fewer epochs for pretraining. In particular, our MixMIM-B achieves 84.6% top-1 accuracy with 300 epochs of pretraining, 1.4% better than BEiT <ref type="bibr" target="#b3">[4]</ref> with 62.5% fewer epochs for pretraining. Besides, our MixMIM also enjoys longer pretraining as previous methods <ref type="bibr" target="#b18">[19]</ref>. Specifically, we obtain strong 85.1% top-1 accuracy with only 600 epochs of pretraining, setting a new record for models that has a similar model size to ViT-B <ref type="bibr" target="#b13">[14]</ref> among MIM methods.  While previous works design various reconstruction targets to speed up the pretraining process <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref>, our MixMIM reconstructs simply normalized pixels <ref type="bibr" target="#b18">[19]</ref> and demonstrates strong pretraining efficiency. Compared to MaskFeat <ref type="bibr" target="#b41">[42]</ref>, our MixMIM obtains +1% better accuracy with the same pretraining epochs. PeCo <ref type="bibr" target="#b12">[13]</ref> proposed to reconstruct the perceptual codebook from a pretrained MoCo v3 network <ref type="bibr" target="#b7">[8]</ref> and can achieve better performance to some extent. In comparison, our MixMIM obtains even better performance (+0.6%) than PeCo with less pretraining epochs (-200). The superior performance of MixMIM comes from our mixed pretraining as well as the hierarchical vision transformer. However, SimMIM <ref type="bibr" target="#b45">[46]</ref> also utilizes a hierarchical Swin Transformer <ref type="bibr" target="#b28">[29]</ref>, but its performance is worse (-1.1%) than MixMIM with more pretraining epochs (+200).</p><p>We test with scaling MixMIM up to 600M parameters, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (Left). MixMIM has better FLOPs vs. accuracy tradeoff than other approaches. In particular, MixMIM-L and -H achieve 85.8% and 86.8% top-1 accuracy, respectively, being comparable with MAE-L (85.9%) and -H (86.9%) but requiring fewer FLOPs for inference (-40% for -L and -30% for -H).</p><p>Integrating MixMIM to other backbones. While previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> may be restricted to a specific architecture, our proposed MixMIM can generalize to various visual backbones, including plain ViT <ref type="bibr" target="#b13">[14]</ref>, Swin Transformer <ref type="bibr" target="#b28">[29]</ref>, and PVT <ref type="bibr" target="#b40">[41]</ref>. We conduct a thorough comparison with other MIM approaches with fixed encoders. As shown in <ref type="table" target="#tab_3">Table 3</ref>, MixMIM consumes the same or fewer epochs for pretraining but obtains consistently better performance on hierarchical ViTs. In particular, our MixMIM achieves 84.4% top-1 accuracy with Swin-B, +0.4% better than SimMIM <ref type="bibr" target="#b45">[46]</ref> while requiring 200 fewer epochs for pretraining. With Swin-L, our MixMIM obtains 85.7% top-1 accuracy with 600 epochs of pretraining and 50 epochs of finetuning, showing higher efficiency than SimMIM. Besides, our MixMIM achieves 83.2% top-1 accuracy with PVT-L <ref type="bibr" target="#b40">[41]</ref>, improving the supervised baseline by a non-trivial margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results of Transferring to Downstream Tasks</head><p>To further demonstrate the effectiveness of the visual representations learned by MixMIM, we transfer MixMIM to various visual benchmarks with settings described in section 3.</p><p>Object detection and instance segmentation. We show the results on COCO <ref type="bibr" target="#b26">[27]</ref> in <ref type="table" target="#tab_4">Table 4</ref>. By pretraining for 600 epochs on ImageNet-1K, our MixMIM-B achieves 52.2 AP box and 46.5 AP mask , surpassing previous self-supervised approaches with fewer FLOPs and parameters. Compared to BEiT <ref type="bibr" target="#b3">[4]</ref>, our MixMIM obtains higher AP box (+2.4) and AP mask (+2.1) with less pretraining epochs <ref type="bibr">(-200)</ref>. Note that our hierarchical backbone can naturally be transferred to object detection without re-designing <ref type="bibr" target="#b24">[25]</ref> network architectures such as FPN <ref type="bibr" target="#b25">[26]</ref>.</p><p>Our MixMIM can also scale up to larger models in object detection task and obtains better performance. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our MixMIM-L achieves 54.1 AP box (48.1 AP mask ), +0.8 (+1.0) better than BEiT while requiring 200 less epochs for pretraining and 41% less FLOPs for inference. We further evaluate the tradeoff of FLOPs vs. AP box in <ref type="figure" target="#fig_1">Figure 2</ref> (Middle). We also found that MixMIM outperforms other approaches by large margins.</p><p>Semantic segmentation. <ref type="table" target="#tab_4">Table 4</ref> also presents the results of MixMIM on ADE20K <ref type="bibr" target="#b50">[51]</ref>. We compare its Mean Intersection over Union (mIoU) on ADE20K with other self-supervised approaches. Our MixMIM-B achieves 50.3 mIoU, +3.2 better than BEiT while requiring only half of FLOPs for inference. We note that the performance is not saturated and can be improved for +0.7 mIoU with 900 epochs of pretraining, as shown in <ref type="table" target="#tab_9">Table 9</ref>. Besides, we obtain 53.8 mIoU by scaling up the model to MixMIM-L. Thanks to the hierarchical design, our MixMIM consumes much fewer FLOPs for inference compared to other approaches with plain ViT. In <ref type="figure" target="#fig_1">Figure 2</ref> (Right), our MixMIM outperforms other approaches by large margins.</p><p>Image classification. We further transfer MixMIM to other 4 classification datasets and show the results in <ref type="table" target="#tab_5">Table 5</ref>. These datasets are challenging as the accuracies are relatively low, e.g., 57.9% top-1 accuracy on Places365 <ref type="bibr" target="#b49">[50]</ref> for MAE-B <ref type="bibr" target="#b18">[19]</ref>. However, our MixMIM can still outperform previous self-supervised approaches. In particular, our MixMIM-B has an average +2.4% performance gain compared to MAE-B. Besides, our MixMIM-L has an average +1.3% performance gain over MAE-L while requiring only 58% FLOPs for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>In this section, we ablate the key designs of MixMIM and report the transferring results of each ablation. By default, we report the top-1 accuracy on ImageNet-1K <ref type="bibr" target="#b10">[11]</ref> and mIoU on ADE20K <ref type="bibr" target="#b50">[51]</ref>. Unless otherwise specified, we pretrain MixMIM-B for 300 epochs by default.      Content to filling. While MixMIM default fills the masked tokens of one image with visible tokens from another image, we also explore more design choices. Specifically, we try to fill the masked tokens with the following contents.</p><p>? Zero: Filling the masked tokens with zeros. This approach causes serious mismatches between the masked tokens and the visible tokens.</p><p>? Learnable: Following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref>, we fill the masked tokens with a shared learnable token. The difference between the zero approach is that learnable tokens can be adapted to visible tokens to match the distribution of the training set.</p><p>? Shuffle: We randomly shuffle the masked tokens, and then fill the masked locations with the shuffled tokens.We note that this approach is similar to solving jigsaw puzzles <ref type="bibr" target="#b32">[33]</ref> with the difference that we need to fully regress the pixels.</p><p>? Zoomin: We zoom in the original image and randomly crop an image patch with the size of the original image. We then fill the masked tokens with tokens from the cropped image. This approach also provides masking tokens that are similar to visible ones but is harder than the shuffle approach.</p><p>We visualize the four approaches in <ref type="figure" target="#fig_2">Figure 3</ref>. We compare the performances of the four approaches in <ref type="table" target="#tab_6">Table 6</ref>. Our default choice Mix performs best in terms of accuracy on ImageNet-1K and mIoU on ADE20K. We find the learnable approach has a similar performance on ImageNet-1K but better performance on ADE20K compared to Zero. We hypothesize that the training-finetuning inconsistency has a larger impact on tasks without a lot of labeled images. The shuffle and zoomin approaches perform much worse than other approaches. Those two strategies cause easier pretext task and have lower pretraining loss. However, the learned representation quality is lower.</p><p>Dual reconstruction. We ablate the proposed dual reconstruction in <ref type="table" target="#tab_7">Table 7</ref>. We find that dual reconstruction greatly boosts the performance on downstream tasks. The performance gap is larger on ADE20K, where we observe +2.6 mIoU with the dual reconstruction. Note that the computation overhead of dual reconstruction is negligible as the decoder is lightweight.</p><p>Masking ratio and masking type. While previous mixing-based works, such as CutMix <ref type="bibr" target="#b46">[47]</ref>, prefer to mask a block of the input image, we follow common practices in Masked Image Modeling (MIM) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46]</ref>, and perform random masking. While our default masking ratio is 0.5, we also explore different masking ratios. We implement by adding a uniformly random coefficient to the default ratio 0.5. We ablate the masking strategy in <ref type="table" target="#tab_8">Table 8</ref>. Compared to block masking, using random masking obtains better performance on downstream tasks (+1.1% accuracy on ImageNet-1K and +2.8 mIoU on ADE20K). We find using block masking tends to produce much larger pretraining loss, indicating the pretext task is harder to learn. Besides, we find using a masking ratio of 0.5 performs best.</p><p>Pretraining epochs. Thanks to the dual reconstruction, our MixMIM can achieve strong performance with few pretraining epochs. We ablate the pretraining epochs in <ref type="table" target="#tab_9">Table 9</ref>. We find that the mIoU on ADE20K can be further improved with more pretraining epochs. We achieve 51.0 mIoU with 900 epochs of pretraining. In contrast, the accuracy on ImageNet-1K does not improve after 600 epochs. It might be because the finetuning on ImageNet-1K is more adequate.</p><p>Reducing the difficulty. As stated in section 2.3, directly performing reconstruction with the mixed input is a much more challenging optimization problem. Hence, we provide two practical approaches to reduce the difficulty. We ablate the design in <ref type="table" target="#tab_1">Table 10</ref>. We note that all the approaches do not bring nonnegligible FLOPs or parameters. We find that the performance of the approach without unmixing is worst even when trained for more epochs. In contrast, using mix embedding alleviates the problem and improves its performance with longer pretraining. However, using masked self-attention in our final solution is much more efficient, and has better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>Inspired by BERT <ref type="bibr" target="#b11">[12]</ref> for Masked Language Modeling, Masked Image Modeling(MIM) becomes a popular pretext task for visual representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2]</ref>. MIM aims to reconstruct the masked tokens from a corrupted input. Current MIM approaches can be divided into two categories by the reconstruction targets. SimMIM <ref type="bibr" target="#b45">[46]</ref> points out that raw pixel values of the randomly masked patches are a good reconstruction target and a lightweight prediction head is sufficient for pretraining. Different from SimMIM, MAE <ref type="bibr" target="#b18">[19]</ref> only takes the visible patches as the input of the encoder. Mask tokens are added in the middle of the encoder and the decoder. Such an asymmetric design greatly reduces the computation overhead of the encoder. To further enhance the feature extraction capability of the encoder, CAE <ref type="bibr" target="#b6">[7]</ref> separates the encoder and decoder explicitly by adding a feature alignment module in the middle of them. Jean-Baptiste et al. <ref type="bibr" target="#b0">[1]</ref> proposes to learn representations by reconstructing original videos from synthetically mixed one.</p><p>Instead of building the reconstruction target manually, using a network to generate the reconstruction target has also been widely applied. In such works, an image tokenizer is used to transform an image into visual tokens. BEiT <ref type="bibr" target="#b3">[4]</ref> utilizes a pretrained discrete VAE (dVAE) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref> as the tokenizer. However, the originally used MSE loss in dVAE is insufficient to force the tokenizer to capture high-level semantics. PeCo <ref type="bibr" target="#b12">[13]</ref> finds that applying perceptual similarity loss on the training of dVAE can drive the tokenizer to generate better semantic visual tokens, which helps pretraining. Moreover, the tokenizer in BEiT <ref type="bibr" target="#b3">[4]</ref> needs to be offline pretrained, which limits the model's adaption. To address the problem, iBOT <ref type="bibr" target="#b51">[52]</ref> proposed to use an online tokenizer to generate the visual tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusion</head><p>This paper proposes Mixed and Masked Image Modeling (MixMIM) for efficient visual representation learning. Our MixMIM uses a mixed input created by mixing two images with random masks, and applies dual reconstruction to recover the original two images from the mixed input. We further explore a simpler but stronger hierarchical Transformer for efficient learning. Empirical results on 7 visual benchmarks demonstrate MixMIM can learn high-quality visual representations efficiently and has better FLOPs / performance tradeoff than previous MIM works. While this paper focuses on the vision field, we hope our work will inspire future works in other modalities, such as text and audio.</p><p>Limitation and Potential Negative Societal Impact. While our MixMIM enables efficient visual representation learning, it may be further improved by mixing with priors, such as only mixing the salient regions <ref type="bibr" target="#b22">[23]</ref>. This work does not have a direct negative societal impact. However, we should be aware that our pretrained neural network can be further finetuned on applications that involving discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Hyperparameters of Pretraining and Finetuning</head><p>We include details about the hyperparameters for reimplementation.</p><p>Pretraining. The default setting is in <ref type="table" target="#tab_1">Table 11</ref>. We use xavier_uniform <ref type="bibr" target="#b15">[16]</ref> to initialize all Transformer blocks following original ViT <ref type="bibr" target="#b13">[14]</ref>. We by default use batch size of 1024 and scale the learning rate with linear rule <ref type="bibr" target="#b16">[17]</ref>: lr=base_lr ? batch_size / 256. Finetuning on ImageNet-1K. The default setting is in <ref type="table" target="#tab_1">Table 12</ref>. We use layer-wise learning rate decay following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. The decay ratio is swept in {0.7, 0.75, 0.8}, and we find 0.7 performs best.  <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> 0.7 batch size 1024 weight decay 0.05 optimizer momentum ? 1 , ? 2 =0.9,0.999 learning rate schedule cosine decay warmup epochs 5 training epochs 100 (B), 50 (L/H) augmentation RandAug(9, 0.5) [10] LabelSmooth <ref type="bibr" target="#b36">[37]</ref> 0.1 Mixup <ref type="bibr" target="#b48">[49]</ref> 0.8 CutMix <ref type="bibr" target="#b47">[48]</ref> 1.0 drop path <ref type="bibr" target="#b21">[22]</ref> 0.1 (B), 0.2 (L), 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">(H)</head><p>Finetuning on other classification datasets. We reuse the setting in <ref type="table" target="#tab_1">Table 12</ref>. We adjust the drop path rate for each dataset.</p><p>Finetuning on COCO. We use the Mask RCNN <ref type="bibr" target="#b19">[20]</ref> framework with the encoder of MixMIM as its backbone. We follow the training setting in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19]</ref>. In particular, we use large-scale jitter <ref type="bibr" target="#b14">[15]</ref> augmentation with 1024?1024 resolution and [0.1, 2.0] scale range. We use step learning rate schedule with 0.25 epochs of warmup. We finetune MixMIM-B/-L for 55/80 epochs. We use a layer-wise learning rate and set the decay ratio to 0.85/0.9 for MixMIM-B/-L.</p><p>Finetuning on ADE20K. We use the UperNet <ref type="bibr" target="#b43">[44]</ref> framework with the encoder of MixMIM as its backbone. We finetune for 16K iterations with a batch size of 16. We use the layer-wise learning rate and set the decay ratio to 0.85/0.9 for MixMIM-B/-L. We adopt others settings from BEiT <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Extend to ConvNets</head><p>While our MixMIM uses a hierarchical Transformer as the encoder, we also explore popular ConvNets.</p><p>In particular, we use ResNet50x3 and ResNet101x3 as the encoder and compare the finetuning results on ImageNet-1K with BiT <ref type="bibr" target="#b23">[24]</ref>. To reduce the difficulty of the pretext task, we extend the idea of partial convoluation <ref type="bibr" target="#b27">[28]</ref> and propose a mixed version, as illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>.  We compare the results in <ref type="table" target="#tab_1">Table 13</ref>. In particular, our MixMIM outperforms BiT-S by a large margin with half the input size. We note that BiT-M achieves better results by pretraining with 10? larger dataset ImageNet-21K. We believe the results of MixMIM can be further improved by using much larger datasets as shown in <ref type="bibr" target="#b3">[4]</ref>, and we leave it as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>MixMIM-B: C = (128, 256, 512, 1024), H = (4, 8, 16, 32), B = (2, 2, 18, 2), ? MixMIM-L: C = (192, 384, 768, 1536), H = (6, 12, 24, 48), B = (2, 2, 18, 2), ? MixMIM-H: C = (352, 704, 1408, 2816), H = (11, 22, 44, 88), B = (2, 2, 18, 2),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Tradeoffs of FLOPs vs. (Left) top-1 accuracy on ImageNet-1K, (Middle) AP box on COCO,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples images for different filling contents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Mixed convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Key differences between MixMIM and related works.</figDesc><table /><note>Approach Pretraining efficient Pretraining-finetuning consistent Applicable to hierarchical ViT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art MIM methods. All entries are results of base-level models and have comparable model sizes. We report the finetuning accuracy on ImageNet-1K. The FLOPs and Params. are calculated for the encoders. ? denotes the number of epochs is based on JFT<ref type="bibr" target="#b35">[36]</ref>. ? results are from<ref type="bibr" target="#b42">[43]</ref>.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="5">FLOPs (G) Param. (M) Supervision Pretrain Epochs Top-1 Acc.</cell></row><row><cell>ViT [14]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>RGB</cell><cell>14  ?</cell><cell>79.9</cell></row><row><cell>BEiT [4]</cell><cell>ViT-B</cell><cell>17.6</cell><cell>87</cell><cell>DALL-E</cell><cell>800</cell><cell>83.2</cell></row><row><cell>CAE [7]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>DALL-E</cell><cell>800</cell><cell>83.6</cell></row><row><cell>MAE [19]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>RGB</cell><cell>1600</cell><cell>83.6</cell></row><row><cell>MaskFeat [42]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>HOG</cell><cell>300</cell><cell>83.6</cell></row><row><cell>data2vec [3]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>Feature</cell><cell>800</cell><cell>84.2</cell></row><row><cell>iBOT [52]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>Momentum</cell><cell>1600</cell><cell>84.0</cell></row><row><cell>PeCo [13]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>MoCo v3</cell><cell>800</cell><cell>84.5</cell></row><row><cell>SimMIM [46]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>RGB</cell><cell>800</cell><cell>83.8</cell></row><row><cell>SimMIM [46]</cell><cell>Swin-B</cell><cell>15.6</cell><cell>88</cell><cell>RGB</cell><cell>800</cell><cell>84.0</cell></row><row><cell>MixMIM</cell><cell>MixMIM-B</cell><cell>16.3</cell><cell>88</cell><cell>RGB</cell><cell>300</cell><cell>84.6</cell></row><row><cell>MixMIM</cell><cell>MixMIM-B</cell><cell>16.3</cell><cell>88</cell><cell>RGB</cell><cell>600</cell><cell>85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Supervised [40]</cell><cell>ViT-B</cell><cell>-</cell><cell>300</cell><cell>81.8</cell></row><row><cell>MAE [19]</cell><cell>ViT-B</cell><cell>400</cell><cell>100</cell><cell>83.3</cell></row><row><cell>BEiT [4]</cell><cell>ViT-B</cell><cell>300</cell><cell>100</cell><cell>82.8</cell></row><row><cell>MixMIM</cell><cell>ViT-B</cell><cell>300</cell><cell>100</cell><cell>83.2</cell></row><row><cell>Supervised [19]</cell><cell>ViT-L</cell><cell>-</cell><cell>200</cell><cell>82.5</cell></row><row><cell>MAE [19]</cell><cell>ViT-L</cell><cell>300</cell><cell>50</cell><cell>84.2</cell></row><row><cell>MixMIM</cell><cell>ViT-L</cell><cell>300</cell><cell>50</cell><cell>85.0</cell></row><row><cell>Supervised [29]</cell><cell>Swin-B</cell><cell>-</cell><cell>300</cell><cell>83.5</cell></row><row><cell>SimMIM [46]</cell><cell>Swin-B</cell><cell>800</cell><cell>100</cell><cell>84.0</cell></row><row><cell>MixMIM</cell><cell>Swin-B</cell><cell>600</cell><cell>100</cell><cell>84.4</cell></row><row><cell>Supervised [46]</cell><cell>Swin-L</cell><cell>-</cell><cell>300</cell><cell>83.5</cell></row><row><cell>SimMIM [46]</cell><cell>Swin-L</cell><cell>800</cell><cell>100</cell><cell>85.4</cell></row><row><cell>MixMIM</cell><cell>Swin-L</cell><cell>600</cell><cell>50</cell><cell>85.7</cell></row><row><cell>Supervised [41]</cell><cell>PVT-L</cell><cell>-</cell><cell>300</cell><cell>81.7</cell></row><row><cell>MixMIM</cell><cell>PVT-L</cell><cell>300</cell><cell>50</cell><cell>83.2</cell></row></table><note>Comparison with state-of-the-art MIM works using the same encoder. We report the finetuning accuracy on ImageNet-1K.Pretrain Method Backbone Pretrain Epochs Finetune Epochs Top-1 Acc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with other self-supervised approaches on COCO and ADK20K. We report AP box and AP mask on COCO, and mIoU on ADE20K. The results of BEiT and MoCo v3 are from MAE<ref type="bibr" target="#b18">[19]</ref>. The results of EsViT are from<ref type="bibr" target="#b42">[43]</ref>. ? denotes using supervised finetuning on ImageNet.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Pretrain FLOPs Params. Epochs (G) (M)</cell><cell cols="2">COCO AP box AP mask</cell><cell cols="3">FLOPs Params. ADE20K (G) (M) mIoU</cell></row><row><cell>MoCo v3 [8]</cell><cell>ViT-B</cell><cell>300</cell><cell>853</cell><cell>116</cell><cell>47.9</cell><cell>42.9</cell><cell>606</cell><cell>164</cell><cell>47.3</cell></row><row><cell>BEiT [4]</cell><cell>ViT-B</cell><cell>800</cell><cell>853</cell><cell>116</cell><cell>49.8</cell><cell>44.4</cell><cell>606</cell><cell>164</cell><cell>47.1</cell></row><row><cell>MAE [19]</cell><cell>ViT-B</cell><cell>1600</cell><cell>853</cell><cell>116</cell><cell>50.3</cell><cell>44.9</cell><cell>606</cell><cell>164</cell><cell>48.1</cell></row><row><cell>iBOT [52]</cell><cell>ViT-B</cell><cell>1600</cell><cell>-</cell><cell>-</cell><cell>51.2</cell><cell>44.2</cell><cell>-</cell><cell>-</cell><cell>50.0</cell></row><row><cell>SimMIM [46]</cell><cell>Swin-B</cell><cell>800</cell><cell>-</cell><cell>-</cell><cell>52.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.8  ?</cell></row><row><cell>EsViT [46]</cell><cell>Swin-B</cell><cell>300</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.3</cell></row><row><cell>MixMIM</cell><cell>MixMIM-B</cell><cell>600</cell><cell>701</cell><cell>110</cell><cell>52.2</cell><cell>46.5</cell><cell>302</cell><cell>122</cell><cell>50.3</cell></row><row><cell>MoCo v3 [8]</cell><cell>ViT-B</cell><cell>300</cell><cell>1907</cell><cell>339</cell><cell>49.3</cell><cell>43.9</cell><cell>877</cell><cell>392</cell><cell>49.1</cell></row><row><cell>BEiT [4]</cell><cell>ViT-B</cell><cell>800</cell><cell>1907</cell><cell>339</cell><cell>53.3</cell><cell>47.1</cell><cell>877</cell><cell>392</cell><cell>53.3</cell></row><row><cell>MAE [19]</cell><cell>ViT-L</cell><cell>1600</cell><cell>1907</cell><cell>339</cell><cell>53.3</cell><cell>47.2</cell><cell>877</cell><cell>392</cell><cell>53.6</cell></row><row><cell>SimMIM [46]</cell><cell>Swin-L</cell><cell>800</cell><cell>-</cell><cell>-</cell><cell>53.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.5  ?</cell></row><row><cell>MixMIM</cell><cell>MixMIM-L</cell><cell>600</cell><cell>1119</cell><cell>319</cell><cell>54.1</cell><cell>48.1</cell><cell>460</cell><cell>236</cell><cell>53.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with other self-supervised approaches on classification tasks. We report the top-1 accuracy and average accuracy of all datasets.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="7">FLOPs (G) Params. (M) INat2018 INat2019 Places205 Places365 Average</cell></row><row><cell>DINO [5]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>72.6</cell><cell>78.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MAE [19]</cell><cell>ViT-B</cell><cell>17.5</cell><cell>86</cell><cell>75.4</cell><cell>80.5</cell><cell>63.9</cell><cell>57.9</cell><cell>69.4</cell></row><row><cell>MixMIM</cell><cell>MixMIM-B</cell><cell>16.3</cell><cell>88</cell><cell>77.5</cell><cell>82.6</cell><cell>68.3</cell><cell>58.9</cell><cell>71.8</cell></row><row><cell>MAE [19]</cell><cell>ViT-L</cell><cell>61.3</cell><cell>304</cell><cell>80.1</cell><cell>83.4</cell><cell>65.8</cell><cell>59.4</cell><cell>72.1</cell></row><row><cell>MixMIM</cell><cell>MixMIM-L</cell><cell>35.8</cell><cell>235</cell><cell>80.3</cell><cell>83.9</cell><cell>69.3</cell><cell>60.3</cell><cell>73.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Filling content.</figDesc><table><row><cell>Type</cell><cell cols="2">Top-1 Acc. mIoU</cell></row><row><cell>Mix</cell><cell>84.6</cell><cell>49.9</cell></row><row><cell>Zero</cell><cell>84.1</cell><cell>48.0</cell></row><row><cell>Learnable</cell><cell>84.1</cell><cell>48.9</cell></row><row><cell>Shuffle</cell><cell>82.6</cell><cell>43.0</cell></row><row><cell>Zoomin</cell><cell>83.5</cell><cell>44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Dual reconstruction.</figDesc><table><row><cell cols="2">Dual Top-1 Acc. mIoU</cell></row><row><cell>84.6</cell><cell>49.9</cell></row><row><cell>84.0</cell><cell>47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Masking strategy.</figDesc><table><row><cell>Type (ratio)</cell><cell cols="2">Top-1 Acc. mIoU</cell></row><row><cell>Random (0.5)</cell><cell>84.6</cell><cell>49.9</cell></row><row><cell>Block (0.5)</cell><cell>83.5</cell><cell>47.1</cell></row><row><cell>Random (0.4-0.6)</cell><cell>84.5</cell><cell>49.0</cell></row><row><cell>Random (0.3-0.7)</cell><cell>84.5</cell><cell>48.8</cell></row><row><cell>Random (0.2-0.8)</cell><cell>84.4</cell><cell>48.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Pretraining epochs.</figDesc><table><row><cell cols="3"># Epochs Top-1 Acc. mIoU</cell></row><row><cell>300</cell><cell>84.6</cell><cell>49.9</cell></row><row><cell>600</cell><cell>85.1</cell><cell>50.3</cell></row><row><cell>900</cell><cell>85.1</cell><cell>51.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablation on reducing the difficulty of the pretext task. We report the top-1 accuracy on ImageNet-1K for each approach with different pretraining epochs. Ours w/o unmixing denotes that we do not reduce the difficulty of the pretext task. Details of the other two approaches are described in section 2.3.</figDesc><table><row><cell>Approach</cell><cell cols="3">FLOPs (G) Params. (M) 300 600 900</cell></row><row><cell>Ours w/o unmixing</cell><cell>115</cell><cell>19.0</cell><cell>84.4 84.4 84.4</cell></row><row><cell>Ours w/ mix embedding</cell><cell>115</cell><cell>19.1</cell><cell>84.4 84.6 84.8</cell></row><row><cell>Ours w/ masked self-attention</cell><cell>115</cell><cell>19.0</cell><cell>84.6 85.1 85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table><row><cell cols="2">Pretraining on ImageNet-1K.</cell></row><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW [32]</cell></row><row><cell>base learning rate</cell><cell>1.5e-4</cell></row><row><cell>weight decay</cell><cell>0.05</cell></row><row><cell>optimizer momentum</cell><cell>? 1 , ? 2 =0.9,0.95 [6]</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay [31]</cell></row><row><cell>warmup epochs</cell><cell>40</cell></row><row><cell>augmentation</cell><cell>RandomResizedCrop</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table><row><cell cols="2">Finetuning on ImageNet-1K.</cell></row><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW</cell></row><row><cell>base learning rate</cell><cell>5e-4</cell></row><row><cell>layer-wise lr decay</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Results on ConvNets. All results of MixMIM are obtained by pretraining for 300 epochs and finetuning for 100 epochs on ImageNet-1K. We report the top-1 accuracy on ImageNet-1K.</figDesc><table><row><cell>Method</cell><cell>Backbone Input Size Pretrain Data</cell><cell>Top-1 Acc.</cell></row><row><cell>BiT-S [24]</cell><cell>Res50x3 448 ? 448 ImageNet-1K</cell><cell>80.0</cell></row><row><cell cols="2">BiT-M [24] Res50x3 448 ? 448 ImageNet-21K</cell><cell>84.0</cell></row><row><cell>MixMIM</cell><cell>Res50x3 224 ? 224 ImageNet-1K (w/o labels)</cell><cell>81.8</cell></row><row><cell>BiT-S [24]</cell><cell>Res101x3 448 ? 448 ImageNet-1K</cell><cell>80.3</cell></row><row><cell cols="2">BiT-M [24] Res101x3 448 ? 448 ImageNet-21K</cell><cell>84.3</cell></row><row><cell>MixMIM</cell><cell>Res101x3 224 ? 224 ImageNet-1K (w/o labels)</cell><cell>82.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The feature map resolution is 14?14 / 7?7 for stage-3 / -4. A 14?14 / 7?7 window attention is equivalent to global self-attention.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The visual centrifuge: Model-free layered video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sit: Self-supervised vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03602</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03555</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Context autoencoder for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shentong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03026</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Peco: Perceptual codebook for bert pre-training of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fr?nd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Benchmarking detection transfer learning with vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11718</idno>
		<title level="m">Partial convolution based padding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaozheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discrete variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolfe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12602</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Contrastive learning rivals masked image modeling in fine-tuning via feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14141</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Masked visual pre-training for motor control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. In ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ibot: Image bert pre-training with online tokenizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
