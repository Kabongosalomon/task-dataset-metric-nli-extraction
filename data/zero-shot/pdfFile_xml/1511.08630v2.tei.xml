<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A C-LSTM Neural Network for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-11-30">30 Nov 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Innovation Experiment</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chonglin</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Innovation Experiment</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A C-LSTM Neural Network for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-11-30">30 Nov 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As one of the core steps in NLP, sentence modeling aims at representing sentences as meaningful features for tasks such as sentiment classification. Traditional sentence modeling uses the bag-ofwords model which often suffers from the curse of dimensionality; others use composition based methods instead, e.g., an algebraic operation over semantic word vectors to produce the semantic sentence vector. However, such methods may not perform well due to the loss of word order information. More recent models for distributed sentence representation fall into two categories according to the form of input sentence: sequence-based models and tree-structured models. Sequence-based models construct sentence representations from word sequences by taking in account the relationship between successive words <ref type="bibr" target="#b5">(Johnson and Zhang, 2015)</ref>. Tree-structured models treat each word token as a node in a syntactic parse tree and learn sentence representations from leaves to the root in a recursive manner <ref type="bibr" target="#b11">(Socher et al., 2013b)</ref>.</p><p>Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models <ref type="bibr" target="#b12">(Tai et al., 2015;</ref><ref type="bibr" target="#b7">Lei et al., 2015;</ref><ref type="bibr" target="#b13">Tang et al., 2015;</ref><ref type="bibr" target="#b6">Kim, 2014;</ref><ref type="bibr" target="#b2">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b9">Mou et al., 2015)</ref>.</p><p>Owing to the capability of capturing local correlations of spatial or temporal structures, CNNs have achieved top performance in computer vision, speech recognition and NLP. For sentence modeling, CNNs perform excellently in extracting n-gram features at different positions of a sentence through convolutional filters, and can learn short and long-range relations through pooling operations. CNNs have been successfully combined with both sequence-based model <ref type="bibr" target="#b2">(Denil et al., 2014;</ref><ref type="bibr" target="#b2">Kalchbrenner et al., 2014)</ref> and tree-structured model <ref type="bibr" target="#b9">(Mou et al., 2015)</ref> in sentence modeling.</p><p>The other popular neural network architecture -RNN -is able to handle sequences of any length and capture long-term dependencies. To avoid the problem of gradient exploding or vanishing in the standard RNN, Long Short-term Memory RNN (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref> and other variants  were designed for better remembering and memory accesses. Along with the sequence-based <ref type="bibr" target="#b13">(Tang et al., 2015)</ref> or the tree-structured <ref type="bibr" target="#b12">(Tai et al., 2015)</ref> models, RNNs have achieved remarkable results in sentence or document modeling.</p><p>To conclude, CNN is able to learn local response from temporal or spatial data but lacks the ability of learning sequential correlations; on the other hand, RNN is specilized for sequential modelling but unable to extract features in a parallel way. It has been shown that higher-level modeling of x t can help to disentangle underlying factors of variation within the input, which should then make it easier to learn temporal structure between successive time steps <ref type="bibr" target="#b10">(Pascanu et al., 2014)</ref>. For example, Sainath et al. <ref type="bibr" target="#b11">(Sainath et al., 2015)</ref> have obtained respectable improvements in WER by learning a deep LSTM from multi-scale inputs. We explore training the LSTM model directly from sequences of higherlevel representaions while preserving the sequence order of these representaions. In this paper, we introduce a new architecture short for C-LSTM by combining CNN and LSTM to model sentences. To benefit from the advantages of both CNN and RNN, we design a simple end-to-end, unified architecture by feeding the output of a one-layer CNN into LSTM. The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher-level representions of n-grams. Then to learn sequential correlations from higher-level suqence representations, the feature maps of CNN are organized as sequential window features to serve as the input of LSTM. In this way, instead of constructing LSTM directly from the input sentence, we first transform each sentence into successive window (n-gram) features to help disentangle factors of variations within sentences. We choose sequence-based input other than relying on the syntactic parse trees before feeding in the neural network, thus our model doesn't rely on any external language knowledge and complicated pre-processing.</p><p>In our experiments, we evaluate the semantic sentence representations learned from C-LSTM with two tasks: sentiment classification and 6-way question classification. Our evaluations show that the C-LSTM model can achieve excellent results with several benchmarks as compared with a wide range of baseline models. We also show that the combination of CNN and LSTM outperforms individual multi-layer CNN models and RNN models, which indicates that LSTM can learn longterm dependencies from sequences of higher-level representations better than the other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning based neural network models have achieved great success in many NLP tasks, including learning distributed word, sentence and document representation <ref type="bibr" target="#b8">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b6">Le and</ref><ref type="bibr">Mikolov, 2014), parsing (Socher et al., 2013a)</ref>, statistical machine translation <ref type="bibr">(Devlin et al., 2014)</ref>, sentiment classification <ref type="bibr" target="#b6">(Kim, 2014)</ref>, etc. Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification, text categorization.</p><p>In many recent sentence representation learning works, neural network models are constructed upon either the input word sequences or the transformed syntactic parse tree. Among them, convolutional neural network (CNN) and recurrent neural network (RNN) are two popular ones.</p><p>The capability of capturing local correlations along with extracting higher-level correlations through pooling empowers CNN to model sentences naturally from consecutive context windows. In <ref type="bibr">(Collobert et al., 2011)</ref>, Collobert et al. applied convolutional filters to successive windows for a given sequence to extract global features by max-pooling. As a slight variant, <ref type="bibr" target="#b6">Kim et al. (2014)</ref> proposed a CNN architecture with multiple filters (with a varying window size) and two 'channels' of word vectors. To capture word relations of varying sizes, <ref type="bibr" target="#b2">Kalchbrenner et al. (2014)</ref> proposed a dynamic k-max pooling mechanism. In a more recent work <ref type="bibr" target="#b7">(Lei et al., 2015)</ref>, Tao et al. apply tensor-based operations between words to replace linear operations on concatenated word vectors in the standard convolutional layer and explore the non-linear interactions between nonconsective n-grams. <ref type="bibr" target="#b9">Mou et al. (2015)</ref> also explores convolutional models on tree-structured sentences.</p><p>As a sequence model, RNN is able to deal with variable-length input sequences and discover long-term dependencies. Various variants of RNN have been proposed to better store and access memories <ref type="bibr">(Hochreiter and Schmidhuber, 1997;</ref>. With the ability of explicitly modeling time-series data, RNNs are being increasingly applied to sentence modeling. For example, <ref type="bibr" target="#b12">Tai et al. (2015)</ref> adjusted the standard LSTM to tree-structured topologies and obtained superior results over a sequential LSTM on related tasks.</p><p>In this paper, we stack CNN and LSTM in a unified architecture for semantic sentence modeling. The combination of CNN and LSTM can be seen in some computer vision tasks like image caption  and speech recognition <ref type="bibr" target="#b11">(Sainath et al., 2015)</ref>. Most of these models use multi-layer CNNs and train CNNs and RNNs separately or throw the output of a fully connected layer of CNN into RNN as inputs. Our approach is different: we apply CNN to text data and feed consecutive window features directly to LSTM, and so our architecture enables LSTM to learn long-range dependencies from higher-order sequential features. In , the authors suggest that sequence-based models are sufficient to capture the compositional semantics for many NLP tasks, thus in this work the CNN is directly built upon word sequences other than the syntactic parse tree. Our experiments on sentiment classification and 6-way question classification tasks clearly demonstrate the superiority of our model over single CNN or LSTM model and other related sequence-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">C-LSTM Model</head><p>The architecture of the C-LSTM model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which consists of two main components: convolutional neural network (CNN) and long shortterm memory network (LSTM). The following two subsections describe how we apply CNN to extract higher-level sequences of word features and LSTM to capture long-term dependencies over window feature sequences respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">N-gram Feature Extraction through Convolution</head><p>The one-dimensional convolution involves a filter vector sliding over a sequence and detecting features at different positions. Let x i ? R d be the d-dimensional word vectors for the i-th word in a sentence. Let x ? R L?d denote the input sentence where L is the length of the sentence. Let k be the length of the filter, and the vector m ? R k?d is a filter for the convolution operation. For each position j in the sentence, we have a window vector w j with k consecutive word vectors, denoted as:</p><formula xml:id="formula_0">w j = [x j , x j+1 , ? ? ? , x j+k?1 ]<label>(1)</label></formula><p>Here, the commas represent row vector concatenation. A filter m convolves with the window vectors (k-grams) at each position in a valid way to generate a feature map c ? R L?k+1 ; each element c j of the feature map for window vector w j is produced as follows:</p><formula xml:id="formula_1">c j = f (w j ? m + b),<label>(2)</label></formula><p>where ? is element-wise multiplication, b ? R is a bias term and f is a nonlinear transformation function that can be sigmoid, hyperbolic tangent, etc. In our case, we choose ReLU (Nair and Hinton, 2010) as the nonlinear function. The C-LSTM model uses multiple filters to generate multiple feature maps. For n filters with the same length, the generated n feature maps can be rearranged as feature representations for each window w j ,</p><formula xml:id="formula_2">W = [c 1 ; c 2 ; ? ? ? ; c n ]<label>(3)</label></formula><p>Here, semicolons represent column vector concatenation and c i is the feature map generated with the i-th filter. Each row W j of W ? R (L?k+1)?n is the new feature representation generated from n filters for the window vector at position j. The new successive higher-order window representations then are fed into LSTM which is described below.</p><p>A max-over-pooling or dynamic k-max pooling is often applied to feature maps after the convolution to select the most or the k-most important features. However, LSTM is specified for sequence input, and pooling will break such sequence organization due to the discontinuous selected features. Since we stack an LSTM neural neural network on top of the CNN, we will not apply pooling after the convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Long Short-Term Memory Networks</head><p>Recurrent neural networks (RNNs) are able to propagate historical information via a chain-like neural network architecture. While processing sequential data, it looks at the current input x t as well as the previous output of hidden state h t?1 at each time step. However, standard RNNs becomes unable to learn long-term dependencies as the gap between two time steps becomes large. To address this issue, LSTM was first introduced in <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) and reemerged as a successful architecture since <ref type="bibr">Ilya et al. (2014)</ref> obtained remarkable performance in statistical machine translation. Although many variants of LSTM were proposed, we adopt the standard architecture <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref> in this work.</p><p>The LSTM architecture has a range of repeated modules for each time step as in a standard RNN. At each time step, the output of the module is controlled by a set of gates in R d as a function of the old hidden state h t?1 and the input at the current time step x t : the forget gate f t , the input gate i t , and the output gate o t . These gates collectively decide how to update the current memory cell c t and the current hidden state h t . We use d to denote the memory dimension in the LSTM and all vectors in this architecture share the same dimension. The LSTM transition functions are defined as follows:</p><formula xml:id="formula_3">i t = ?(W i ? [h t?1 , x t ] + b i ) (4) f t = ?(W f ? [h t?1 , x t ] + b f ) q t = tanh(W q ? [h t?1 , x t ] + b q ) o t = ?(W o ? [h t?1 , x t ] + b o ) c t = f t ? c t?1 + i t ? q t h t = o t ? tanh(c t )</formula><p>Here, ? is the logistic sigmoid function that has an output in [0, 1], tanh denotes the hyperbolic tangent function that has an output in [?1, 1], and ? denotes the elementwise multiplication. To understand the mechanism behind the architecture, we can view f t as the function to control to what extent the information from the old memory cell is going to be thrown away, i t to control how much new information is going to be stored in the current memory cell, and o t to control what to output based on the memory cell c t . LSTM is explicitly designed for time-series data for learning long-term dependencies, and therefore we choose LSTM upon the convolution layer to learn such dependencies in the sequence of higher-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning C-LSTM for Text Classification</head><p>For text classification, we regard the output of the hidden state at the last time step of LSTM as the document representation and we add a softmax layer on top. We train the entire model by minimizing the cross-entropy error. Given a training sample x (i) and its true label y (i) ? {1, 2, ? ? ? , k} where k is the number of possible labels and the estimated probabilities y (i) j ? [0, 1] for each label j ? {1, 2, ? ? ? , k}, the error is defined as:</p><formula xml:id="formula_4">L(x (i) , y (i) ) = k j=1 1{y (i) = j} log( y (i) j ),<label>(5)</label></formula><p>where 1{condition} is an indicator such that 1{condition is true} = 1 otherwise 1{condition is false} = 0. We employ stochastic gradient descent (SGD) to learn the model parameters and adopt the optimizer RM-Sprop (Tieleman and Hinton, 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Padding and Word Vector Initialization</head><p>First, we use maxlen to denote the maximum length of the sentence in the training set. As the convolution layer in our model requires fixed-length input, we pad each sentence that has a length less than maxlen with special symbols at the end that indicate the unknown words. For a sentence in the test dataset, we pad sentences that are shorter than maxlen in the same way, but for sentences that have a length longer than maxlen, we simply cut extra words at the end of these sentences to reach maxlen.</p><p>We initialize word vectors with the publicly available word2vec vectors 1 that are pre-trained using about 100B words from the Google News Dataset. The dimensionality of the word vectors is 300. We also initialize the word vector for the unknown words from the uniform distribution [-0.25, 0.25]. We then fine-tune the word vectors along with other model parameters during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Regularization</head><p>For regularization, we employ two commonly used techniques: dropout (Hinton et al., 2012) and L2 weight regularization. We apply dropout to prevent co-adaptation. In our model, we either apply dropout to word vectors before feeding the sequence of words into the convolutional layer or to the output of LSTM before the softmax layer. The L2 regularization is applied to the weight of the softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the C-LSTM model on two tasks: (1) sentiment classification, and (2) question type classification. In this section, we introduce the datasets and the experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Sentiment Classification: Our task in this regard is to predict the sentiment polarity of movie reviews. We use the Stanford Sentiment Treebank (SST) benchmark <ref type="bibr" target="#b11">(Socher et al., 2013b)</ref>.</p><p>This dataset consists of 11855 movie reviews and are split into train (8544), dev (1101), and test (2210). Sentences in this corpus are parsed and all phrases along with the sentences are fully annotated with 5 labels: very positive, positive, neural, negative, very negative. We consider two classification tasks on this dataset: fine-grained classification with 5 labels and binary classification by removing neural labels. For the binary classification, the dataset has a split of train (6920) / dev (872) / test (1821). Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works <ref type="bibr" target="#b11">(Socher et al., 2013b;</ref><ref type="bibr" target="#b2">Kalchbrenner et al., 2014)</ref>. Question type classification: Question classification is an important step in a question answering system that classifies a question into a specific type, e.g. "what is the highest waterfall in the United States?" is a question that belongs to "location". For this task, we use the benchmark TREC <ref type="bibr">(Li and Roth, 2002)</ref>. TREC divides all questions into 6 categories, including location, human, entity, abbreviation, description and numeric.</p><p>The training dataset contains 5452 labelled questions while the testing dataset contains 500 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>We implement our model based on Theano (Bastien et al., 2012) -a python library, which supports efficient symbolic differentiation and transparent use of a GPU. To benefit from the efficiency of parallel computation of the tensors, we train the model on a GPU. For text preprocessing, we only convert all characters in the dataset to lower case.</p><p>For SST, we conduct hyperparameter (number of filters, filter length in CNN; memory dimension in LSTM; dropout rate and which layer to apply, etc.) tuning on the validation data in the standard split. For TREC, we hold out 1000 samples from the training dataset for hyperparameter search and train the model using the remaining data.</p><p>In our final settings, we only use one convolutional layer and one LSTM layer for both tasks. For the filter size, we investigated filter lengths of 2, 3 and 4 in two cases: a) single convolutional layer with the same filter length, and b) multiple convolutional layers with different lengths of filters in parallel. Here we denote the number of filters of length i by n i for ease of clarification. For the first case, each n-gram window is transformed into n i convoluted   <ref type="table">Table 1</ref>: Comparisons with baseline models on Stanford Sentiment Treebank. Fine-grained is a 5-class classification task. Binary is a 2-classification task. The second block contains the recursive models. The third block are methods related to convolutional neural networks. The fourth block contains methods using LSTM (the first two methods in this block also use syntactic parsing trees). The first block contains other baseline methods. The last block is our model.</p><p>features after convolution and the sequence of window representations is fed into LSTM. For the latter case, since the number of windows generated from each convolution layer varies when the filter length varies (see L ? k + 1 below equation <ref type="formula" target="#formula_2">(3)</ref>), we cut the window sequence at the end based on the maximum filter length that gives the shortest number of windows. Each window is represented as the concatenation of outputs from different convolutional layers. We also exploit different combinations of different filter lengths. We further present experimental analysis of the exploration on filter size later. According to the experiments, we choose a single convolutional layer with filter length 3. For SST, the number of filters of length 3 is set to be 150 and the memory dimension of LSTM is set to be 150, too. The word vector layer and the LSTM layer are dropped out with a probability of 0.5. For TREC, the number of filters is set to be 300 and the memory dimension is set to be 300. The word vector layer and the LSTM layer are dropped out with a probability of 0.5. We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Model Analysis</head><p>In this section, we show our evaluation results on sentiment classification and question type classification tasks. Moreover, we give some model analysis on the filter size configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sentiment Classification</head><p>The results are shown in <ref type="table">Table 1</ref>. We compare our model with a large set of well-performed models on the Stanford Sentiment Treebank.</p><p>Generally, the baseline models consist of recursive models, convolutional neural network models, LSTM related models and others. The recursive models employ a syntactic parse tree as the sentence structure and the sentence representation is computed recursively in a bottom-up manner along the parse tree. Under this category, we choose recursive autoencoder (RAE), matrix-vector (MV-RNN), tensor based composition (RNTN) and multi-layer stacked (DRNN) recursive neural network as baselines. Among CNNs, we compare with <ref type="bibr" target="#b6">Kim's (2014)</ref> CNN model with fine-tuned word vectors (CNN-non-static) and multi-channels (CNNmultichannel), DCNN with dynamic k-max pool-   <ref type="bibr" target="#b12">(Tai et al., 2015)</ref> even if following their untied weight configuration, we report our own results. For other baseline methods, we compare against SVM with unigram and bigram features, NBoW with average word vector features and paragraph vector that infers the new paragraph vector for unseen documents.</p><p>To the best of our knowledge, we achieve the fourth best published result for the 5-class classification task on this dataset. For the binary classification task, we achieve comparable results with respect to the state-of-the-art ones. From <ref type="table">Table 1</ref>, we have the following observations: (1) Although we did not beat the state-of-the-art ones, as an endto-end model, the result is still promising and comparable with thoes models that heavily rely on linguistic annotations and knowledge, especially syntactic parse trees. This indicates C-LSTM will be more feasible for various scenarios. (2) Comparing our results against single CNN and LSTM models shows that LSTM does learn long-term dependencies across sequences of higher-level representations better. We could explore in the future how to learn more compact higher-level representations by replacing standard convolution with other non-linear feature mapping functions or appealing to tree-structured topologies before the convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Question Type Classification</head><p>The prediction accuracy on TREC question classification is reported in <ref type="table" target="#tab_2">Table 2</ref>. We compare our model with a variety of models. The SVM classifier uses unigrams, bigrams, wh-word, head word, POS tags, parser, hypernyms, WordNet synsets as engineered features and 60 hand-coded rules. Ada-CNN is a self-adaptiive hierarchical sentence model with gating networks. Other baseline models have been introduced in the last task. From <ref type="table" target="#tab_2">Table 2</ref>, we have the following observations: (1) Our result consistently outperforms all published neural baseline models, which means that C-LSTM captures intentions of TREC questions well. (2) Our result is close to that of the state-of-the-art SVM that depends on highly engineered features. Such engineered features not only demands human laboring but also leads to the error propagation in the existing NLP tools, thus couldn't generalize well in other datasets and tasks. With the ability of automatically learning semantic sentence representations, C-LSTM doesn't require any human-designed features and has a better scalibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Analysis</head><p>Here we investigate the impact of different filter configurations in the convolutional layer on the model performance.</p><p>In the convolutional layer of our model, filters are used to capture local n-gram features. Intuitively, multiple convolutional layers in parallel with differ- ent filter sizes should perform better than single convolutional layers with the same length filters in that different filter sizes could exploit features of different n-grams. However, we found in our experiments that single convolutional layer with filter length 3 always outperforms the other cases. We show in <ref type="figure" target="#fig_2">Figure 2</ref> the prediction accuracies on the 6-way question classification task using different filter configurations. Note that we also observe the similar phenomenon in the sentiment classification task. For each filter configuration, we report in <ref type="figure" target="#fig_2">Figure 2</ref> the best result under extensive grid-search on hyperparameters. It it shown that single convolutional layer with filter length 3 performs best among all filter configurations. For the case of multiple convolutional layers in parallel, it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters, which further confirms that tri-gram features do play a significant role in capturing local features in our tasks. We conjecture that LSTM could learn better semantic sentence representations from sequences of tri-gram features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have described a novel, unified model called C-LSTM that combines convolutional neural network with long short-term memory network (LSTM). C-LSTM is able to learn phrase-level features through a convolutional layer; sequences of such higherlevel representations are then fed into the LSTM to learn long-term dependencies. We evaluated the learned semantic sentence representations on sentiment classification and question type classification tasks with very satisfactory results.</p><p>We could explore in the future ways to replace the standard convolution with tensor-based operations or tree-structured convolutions. We believe LSTM will benefit from more structured higher-level representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of C-LSTM for sentence modeling. Blocks of the same color in the feature map layer and window feature sequence layer corresponds to features for the same window. The dashed lines connect the feature of a window with the source feature map. The final output of the entire model is the last hidden unit of LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Prediction accuracies on TREC questions with different filter size strategies. For the horizontal axis, S means single convolutional layer with the same filter length, and M means multiple convolutional layers in parallel with different filter lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The 6-way question type classification accuracy on TREC.</figDesc><table><row><cell>ing, Tao's CNN (Molding-CNN) with low-rank ten-</cell></row><row><cell>sor based non-linear and non-consecutive convo-</cell></row><row><cell>lutions. Among LSTM related models, we first</cell></row><row><cell>compare with two tree-structured LSTM models</cell></row><row><cell>(Dependence Tree-LSTM and Constituency Tree-</cell></row><row><cell>LSTM) that adjust LSTM to tree-structured network</cell></row><row><cell>topologies. Then we implement one-layer LSTM</cell></row><row><cell>and Bi-LSTM by ourselves. Since we could not tune</cell></row><row><cell>the result of Bi-LSTM to be as good as what has</cell></row><row><cell>been reported in</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://code.google.com/p/word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastien</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<editor>Ronan Collobert, Jason Weston, L?on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Natural language processing (almost) from scratch</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modelling, visualising and summarising documents with a single convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3830</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<editor>Devlin et al.2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul</editor>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Fast and robust neural network joint models for statistical machine translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
	<note>The Computing Research Repository (CoRR)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and J?rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire Cardie ; Rie Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL</title>
		<meeting><address><addrLine>Edward Grefenstette, and Phil Blunsom</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
	<note>A convolutional neural network for modelling sentences</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. ; Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>Proceedings of Empirical Methods on Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<editor>Li and Roth2002] Xin Li and Dan Roth</editor>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Proceedings of Empirical Methods on Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods on Natural Language Processing</title>
		<meeting>Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Version, 5. [Nair and Hinton2010] Vinod Nair and Geoffrey E Hinton</title>
		<ptr target="http://arxiv.org/abs/1504.01106v5" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
	<note>Rectified linear units improve restricted boltzmann machines</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on International Conference on Learning Representations</title>
		<meeting>the conference on International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods on Natural Language Processing</title>
		<meeting>Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>coursera: Neural networks for machine learning</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015th International Conference on Machine Learning</title>
		<meeting>2015th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conferences on Artificial Intelligence</title>
		<meeting>International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
