<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiview Contextual Commonsense Inference: A New Dataset and Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Shen</surname></persName>
							<email>shensq@umich.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
							<email>deepanway_ghosal@mymail.sutd.edu.sgnavonil_majumder@</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Declare</forename><surname>Navonil</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majumder</forename><surname>Declare</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Lim</surname></persName>
							<email>henry_lim@</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Declare</forename><surname>Rada</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihalcea</forename><surname>Soujanya</surname></persName>
							<email>mihalcea@umich.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poria</forename><surname>Declare</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">DeCLaRe DeCLaRe Lab</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiview Contextual Commonsense Inference: A New Dataset and Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CICERO v2 is available at: https://declare-lab.github.io/CICERO</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multiview contextual commonsense inference is the task of determining commonsense explanations around the events in a dyadic dialogue, where multiview refers to the characteristic that there can be multiple plausible but independent inferences. Producing a coherent and non-trivial explanation requires awareness of the dialogue's structure and how an event is grounded in the context, yet there is a lack of high-quality resources dedicated to the task. In this work, we create CICERO v2 , a dataset consisting of 8,351 instances from 2,379 dialogues, containing multiple human-written answers for each contextual commonsense inference question, representing a type of explanation on cause, subsequent event, motivation, and emotional reaction. We show that the inferences in CICERO v2 are of higher semantic diversity than other contextual commonsense inference datasets. In addition, we propose a collection of pretraining objectives, including concept denoising and utterance sorting, to help adapt language models for the multiview contextual commonsense inference task. Evaluation results show the effectiveness of the pretraining stage, as there is a universal improvement in accuracy for all inference types.</p><p>Linda would you care for some candies or cookies?</p><p>No don't try to tend me. I'm becoming chubby and I have to slender down.</p><p>You are not really chubby. You are actually thin enough.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Perhaps unwittingly, commonsense is a key part of daily conversations. Rather than being explicit, interlocutors usually rely on shared context and commonsense knowledge to make sense of the inbound utterances and respond as succinctly as possible to maximize information flow <ref type="bibr">(Grice, 1975)</ref>. The scope of this shared context, however, is quite often broad enough to span beyond the scope of the given conversation. Understanding various dimensions of such conversations for NLP systems is thus rather challenging without the aid of commonsense-based reasoning. Some of the useful dimensions, such as cause, subsequent events, and motivation behind some given utterance, can be extracted from the explicit context. Otherwise, the broader context that fits the explicit context must be imagined. Either way, commonsense knowledge must be employed with the context in mind to broaden the context if necessary and arrive at a fitting explanation. Inferring such explanations for various dimensions with the context and commonsense-based reasoning is called contextual commonsense inference. An accurate understanding of dialogues achieved through contextual commonsense inference can assist in meaningful indexing, filtering, and searching of the copious amount of conversational content available on the internet. Tasks like affect analysis and relation extraction in dialogues may also benefit from such explanations.</p><p>To this end, the CICERO dataset <ref type="bibr" target="#b3">(Ghosal et al., 2022)</ref> collects five dimensions of contextual commonsense inferences for utterances in dialogues. However, for each present dimension-utterance pair, only one human-annotated explanation is collected. The remaining explanations, if any, are picked using adversarial filtering <ref type="bibr" target="#b6">(Zellers et al., 2018a</ref>) from a set of fine-tuned language modelgenerated explanations. These auto-generated explanations are both lexically and semantically very close to the human-annotated explanation. This contradicts the intuitive multiview nature of these explanations, where multiple disparate explanations for the same event may exist (see <ref type="figure" target="#fig_0">Fig. 1</ref>). CICERO v2 seeks to address this issue by collecting multiple distinct human-annotated explanations, leading to the enrichment of the downstream models for contextual commonsense inference task.</p><p>The availability of multiple correct answers brings the need for methods that can simultaneously select multiple correct answers from a mixture of correct and incorrect answers given a context. <ref type="bibr" target="#b3">Ghosal et al. (2022)</ref> shows that given a context, selecting two correct answers is harder than selecting just one. On CICERO, T5-Large attains an Exact Match (EM) score of 95% on the single answer selection task but this score drops to 20% on the multiple answer selection task. Models need to encode rich commonsense knowledge to solve this task due to its hardness. In this work, we attempt to encode commonsense knowledge to a large pre-trained language model T5-Large by continuing training it on a dialogue-level commonsense dataset CICERO <ref type="bibr" target="#b3">(Ghosal et al., 2022)</ref> using a set of commonsense-aware pre-training objectives. Large pre-trained language models, such as <ref type="bibr">GPT-2 (Radford et al., 2019)</ref> and <ref type="bibr">T5 (Raffel et al., 2020b)</ref>, seem attractive frameworks to solve contextual commonsense inference task. Through fine-tuning, these models have become state of the art in several natural language understanding tasks, such as SuperGLUE <ref type="bibr" target="#b5">(Wang et al., 2019)</ref>. Additionally, being trained on several hundreds of GB of text may have endowed these models with much commonsense knowledge <ref type="bibr">(Petroni et al., 2019)</ref>.</p><p>However, the fine-tuning approach may not suffice for tasks with limited training samples. <ref type="bibr">Nonetheless, previous work (Gururangan et al., 2020;</ref><ref type="bibr">Zhou et al., 2021a)</ref> has shown that, prior to fine-tuning, pre-training with objectives catered to the target tasks may improve performance on such tasks. Following this intuition, we propose a set of self-supervised pre-training objectives to adapt the language models for the contextual commonsense inference task, specifically addressing the task of multi-choice answer selection.</p><p>Thus, our contribution in this paper is twofold: i) we curate CICERO v2 , containing multiple distinct contextual commonsense inferences per dimension, and ii) we propose a set of pre-training objectives for contextual commonsense inference that improves over the vanilla fine-tuning by about 1.9% for the multi-choice answer selection task, defined on both CICERO and CICERO v2 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Primer on CICERO</head><p>The dialogues in CICERO <ref type="bibr" target="#b3">(Ghosal et al., 2022)</ref> are sourced from three different datasets: <ref type="bibr">Daily-Dialog (Li et al., 2017)</ref>, MuTual <ref type="bibr" target="#b2">(Cui et al., 2020)</ref>, and DREAM <ref type="bibr">(Sun et al., 2019)</ref>. All dialogues are dyadic and their inherent nature is particularly conducive to qualitatively rich utterance-level inferences. These annotated inferences are categorized into five dimensions: cause, subsequent event, prerequisite, motivation, and emotional reaction. The tasks proposed on these inferences require contextual understanding, multi-utterance reasoning, and commonsense knowledge.</p><p>In addition to introducing CICERO, <ref type="bibr" target="#b3">Ghosal et al. (2022)</ref> also defines a multi-choice answer selection task (MCQ), where the original annotation is considered as the primary correct answer. The candidates for the remaining correct and incorrect answers are generated using fine-tuned T5 models <ref type="bibr">(Raffel et al., 2020a)</ref>. Adversarial filtering <ref type="bibr" target="#b6">(Zellers et al., 2018a)</ref> is applied to these candidates to identify the hard-to-distinguish answers, which are manually labeled as correct or incorrect.</p><p>Drawbacks of CICERO. The automaticallygenerated and labeled-as-correct answers are the only sources of secondary correct answers in the CICERO dataset. In total, close to 15% of the instances contain multiple correct answers (inferences). We empirically analyzed these instances and found that the adversarial filtering algorithm favors the selection of alternate answers that are lexically close to the primary correct answer. As such, both correct and incorrect answers bear a relatively high degree of token-level and semantic similarity with each other as indicated in <ref type="table" target="#tab_2">Table 2</ref> in terms of BLEU, ROUGE-L, CIDER and semanticsimilarity metrics. This belies the multiview nature of commonsense-based explanations, where multiple either independent or related explanations of the same event may exist. This is demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref> where the target utterance "I don't think so. I know I've put on weight this winter." can be a consequence of multiple possible events. Particularly, the event of weight gain can be caused by lack of physical activity and exercise or unhealthy diet or perhaps both. There are myriad of other possible factors that may contribute to the weight gain, such as disease, but those multitudes of possibilities or views are not captured in CICERO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CICEROv2</head><p>To address the drawbacks highlighted earlier, we introduce CICERO v2 , to improve the generalization ability of the models trained on this data. CICERO v2 contains commonsense inferences from target utterances of dyadic dialogues sampled from CICERO. A human annotator is given a dialogue with a target utterance and asked a question about the target utterance. The annotator writes multiple distinct correct answers and two or more incorrect answers for the question.</p><p>We start by sampling (dialogue, target, question) triplets from CICERO. For these instances, we show the original correct answer from CICERO to the annotators to avoid duplication. The annotators write at least one more correct and at least two incorrect answers that are semantically distinct from each other and the answer from CICERO. This original answer and the newly written answer(s) constitute the set of answers for these instances.</p><p>We also sample new (dialogue, target, question) triplets, not present in CICERO. The annotators write at least two correct answers and two incorrect answers for these instances.</p><p>The above strategy ensures that all instances in CICERO v2 have at least two correct and two incorrect answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation Instructions</head><p>Guidelines for Writing Correct Answers. We instruct the annotators to write context-congruent correct answers that are grammatically sound and concise sentences. The answers may contain some important terms from the context and must be commonsense-based, factual, and plausible.</p><p>Guidelines for Writing Incorrect Answers. The incorrect answers are also grammatically correct and concise but must contradict some information in the dialogue. Incorrect answers should contain some important terms from the context and must be commonsense-based and factual. Annotators were instructed not to write incorrect answers that are clearly outlandish in the given context.</p><p>We also ask the annotators to write sufficiently diverse and distinct correct and incorrect answers. This diversity may stem from token-level differences, semantic differences, or various likely speculative scenarios around the given context. Humanwritten diverse incorrect answers is a major contribution in CICERO v2 , which is absent CICERO. We discuss the diversity of answers in CICERO and CICERO v2 in more detail in ?3.3.</p><p>We collect inferences across four different dimensions in CICERO v2 : subsequent event, cause, motivation, and emotional reaction w.r.t the target. Prerequisite dimension from CICERO is skipped as the annotators found it difficult to distinguish from cause during annotation training. The annotators are asked to write correct and incorrect answer(s) to the questions representing each of the four inference dimensions. We expand on the annotation instructions outlined by <ref type="bibr" target="#b3">Ghosal et al. (2022)</ref> for answer writing. Both correct and incorrect answers may describe either an overt or a speculative scenario, as illustrated in CICERO. An overt answer is explicitly or implicitly present in the dialogue context. However, when a dialogue does not explicitly or implicitly hold the answer to a question about a particular target, the answer is speculated within the dialogue context imagined and broadened using commonsense and world knowledge.</p><p>The following illustrates the questions and possible correct and incorrect answer(s) for the (dialogue, target) pair shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>What's that smell?</p><p>No, I'm making chocolate banana cookies I smell something different, pears?</p><p>At first I was going to use the oranges, but I think these will taste better Are you making a chocolate cake? Q1. What subsequent event happens (overt) or could happen (speculative) following the Target? The annotators write about the event that happens or could happen following the target. They are also made aware that at times such subsequent events could be triggered by the target itself. CICERO Correct Answer: The speaker made delicious banana cookies. Incorrect Answers: i) The speaker is making a chocolate cake. ii) The speaker was baking a cake. CICERO v2 Correct Answer: The speaker threw the leftover oranges into the rubbish bin. Incorrect Answers: i) The listener requests to taste the orange cookies. ii) The listener started to make orange chocolate cookies.</p><p>Q2. What is the event that directly causes (overt) or could cause (speculative) Target? The annotators consider the events antecedent to the target that cause or likely cause the target. CICERO Correct Answer: The speaker was making banana cookies. Incorrect Answers: i) The speaker is making a chocolate cake. ii) The speaker was baking a cake. CICERO v2 Correct Answers: i) It is too difficult to process the orange pulp. ii) The orange smell doesn't match well with chocolate. Incorrect Answers i) The orange smell matches much better with chocolate compared with banana. ii) The speaker loves the taste of orange and the texture of its pulp.</p><p>Q3. What is the emotion or basic human drive that motivates or could motivate Target? We ask the annotators to consider the basic human drives and needs of the speaker of the target utterances. The basic human drives include food, water, clothing, rest, safety, friends, relationships, enjoyment, etc. Do or may any of the human drives/states of mind/emotional feelings motivate the target? CICERO Answers: Instance not present. CICERO v2 Correct Answers: i) The speaker wants the cookies to be delicious. ii) The oranges were not sweet enough for the cookies. Incorrect Answers: i) The speaker prefers spicy cookies. ii) The speaker wants to use the leftover pears before they go bad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q4</head><p>. What is the possible emotional reaction of the listener: A (or B)? What could be the possible emotional reaction of the listener to the target?</p><p>The annotators capture the appropriate emotion of the listener using the emotion terms listed in the Appendix in <ref type="table" target="#tab_13">Table 7</ref> using verbatim or related words (e.g., anxious, confused, interested). CICERO Correct Answer: The listener is excited to eat the cookies. Incorrect Answers i) The listener is excited to eat the salad. ii) The listener is excited to eat the muffins instead. CICERO v2 Correct Answer: The listener feels pity that they cannot have orange cookies. Incorrect Answers i) The listener is happy to taste orange cookies. ii) The listener is annoyed by the banana smell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling of Dialogues and Targets</head><p>From the (dialogue, target, question) triplets in CI-CERO, the following criteria is used to subsample a set of triplets for annotation:</p><p>? The target utterance must contain at least one non-stop verb word and more non-stop words than stop words.</p><p>? If the dialogue is from DailyDialog, then the dialogue-act label of the target utterance must either be directive or commissive <ref type="bibr">(Li et al., 2017)</ref>.</p><p>These sampled target utterances often describe some action or activity, which the annotators found easier to annotate across the four question types. Overall, 17% of the correct answer annotations in CICERO v2 also appear in CICERO. However, there is no overlap between the incorrect answers in the two datasets. Crucially, CICERO v2 contains all manually annotated and semantically diverse set of commonsense-based correct and incorrect answers that capture distinct perspectives or views. We expand upon the diversity of the answers next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Diversity of Answers</head><p>Answers in CICERO v2 are significantly more diverse than CICERO. We observe this trend among both correct and incorrect answers. As such, CICERO v2 provides much richer and diversified   multiview commonsense inferences than CICERO. We show a comparative example of annotations in CICERO and CICERO v2 in <ref type="table" target="#tab_1">Table 10</ref>. We compute the instance-level average of BLEU <ref type="bibr">(Papineni et al., 2002)</ref>, <ref type="bibr">ROUGE (Lin, 2004)</ref>, CIDEr <ref type="bibr" target="#b4">(Vedantam et al., 2015)</ref>, and semantic similarity among all (correct, correct), (incorrect, incorrect) and (correct, incorrect) answer pairs in <ref type="table" target="#tab_2">Table 2</ref>. We use the all-mpnet-base-v2 model (Reimers and Gurevych, 2019) to compute the semantic similarity. All scores are reported between 0-1, with a higher score indicating more similarity. The numbers reported in <ref type="table" target="#tab_2">Table 2</ref> clearly indicate that answers in CICERO are significantly less diverse. We also conclude that annotations in CICERO v2 provide a superior quality of multiview commonsense inferences. Similar to <ref type="bibr" target="#b3">(Ghosal et al., 2022)</ref>, we carry out a quality assurance stage on CICERO v2 , details of which can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DIALeCT</head><p>We propose DIALogue-level Commonsense Transformer -DIALeCT, a pretrained transformer for commonsense inference in dialogues. It is a model trained on a variety of dialogue-related tasks, which help the model better leverage the structural information from the dialogues. The model can be used as the initial weight for further downstream task finetuning and have the capability of making zeroshot inference on its own.</p><p>The pretraining tasks include choosing the correct options illustrated, recovering corrupted input, sorting, and generation based on concepts augmented input. All the pretraining tasks are built based on only the training set of CICERO to avoid information leaking from seeing the dialogues in the test set. We describe details for each training objective in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>Given a dialogue D consists of n utterances: [u 1 , u 2 , ..., u n ], our task is to predict the correct answers from a set of choices for questions on a target utterance u i in CICERO as illustrated in ?3: cause (c), effect (e), motivation (m), prerequisite (p), and reaction (r). We denote the questions as Q = [Q j ], where j = [0, 1, .., 4] corresponds to the relation type being asked. The annotated answers on target utterance u i are represented as</p><formula xml:id="formula_0">A i = [a j i ] = [c i , e i , m i , p i , r i ],</formula><p>for each aforementioned question type respectively. Each a j i consists of multiple choices, among which at least two are correct answers. For the pretraining, we use a j i to refer to one of the correct answers if not indicated otherwise. We denote the non-stopword nouns and verbs for either utterance u i or the corresponding answer as concept c i . Note that not all five questions are annotated for each target utterance in CI-CERO. Hence, for a particular target utterance, A i and Q contain only a subset of the question types, making the value of j no more than four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-training objectives</head><p>We propose a set of objectives to train the model in a text-to-text manner. The input usually consists of a combination of the prompt text denoting the task referred to as p, the concatenation of utterances in the dialogue D referred to as context x, and objective-specific information detailed in the following section. Different parts of the input are concatenated to form the input sequence, separated by special tokens and text indicating the parts. We give details on the input formats and prompts used for all the pre-training objectives in <ref type="table" target="#tab_1">Table 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Primary Objectives (PO)</head><p>Primary objectives train open-ended text generation without any set of options to choose from, as in the contextual commonsense inference task:</p><p>(i) Given context x, target utterance u i , and question Q j , generate the corresponding answer a j i .</p><p>(ii) Given context x, question Q j , and its answer a j i , generate the corresponding target utterance u i . (iii) Given context x, target utterance u i , question Q j , answer a j i , and question or another type Q k , generate the corresponding answer a k i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Single Correct Answer Objectives (SCAO)</head><p>These objectives ask the model to generate the right choice from the given options or a closed set of relations, with access to the dialogue context.</p><p>(i) Given context x, target utterance u i , question Q j , multiple answer choices? j i , generate the correct answer a j i . The answer choices? j i includes correct answer a j i and incorrect answers a j? i . We concatenate the question, target, context, and answer choices with separators to form the input. (ii) Given context x, target utterance u i , answer a j i , generate the question type of Q j . We concatenate the answer, target, and context to form the input. The output is one of the five question type strings: cause, effect, motivation, prerequisite, or reaction.</p><p>(iii) Given context x, answer a j i , question Q j , a pool of possible target utterances? i , choose the correct target utterance u i . The pool includes correct target utterance u i and three other utterances u ? i randomly sampled from the same dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Concept-Based Objectives (CO)</head><p>These objectives train to reconstruct a sentence from the set of concepts it contains, and generate the answer or target from the concepts in the target or answer, respectively. The concepts are selected based on the part-of-speech tags parsed by Spacy 1 after removing stop words.</p><p>(i) Given context x, question Q j , concepts c j i from answer a j i , generate the target utterance u i . We use the concatenation of a template question and context as the input. (ii) Given context x, concepts c i from target utterance u i , question Q j , answer a j i , generate the target utterance u i . Following a strategy similar to the previous case, we use x, a j i , c i , Q j along with a template question to form the input. (iii) Given context x, question Q j , and concepts c i from target utterance u i , generate the answer a j i . We concatenate the question, concepts, and context to form the input. 1 https://spacy.io/ (iv) Given context x, target utterance u i , question Q j , and concepts c j i from answer a j i , generate the answer a j i . We concatenate the question, target, concepts, and context to form the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Denoising Objectives (DO)</head><p>These objectives train to restore and order the corrupted concepts in the target utterance or answer. Corruption is performed by randomly changing the order of the concepts, and randomly removing one concept in the original utterance or answer. A similar concept order recovery has previously been explored by <ref type="bibr" target="#b9">Zhou et al. (2021b)</ref>.</p><formula xml:id="formula_1">(i) Given context x, target utterance u i , question Q j , corrupted conceptsc j i for answer a j i , generate correct concepts c j i . (ii) Given context x, question Q j , answer a j i , cor- rupted conceptsc i for utterance u i , generate correct concepts c i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Sorting-Based Objectives (SO)</head><p>Sorting-based objectives require the model to be aware of the order of the utterances and the order of questions asked in the dialogue.</p><p>(i) We consider the following precedence order of the relations: c ? p ? m ? e ? r. Now, given context x and a randomly ordered subset of answer? a from A, the objective is to generate the sorted order of? according to utterance location and relation precedence. The output to be generated is formulated according to indices of answers in the subset a. For instance, if? = [r 5 , p 2 , e 0 , c 0 , m 2 ], the output to generate would be 3 2 1 4 0, denoting the sorted order c 0 ? e 0 ? p 2 ? m 2 ? r 5 . (ii) Given a randomly ordered set of utterances? from D, identify the correct order. For example if u = [u 3 , u 1 , u 2 ], the output to be predicted is the string 1 2 0, assuming indexing starts from 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the effectiveness of DIALeCT on commonsense inference tasks with the multi-choice question answering (MCQ) format under various settings, where we compare the performance of models finetuned on the MCQ task based on DI-ALeCT with the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Pretraining. We pretrain DIALeCT using T5-large as the backbone (770M parameters). We initialize the parameters with the checkpoint  released by <ref type="bibr">(Raffel et al., 2020a)</ref> and continue pretraining in a text-to-text manner instead of span filling. We use the Adafactor (Shazeer and Stern, 2018) optimizer with a weight decay of 0.005 and a learning rate of 1e-5. Note that Adafactor significantly reduces the memory footprint for conversational tasks with long text input. We train the model for 75000 steps with a batch size of 16. The training takes around 22 hours on two A40 GPUs.</p><p>Finetuning. We finetuned the model based on either DIALeCT or T5-large. We use the Adafactor optimizer during pretraining with a learning rate of 3e-5. All finetuning experiments are run for 5 epochs with five different random seeds. Each trial takes 30 minutes on an A40 GPU.</p><p>Evaluation Metrics We use macro-F1 and Exact Match to evaluate the performance of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Results on CICERO</head><p>We evaluate DIALeCT with MCQ from the CI-CERO dataset it pretrained on. <ref type="table" target="#tab_4">Table 3</ref> shows the performance on the MCQ task. We find that DI-ALeCT improves the performance compared to the baseline on all metrics except recall. For the exact match, there is around 2% universal improvement for all inference types, indicating that the pretraining is not limited to a certain type of commonsense inference. The results suggest that, although having the same access to dialogue context and questionanswer pairs from the same dataset, the pretraining helps exploit the information in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transferability of Pretraining</head><p>To further investigate if the performance boost comes from merely seeing the questions and choices in advance. We test DIALeCT on newly collected CICERO v2 .  of DIALeCT on CICERO is worse than its performance on CICERO v2 . We think this could be due to the high lexical overlap and semantic similarity between correct and incorrect answers in CICERO (as shown in <ref type="table" target="#tab_2">Table 2</ref>) that might cause confusion in easily finding the decision boundary. As a result, both T5-large and DIALeCT perform poorly to predict multiple correct answers in CICERO.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study of Pretraining Objectives</head><p>For a fair comparison, we remove a group of pretraining objectives for each setting and pretrain the model with the exact same set of hyper-parameters, including the random seeds, all for five epochs. <ref type="table" target="#tab_6">Table 4</ref> shows that all the ablation models still outperform the baseline, meaning that there is at least more than one group of helpful objectives. Removing the Single Correct Answer Objectives i.e., SCAO causes the largest drop among all metrics, suggesting it carries essential information. On contrary, removing Primary Objectives and Sorting Based Objectives leads to slightly higher metrics. One plausible explanation is that the gap in the input format for these objectives causes trouble for later finetuning. For example, Sorting Objectives ask the model to predict a sequence of integers, which may be confused with the multiple-choice marker. The results for CICERO v2 is shown in <ref type="table" target="#tab_8">Table 5</ref>. It holds the same conclusion that all ablation models perform better than the baseline. It is also interesting that the Concept Objective i.e.,   CO ablation group gets the highest performance on most of the metrics, suggesting that the concepts from CICERO may misalign with the ones in CICERO v2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Performance Analysis</head><p>Cross-Dataset Performance. <ref type="table" target="#tab_10">Table 6</ref> shows cross-dataset adaptability of DIALeCT. To circumvent the influence of the variability of answer counts in CICERO and CICERO v2 , we cull the samples of both datasets to have exactly four answers. For each sample with more than four answers, two correct answers are randomly picked without replacement, and then two more answers are randomly chosen from the rest. This results in at least two correct answers per sample. As expected, both cross-dataset transfers lead to diminished performance due to the starker difference in distribution between training and test set. Interestingly, the performance drop of 29.81% for CICERO to CICERO v2 transfer is far more severe than the drop of 7.63% for CICERO v2 to CICERO transfer. This observation strongly implies that CICERO v2 allows for a much more robust cross-dataset transfer than CICERO. This is likely a consequence of the larger diversity of answers in the training samples of CICERO v2 , as indicated in ?3.3. Another observation is the performance improvement (7.03%) and degradation (10.78%) on in-dataset transfer for culled CICERO and CICERO v2 , respectively. This is indicative of the strong influence of negative samples over the overall performance of DIALeCT on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance with Fewer Training Examples.</head><p>To assess our proposed objectives' efficacy in the low-resource setting, we compare the fine-tuning performance of DIALeCT with T5-Large using different fragments of the training data. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, DIALeCT consistently attains better exact match accuracy than the T5-Large baseline on both CICERO and CICERO v2 . It can be seen that the performance improvement by DIALeCT is more significant under the low resource setting. When finetuned with 20% of the training data, DIALeCT offers over 5% performance boost on both datasets, compared with around 2% for the full dataset. This indicates that DIALeCT might be endowed with some commonsense knowledge through its pre-training using our proposed objectives. As a result, DIALeCT does not require much training data before attaining a decent performance. Note that, although building the pre-training objectives relies on the training set of CICERO, the training set of CICERO v2 is not used, and thus can be considered as a "true low resource setting". In contrast, the baseline T5-Large model needs more training data before obtaining a good fine-tuning performance. Based on these observations, we may conjecture that T5-Large lacks the required commonsense knowledge that DIALeCT encodes in its parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>The area of commonsense reasoning has received significant attention recently, with the introduction of several new benchmarks <ref type="bibr" target="#b7">(Zellers et al., 2018b;</ref><ref type="bibr">Talmor et al., 2019;</ref><ref type="bibr" target="#b1">Bisk et al., 2020</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce CICERO v2 , a human-written dataset for distinct multiview commonsense inferences in dialogues. The dataset contains ?8.3k instances from ?2.3k dialogues across four commonsense dimensions -cause, subsequent event, motivation, and reaction. We also propose DI-ALeCT, which is pre-trained on a collection of dialogue understanding objectives. We evaluate it on the multiview commonsense inference task and analyze its performance across various settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>Our model DIALeCT can only perform the answer selection task (MCQ). Wherein the commonsense inference generation as proposed in <ref type="bibr" target="#b3">(Ghosal et al., 2022)</ref> is more challenging which DIALeCT can not solve. Besides, DIALeCT requires heavy computing power for pre-training and fine-tuning. As a consequence, it can not be deployed on mobile devices with very low computational power. On the other hand, our proposed CICERO v2 only contains inferences across four different commonsense dimensions -cause, subsequent event, motivation, and reaction. Hence, models e.g., DIALeCT trained on CICERO v2 could be limited in their capacity to infer other types of commonsense relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The annotators for CICERO v2 were hired through a data annotation service. The compensation was derived based on the country of residence of the annotators, as deemed by the company. The study has been categorized as "exempt" by the IRB. Annotators were strictly asked not to write any toxic content (hateful or offensive toward any gender, race, sex, or religion). They were asked to consider gender-neutral settings in dialogues whenever possible.</p><p>The source dialogue datasets -DailyDialog, Mu-Tual, and DREAM are high-quality multi-turn dialogue datasets manually annotated by experts in dialogue, communication theory, and linguistics.</p><p>All three datasets have been extensively used and studied in the natural language processing literature. The three source datasets and our annotations in CICERO v2 do not contain any personal data or any information that can uniquely identify individual people or groups.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Annotation of Emotional Reaction</head><p>The annotators capture the appropriate emotion of the listener using the emotion terms listed in <ref type="table" target="#tab_13">Table 7</ref> using verbatim or related words, to write the answer for the question What is the possible emotional reaction of the listener: A (or B)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Quality Assurance of CICERO v2</head><p>The dataset quality is ensured with the following steps:</p><p>? Initially, we sample 30 random dialogues and manually annotate all the questions in those. Each annotator is then evaluated on those dialogues and is selected for the annotation task if 95% of his/her annotations are approved by us.</p><p>? We constantly review and provide feedback to the annotators during the annotation process. Annotators are also instructed to amend their answers.</p><p>? Upon completion of the annotation, we employ three additional annotators who manually check the annotated samples and score their acceptability. These annotators reached a consensus for approving 96.2% of these samples. The samples not bearing majority agreement were removed from the dataset. The statistics of the annotated dataset are shown in <ref type="table" target="#tab_1">Table 1</ref>. A number of annotated examples from CICERO v2 are also shown in <ref type="table" target="#tab_1">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Details on the Pre-training</head><p>We use the CICERO dataset to pre-train DIALeCT. Detailed statistics of this CICERO dataset are presented in  Total 394,379 -- <ref type="table">Table 9</ref>: The number of instances for each group and corresponding sub-groups of objective functions as described in ?4.2. The number of training instances in CI-CERO is 31,418, which is also the number of instances in some of the sub-groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Annotation Details</head><p>We recruited 32 student helpers who are undergraduate students studying computer science and fluent in speaking and writing English. These students have knowledge of Artificial Intelligence. The annotators were paid 7.5 USD per hour which is a standard rate for hiring student helpers at our university. In total, the total cost of the annotation was 2955 USD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Performance Analysis</head><p>Impact of Lexical Overlap of Answers and Context. We use ROUGE-1 precision as the lexical similarity measure between answers and context. For each sample in the training set, we calculate the average ROUGE score for its answers having dialogue context as the reference of this calculation. The distribution of ROUGE scores is shown At first I was going to use the oranges , but I think these will taste better.</p><p>Targetu 4 u 4 u 4 ; Question: Motivation ; Correct Answers in CICERO v2 : i) The speaker has leftover chocolate and bananas and wants to consume them quickly. ii) The speaker likes chocolate sweets. Incorrect Answers in CICERO v2 : i) The speaker wants to make the kitchen smelly to stop the listener entering. ii) The speaker is hungry and chocolate is not filling enough.</p><p>Targetu 4 u 4 u 4 ; Question: Subsequent Event ; Correct Answers in CICERO: i) The listener will request his friend to taste the cookies he prepared just now. Correct Answers in CICERO v2 : i) The speaker asks the listener to pass the spatula to her. Incorrect Answers in CICERO: i) The listener will ask his friends to taste the cake he prepared just now. ii) The listener will request his friends to taste the chocolate cake he prepared just now. Incorrect Answers CICERO v2 : i) The speaker invites the speaker to taste the orange cookies. ii) The listener asks the speaker to get out of the kitchen then takes over the cookies.</p><p>Targetu 5 u 5 u 5 ; Question: Cause ; Correct Answers in CICERO: i) The speaker was making banana cookies. Correct Answers in CICERO v2 : i) It is too difficult to process the orange pulp.</p><p>ii) The orange smell doesn't match well with chocolate. Incorrect Answers in CICERO: i) The speaker is making a chocolate cake. ii) The speaker was baking a cake. Incorrect Answers in CICERO v2 : i) The orange smell matches much better with chocolate compared with banana. ii) The speaker loves the taste of orange and the texture of its pulp.</p><p>Targetu 5 u 5 u 5 ; Question: Emotional Reaction ; Correct Answers in CICERO: i) The listener is excited to eat the cookies. Correct Answers in CICERO v2 : i) The listener feels pity that she cannot have orange cookies. Incorrect Answers in CICERO: i) The listener is excited eats the salad. ii) The listener is excited to eat the muffins instead. Incorrect Answers in CICERO v2 : i) The listener is happy to taste orange cookies. ii) The listener is annoyed by the banana smell. <ref type="table" target="#tab_1">Table 10</ref>: Annotated examples in CICERO and CICERO v2 marked with the target utterance and the question type. The first (dialogue, target, question) instance is not present in CICERO. We show the incorrect answers in CICERO for the other three instances. For these instances, the first correct answer is the primary human written answer in CICERO. Incorrect answers in CICERO are significantly less diverse than CICERO v2 . <ref type="figure">Figure 4</ref>: The distribution of Rouge1-P for correct/incorrect answers in CICERO and CICERO v2 . CICERO has more samples with a zero ROUGE score, and both correct and incorrect answers from CICERO v2 have a slightly higher average ROUGE score than its counterpart. in <ref type="figure">Fig. 4</ref>.</p><p>We set the lower quartile of the average ROUGE score of training samples as the low threshold and the upper quartile as the high threshold. Based on the two thresholds, we then filter the samples in the test set into low-ROUGE and high-ROUGE groups. <ref type="table" target="#tab_1">Table 11</ref> shows that the models perform better on the high-ROUGE group of CICERO. This follows the intuition that samples from the high-ROUGE group are easier to predict as they overlap more with the context.</p><p>That is not the case for CICERO v2 , where all models perform better in the low-ROUGE group. Upon deeper inspection, we find that the models' performance might be influenced by the gap between the overlap, quantified by the ROUGE score, of the correct (R c ) and incorrect answers (R i ), in-</p><formula xml:id="formula_2">Train Test R c R i |R c -R i | Average Exact Match All</formula><p>All -PO All -SCAO All -CO All -DO All -SO  <ref type="table" target="#tab_1">Table 11</ref>: The lexical similarity between answers and context also impact models' performance significantly. lowr and highr denote low rouge precision and high rouge precision groups respectively. R c and R i denote the average ROUGE score of correct and incorrect answers, respectively. <ref type="figure">Figure 5</ref>: Learning Curve of the Pretraining. The validation loss plateaus and starts to increase after 50k. The best-pretrained model is selected based on its performance on the downstream task of multiple-answer selection.</p><p>stead of the absolute overlap of the correct answers. For the high-ROUGE group of CICERO v2 , the incorrect answers have an average ROUGE score of 0.45, which is very close to the score of 0.49 for the correct answers. That may make the separation between the correct and incorrect answers difficult. Note that the incorrect answers in CICERO have almost the same average ROUGE scores across both groups, as they are generated automatically. The correct answers in the high-ROUGE group of CICERO have an average ROUGE score of 0.47, and the incorrect answers in the same group have an average ROUGE score of 0.20. On the other hand, in the low-ROUGE group, correct and incorrect answers have an average ROUGE score of 0.01 and 0.19, respectively. We surmise, as compared to the lower-ROUGE group, the larger gap between the ROUGE scores of the correct and incorrect answers in the high-ROUGE group of CICERO aids DIALeCT to attain better performance in this group.</p><p>Pre-training Steps Required to Converge. Examples of Generated Outputs from the Pretraining Stage. We provide examples of inputs, ground truth and generated outputs by DIALeCT in <ref type="table" target="#tab_1">Table 13</ref>.</p><p>Examples of Generated Output for Multiview Contextual Commonsense inference. We provide a few examples where DIALeCT makes the correct predictions while the baseline model makes commonsense mistakes in <ref type="table" target="#tab_1">Table 14</ref>. For example, in the first dialogue, the model needs to guess what will happen next after the speaker complains Aspirin is not strong enough. The baseline model mistakenly selects option 4, suggesting the listener to visit the emergency room to get medicines. Similarly, in the second example, the baseline model predicts that a thief pulled out a knife will ask if he was okay as the next movement. In the following example, the waiter is confirming if the guest wants to book the room which requires the room to be not occupied and again contradicts option 4 predicted by the baseline model. DIALeCT makes such commonsense mistakes much less compared to the baseline. The last example illustrates a case where DIALeCT makes a prediction that contains the words upstairs but fails to understand the relative spatial information of the speaker and listener and as a result, makes a commonsense mistake. It suggests that the model's ability to do inference Targetu 11 u 11 u 11 ; Question: What is or could be the life goal of the target? Inference: The speaker is hopeful of getting a re-test.</p><p>Targetu 12 u 12 u 12 ; Question: What is or could be the physical requirement of the target? Inference: The speaker has a good driving record.</p><p>Targetu 12 u 12 u 12 ; Question: What is or could be the intention of the target? Inference: The speaker is encouraging the listener to re-appear in the driving test.</p><formula xml:id="formula_3">A (u 1 ) (u 1 ) (u 1 ): David, do you like ice cream? B (u 2 ) (u 2 ) (u 2 ): Yes I do, a lot! A (u 3 ) (u 3 ) (u 3 ):</formula><p>Well, why don't we go get some today? B (u 4 ) (u 4 ) (u 4 ): Sorry, I can not make it today as I have some other plans.</p><p>Targetu 4 u 4 u 4 ; Question: What is the goal of the speaker in the target? Inference: The speaker has to attend a meeting.</p><p>Targetu 4 u 4 u 4 ; Question: What is the emotion of the speaker in the target? Inference: The speaker is disappointed as he is unable to go for ice cream.</p><formula xml:id="formula_4">A (u 1 ) (u 1 ) (u 1 ): David, do you like ice cream? B (u 2 ) (u 2 ) (u 2 ): Yes I do, a lot! A (u 3 ) (u 3 ) (u 3 ): Well, why don't we go get some today? B (u 4 ) (u 4 ) (u 4 ): I can't wait.</formula><p>Targetu 4 u 4 u 4 ; Question: What is the goal of the speaker in the target? Inference: The speaker and david are craving for ice cream.</p><p>Targetu 4 u 4 u 4 ; Question: What is the emotion of the speaker in the target? Inference: The speaker is excited to go to the ice cream shop.  <ref type="table" target="#tab_1">Table 12</ref>. DIALeCT provides correct inferences for life goal, physical requirement, and intention dimension in the first dialogue for different target utterances. The second and third dialogue contexts are constructed in a way such that the first three utterances are identical and the fourth utterance is different. We then ask questions about goal and emotion of the speaker for the fourth utterance. DIALeCT again generates accurate inferences for the questions. The inferences also change appropriately based on the distinct fourth utterances in the two dialogues.  Subseq: (0) The speaker would tell the listener to visit the doctor to get some better medicines.</p><p>(1) The speaker would tell the listener to call the doctor who would prescribe them medicine.</p><p>(2) The speaker would tell the listener to call the doctor to see if they could get some more medicine.</p><p>(3) The speaker would tell the listener to visit the medical store nearby to get some better medicines. (4) The speaker would tell the listener to visit the emergency room to get some better medicines. Subseq: (0) Joan would tell the listener that the thief asked him if he was okay.</p><p>(1) Joan would tell the listener that the thief asked him to give him money and a watch.</p><p>(2) Joan would tell the listener that the thief asked him to give him money and a cell phone.</p><p>(3) Joan would tell the listener that the thief asked him to tell the police about the crime. (4) Joan told the listener that the thief asked him to hand over his keys. Subseq: (0) Paul will get down to pick up the computer from mary. (1) Paul will get downstairs to help mary in lifting the computer upstairs. (2) Paul will get upstairs to help mary in lifting the box upstairs.</p><p>(3) Paul will get downstairs to help mary in lifting the box upstairs. (4) Mary will help paul lift the computer.</p><p>1, 3 1, 2, 3 1, 3 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Demonstration of multiple possible contextual explanations through multiple commonsense-based mechanisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A (dialogue, target) pair; the utterance with the red border is the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The performance of finetuned models trained with fewer samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5depicts the number of training steps required to converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of CICEROv2. We report the numbers only for the multiple correct answer subset in CICERO.</figDesc><table><row><cell cols="7">Data (x, y) BLEU1 BLEU2 BLEU4 ROUGE-L CIDER Sem-Sim</cell></row><row><cell>v1 (C, C)</cell><cell>0.7082</cell><cell>0.6340</cell><cell>0.4817</cell><cell>0.7323</cell><cell>0.2918</cell><cell>0.7974</cell></row><row><cell>v1 (I, I)</cell><cell>0.5966</cell><cell>0.5036</cell><cell>0.3442</cell><cell>0.6119</cell><cell>0.7434</cell><cell>0.7120</cell></row><row><cell>v1 (C, I)</cell><cell>0.6797</cell><cell>0.6028</cell><cell>0.4565</cell><cell>0.7016</cell><cell>0.1268</cell><cell>0.7355</cell></row><row><cell>v2 (C, C)</cell><cell>0.3265</cell><cell>0.1966</cell><cell>0.0501</cell><cell>0.3533</cell><cell>0.0028</cell><cell>0.5934</cell></row><row><cell>v2 (I, I)</cell><cell>0.3455</cell><cell>0.2164</cell><cell>0.0625</cell><cell>0.3738</cell><cell>0.0009</cell><cell>0.5425</cell></row><row><cell>v2 (C, I)</cell><cell>0.3367</cell><cell>0.2214</cell><cell>0.0685</cell><cell>0.3614</cell><cell>0.3421</cell><cell>0.5097</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>(x, y) indicates source-target pair. v1, v2, C, I indi- cate CICERO, CICEROv2, correct answer set, and incorrect answer set, respectively. We show the instance-level aver- age similarity between pairs of (correct, correct), (incorrect, incorrect), and (correct, incorrect) answers in CICERO and CICEROv2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of DIALeCT on CICERO andCICEROv2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Objectives</cell><cell>Avg Macro F1</cell><cell cols="3">Exact Match Cause Subseq Prereq Motiv Reaction Average</cell></row><row><cell>T5-Large</cell><cell>0.7001</cell><cell>0.2521 0.2358 0.2430 0.3258</cell><cell>0.3258</cell><cell>0.2566</cell></row><row><cell>All</cell><cell>0.7066</cell><cell>0.2736 0.2560 0.2457 0.3539</cell><cell>0.3420</cell><cell>0.2754</cell></row><row><cell>-PO</cell><cell>0.7069</cell><cell>0.2964 0.2534 0.2722 0.3660</cell><cell>0.3190</cell><cell>0.2841</cell></row><row><cell>-SCAO</cell><cell>0.6963</cell><cell>0.2613 0.2797 0.2324 0.3419</cell><cell>0.3276</cell><cell>0.269</cell></row><row><cell>-CO</cell><cell>0.7096</cell><cell>0.2867 0.2587 0.2563 0.3505</cell><cell>0.3276</cell><cell>0.2803</cell></row><row><cell>-DO</cell><cell>0.7036</cell><cell>0.2737 0.2530 0.2430 0.3505</cell><cell>0.2931</cell><cell>0.2703</cell></row><row><cell>-SO</cell><cell>0.7090</cell><cell>0.2834 0.2609 0.2656 0.3626</cell><cell>0.3074</cell><cell>0.2815</cell></row><row><cell>Ensemble</cell><cell>-</cell><cell>0.2964 0.2797 0.2722 0.3660</cell><cell>0.3420</cell><cell>0.3112</cell></row></table><note>shows that DI- ALeCT outperform the T5-large baseline on all metrics again. There is a similar trend of improve- ment across inference types for exact matches. The results show information learned in DIALeCT gen- eralize to MCQ samples drawn from a different distribution. Interestingly, despite seeing answers of CICERO during pre-training, the performance</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on CICERO. Reported results are the average of five different runs. The Ensemble model selects the best-performing ablated model for a particular relation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on CICEROv2. Reported results are the average of five different runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Cross-dataset results (avg. of five runs) of DIALeCT;v1-four and v2-four stand for CICERO and CICEROv2, respectively, culled to have four options per sample.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>List of possible emotional reactions of the listener.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 .</head><label>8</label><figDesc>The objective-wise statistics of the training dataset used in pre-training DIALeCT are reported inTable 9.</figDesc><table><row><cell>Description</cell><cell># Instances</cell><cell>Percentage</cell></row><row><cell># Dialogues / # Inferences</cell><cell></cell><cell></cell></row><row><cell>DailyDialog</cell><cell>3,280 / 30,509</cell><cell>57.82 / 57.34</cell></row><row><cell>MuTual</cell><cell>1,640 / 14,207</cell><cell>28.91 / 26.70</cell></row><row><cell>DREAM</cell><cell>753 / 8,488</cell><cell>13.27 / 15.95</cell></row><row><cell>Total</cell><cell>5,673 / 53,204</cell><cell>-</cell></row><row><cell># Dialogues with # Inferences</cell><cell></cell><cell></cell></row><row><cell>less than 10</cell><cell>3,140</cell><cell>55.35</cell></row><row><cell>between 10-20</cell><cell>2,518</cell><cell>44.39</cell></row><row><cell>between 21-30</cell><cell>15</cell><cell>0.26</cell></row><row><cell>Avg. # Inferences per Dialogue</cell><cell>9.38</cell><cell>-</cell></row><row><cell>Instances with</cell><cell></cell><cell></cell></row><row><cell># Correct Answers</cell><cell></cell><cell></cell></row><row><cell>only 1</cell><cell>45759</cell><cell>86.01</cell></row><row><cell>only 2</cell><cell>4985</cell><cell>9.37</cell></row><row><cell>&gt; 2</cell><cell>2460</cell><cell>4.62</cell></row><row><cell>Inference Types in</cell><cell></cell><cell></cell></row><row><cell>Train / Validation / Test</cell><cell></cell><cell></cell></row><row><cell>Cause</cell><cell cols="2">10,386 / 3,060 / 3,071 33.06 / 28.10 / 28.18</cell></row><row><cell>Subsequent Event</cell><cell cols="2">6,617 / 4,021 / 4,050 21.06 / 36.93 / 37.16</cell></row><row><cell>Prerequisite</cell><cell cols="2">7,501 / 1,347 / 1,396 23.87 / 12.37 / 12.81</cell></row><row><cell>Motivation</cell><cell cols="2">4,412 / 1,420 / 1,401 14.04 / 13.04 / 12.86</cell></row><row><cell>Reaction</cell><cell>2,502 / 1,040 / 980</cell><cell>7.96 / 9.55 / 8.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Statistics of CICERO<ref type="bibr" target="#b3">(Ghosal et al., 2022)</ref>.</figDesc><table><row><cell cols="3">Group # Instances Sub-group</cell><cell>Sub-group # Instances</cell></row><row><cell>PO</cell><cell>107,198</cell><cell>(i), (ii) (iii)</cell><cell>31,418 44,362</cell></row><row><cell>SCAO</cell><cell>94,254</cell><cell>(i) -(iii)</cell><cell>31,418</cell></row><row><cell>CO</cell><cell>125,672</cell><cell>(i) -(iv)</cell><cell>31,418</cell></row><row><cell>DO</cell><cell>60,302</cell><cell>(i) (ii)</cell><cell>31,369 28,933</cell></row><row><cell>SO</cell><cell>6,953</cell><cell>(i) (ii)</cell><cell>3,476 3,477</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>To sum it all up, you really just don't know how to drive. A (u 11 ) (u 11 ) (u 11 ): Thanks. Will I be able to take a retest? B (u 12 ) (u 12 ) (u 12 ): Sure you can , in about two and a half weeks.</figDesc><table><row><cell>A (u 1 ) (u 1 ) (u 1 ): Did I do well on my test? B (u 2 ) (u 2 ) (u 2 ): Do you want to know the honest answer? A (u 3 ) (u 3 ) (u 3 ): Why</cell></row><row><cell>wouldn't I want to know? B (u 4 ) (u 4 ) (u 4 ): You had pretty bad scores. A (u 5 ) (u 5 ) (u 5 ): Exactly what do you mean by</cell></row><row><cell>bad? B (u 6 ) (u 6 ) (u 6 ): You failed. A (u 7 ) (u 7 ) (u 7 ): How'd I fail it? B (u 8 ) (u 8 ) (u 8 ): There are a couple of reasons why you</cell></row><row><cell>didn't pass. A (u 9 ) (u 9 ) (u 9 ): What did I do wrong? B (u 10 ) (u 10 ) (u 10 ):</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Examples of zero-shot question types and inferences.still needs to be improved.Zero-shot Transfer with DIALeCTWe examine if DIALeCT is capable of performing zero-shot inferences on unseen questions beyond the pretraining corpus. We show some examples of such inferences in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>What is or could be the cause of target? &lt;sep&gt; target: Drive slowly, David. You could have an accident. &lt;sep&gt; context: x David is driving very fast to flaunt his driving skills to the speaker.The speaker is warning david not to drive too fast.(ii)For which utterance in the context the cause is the following: David is driving very fast to flaunt his driving skills to the speaker. &lt;sep&gt; context:x Drive slowly, David. You could have an accident. You can count on me. I have been driving for years. (iii) target: Drive slowly, David. You could have an accident. &lt;sep&gt; The cause of the target: David is driving very fast to flaunt his driving skills to the speaker. &lt;sep&gt; What is the subsequent event of the target? &lt;sep&gt; context: x &lt;sep&gt; David is driving very fast to flaunt his driving skills to the speaker. &lt;sep&gt; A policeman caught david for breaking traffic rules. &lt;sep&gt; David was driving very fast and broked traffic rules. &lt;sep&gt; The speaker would tell the listener to apply brakes. &lt;sep&gt; David ignores the speaker's advice and continues driving with the same pace. &lt;sep&gt; David is confident in his driving skills. &lt;sep&gt; The speaker is driving with overconfidence that leads him to miss the traffic signal. You can count on me. I have been driving for years. &lt;utt&gt; A: Look out! Red light! &lt;utt&gt; B: It doesn't matter. It is late. There is no one around. &lt;utt&gt; A: Don't let the police catch you. Oh, David, that's a policeman. He is waving over us. &lt;utt&gt; A: Drive slowly, David. You could have an accident.</figDesc><table><row><cell cols="2">Group #</cell><cell>Input</cell><cell>Reference</cell><cell>Generated Output</cell></row><row><cell></cell><cell>(i)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PO</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>David ignores the speaker's</cell><cell>The speaker warns david</cell></row><row><cell></cell><cell></cell><cell></cell><cell>advice and continues</cell><cell>that if he drives too fast he</cell></row><row><cell></cell><cell></cell><cell></cell><cell>driving with the same pace.</cell><cell>will get into an accident.</cell></row><row><cell></cell><cell>(i)</cell><cell>What is or could be the cause of target? &lt;sep&gt; target: Drive</cell><cell>David is driving very fast to</cell><cell>David is driving very fast to</cell></row><row><cell></cell><cell></cell><cell>slowly, David. You could have an accident. &lt;sep&gt; (0) David</cell><cell>flaunt his driving skills to</cell><cell>flaunt his driving skills to</cell></row><row><cell></cell><cell></cell><cell>drives very slowly to flaunt his walking skills to the speaker. (1)</cell><cell>the speaker.</cell><cell>the speaker.</cell></row><row><cell></cell><cell></cell><cell>David drives very slowly to flaunt his driving skills to the speaker.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(2) David is driving very slowly to flaunt his driving skills to the</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>speaker. (3) David is driving very fast to flaunt his driving skills to</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>the speaker. (4) David walks very fast to flaunt his driving skills to</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>the speaker. &lt;sep&gt; context: x</cell><cell></cell><cell></cell></row><row><cell>SCAO</cell><cell>(ii)</cell><cell>answer: David is driving very fast to flaunt his driving skills to the</cell><cell>cause</cell><cell>subsequent event</cell></row><row><cell></cell><cell></cell><cell>speaker. &lt;sep&gt; target: Drive slowly, David. You could have an</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>accident. &lt;sep&gt; context: x</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(iii) The cause of the target: David is driving very fast to flaunt his</cell><cell>Drive slowly, David. You</cell><cell>You can count on me. I</cell></row><row><cell></cell><cell></cell><cell>driving skills to the speaker. &lt;sep&gt; target options: Drive slowly,</cell><cell>could have an accident.</cell><cell>have been driving for years.</cell></row><row><cell></cell><cell></cell><cell>David. You could have an accident. &lt;utt&gt; Look out! Red light!</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>&lt;utt&gt; It doesn't matter. It is late. There is no one around. &lt;utt&gt;</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>You can count on me. I have been driving for years. &lt;sep&gt;</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>context: x</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(i)</cell><cell>For which utterance in the context the cause is related to the</cell><cell>Drive slowly, David. You</cell><cell>You can count on me. I</cell></row><row><cell></cell><cell></cell><cell>following concepts: drive, flaunt, driving, skill, speaker &lt;sep&gt;</cell><cell>could have an accident.</cell><cell>have been driving for years.</cell></row><row><cell></cell><cell></cell><cell>context: x</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(ii)</cell><cell>For which utterance in the context the cause is the following:</cell><cell>Drive slowly, David. You</cell><cell>Drive slowly, David. You</cell></row><row><cell></cell><cell></cell><cell>David is driving very fast to flaunt his driving skills to the speaker.</cell><cell>could have an accident.</cell><cell>could have an accident.</cell></row><row><cell>CO</cell><cell></cell><cell>&lt;sep&gt; concept: drive, accident &lt;sep&gt; context: x</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(iii) What is or could be the cause of target? &lt;sep&gt; concepts in the</cell><cell>David is driving very fast to</cell><cell>David was driving at a high</cell></row><row><cell></cell><cell></cell><cell>target: drive, accident &lt;sep&gt; context: x</cell><cell>flaunt his driving skills to</cell><cell>speed.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the speaker.</cell><cell></cell></row><row><cell></cell><cell cols="2">(iv) What is or could be the cause of target? &lt;sep&gt; target: Drive</cell><cell>David is driving very fast to</cell><cell>David was driving fast and</cell></row><row><cell></cell><cell></cell><cell>slowly, David. You could have an accident. &lt;sep&gt; concepts in the</cell><cell>flaunt his driving skills to</cell><cell>flaunting his driving skills</cell></row><row><cell></cell><cell></cell><cell>answer: drive, flaunt, driving, skill, speaker &lt;sep&gt; context: x</cell><cell>the speaker.</cell><cell>to the speaker.</cell></row><row><cell></cell><cell>(i)</cell><cell>target: Drive slowly, David. You could have an accident. &lt;sep&gt;</cell><cell>drive, flaunt, driving, skill,</cell><cell>speaker, flaunt, driving,</cell></row><row><cell></cell><cell></cell><cell>corrupted concepts: drive, driving, flaunt, speaker &lt;sep&gt; context:</cell><cell>speaker</cell><cell>skill, drive</cell></row><row><cell>DO</cell><cell></cell><cell>x &lt;sep&gt; concepts in the answer:</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(ii)</cell><cell>answer: David is driving very fast to flaunt his driving skills to the</cell><cell>drive, accident</cell><cell>drive, accident</cell></row><row><cell></cell><cell></cell><cell>speaker. &lt;sep&gt; corrupted concepts: drive &lt;sep&gt; context: x &lt;sep&gt;</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>concepts in the target:</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(i)</cell><cell cols="2">context: x 0 6 5 4 1 2 3</cell><cell>6 0 1 3 5 4 2</cell></row><row><cell>SO</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(ii)</cell><cell cols="2">B: 4 0 1 2 3</cell><cell>4 0 1 2 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 13 :</head><label>13</label><figDesc>An example of input, reference, and generated output triplets for the various groups of objective functions ( ?4.2) from a dialogue D. PO, SCAO, CO, DO, and SO refers to the Primary Objectives, Single Correct Answer Objectives, Concept-Based Objectives, Denoising Objectives, and Sorting Based Objectives, respectively. The outputs are generated from the pretrained DIALECT model. The context placeholder x is the concatenation of the utterances in the dialogue D, which is the following string: A: Drive slowly, David. You could have an accident. &lt;utt&gt; B: You can count on me. I have been driving for years. &lt;utt&gt; A: Look out! Red light! &lt;utt&gt; B: It doesn't matter. It is late. There is no one around. &lt;utt&gt; A: Don't let the police catch you. Oh, David, that's a policeman. He is waving over us.</figDesc><table><row><cell>Context</cell><cell>Relation + Answers</cell><cell>Label</cell><cell>DIALeCT T5-Large</cell></row><row><cell>A: Wake up. It's almost eight o'clock. B: No,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>please. Let me sleep on! I couldn't get to sleep</cell><cell></cell><cell></cell><cell></cell></row><row><cell>until 3 o'clock this morning. A: Why? What's</cell><cell></cell><cell></cell><cell></cell></row><row><cell>wrong with you? B: I felt pain all over my body.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Can you get me some medicine? A: Will aspirin</cell><cell></cell><cell></cell><cell></cell></row><row><cell>do? B: No, aspirin isn't strong enough. A:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Then I can do nothing but call for a doctor.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Joan. Why are you late today? You are never late for work. B: No, I never. But ... A: Wow! You coat's got very dirty! Did you fall? B: Yes, I had a terrible experience on the underground train. Listen to this! A man came up to me and pulled out a knife. He pointed it right at me! A: Oh, no! Are you all right? Did he hurt you? B: No, he didn't hurt me, but he took my handbag. A: Then what happened? What did you do? B: I caught hold of his knife, and he pushed me to the floor. A: Oh, no! Why did you catch hold of his knife? That's dangerous. B: I don't know. I didn't think. A: What did the other passengers do? Did they help you? B: Yes, they did. Two men ran after the robber and held him. A: Did the police come? B: Yeah. The conductor called a policeman, and he took the robber to the police station. A: Wow! What a story! Thank God you're all right.</figDesc><table><row><cell>0, 1, 3</cell><cell>0, 1, 3</cell><cell>0, 3, 4</cell></row><row><cell>A: Hello,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>Hello , may I help you ? B: Yes.We ' re interested in seeing the rooms for rent . A: Oh , how nice.They ' re bright rooms and the house is very quiet . B: A nice quiet house is exactly what we're looking for . A: Well , gentleman.Each room is $ 40 a week if you think that's OK . B: That sounds just wonderful to us . A: When do you want to move in ? B: How about this afternoon ? A: Fine . I'll be expecting you around two . Paul, is that you? B: Yes, Mary. What can I do for you? A: Sorry to call you. But I just delivered my new computer. I am afraid I can't lift it by myself. Could you give me a hand to get it upstairs? B: Sure. Could you just give me a minute to finish off what I am doing? A: Yes, of course. But please hurry. The box is getting in the way. B: Don't worry. I'll be right down.</figDesc><table><row><cell></cell><cell>1, 2, 3</cell><cell>1, 2, 3</cell><cell>0, 2</cell></row><row><cell>A: Prereq: (0) The rooms showed to the</cell><cell>0, 1</cell><cell>0, 1</cell><cell>1, 4</cell></row><row><cell>person are currently unoccupied. (1) The</cell><cell></cell><cell></cell><cell></cell></row><row><cell>rooms shown to the person are currently</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ready to be occupied. (2) The rooms they</cell><cell></cell><cell></cell><cell></cell></row><row><cell>show are occupied. (3) The rooms showed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>to the person are full and occupied. (4)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>The rooms the person was looking in are</cell><cell></cell><cell></cell><cell></cell></row><row><cell>currently occupied.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Examples of the fine-tuning performance of DIALeCT and its comparison with T5-Large.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual: A dataset for multi-turn dialogue reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Conference of the Association for Computational Linguistics</title>
		<meeting>the 58th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CI-CERO: A dataset for contextualized commonsense inference in dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.344</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5010" to="5028" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SWAG: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bill Yuchen Lin, and Xiang Ren. 2021a. Pre-training text-to-text transformers for concept-centric common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Selvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyeon</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/2011.07956</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pretraining text-to-text transformers for concept-centric common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Selvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Question answering with long multiple-span answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandan</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.342</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3840" to="3849" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
