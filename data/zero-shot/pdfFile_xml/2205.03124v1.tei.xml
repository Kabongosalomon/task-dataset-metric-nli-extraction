<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A High-Accuracy Unsupervised Person Re-identification Method Using Auxiliary Information Mined from Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehan</forename><surname>Teng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
							<email>yuchen.w.guo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cn</email>
						</author>
						<title level="a" type="main">A High-Accuracy Unsupervised Person Re-identification Method Using Auxiliary Information Mined from Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised person re-identification methods rely heavily on high-quality cross-camera training label. This significantly hinders the deployment of re-ID models in real-world applications. The unsupervised person re-ID methods can reduce the cost of data annotation, but their performance is still far lower than the supervised ones. In this paper, we make full use of the auxiliary information mined from the datasets for multi-modal feature learning, including camera information, temporal information and spatial information. By analyzing the style bias of cameras, the characteristics of pedestrians' motion trajectories and the positions of camera network, this paper designs three modules: Time-Overlapping Constraint (TOC), Spatio-Temporal Similarity (STS) and Same-Camera Penalty (SCP) to exploit the auxiliary information. Auxiliary information can improve the model performance and inference accuracy by constructing association constraints or fusing with visual features. In addition, this paper proposes three effective training tricks, including Restricted Label Smoothing Cross Entropy Loss (RLSCE), Weight Adaptive Triplet Loss (WATL) and Dynamic Training Iterations (DTI). The tricks achieve mAP of 72.4% and 81.1% on MARS and DukeMTMC-VideoReID, respectively. Combined with auxiliary information exploiting modules, our methods achieve mAP of 89.9% on DukeMTMC, where TOC, STS and SCP all contributed considerable performance improvements. The method proposed by this paper outperforms most existing unsupervised re-ID methods and narrows the gap between unsupervised and supervised re-ID methods. Our code is at GitHub.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of person re-identification is to retrieve people across cameras. As an important supporting technology for intelligent security systems, person re-ID has been an active research field over the years. However, due to the dif-ferences between academia and industry settings, serious cross-domain accuracy loss, and the extremely high cost of annotation of re-ID datasets, person re-ID still faces the problem of insufficient accuracy and high data labeling cost in application. Aiming at the problems in real application scenarios, more work is devoted to exploring high-accuracy unsupervised person re-ID methods.</p><p>For unsupervised person re-ID methods, due to the lack of real identity labels, pseudo labels need to be constructed before model training. The process of generating pseudo labels is called ID association. The quality of pseudo labels directly affects the performance of model. With accurate pseudo labels the performance of unsupervised methods may approach those of supervised methods, while the noise in pseudo labels indicating wrong identity relationship would misguide the training process and result in bad performance. Existing unsupervised re-ID methods usually generate pseudo labels based on visual features when performing ID association. However, in the early stage of model training, the quality of deep features extracted by the model is poor, which leads to inaccurate pseudo labels, having a negative impact on model training.</p><p>In order to effectively improve the accuracy of the pseudo labels in unsupervised re-ID methods, we fully mine the camera information, temporal information and spatial information from the datasets, and design multimodal feature learning methods based on auxiliary cues. Auxiliary information exploiting modules include Time-Overlapping Constraint (TOC), Spatio-Temporal Similarity (STS) and Same-Camera Penalty (SCP). TOC constructs identity association constraints by mining the timeoverlapping relationship between unlabeled samples, which is then converted into "unconnectable relationship" between points in the clustering process. Based on the traditional clustering method, DBSCAN <ref type="bibr" target="#b3">[4]</ref>, a restricted DBSCAN method was implemented to take such constraints into consideration. STS module measures the time spent by pedestrians passing through the camera pairs and fit a Gaussian mixture distribution for each of the camera pairs. Given two unlabeled samples, the value of the time distribution function of the corresponding camera pair, evaluated at the time difference of the samples, can be seen as the spatiotemporal similarity between them, which is then fused with visual feature distance to obtain a fusion distance that contains both visual information and spatio-temporal information. SCP module applies the same-camera distance penalty to unlabeled sample pairs captured by the same camera to compensate for the distance deviation caused by camera style bias. Auxiliary information exploiting modules improve the accuracy of the pseudo labels by constructing association constraints or fusing with visual features. In addition, the three modules can adjust the ordering of the samples in the gallery set during predicting phase to improve the inference accuracy of the model. In order to alleviate the negative impact of noise in pseudo labels, we propose three training tricks, which improve the ID Loss and Triplet Loss used by traditional re-ID methods into Restricted Label Smoothing Cross Entropy Loss (RLSCE) and Weight Adaptive Triplet Loss (WATL), and replaces the commonly used fixed training iterations with Dynamic Training Iterations (DTI).</p><p>The contributions of this paper could be summarized as three-fold. (1) We proposed three auxiliary information exploiting modules to improve the accuracy of pseudo labels for unsupervised re-ID model training and the inference performance, including Time-Overlapping Constraint (TOC), Spatio-Temporal Similarity (STS) and Same-Camera Penalty (SCP). <ref type="bibr" target="#b1">(2)</ref> In order to alleviate the negative effects of noise in pseudo labels, we propose three training tricks, Restricted Label Smoothing Cross Entropy Loss (RLSCE), Weight Adaptive Triplet Loss (WATL) and Dynamic Training Iterations (DTI). <ref type="bibr" target="#b2">(3)</ref> The experiments show that the three training tricks proposed by this paper achieve mAP of 72.4% and 81.1% on MARS <ref type="bibr" target="#b17">[18]</ref> and DukeMTMC-VideoReID <ref type="bibr" target="#b9">[10]</ref>, respectively. Combined with auxiliary information exploiting modules, our methods achieve mAP of 89.9% on DukeMTMC-VideoReID, outperforming most existing unsupervised re-ID methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review the unsupervised person re-ID methods, where identity labels are not required in the process of model training. The core of unsupervised person re-ID methods is the ID association of samples across cameras, i.e. the generation of pseudo labels. In the absence of real identity labels, ID association is performed according to the features of the samples, and the generated pseudo labels are used for model training. In order to achieve a better training effect, unsupervised person re-ID methods usually adopt an iterative pipeline of model training and pseudo label generation. As training progresses, the accuracy of pseudo labels and the performance of model are continu-ously improved. In this paper, we divide unsupervised re-ID methods into two categories according to the method of pseudo labels generation: anchor-based ID association and clustering-based ID association. Anchor-based ID association is different from the traditional unsupervised person re-ID methods in which all data is unlabeled. The anchor-based ID association methods require that each pedestrian in the dataset has a labeled sample to serve as an anchor. According to the features of the anchors, the unlabeled samples are gradually assigned ID labels. EUG <ref type="bibr" target="#b11">[12]</ref> is a step-wise one-shot learning method, gradually sampling a few candidates with most reliable pseudo labels from unlabeled tracklets to enrich the labeled dataset. RACE <ref type="bibr" target="#b13">[14]</ref> proposed a robust anchor embedding framework which estimates pseudo label via robust anchor embedding and top-k counts label prediction. Note that anchor-based ID association methods require additional information to initialize their learning process, which usually involves extra human labor. Clustering-based ID association maintains state-of-theart performance to date. It discovers cluster structures in unlabeled data and utilizes cluster labels for model training [2] <ref type="bibr" target="#b12">[13]</ref>. PUL <ref type="bibr" target="#b4">[5]</ref> propose a progressive unsupervised learning method that iterates between samples clustering and fine-tuning of the network. For generating pseudo labels with low noise, standard k-means clustering is followed by a selection operation to choose reliable samples. In order to mitigate the effects of noisy pseudo labels, Mutual Mean-Teaching <ref type="bibr" target="#b5">[6]</ref> is proposed to learn better features via off-line refined hard pseudo labels generated by clustering and online refined soft pseudo labels predicted by mean teacher model in an iterative training manner. On clustering algorithm selection, DBSCAN gradually replaces k-means as the clustering algorithm commonly used to obtain pseudo labels in unsupervised re-ID methods. The total number of identities in an unlabeled dataset is unknown, so when the value of k deviates greatly from the total number of IDs during k-means clustering, the accuracy of clustering will be poor. In addition, k-means performs well for regularly shaped clusters and is less effective for clusters of non-convex shapes. DBSCAN is a density-based clustering method that can generate clusters of any shape and does not require a preset of the total number of clusters, which is more suitable for person re-ID scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We adopt a simplification of MMT <ref type="bibr" target="#b5">[6]</ref> [17] which keeps only one network (student) and its past temporal average model (teacher) as our overall training pipeline. The student and teacher are called Net and Mean Net respectively. The Net is trained by utilizing both hard pseudo labels and soft pseudo labels. The former is generated by the clustering method of DBSCAN and latter is constructed by the Mean Net. The Net is trained by jointly optimizing the following loss functions: hard ID loss L id , hard triplet loss L tri , soft ID loss L sid , and soft triplet loss L stri . The iterative pipeline of model training and pseudo label generation is kept. Through experiments, we found that the hard pseudo labels and the loss functions based on the hard pseudo labels have a far more important impact on the performance of the model than the soft ones. We propose three auxiliary information exploiting modules to improve the accuracy of hard pseudo labels generated by DBSCAN (Section 3.2). Meanwhile, to mitigate the effects of noisy pseudo labels, we replace L id and L tri with Restricted Label Smoothing Cross Entropy Loss and Weight Adaptive Triplet Loss, and improve the fixed training iterations into Dynamic Training Iterations (Section 3.3). Besides MMT, the proposed auxiliary information exploiting modules and training tricks also apply to other clustering-based unsupervised person re-ID methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Auxiliary Information Exploiting Modules</head><p>In order to obtain rich auxiliary information and visual features, we utilize video-based re-ID datasets, in which the start and end timestamps (in the raw surveillance video) of each sample and the camera where each sample was captured are known. Based on the information above, auxiliary information exploiting modules, namely Time-Overlapping Constraint, Spatio-Temporal Similarity and Same-Camera Penalty are designed. It should be noted that STS and SCP are also applicable to image-based re-ID methods. Time-Overlapping Constraint When setting up the re-ID datasets shooting network, in order to obtain as many pedestrian pose changes and shooting scenes as possible, on the premise of ensuring that enough pedestrians are captured by multiple cameras, the camera network is usually set up scattered. There is no or very little overlap between the shot areas, so the probability of a pedestrian passing through the overlapping areas can be seen as zero. In this case, it is impossible for any pedestrian to be captured by more than one camera at the same time. For video-based re-ID datasets, we use s i,a and s i,b to represent two samples of the pedestrian with ID i from camera a and camera b, the corresponding time segments of which are denoted [start i,a , end i,a ] and [start i,b , end i,b ], respectively. It follows that [start i,a , end i,a ] and [start i,b , end i,b ] cannot have overlapping time periods. Consequently, for two unlabeled samples s i and s j , if their time segments [start i , end i ] and [start j , end j ] overlap, s i and s j must not be the same pedestrian. Based on the above analysis, we propose Time-Overlapping Constraint (TOC) to assist the ID association for unsupervised re-ID methods. TOC means that for two unlabeled samples, if their time seg-ments overlap, then the samples should not be assigned the same pseudo label.</p><p>TOC works by imposing these constraints during ID association. The constraints are expressed as "unconnectable", or "cannot-link" relationships between points in the DBSCAN process. If P oint i and P oint j have a cannotlink relationship, they should not be assigned to the same cluster. We propose Restricted DBSCAN (Algorithm 1): First, all the cannot-link relationships are processed as the unconnectable point set S of each point. In the clustering process, the unconnectable point set of the current cluster C k , cannotlinks, is maintained until the C k is finished. For an unclassified point P i that meets the requirements to join C k , if P i is in cannotlinks, then P i cannot be added to C k . Otherwise, P i is added to C k and cannotlinks is updated with the union of the unconnectable point set of P i and current cannotlinks. </p><formula xml:id="formula_0">= D; for x j in D do Calculate neighbor set N (x j ); end for x j in D ? ? do Q = {x j }; cannotlinks = {}; while Q = ? do Select a sample q from Q; ? = N (q) ? ? ? cannotlinks; if |?| ? M inP ts then for x i in ? and x i / ? cannotlinks do Add x i to Q; Assign pseudo label k to x i ; ? = ? ? set(x i ); cannotlinks = cannotlinks ? S i ; end end end k = k + 1; end Algorithm 1: Restricted DBSCAN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-Temporal Similarity</head><p>In order to ensure that as many pedestrians as possible are captured by more than one camera, the camera network is usually placed in a bounded area in which pedestrians' motion paths are relatively fixed and limited. Based on the following three conditions: the position of camera network is stationary, (2) the paths of the pedestrians moving between the cameras are fixed and limited, (3) most people move with definite purposes and the speeds of them are within a reasonable range, we conjecture that the time it takes for a pedestrian to move between a certain camera pair obey a certain distribution. For example, the time spent by pedestrians moving between camera a and camera b should be mainly concentrated around the value of d a,b /v average , where d a,b represents the actual distance between the camera pair and v average is the average speed of pedestrians. On the contrary, the probability of the value far from d a,b /v average is relatively low, because it means that the pedestrian's movement speed deviates significantly from the average level of normal people or the pedestrian chooses a very unpopular route. To verify the above conjecture, we utilize DukeMTMC-VideoReID to conduct statistics on pedestrians' time spent across cameras. For any two cameras a and b in the camera network, we calculate the timestamp difference |T i,a ? T i,b | of all sample pairs &lt; s i,a , s i,b &gt; where one component from camera a and another from camera b, for each person ID i. The time differences are plotted as probability distribution histograms, one per camera pair. <ref type="figure" target="#fig_1">Figure 1</ref> shows probability distribution histograms of the cross-camera time for four camera pairs, which can be well fitted by Gaussian mixture distributions. In Gaussian mixture distribution, each independent Gaussian distribution represents a common path between two cameras, and the time spent moving on this path constitutes a Gaussian distribution. Take camera a and camera b as an example, the Gaussian mixture distribution obtained by fitting the time required for pedes-trians to move between a and b is denoted F a,b . For a sample s i,a from camera a and a sample s j,b from camera b, feed the time difference |T i,a ? T j,b | into F a,b and the corresponding probability p i,j = F a,b (|T i,a ? T j,b |) is output. A larger value of p i,j means a higher probability that a pedestrian spends about |T i,a ? T j,b | to move between camera a and camera b, indicating that s i,a and s j,b are more likely to be the same person. A smaller value of p i,j , on the contrary, means that the time required to move between camera a and camera b is generally not close to the value of |T i,a ? T j,b |, indicating the probability that s i,a and s j,b are the same person is relatively low. To sum up, the p i,j can be understood as the similarity between s i,a and s j,b in terms of spatial and temporal. The larger the value of p i,j , the higher the probability that the two samples are the same person. The spatio-temporal similarity p i,j is fused with the visual features according to Equation 1, where d i,j represents the distance of visual deep features between s i,a and s j,b . ? and ? are preset hyperparameters. The joint distance d i,j contains visual information and spatio-temporal information, which achieve better performance in ID association.</p><formula xml:id="formula_1">(1) (a) (b) (c) (d)</formula><formula xml:id="formula_2">d i,j = d i,j ? 1 + ?e ???pi,j<label>(1)</label></formula><p>The accurate Gaussian mixture function F cannot be obtained in unsupervised person re-ID methods because the ID labels of training set are unknown. In order to solve the problem, we propose to generate Gaussian mixture functions of cross-camera time using pseudo labels. The framework is as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. In each epoch of model training, pseudo labels are first generated by the visual features, then calculate the cross-camera time and obtain the Gaussian mixture functions according to the generated pseudo labels. Spatio-temporal similarity is then fused with the distance of visual features according to Equation 1 and the fusion distance is used to generate higher quality pseudo labels by clustering, which are utilized to train the model. Same-Camera Penalty As for person re-ID methods, due to differences in shooting angles, lighting conditions, backgrounds of pedestrians and other aspects between cameras, the photos taken by the same camera are more similar in style than the photos taken by different cameras. In this case, the unlabeled training samples of a certain pedestrian captured by different cameras may be difficult to be classified into the same cluster due to the difference in style during ID association. On the contrary, the training samples captured by same camera may be assigned same pseudo label because of the similar shooting styles even if the groundtruth IDs of the samples are different. In summary, the shooting style difference between cameras negatively affects the accuracy of clustering for ID association in unsupervised re-ID methods.</p><p>In order to alleviate the negative impact of camera style differences on the clustering results, we propose Same- Camera Penalty which is calculated according to Equation 2 and fused with the distance of visual deep feature between unlabeled training samples. If x a and x b come from the same camera, the value of SCP is ? c , otherwise 0. Same-Camera Penalty is added to the original distance of visual features. Distance penalty ? c is imposed to compensate for the distance deviation caused by similar shooting styles and the corrected distance is used for clustering to obtain pseudo labels, which are less affected by shooting styles.</p><formula xml:id="formula_3">SCP (x a , x b ) = ? c , c a = c b 0, c a = c b<label>(2)</label></formula><p>Summary We propose three auxiliary information exploiting modules including Time-Overlapping Constraint, Spatio-Temporal Similarity and Same-Camera Penalty to improve the training performance and inference accuracy for re-ID model. For unsupervised re-ID training, the modules improve the quality of pseudo labels generated by DBSCAN through constructing association constraints (TOC) or fusing with visual features (STS and SCP). During the phase of inference, TOC can eliminate the samples in gallery set which have time-overlapping constraints with the query while STS and SCP replaces the distance of visual features between query and gallery with fusion distance to generate more appropriate ordering of the samples in gallery set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Tricks</head><p>The pseudo labels generated by ID association in unsupervised person re-ID methods usually contain noise, especially in the early stage of model training, the low quality of </p><formula xml:id="formula_4">L LSCE = 1 N N i=1 L ce (p i , q i ) = ? 1 N N i=1 M j=1 p j i log q j i p j i = 1 ? ? if j = y ? / (M ? 1) otherwise (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restricted Label Smoothing Cross Entropy Loss Label smoothing according to Equation 3 based on traditional classification loss is often used to solve the overfitting problem in supervised deep learning. Applying Label Smooth</head><p>Cross Entropy Loss (LSCE) to unsupervised person re-ID methods can effectively alleviate the negative impact of noisy pseudo labels, because ?/(M ? 1) encourage the model to be less confident on the pseudo labels. However, LSCE also has the disadvantage that it does not reflect varying degrees of similarity between categories. For example, pedestrian B and C are both in negative categories with respect to pedestrian A, and the appearance of B is much more similar to A than that of C. B and C are different in their similarity to A but the values of p i B i A and p i C i A are both ?/(M ?1) according to <ref type="bibr">Equation 3</ref>, indicating that the common label smoothing operation fails to reflect the similarity between categories.</p><p>We propose Restricted Label Smoothing Cross Entropy Loss to exploit the degree of the similarity between categories implied in the clustering results generated for ID as-sociation. After DBSCAN completed, for each cluster, K nearest clusters are determined according to the distance of deep features. For each unlabeled training sample, in addition to the category assigned by the cluster algorithm, the K nearest clusters are the K categories to which the sample is most likely to belong. As shown in <ref type="bibr">Equation 4</ref>, when performing label smoothing processing, ? is equally distributed to the corresponding K nearest clusters and the probabilities of the other negative categories remain 0. RLSCE can effectively utilize the similarity between categories while suppressing the negative effects of noisy pseudo labels, which achieves better performance than LSCE.</p><formula xml:id="formula_5">p j i = ? ? ? ? ? 1 ? ? if j = y ? / K j is in K-nearest clusters 0 otherwise (4)</formula><p>Weight Adaptive Triplet Loss The triplet loss function is commonly adopted in person re-ID research which is calculated based on the identity relationship between training samples. By optimizing triplet loss, the features of samples with the same ID are pulled closer and that of samples with different IDs are pushed away. The value of triplet loss is proportional to ?d = d i,p ? d i,n , where d i,p and d i,n represent the feature distance between anchor and its hardest positive sample and hardest negative sample respectively. When utilizing the pseudo labels generated by clustering to calculate triplet loss, there are two possible cases of wrong pseudo label assignment. a. Incorrectly assign samples with the same ID to different clusters, that is, the real IDs of a n and a i are same, which causes the value of d i,n to be smaller than its actual value. b. Samples with different IDs are assigned to same cluster. The real IDs of a p and a i are different, leading to a larger value of d i,p . According to the above analysis, both wrong pseudo label assignments will lead to an increase in the value of ?d and thus an increase in the value of triplet loss, having a larger impact to the model weights during backward propagation.</p><formula xml:id="formula_6">w(?d) = 0 if ?d ? ?margin e ?(?d+margin) if ?d &gt; ?margin<label>(5)</label></formula><formula xml:id="formula_7">L W AT L = 1 N N i=1 w (?d) ? max (0, ?d + margin) (6)</formula><p>We propose Weight Adaptive Triplet Loss (WATL) to reduce the weight of the triplet loss, and the weight is determined according to the value of ?d. The calculation formula is shown in <ref type="bibr">Equation 5</ref>, where the hyperparameter ? is a negative number whose absolute value represents the strength of the triplet loss reduction and m is the margin in the original triplet loss. A larger ?d generates a smaller weight because a larger ?d correlates with a higher noise ratio of pseudo labels. Weight Adaptive Triplet Loss is obtained by multiplying the weights by the traditional triplet loss, see Equation <ref type="bibr" target="#b5">6</ref>. Dynamic Training Iterations In each epoch of deep learning model training, the training set is input into the model in batches, and then the loss function is calculated and optimized to update the model weights. Each batch contains batchsize samples from the training set and the training process of a batch is called an iteration. In person re-ID research, in order to make the data of different (pseudo label) IDs relatively balanced and ensure enough positive and negative sample pairs in each batch, the training data sampling is not completely random. First, select N ID IDs that are not already used in the current epoch, and m samples are selected for each ID to form a batch, that is, N ID ? m = batchsize. Therefore, the total number of IDs involved in training for each epoch is N ID ? N iter and the total number of training samples is batchsize ? N iter , where N iter is the number of iterations for each epoch. <ref type="figure" target="#fig_4">Figure 3</ref> shows the number of samples assigned pseudo labels and the number of pseudo labels generated by DB-SCAN as functions of epoch during unsupervised training. On MARS and DukeMTMC-VideoReID, both of them gradually increase as the training progresses. Because in the initial stage of training, the quality of the features extracted by the model is low, causing many training samples to be treated as noise during clustering, which are not assigned pseudo labels and cannot participate in training. In addition, the number of clusters generated by DBSCAN is small as the features are not discriminative enough in the early stage of training.</p><p>From the above analysis, the total number of IDs and that of training samples in each epoch of training are N ID ? N iter and batchsize ? N iter , respectively. N ID and batchsize are hyperparameters. If the value of N iter is fixed, the total number of IDs and training samples involved in each epoch remain unchanged during training. However, as the training progresses of unsupervised re-ID methods, the number of samples assigned pseudo labels (the number of training samples) and the number of pseudo labels generated by clustering (the number of IDs) gradually increase. In addition, the accuracy of pseudo labels gradually improves during training, and for more accurate and reliable labels, more iterations should be used for sufficient learning. Therefore, we propose Dynamic Training Iterations, N iter = N pid ? r iter , where N pid represents the number of pseudo labels of current training epoch and r iter is the training iteration coefficient. As the number of pseudo labels increases, the number of IDs and training samples involved in each training epoch gradually increase, so as to fully utilize the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The methods we proposed were evaluated on two widely-used video-based re-ID datasets, i.e., MARS and DukeMTMC-VideoReID, which contain more abundant visual features and auxiliary information than image-based datasets. MARS contains 17,503 tracklets for 1,261 identities and 3,248 distractor tracklets, which are recorded by six cameras. The dataset is split into 625 identities for training and 636 identities for testing. DukeMTMC-VideoReID consists of 702 identities for training, 702 identities for testing, and 408 identities as distractors. In total there are 2,196 tracklets for training and 2,636 tracklets for testing, where all tracklets are collected from 8 cameras. It should be noted that the experiments about three auxiliary information exploiting modules are only conducted on DukeMTMC-VideoReID because necessary temporal information of training samples in the raw surveillance video is unknown in MARS. We adopted the Cumulative Matching Characteristic (CMC) and the mean average precision (mAP) to evaluate the performance of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We utilize a simplification of MMT which contains only one network and its past temporal average model as our overall framework. The student model and the teacher model have the same structure, whose backbone consists of ResNet50 <ref type="bibr" target="#b6">[7]</ref> and BNNeck <ref type="bibr" target="#b8">[9]</ref>. The iterative pipeline of model training and pseudo label generation are kept. The hard pseudo labels are generated by DBSCAN. In addition, other hyperparameters related to MMT are consistent with those in the original paper.</p><p>Through experiments, for Restricted Label Smoothing Cross Entropy Loss, the ? and K in Equation 4 are set to 0.1 and 60, respectively. For Weight Adaptive Triplet Loss, margin and ? in Equation 5 are set to 0.5 and -0.01, respectively. The r iter of Dynamic Training Iterations is set to 0.6. For auxiliary information exploiting modules, ? and Methods MARS DukeMTMC rank-1 mAP rank-1 mAP DAL <ref type="bibr" target="#b2">[3]</ref> 46.8 21.4 --RACE <ref type="bibr" target="#b13">[14]</ref> 41.0 22.3 --DGM+ <ref type="bibr" target="#b14">[15]</ref> 48.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art</head><p>We compare our proposed methods with six stateof-the-art methods of unsupervised person re-ID on two video-based datasets, MARS and DukeMTMC-VideoReID. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. Our method significantly outperforms existing video unsupervised approaches on MARS and DukeMTMC-VideoReID. "Ours-" means the training framework proposed by this paper utilizing the three training tricks including RLSCE, WATL and DTI, which achieves 80.9% rank-1, 72.4% mAP on MARS and 85.3% rank-1, 81.1% mAP on DukeMTMC-VideoReID. "Ours" uses the three auxiliary information exploiting modules including TOC, STS and SCP on the basis of "Ours-", which achieves 91.9% rank-1, 89.9% mAP on DukeMTMC-VideoReID. "Ours" cannot be applied to MARS due to the lack of auxiliary information. Compared with the state-of-the-art supervised re-ID methods, STRF <ref type="bibr" target="#b0">[1]</ref> and PSTA <ref type="bibr" target="#b10">[11]</ref>, our methods also shorten the performance gap between supervised and unsupervised methods.   We compare the performance of fixed iterations and dynamic iterations in which the training iteration coefficient is set to 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the training tricks</head><p>model that utilize the training framework proposed by this paper with traditional ID loss and triplet loss was used as baseline. The performance of the baseline model is present in <ref type="table" target="#tab_3">Table 2</ref>. Replacing tradition ID loss with RLSCE improves model performance by 1.4% and 4.3% mAP on MARS and DukeMTMC-VideoReID, which also outperforms baseline with normal label smoothing cross entropy loss. Baseline with WATL also outperforms the baseline by 1.1% and 0.9% mAP on the two datasets. The combined use of RLSCE and WATL achieves a model performance of 72.4% and 81.1% mAP on MARS and DukeMTMC-VideoReID, respectively, which show that RLSCE and WATL can effectively boost the performance of model. We also verify the effectiveness of Dynamic Training Iterations, results of which are shown in <ref type="table" target="#tab_4">Table 3</ref>. When r iter is set to 0.6, the dynamic iteration method achieves the best training effect, where the iterations varies from 200 to 500 during training. The performance of DTI outperforms fixed iterations methods in which training iteration is set to 200, 300, 400 and 500. It should be noted that RLSCE, WATL and DTI also have an improved effect on image-based re-ID datasets.</p><p>Effectiveness of the auxiliary information exploiting modules The three proposed auxiliary information exploiting modules can not only boost the performance of the model itself, but also correct the sorting results of the sam-   <ref type="table">Table 6</ref>: Ablation studies on Same-Camera Penalty.</p><p>ples in the gallery during the testing phase to improve the accuracy of inference. As illustrated in <ref type="table">Table 4</ref>, 5 and 6, using Time-Overlapping Constraint, Spatio-Temporal Similarity and Same-Camera Penalty in the training phase or the inference phase can achieve a considerable performance improvement, and using the module in both phases is the best. Jointly utilizing the above three modules in the training phase and the testing phase can reach 91.9% rank-1 and 89.9% mAP on DukeMTMC-VideoReID. It should be noted that STS and SCP are applicable to the image-based re-ID methods as long as the dataset provides temporal information and camera information of each training sample, while TOC can only be used in video-based re-ID methods because TOC is based on time segments overlap between samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a high-accuracy unsupervised person re-ID methods using three auxiliary information exploiting modules and three training tricks. The auxiliary information exploiting modules including Time-Overlapping Constraint, Spatio-Temporal Simialrity and Same-Camera Penalty improve the model performance and inference accuracy by constructing association constraints or fusing</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Input: Set of unlabeled training samples D, hyperparameters and M inP ts Output: Result of clustering R Initialize unconnectable point set of each sample {S 1 , S 2 , ...S m }; Initialize number of clusters k = 0; Initialize set of unclassified points ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figures (a)(b)(c)(d) are probability distribution histograms of the time spent moving between four camera pairs in DukeMTMC-VideoReID. The histograms can be well fitted by Gaussian mixture distribution, whose probability distribution functions are plotted with red line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of model training using Spatio-Temporal Similarity module. At the start of each training epoch, the pseudo labels are firstly generated according to the visual features and then obtain the Gaussian mixture functions for camera network. Spatio-temporal similarity is fused with visual features and the generated fusion distance is used to generate more accurate pseudo labels, which are utilized for model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the deep features extracted by model reduces the accuracy of clustering. In order to alleviate the negative impact of the noise in pseudo labels on model training, we propose three training tricks: Restricted Label Smoothing Cross Entropy Loss, Weight Adaptive Triplet Loss and Dynamic Training Iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) The number of samples assigned pseudo labels as function of epoch during unsupervised re-ID training. (b) The number of pseudo labels generated by DBSCAN as function of epoch during unsupervised re-ID training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Comparisons with state-of-the-art. We compare</cell></row><row><cell>our proposed method with six unsupervised state-of-the-art</cell></row><row><cell>methods and two supervised methods on two video re-ID</cell></row><row><cell>datasets. "Ours-" is our method with auxiliary information</cell></row><row><cell>exploiting modules omitted. The three auxiliary informa-</cell></row><row><cell>tion exploiting modules are only applied to DukeMTMC-</cell></row><row><cell>VideoReID because of the lack of auxiliary information in</cell></row><row><cell>MARS.</cell></row><row><cell>? in Equation 1 are set to 0.6 and 8.0; ? c in Equation 2 is</cell></row><row><cell>set to 0.006.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>To investigate the effectiveness of the proposed Restricted Label Smoothing Cross Entropy Loss and Weight Adaptive Triplet Loss, a</figDesc><table><row><cell>Methods</cell><cell cols="4">MARS rank-1 mAP rank-1 mAP DukeMTMC</cell></row><row><cell>baseline</cell><cell>79.0</cell><cell>70.3</cell><cell>80.6</cell><cell>75.6</cell></row><row><cell>baseline+LSCE</cell><cell>78.8</cell><cell>70.5</cell><cell>82.3</cell><cell>78.4</cell></row><row><cell>baseline+RLSCE</cell><cell>79.7</cell><cell>71.7</cell><cell>83.6</cell><cell>79.9</cell></row><row><cell>baseline+WATL</cell><cell>79.8</cell><cell>71.4</cell><cell>81.3</cell><cell>76.5</cell></row><row><cell>baseline+RLSCE+WATL</cell><cell>80.9</cell><cell>72.4</cell><cell>85.3</cell><cell>81.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on Restricted Label Smoothing Cross Entropy Loss and Weight Adaptive Triplet Loss. "Baseline" means the training framework proposed by this paper with traditional ID loss and triplet loss.</figDesc><table><row><cell>N iter</cell><cell cols="4">MARS rank-1 mAP rank-1 mAP DukeMTMC</cell></row><row><cell>r iter = 0.6</cell><cell>80.9</cell><cell>72.4</cell><cell>85.3</cell><cell>81.1</cell></row><row><cell>200</cell><cell>77.2</cell><cell>67.2</cell><cell>80.9</cell><cell>76.1</cell></row><row><cell>300</cell><cell>79.7</cell><cell>70.5</cell><cell>82.1</cell><cell>77.3</cell></row><row><cell>400</cell><cell>78.6</cell><cell>69.9</cell><cell>82.5</cell><cell>78.2</cell></row><row><cell>500</cell><cell>75.6</cell><cell>65.4</cell><cell>79.1</cell><cell>72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on Dynamic Training Iterations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies on Spatio-Temporal Similarity.</figDesc><table><row><cell cols="2">Train Inference</cell><cell cols="2">DukeMTMC rank-1 mAP</cell></row><row><cell>w/o</cell><cell>w/o</cell><cell>80.6</cell><cell>75.6</cell></row><row><cell>w</cell><cell>w/o</cell><cell>81.9</cell><cell>78.8</cell></row><row><cell>w/o</cell><cell>w</cell><cell>82.3</cell><cell>77.1</cell></row><row><cell>w</cell><cell>w</cell><cell>82.9</cell><cell>79.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>with visual features. In order to alleviate the negative impact of noise in pseudo labels, we propose three training tricks, including Restricted Label Smoothing Cross Entropy Loss, Weight Adaptive Triplet Loss and Dynamic Training Iterations. Extensive experiment results on MARS and DukeMTMC-VideoReID show that the above proposed modules or tricks can achieve performance improvements and our framework outperforms existing unsupervised re-ID methods, and it also shorten the performance gap between supervised and unsupervised methods.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatiotemporal representation factorization for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep association learning for unsupervised video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07301</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">kdd</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised tracklet person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1770" to="1782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pyramid spatial-temporal aggregation for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingquan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust anchor embedding for unsupervised video person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic graph co-matching for unsupervised videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2976" to="2990" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting robust unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghao</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IET Image Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
