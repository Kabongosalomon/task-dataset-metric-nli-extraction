<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Training of Audio Transformers with Patchout</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Koutini</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">&amp; LIT AI Lab 2</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Schl?ter</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">&amp; LIT AI Lab 2</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Eghbal-Zadeh</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">&amp; LIT AI Lab 2</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">&amp; LIT AI Lab 2</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computational Perception</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Training of Audio Transformers with Patchout</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: transformers</term>
					<term>audio-tagging</term>
					<term>attention models</term>
					<term>audio classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The transformer architecture <ref type="bibr" target="#b0">[1]</ref> has proven very successful in sequence modeling. It allows learning dependencies between different items in the sequence regardless of their positions or their separation in the sequence. Transformers are the state-of-the-art models in different natural language processing tasks <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. More recently, they have been adapted to computer vision <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> by extracting small patches from the input image and adding a learnable positional encoding to each patch. The resulting patches form a sequence that can be fed to the transformer. These vision transformer models achieve state-of-theart performance on image classification tasks, but require large amounts of training data (e.g, in Vision Transformer (ViT) <ref type="bibr" target="#b4">[5]</ref>), or heavily depend on extensive data augmentation and knowledge distillation from a CNN model (e.g, in Data-efficient Image Transformers (DeiT) <ref type="bibr" target="#b7">[8]</ref>). Gong et al. <ref type="bibr" target="#b8">[9]</ref> further adapted vision transformers to audio spectrograms, achieving state-ofthe-art performance on Audioset <ref type="bibr" target="#b9">[10]</ref> by using pre-trained models from computer vision and using overlapping patches from audio spectrograms for fine-tuning.</p><p>The transformer architecture consists of a series of selfattention layers <ref type="bibr" target="#b0">[1]</ref>. Each layer relies on calculating a distance between each pair of items from the input sequence. Although this allows each input item to attend to any other item in the sequence, this results in a complexity of O(n 2 ) with respect to the input sequence length n, in terms of both memory and computation effort. Reducing the quadratic complexity has been the target of several approaches in natural language processing, the idea being to restrict each input item (token) to attend On the largest publicly available audio dataset, our approach PaSST-S can reach the state-of-the-art performance in less than 2 days on a single consumer GPU. Details are presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>only to a pre-selected subset of input items (tokens). One example is to allow attending only to neighbours inside a sliding window <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Additionally, Kitaev et al. <ref type="bibr" target="#b12">[13]</ref> use localitysensitive hashing to approximate attention, reducing the attention complexity to O(n log n). BigBird <ref type="bibr" target="#b13">[14]</ref> combines sliding windows, global attention, and random interaction between the sequence items. Masking portions of the input sequence has been shown to be an extremely effective method for training encoder/decoder transformers in NLP <ref type="bibr" target="#b1">[2]</ref>. In computer vision, the idea of removing patches during inference was investigated to assess the vision transformer's robustness against input perturbations <ref type="bibr" target="#b14">[15]</ref>. In this paper, we focus on applying transformers to audio processing. We address the shortcomings of current audio transformers from the aspect of computational complexity and memory requirements by introducing a new simple yet effective method for training transformers on spectrograms. In summary, the main contributions of our work are as follows:</p><p>? We propose Patchout, a method that significantly reduces the computation and memory complexity of training transformers for the audio domain. Patchout also functions as a regularizer, improving the generalization of the trained transformers.</p><p>? We disentangle the transformer's positional encoding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> into time and frequency positional encoding, allowing for straightforward inference on audio snippets of variable length without the need for fine-tuning or interpolating positional encodings.</p><p>? We investigate additional methods for reducing training complexity and demonstrate how they affect performance on the larger general-purpose Audioset as well as domain-specific downstream tasks.</p><p>Our proposed models can achieve state-of-the-art performance on several audio tagging and classification tasks using a single consumer GPU, in a relatively short time (see <ref type="figure" target="#fig_0">Figure 1</ref>). When different complexity reduction methods are combined, the models outperform CNNs in terms of training speed and memory requirements, in addition to generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Patchout faSt Spectrogram Transformer (PaSST)</head><p>The Vision Transformer (ViT) <ref type="bibr" target="#b4">[5]</ref> works by extracting small patches from an input image and projecting these patches linearly onto a sequence of embeddings. The sequence is then augmented by adding trainable positional encodings as biases to the input sequence. A special classification embedding (classification token) is then appended to the sequence, which is connected to a classifier after the self-attention layers. In Dataefficient Image Transformers (Deit) <ref type="bibr" target="#b7">[8]</ref> another special embedding for distillation (distillation token) is added. Gong et al. <ref type="bibr" target="#b8">[9]</ref> showed that overlapping the extracted patches improves the performance when training ViT on spectrograms. On the other hand, patch overlapping increases the total number of patches, i.e., the input sequence length. Therefore, overlapping greatly increase the memory and compute requirements for training the transformers. We propose a new method called Patchout (Section 2.2) to overcome these issues. <ref type="figure">Figure 2</ref> summarizes the proposed transformer architecture: The pipeline starts at the upper left with an audio spectrogram being fed into the model as input. (1) is the patch extraction and linear projection steps as explained in <ref type="bibr" target="#b4">[5]</ref>. In (2), frequency and time positional encodings are added, as discussed below. In (3), we apply Patchout as explained in Section 2.2, and add the classification token. In (4), we flatten the sequence and pass through d layers blocks of self-attention (d is the depth of the transformer). Finally, a classifier operates on the mean of the transformed C and D tokens.</p><p>In Addition to Patchout (step 3 in <ref type="figure">Figure 2</ref>), the second main difference between our work and previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> is that we disentangle the positional encoding for the time and frequency dimensions, and as a result, we have two positional encodings: one representing the frequency, and one for time (step 2 in <ref type="figure">Figure 2</ref>). This makes inference and the tuning of the pre-trained models on downstream tasks with shorter audio length simpler. When fine-tuning or inference on shorter audio clips, we simply crop the time positional encoding parameters, without changing the frequency encoding parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Complexity Analysis</head><p>Multi-head attention layers <ref type="bibr" target="#b0">[1]</ref> rely on computing a distance between each pair of positions in the input sequence (in the form of an attention matrix), therefore having a complexity of O(n 2 ) where n is the input sequence length. As the sequence length grows -for example, when overlapping input patches or for longer audio clips -the compute and memory requirements quickly become problematic. More specifically, given an input of b samples of the dimension (n ? e), where b is the batch size, <ref type="figure">Figure 2</ref>: The Patchout transformer (PaSST) architecture as explained in Section 2. The Self-attention layer + FFN (Feedforward network) is explained in detail in <ref type="bibr" target="#b4">[5]</ref>. C: classification token. D: distillation token (only for models based on DeiT).</p><p>n is the input sequence length, e is the embeddings size, each multi-head attention layer projects each input sample to h query Q, key K, and value V matrices, where h is the number of attention heads <ref type="bibr" target="#b0">[1]</ref>. Each of Q, K and V has a shape of (n ? e h ). The attention matrix A is then computed by the matrix multiplication Q ? K T , scaling, and applying the soft-max activation</p><formula xml:id="formula_0">function A = Sof tmax( Q?K T ? d ).</formula><p>A has a shape of (n ? n) and is multiplied with V giving the attention output: A ? V resulting in a new sequence with the same shape as the input (n ? e). As a result, the computation complexity (and memory requirements) for all the operations on the attention matrix A grow quadratically O(n 2 ) with sequence length n, while the operations in the rest of the network have a linear complexity relationship with n <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>. In short, reducing the sequence length would have a large impact on the computational complexity of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Patchout</head><p>Motivated by (a) the impact of reducing the sequence length on the computation complexity of training transformer models; (b) the fact that audio events are expected to be spread out in time and frequency in an audio clip; (c) the insight that CNNs can benefit from having a small receptive field during training for different audio tasks, as shown in <ref type="bibr" target="#b15">[16]</ref>, we propose Patchout, a method to efficiently train transformer models on audio spectrograms. The idea is to drop parts of the transformer's input sequence when training, encouraging the transformer to perform the classification using an incomplete sequence. We first extract small overlapping patches from the input spectrograms and linearly project them to vectors, forming the transformer input sequence. We augment the patches with both frequency and time encoding. When training, we randomly drop parts of the sequence, reducing the sequence length, and effectively regularizing the training process. Similar to DropOut <ref type="bibr" target="#b16">[17]</ref>, during inference, the whole input sequence is presented to the transformer. We distinguish between different types of Patchout as follows:</p><p>Unstructured Patchout is the basic form of Patchout, where we select the patches randomly regardless of their position. We refer to models trained with this method as PaSST-U. Structured Patchout: We randomly pick some frequency bins/time frames and remove a whole column/row of extracted patches. This structure is inspired by SpecAugment <ref type="bibr" target="#b17">[18]</ref>. We refer to models trained with this method as PaSST-S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Further Complexity Reduction Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Reducing the extracted patches' overlap</head><p>Reducing the overlap between patches results in a lower number of extracted patches, and therefore a smaller transformer input sequence length. However, Gong et al. <ref type="bibr" target="#b8">[9]</ref> showed that reducing the overlap (or training without overlapping) degrades the performance of the transformer on Audioset. Patchout can also be used even when there is no overlap between patches. We refer to the system without patch overlapping as PaSST-N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Reducing the depth of the transformer</head><p>The depth of the transformer is the number of successive selfattention blocks ( d in <ref type="figure">Figure 2</ref>). The depth has a linear relationship with the overall training and inference complexity and influences the total number of parameters of the model. Since we are starting the training from models pre-trained on Imagenet <ref type="bibr" target="#b18">[19]</ref> (as explained in Section 3.2), we remove every other self-attention block. This allows us to benefit from the pretraining, compared to removing consecutive blocks, since the residual activations will have a less sudden change. We will refer to the model with the removed blocks as PaSST-L. It has d = 7 self-attention blocks and 50M parameters compared to 87M in the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Setup</head><p>We train our models on Audioset <ref type="bibr" target="#b9">[10]</ref>, the largest publicly available audio dataset, consisting of around 2 million audio clips from Youtube. The task is to tag the audio clips with labels from 527 possible classes. Furthermore, we fine-tune the models trained on Audioset on various audio classification and tagging tasks, namely, instrument recognition, environmental audio classification, and acoustic scene classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preprocessing and Training Setup</head><p>We use mono audio with a sampling rate of 32 kHz. We extract Mel features from a window of 25 ms with a hop length of 10 ms, resulting in 128 mel bands, similar to <ref type="bibr" target="#b8">[9]</ref>. Kong et al. <ref type="bibr" target="#b19">[20]</ref> showed the importance of balancing Audioset; therefore, we balance our training data using importance sampling. We assign a sampling weight to each sample proportional to the inverse frequency of its label 1 freq(label)+100 . We train on 1, 893, 693 (approx. 2M) training segments, and evaluate on 18, 951 audio clips. For each epoch, we sample 200k samples from the full 2M Audioset without replacement. We use the AdamW <ref type="bibr" target="#b20">[21]</ref> optimizer with weight decay of 10 ?4 , with a maximum learning rate of 10 ?5 . We use a linear learning rate decay from epoch 50 to 100, dropping the learning rate to 10 ?7 and fine-tune the model for a further 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ImageNet Pretraining</head><p>Gong et al. <ref type="bibr" target="#b8">[9]</ref> showed that using pre-trained models on Imagenet significantly improves their performance on Audioset. Therefore, we will use pre-tranined models in all our experi-mAP Speed (? AST) Mem Seq Baseline <ref type="bibr" target="#b9">[10]</ref> .314 ---PANNs <ref type="bibr" target="#b19">[20]</ref> .439 131.0 (5.7?) .213 -AST <ref type="bibr" target="#b8">[9]</ref> .459 23.1 (1?) 2.33 1212 AST <ref type="bibr" target="#b8">[9]</ref> .459 23.1 (1?) 2.33 1190 AST-N <ref type="bibr" target="#b8">[9]</ref> .454 80.</p><p>(3.4?) .534 498 CNN <ref type="bibr" target="#b22">[23]</ref> . ments.Our base model is DeiT B?384 <ref type="bibr" target="#b7">[8]</ref>. We also achieve a comparable performance using computationally more complex ViT models such as stripped-down ViT-hug224 <ref type="bibr" target="#b4">[5]</ref>; by removing half of the self-attention blocks, its depth was reduced to only 16 blocks (with the methods explained in Section 2.3.2); this will not be further explored in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation</head><p>The transformer models are very prone to overfitting, therefore data augmentation plays an essential role in the training process <ref type="bibr" target="#b7">[8]</ref>. In our experiments, the following augmentation strategies are used:</p><p>Two-level Mix-Up: We use Mix-up <ref type="bibr" target="#b21">[22]</ref> since it has been shown to improve performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. We mix both the raw waveforms randomly from the dataset as well as the final spectrograms.</p><p>Specaugment: We use SpecAugment <ref type="bibr" target="#b17">[18]</ref> by masking up to 48 frequency bins and 192 time frames similar to <ref type="bibr" target="#b8">[9]</ref>.</p><p>Rolling: We roll the waveforms randomly over time.</p><p>Random Gain: We multiply the audio waveforms to change the gain by ?7 dB. <ref type="table" target="#tab_0">Table 1</ref> shows the mean average precision mAP (also referred to as precision-recall area under-curve) results on Audioset <ref type="bibr" target="#b9">[10]</ref>. As can be seen, the proposed model PaSST achieves a new state-of-the-art performance on the largest available audio tagging dataset. The proposed model outperforms AST <ref type="bibr" target="#b8">[9]</ref> and significantly outperforms CNNs. Using Patchout not only improves the performance of the transformer architecture, but also increases the training speed approximately 4 times, and reduces the required GPU memory to less than 25%. As a result, it is possible to train PaSST on a single Nvidia RTX 2080ti (consumer GPU), achieving state-of-the-art performance in 50 hours. Furthermore, PaSST-L-S (with a scaled down depth of d = 7) and PaSST-S-N (without patch overlap) significantly outperform CNNs while maintaining a higher training throughput, and with similar GPU memory requirements. PaSST-S-L and PaSST-S-N can be trained on a single GPU to reach .459 and .466 mAP in approximately 25 hours. Applying Patchout on the transformer without overlap (PaSST-S-N) outperforms the baseline PaSST-B (without Patchout) and AST <ref type="bibr" target="#b8">[9]</ref> while being up to 8 times faster, and requiring less than 10% of the GPU memory for training. The results are also illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The only difference between the baseline PaSST-B and the AST model is the positional encoding. AST <ref type="bibr" target="#b8">[9]</ref>, like vision transformers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>, employs grid positional encoding. PaSST-B, on the other hand, utilises disentangled time and frequency positional encoding (see Section 2). mAP Baseline <ref type="bibr" target="#b9">[10]</ref> .314 PSLA (Ensemble-S) <ref type="bibr" target="#b23">[24]</ref> .469 PSLA (Ensemble-M) <ref type="bibr" target="#b23">[24]</ref> .474 AST (Ensemble-M) <ref type="bibr" target="#b8">[9]</ref> .475 AST (Ensemble-M) <ref type="bibr" target="#b8">[9]</ref> .485 PaSST-S S16,14 <ref type="bibr">(2 models)</ref> .486 PaSST-S S10-16 (4 models)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Audio Tagging on Audioset</head><p>.493 PaSST-S S10-16 <ref type="bibr">(5 models)</ref> .495 PaSST-S S10-16 (9 models)</p><p>.496 <ref type="table">Table 2</ref>: Ensemble results on Audioset.</p><p>indicates our run. mAP is the mean average precision (also referred to as precision/recall area under curve). <ref type="table">Table 2</ref> shows the result of ensemble models. We ensemble models with different overlap values between input patches <ref type="figure">(Figure 2</ref>). S indicates the patches stride, S16 means no overlap between the patches. The first ensemble (2 models) averages the logits of a model with no patches overlap and a model with an overlap of 2 (stride=14). S10-S16 indicates that the models used have strides of 10,12,14 and 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-tuning and Transfer to Downstream Tasks</head><p>We fine-tune the pre-trained (on Audioset) models on several downstream audio tagging and classification tasks with different dataset sizes, <ref type="table">Table 3</ref> summarizes the results. PaSST-(B,U,S) models use the pre-trained PaSST-S on Audioset, but for fine-tuning, we use no Patchout, unstructured Patchout, and structured Patchout respectively. It is worth noting that the transformer models can be fine-tuned using a small number of epochs. The results suggest that researchers and practitioners can use pre-trained PaSST and fine-tune them on downstream tasks without the need for large computational resources.</p><p>In summary, fine-tuning the transformer model outperforms state-of-the-art CNNs on all tasks. Patchout results in significant speedups and, in many cases, improved generalization. When combined with Structured Patchout (-S), reducing complexity by removing patch overlap (-N) performs better than reducing transformer depth (-L) and enables faster fine-tuning.</p><p>We only replace the MLP classifier in the pre-trained models for fine-tuning. When we use Patchout, we randomly remove roughly half of the input sequence. Each experiment was repeated three times, and the average results are reported. The speedup in <ref type="table">Table 3</ref> is relative to PaSST-B and is rounded up to the nearest integer. Details on the setup of each task can be found in our github repository. Polyphonic Musical Instrument Recognition: The task here is to detect all the instruments present in an audio clip. The</p><p>OpenMIC dataset <ref type="bibr" target="#b24">[25]</ref> consists 20,000 audio clips. Each clip is 10 seconds long and can be assigned multiple tags out of 20 classes. The metric for the task is the mean average precision. The state-of-the-art methods for this task are CNNs with restricted receptive fields <ref type="bibr" target="#b22">[23]</ref>. PaSST-S-N reaches the state-ofthe-art performance in less than 30 minutes on a single consumer GPU. Environmental Sound Classification: The ESC50 dataset <ref type="bibr" target="#b25">[26]</ref> consists of 2,000 environmental 5-second audio clips. The task is to classify each clip into one out of 50 possible classes. We report the accuracy averaged over the 5 official folds <ref type="bibr" target="#b25">[26]</ref>. All PaSST variants (with Patchout) can be fine-tuned on this dataset in less than 5 minutes on a single GPU. The state-of-the-art performance was achieved using the AST transformer model <ref type="bibr" target="#b8">[9]</ref>. The difference between AST and PaSST-B is in the positional encoding, as explained in Section 2. Acoustic Scene Classification: The task is to recognize the acoustic scene of 10-second audio clips. We use the TAU Urban Acoustic Scenes 2020 Mobile dataset <ref type="bibr" target="#b26">[27]</ref> as used in the DCASE 2020 challenge (DCASE20). The audio clips are recorded with different devices and further simulated devices are introduced. The performance is measured using accuracy on a dataset including unseen devices. The first place in the challenge used CNNs <ref type="bibr" target="#b27">[28]</ref>. Patchout accelerates training on this task, reaching state-of-the-art in less than an hour. Patchout also allows for fine-tuning on a single consumer GPU. It does, however, lead to a decrease in accuracy. Sound Event Recognition (Tagging) on FSD50K: The FSD50K dataset <ref type="bibr" target="#b28">[29]</ref> consists of 51K audio clips annotated with 200 sound event classes taken from the Audioset ontology <ref type="bibr" target="#b9">[10]</ref>. The dataset contains 100 hours of audio and is the second largest publicly available general purpose sound event recognition dataset after Audioset. Furthermore, the FSD50K evaluation set is of high quality, with each evaluation label being double-checked and assessed by two to five independent annotators <ref type="bibr" target="#b28">[29]</ref>. The reported results are on the official evaluation subset of FSD50K using the best model on the validation subset. The state-of-the-art in PSLA <ref type="bibr" target="#b23">[24]</ref> is achieved through CNN architecture and a collection of performance-improving methods such as ImageNet pre-training, label enhancement, balancing, data augmentation, and weight averaging. On this dataset, our approach significantly outperforms the current state-of-the-art. Fine-tuning PaSST-S and PaSST-S-N takes less than 2 hours and 1 hour, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OpenMIC ESC50</head><p>DCASE20 FSD50K Baseline .795 <ref type="bibr" target="#b24">[25]</ref> 76.9 <ref type="bibr" target="#b25">[26]</ref> 54.1 <ref type="bibr" target="#b26">[27]</ref> .434 <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>: Results in performance and speedup compared to the base model PaSST-B for the downstream tasks: polyphonic instrument tagging using OpenMIC <ref type="bibr" target="#b24">[25]</ref> dataset (mean average precision), Environmental Sound Classification ESC50 <ref type="bibr" target="#b25">[26]</ref> (accuracy), Cross-device Acoustic Scene Classification DCASE20 <ref type="bibr" target="#b26">[27]</ref> (accuracy). Sound Event Recognition (Tagging) in FSD50K <ref type="bibr" target="#b28">[29]</ref>(mean average precision). The second part of the table compares different PaSST variants ( Table 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a new method for efficiently training transformers on audio spectrograms, achieving state-of-the-art performance on Audioset as well as several downstream tasks. Furthermore, Patchout significantly reduces compute complexity and memory requirements for training transformers. We investigate additional methods for reducing training complexity and propose two models, PaSST-S-L and PaSST-S-N, that outperform CNNs while having a faster training speed and comparable memory requirements. Our pre-trained models can be fine-tuned on several audio downstream tasks with little resources and little additional training time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance vs training speed on Audioset. The radius of the circle indicates the required GPU memory per sample for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This work has been supported by the COMET-K2 Center of the Linz Center of Mechatronics (LCM) funded by the Austrian Federal Government and the Federal State of Upper Austria. The LIT AI Lab is financed by the Federal State of Upper Austria. The computational results presented have been achieved in part using the Vienna Scientific Cluster (VSC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Single-model results on Audioset. indicates our run. mAP is the mean average precision (also referred to as precision/recall area under curve). Speed: training throughput in spectrograms per second on an Nvidia Titan RTX GPU (showing the speedup compared to AST [9]). Mem: the required GPU memory to train per sample. Seq: the training sequence length. B: Baseline without Patchout. U: Unstructured Patchout. S: Structured Patchout. N: no-overlap of the extracted patches. L: lighter model with reduced depth=7.</figDesc><table><row><cell></cell><cell>438</cell><cell>126.3</cell><cell>(5.5?)</cell><cell>.213</cell><cell>-</cell></row><row><cell>PaSST-B</cell><cell>.462</cell><cell>23.1</cell><cell>(1?)</cell><cell>2.33</cell><cell>1190</cell></row><row><cell>PaSST-U</cell><cell>.466</cell><cell>43.2</cell><cell>(1.9?)</cell><cell>1.14</cell><cell>790</cell></row><row><cell>PaSST-S</cell><cell cols="2">.471 88.7</cell><cell>(3.8?)</cell><cell>.513</cell><cell>474</cell></row><row><cell>PaSST-S-L</cell><cell>.459</cell><cell>148.6</cell><cell>(6.4?)</cell><cell>.311</cell><cell>474</cell></row><row><cell>PaSST-S-N</cell><cell>.466</cell><cell>184.2</cell><cell>(8?)</cell><cell>.202</cell><cell>254</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code and pretrained models: https://github.com/ kkoutini/PaSST</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Primer: Searching for efficient transformers for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Manke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LUKE: deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations ICLR 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021</title>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2021</title>
		<meeting><address><addrLine>Brno, Czechia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>New Orleans, LA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multipassage BERT: A globally normalized BERT model for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5877" to="5881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-02" />
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Receptive Field as a Regularizer in Deep Convolutional Neural Networks for Acoustic Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>EUSIPCO 2019, A Coru?a, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-09-19" />
			<biblScope unit="page" from="2613" to="2617" />
			<pubPlace>Graz, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference On Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Receptive field regularization techniques for audio classification and tagging with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PSLA: Improving audio tagging with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">OpenMIC-2018: An open data-set for multiple instrument recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09-23" />
			<biblScope unit="page" from="438" to="444" />
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acoustic scene classification in DCASE 2020 Challenge: generalization across devices and low complexity solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE2020 Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Designing Acoustic Scene Classification Models with CNN Variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2020 Challenge</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FSD50K: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="829" to="852" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
