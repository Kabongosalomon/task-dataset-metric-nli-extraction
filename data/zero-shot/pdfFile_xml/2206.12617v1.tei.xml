<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Models as Knowledge Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Fudan-Aishu Cognitive Intelligence Joint Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Models as Knowledge Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or descriptionbased. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental results show that LMKE achieves state-of-the-art performance on KE benchmarks of link prediction and triple classification, especially for long-tail entities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are multi-relational graphs composed of entities as nodes and relations as edges, such as WordNet <ref type="bibr" target="#b1">[Miller, 1995]</ref>. They have been used to support a wide range of applications, including information retrieval, recommender systems and question answering. For better applications of symbolic knowledge in KGs in recent machine learning models, many efforts have been devoted to embedding KGs into low-dimension vector spaces, which are referred to as knowledge embeddings (KEs). The principal applications of KEs are link prediction and triple classification in KGs, while there is also an increasing trend to use KEs in natural language processing (NLP) tasks like text generation.</p><p>There are mainly two kinds of existing KE methods, namely traditional structure-based methods and emerging description-based methods. Structure-based methods, such as TransE <ref type="bibr" target="#b0">[Bordes et al., 2013]</ref> and <ref type="bibr">RotatE [Sun et al., 2019]</ref>, are trained to preserve the inherent structure of KGs. These methods cannot well represent long-tail entities, as they depend solely on the structure of KGs and thus favor entities rich in structure information (i.e., linked with plentiful entities). However, real-world KGs are widely observed to have a right-skewed degree distribution, i.e., degrees of entities approximately follow the power-law distribution that forms a long tail, as is illustrated in <ref type="figure" target="#fig_0">Fig.1</ref>. These KGs are populated with massive unpopular entities of low degrees. For example, on WN18RR, 14.1% entities are with degree 1, and 60.7% entities have no more than 3 neighbors. Their embeddings consequently suffer from limited structural connectivity. This problem is justified by the declined performance of structurebased methods on long-tail entities shown in <ref type="figure" target="#fig_0">Fig.1</ref>, which suggests that their embeddings are still unsatisfactory. Description-based KE methods represent entities in KGs by encoding their descriptions with language models, such as DKRL <ref type="bibr" target="#b5">[Xie et al., 2016]</ref> and KEPLER <ref type="bibr" target="#b5">[Wang et al., 2019b]</ref>. Textual descriptions provide rich information for numerous semantic-related tasks, which brings an opportunity to learn informative representations for long-tail entities. In an extreme case, an emerging entity that is novel to an existing KG (in other words, having zero degrees in the KG) can still be well represented with its textual information. This feature is referred to as a capacity in the "inductive (zero-shot) setting" by prior approaches. Besides, lots of missing knowledge in KGs could be covered by rich textual information contained in entity descriptions or learned by pretrained language mod-els (PLMs), given that current text corpora outstrip KGs in scale and contain much more information. However, existing description-based methods barely outperform structure-based methods and suffer from the following problems: In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enhancing representations of long-tail entities and solving the above issues in prior description-based KEs. LMKE leverages the inductive capacity of description-based methods to enrich the representations of long-tail entities. In LMKE, entities and relations are regarded as special tokens whose output embeddings are contextualized in and learned from related entities, relations, and their textual information, so LMKE is also suitable for entities with no description. A contrastive learning framework is further proposed where entity embeddings within a mini-batch serve as negative samples for each other to avoid the additional cost of encoding negative samples. In summary, our contributions are listed as follows: 1 1. We identify the problem of structure-based KEs in representing long-tail entities, and generalize the inductive capacity of description-based KEs to solve this problem.</p><p>To the best of our knowledge, we are the first to propose leveraging textual information and language models to enrich representations of long-tail entities.</p><p>2. We propose a novel method LMKE that tackles two deficiencies of existing description-based methods, namely expensive negative sampling and restrictive description demand. We are also the first to formulate descriptionbased KE learning as a contrastive learning problem.</p><p>3. We conduct extensive experiments on widely-used KE benchmarks, and show that LMKE achieves the state-of-the-art performance on both link prediction and triple classification, significantly surpassing existing structure-based and description-based methods, especially for long-tail entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Knowledge embeddings represent entities and relations of KGs in low-dimension vector spaces. KGs are composed of triples, where a triple (h, r, t) means there is a relation r ? R between the head entity h ? E and the tail entity t ? E. E and R denote the entity set and the relation set respectively.</p><p>Structure-Based Knowledge Embeddings. Existing KEs are mainly structure-based, deriving the embeddings by 1 Our codes are available at https://github.com/Neph0s/LMKE solely structure information of KGs. These methods are further distinguished into translation-based models and semantic matching models in terms of their scoring functions <ref type="bibr" target="#b5">[Wang et al., 2017]</ref>. Translation-based models adopt distance-based scoring functions, which measure the plausibility of a triple (h, r, t) by the distance between entity embeddings h and t after a translation specific to the relation. The most representative model is TransE. It embeds entities and relations as h, r, t ? R d in a shared vector space with a dimension d. Its loss function is defined as ?h + r ? t? so as to make h close to t after a translation r. TransH <ref type="bibr" target="#b5">[Wang et al., 2014]</ref> proposes to project entity embeddings h and t to a relation-specific hyperplane, and TransR <ref type="bibr" target="#b0">[Lin et al., 2015]</ref> further proposes projections into relation-specific spaces. RotatE defines a relation as a rotation from entity h to entity t in a complex vector space, so their embeddings h, r, t ? C d are expected to satisfy h ? r ? t, where ? stands for element-wise product. Semantic matching models adopt similarity-based scoring functions, which measure the plausibility of a triple (h, r, t) by matching latent semantics of h, r, t. RESCAL <ref type="bibr" target="#b1">[Nickel et al., 2011]</ref> represents the relation r as a matrix M r and use a bilinear function h ? M r t to score (h, r, t). DistMult <ref type="bibr" target="#b6">[Yang et al., 2014]</ref> makes M r diagonal for simplicity and efficiency. CoKE <ref type="bibr" target="#b5">[Wang et al., 2019a]</ref> employs Transformers <ref type="bibr">[Vaswani et al., 2017]</ref> to derive contextualized embeddings, where triples or relation paths serve as the input token sequences. Description-based Knowledge Embeddings. Recently, description-based KE methods are gaining increasing attention for the importance of textual information and development in NLP. DKRL <ref type="bibr" target="#b5">[Xie et al., 2016]</ref> first introduces entities' descriptions and encodes them via a convolutional neural network for description-based embeddings. KEPLER <ref type="bibr" target="#b5">[Wang et al., 2019b]</ref> uses PLMs as the encoder to derive descriptionbased embeddings and is trained with the objectives of both KE and PLM. Pretrain-KGE [Zhang et al., 2020b] proposes a general description-based KE framework, which initializes another learnable KE with description-based embeddings and discards PLMs for efficiency after fine-tuning PLMs. <ref type="bibr">KG-BERT [Yao et al., 2019]</ref> concatenates descriptions of h, r, t as an input sequence to PLMs, and scores the triple by the sequence embedding. StAR <ref type="bibr" target="#b5">[Wang et al., 2020]</ref> thus partitions a triple into two asymmetric parts between which it performs semantic matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we introduce LMKE and its variants. We first provide background on language models (Sec 3.1). Afterward, we elaborate on how LMKE adopts language models to derive knowledge embeddings (Sec 3.2), and its contrastive variants for both zero-cost negative sampling in training and efficient link prediction in evaluation (Sec 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Models</head><p>Pretrained language models have been increasingly prevalent in NLP. They have been pretrained on large-scale corpora to store vast amounts of general knowledge. For example, <ref type="bibr">BERT [Devlin et al., 2018]</ref> is a Transformer encoder pretrained to predict randomly masked tokens. Afterward, PLMs  can be readily used to achieve excellent performance in various downstream tasks with fine-tuning.</p><p>To better understand such excellence, knowledge probing is proposed <ref type="bibr">[Petroni et al., 2019]</ref>, which questions PLMs with masked cloze sentences. Researches in this direction have demonstrated that PLMs contain abundant factual knowledge and have the potential to be used as knowledge bases. Interestingly, PLMs are also shown to be capable of learning relations satisfying one-hop rules like equivalence, symmetry, inversion, and implication <ref type="bibr">[Kassner et al., 2020]</ref>, which is also the desiderata of knowledge embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LMKE</head><p>LMKE adopts language models as knowledge embeddings, i.e., deriving embeddings of entities and relations that can support prediction of the plausibility of given triples.</p><p>LMKE learns embeddings of entities and relations in the same space as word tokens. Given a specific triple u = (h, r, t), LMKE leverages descriptions s h , s r , s t of h, r, t as additional input, and outputs its probability p(u). LMKE makes every entity and relation a token and gets their embeddings via pretrained language models. The triple is transformed into a sequence of tokens of h, r, t and the words in their descriptions, which serves as the input to PLMs. The description of an entity (or relation) e is a sequence of tokens s e = (x 1 , . . . , x n e ) where n e denotes the length. The input sequence is thus</p><formula xml:id="formula_0">s u = (h, s h , r, s r , t, s t ) = (h, x h 1 , . . . , x h n h , r, x r 1 , . . . , x r n r , t, x t 1 , . . . , x t n t ).</formula><p>Then, two special tokens [CLS] and <ref type="bibr">[SEP]</ref> are inserted in the front and back of s u . Feeding forward s u into the PLM, LMKE generates the encoded embeddings h, r, t ? R d of the three tokens h, r, t. In this way, we embed entities, relations, and words of their descriptions in a shared vector space. The embedding of one entity (or relation) is contextualized in and learned from not only its own textual information but also the other two components of the triple and their textual information. Therefore, long-tail entities can be well represented with their descriptions, and entities without descriptions can also learn representations from related entities' descriptions. The output embedding of [CLS] aggregates information of the whole sequence, so we regard it as embedding u for the entire triple. To score plausibility p(u) of the triple, LMKE inputs u into a linear layer like KG-BERT:</p><formula xml:id="formula_1">p(u) = ?(wu + b)<label>(1)</label></formula><p>where w ? R d and b are learnable weight and bias. We adopt binary cross-entropy as the loss function for training. Besides, LMKE can also be used for prediction over entities or relations with h, r, t. The architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The principal applications of KEs are predicting missing links and classifying possible triples, which are formulated as two benchmark tasks for KE evaluation, namely link prediction <ref type="bibr" target="#b0">[Bordes et al., 2013]</ref> and triple classification <ref type="bibr">[Socher et al., 2013]</ref>. Triple classification is a binary classification task that judges whether a triple u is true or not, which can be directly performed with p(u). Link prediction is to predict the missing entity in a corrupted triple (?, r, t) or (h, r, ?) where ? denotes an entity removed for prediction. In this task, models need to corrupt a triple by replacing its head or tail entity with every entity in the KG, score the replaced triples, and rank the entities in descending order of the scores. However, it is hardly affordable for LMKE to score every replaced triple as an integral with PLMs, concerning the time complexity shown in <ref type="table" target="#tab_0">Table 1</ref>. Even for a middle-sized dataset FB15k-237, there would be 254 million triples to be encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Link Prediction |E| or |R| denotes the number of entities or relations in the KG. N train or N eval is the number of triples in the training or evaluation split. N neg is the negative sampling size. C-LMKE denotes contrastive LMKE, whose complexity is lower than prior approaches.</p><formula xml:id="formula_2">KG-BERT O(N train N neg ) O(|E|N eval ) StAR O(N train N neg ) O(|E||R|) LMKE O(N train N neg ) O(|E|N eval ) C-LMKE O(N train ) O(N eval + |E|)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive KE Learning</head><p>Towards efficient link prediction with language models, one solution is to encode the triples partially. Masked entity model (MEM-KGC) <ref type="bibr" target="#b0">[Choi et al., 2021]</ref> replaces the removed entity and its description with a mask q, and predicts the missing entity by feeding its output embedding q into a linear layer. It can be viewed as a masked variant of LMKE that trades off the exploitation of textual information for time complexity. Since only one masked incomplete triple is encoded, the complexity is downgraded. Nevertheless, it drops textual information of the target entities to be predicted, thus hurting the utility of textual information.  We propose a contrastive learning framework to better exploit description-based KEs for link prediction, where the given entity-relation pair and the target entity serve as the query q and the key k to be matched in contrastive learning.</p><p>From this point of view, the output embedding q of the masked entity in MEM-KGC is the encoded query, and the i-th row in the weights W e of the linear layer serves as the key that corresponds to the i-th entity for each 1 ? i ? |E|. Therefore, feeding q into the linear layer can be viewed as matching the query q with the keys. The difference is that the keys are directly learned representations instead of encodings of textual information like the queries.</p><p>Contrastive LMKE (C-LMKE) is an LMKE variant under this framework that replaces the learned entity representations (rows of W e ) with encodings of target entities' descriptions, as is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. It features the contrastive matching within the mini-batch, which allows efficient link prediction without loss of information exploitation while avoiding the additional cost of encoding negative samples. The candidate keys of a query are defined as keys of the queries in the batch. C-LMKE's time complexity is analyzed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>This practice solves the problem of expensive negative sampling and allows description-based KEs to learn from much more negative samples. While negative samples are vital to KE learning, most existing description-based methods are allowed to have only a few for each positive sample (usually ranging from 1 to 5) because of the cost of language models. C-LMKE currently binds the negative sampling size to the batch size, and with our contrastive framework, existing approaches in contrastive learning like memory bank can be further introduced to achieve more improvement.</p><p>We match an encoded query q with an encoded key k by a two-layer MLP (multi-layer perceptron), instead of cosine similarity that is commonly adopted in contrastive learning, because there may exist multiple keys matching q. If both k 1 and k 2 match q and we maximize similarity between (q, k 1 ) and (q, k 2 ), (k 1 , k 2 ) will also be enforced to be similar, which is not desirable. Thus, the probability that q matches k is:</p><formula xml:id="formula_3">p(q, k) = ?(MLP[q; k; q ? k; q ? k; d])<label>(2)</label></formula><p>where d = [log (d q + 1); log (d k + 1)] is the logarithm of entity degrees. d q and d k are the degrees of the given entity in q and the target entity k counted on the training set. If the training set and the test set follow the same distribution, entities of higher degrees will also be more likely to appear in test triples, so degrees are important structural information to be concerned. While structure-based KEs learn degree information as aggregation into clusters <ref type="bibr" target="#b1">[Pei et al., 2019]</ref> and MEM-KGC learns it as the imbalance of entity labels, our matching function cannot capture this information, so we explicitly include it as additional features for prediction. Since there are usually multiple correct entities for a corrupted triple regarding relations that are not one-to-one, we adopt binary cross-entropy to judge whether each entity is a correct key (multi-label classification), instead of multiclass cross-entropy to find the most likely entity. Given that most of the keys are negative, we average losses over positive ones and negative ones respectively, and sum them up. The loss of matching a query q with the keys K is thus formulated as L(q,</p><formula xml:id="formula_4">K) = ? ? k + ?K + w q,k + log(p(q, k + )) ? ? k ? ?K ? w q,k ? log(1 ? p(q, k ? )), where K + , K ? denotes the</formula><p>positive and negative keys respectively and K = K + ?K ? . We adopt self-adversarial negative sampling  for efficient KE learning, which calculates the weights as w q,k + = 1 |K + | and w q,k ? = exp(p(q,k ? )) ? k?K ? exp(p(q,k)) . In this case, loss of false-positive samples is amplified and loss of truenegative samples is diminished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analyses</head><p>In this section, we evaluate the effectiveness of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. We experiment on four popular benchmark datasets: FB13 <ref type="bibr">[Socher et al., 2013]</ref>, FB15k-237 <ref type="bibr" target="#b2">[Toutanova, 2015]</ref>   Baselines. We compare our methods with structure-based and description-based methods. The structure-based methods include TransE, TransH, TransR, DistMult, ComplEx, Ro-tatE. The description-based methods include Pretrain-KGE, KG-BERT, exBERT, and StAR. For a fair comparison, we reimplement StAR on FB15k-237 with BERT-base. Metrics. For triple classification, we report accuracy. For link prediction, we report Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@{1, 3, 10} in the "filtered" setting. Metrics for link prediction are based on the rank of the correct entity in a list of all entities ranked by their plausibility. The "filtered" setting is a common practice that removes other correct entities (which also constitute triples existing in the KG) from the list. Hits@K measures the proportion of correct entities ranked in the top K. The results are averaged over test triples and over predicting missing head and tail entities. Generally, a good model is expected to achieve higher MRR, Hits@N, and lower MR. Settings. We evaluate LMKE on triple classification and C-LMKE on link prediction with <ref type="bibr">BERT-base [Devlin et al., 2018]</ref> and BERT-tiny <ref type="bibr" target="#b4">[Turc et al., 2019]</ref> as the language model. Larger models are not considered, in which case we have to use a tiny batch size. We search these hyperparameters with BERT-tiny: learning rate of PLMs among {10 ?4 , 5?10 ?5 , 10 ?5 }, learning rate of other components among {10 ?3 , 5?10 ?4 , 10 ?4 , 10 ?5 }, batch size among {12, 16, 32, 64} based on best Hits@10 on the dev set. With BERT-base, we set the batch size as 16 for triple classification and 12 for link prediction, which considers both results of BERT-tiny and limited memory. Our models are fine-tuned with Adam as the optimizer. For triple classification, we sample 1 negative triple for each positive triple. For link prediction, we encode given entity-relation pair queries and target entity keys with two language models. They are equally initialized and share the same embeddings of words, entities, and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">General Performance</head><p>We compare our methods with prior approaches on both link prediction and triple classification. Experimental results in <ref type="table" target="#tab_5">Table 2</ref> and <ref type="table" target="#tab_8">Table 4</ref> show that our methods achieve the stateof-the-art performance on both tasks.</p><p>Results of link prediction on FB15k-237 and WN18RR are demonstrated in <ref type="table" target="#tab_5">Table 2</ref>, which show that our method significantly outperforms existing approaches. C-LMKE with BERT-base achieves surpassing performance on MR, MRR and Hits@{1, 3, 10} on both datasets. The improvement is more significant on WN18RR, where Hits@10 of our method reaches to 0.806 while the best result of previous methods is 0.704 from MEM-KGC. However, its entity prediction task is compatible with our work, without which its Hits@10 declines to 0.636. Our method is also the first description-based one to outperform state-of-the-art structure-based methods on FB15k-237. Even with BERT-tiny, our method achieves better or comparable performance compared with prior approaches built with larger models.</p><p>The results suggest that description-based methods largely outperform structure-based ones on WN18RR, but barely surpass them on FB15k-237. We analyze the difference between these datasets to explain this phenomenon. FB15k-237 differs from WN18RR mainly in two aspects: sparsity and description. According to statistics shown in <ref type="table" target="#tab_6">Table 3</ref>, the average degree on FB15k-237 and WN18RR is 37.4 and 4.2 respectively. The former is about 8.9 times the latter, which indicates that entities in FB15k-237 generally have access to rich structural information while entities in WN18RR are more likely to be long-tail. Also, textual information better covers structural information on WN18RR compared with FB15k-237. Entities on WN18RR are words, and descriptions are exactly their definitions from which structural relations are derived, so descriptions can ideally support the inference of structural relations. However, entities on FB15k-237 are realworld entities whose collected descriptions only partially support the inference. For example, the fact (Albert Einstein, isA, peace activist) is not covered by collected descriptions of these entities. As a result, the overreliance on textual information may hurt performance, and description-based methods did not achieve surpassing performance. This also explains why replacing BERT-base in C-LMKE with BERT-tiny does not decrease the performance.   <ref type="table" target="#tab_8">Table  4</ref> show that LMKE also outperforms existing methods on this task. Comparison between results of LMKE and KG-BERT demonstrates the effectiveness of learning embeddings of entity and relation tokens in the same space as word tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of triple classification on FB13 and UMLS in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Grouped by Entity Degrees</head><p>To show the effectiveness of our methods on long-tail entities, we group entities by the logarithm of degrees, collect relevant triples for each group, and study the average link prediction performance of different methods on different groups. A triple (h, r, t) is relevant to group i if h or t is in group i. The grouping rule is the same as in <ref type="figure" target="#fig_0">Fig.1</ref>. The results on FB15k-237 in <ref type="figure" target="#fig_3">Fig.4</ref> show that description-based methods significantly outperform structure-based ones on long-tail entities in group 0, 1 and 2 (whose degrees are lower than 4), and our C-LMKE significantly surpasses other descriptionbased ones. Comparison between results of C-LMKE with or without degree information indicates that introducing degree information generally improves its performance on entities that are not long-tail. On popular entities, however, structurebased methods generally perform better. Although StAR is also description-based, it achieves the best Hits@10 on group 12 and 13, because it is trained with an additional objective that follows structure-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Importance of Negative Sampling Size</head><p>We study the performance of C-LMKE with different negative sampling sizes N neg to justify its importance. We set the batch size as 32 and limit N neg for each triple by only using several encoded target entities of other triples in the batch as negative keys. We report Hits@10 of C-LMKE with BERTtiny on FB15k-237 for 40 epochs. The results shown in <ref type="figure" target="#fig_4">Fig.  5</ref> indicate that larger N neg continuously brings better performance until convergence. Increasing N neg greatly accelerates training, and improves the eventual performance when N neg is under 8. However, prior description-based methods generally set N neg as only 1 or 5, which limits their performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose LMKE, an effective and efficient method to adopt language models as knowledge embeddings. Motivated by the inability of structure-based KEs to well represent long-tail entities, LMKE leverages textual descriptions and learns embeddings of entities and relations in the same space as word tokens. It solves the restrictive demand of prior description-based approaches. A contrastive learning framework is further proposed that allows zero-cost negative sampling and significantly downgrades time complexity in both training and evaluation. Results of extensive experiments show that our methods achieve state-of-the-art performance on various benchmarks, especially for long-tail entities.</p><p>In the future, we plan to explore the effectiveness of more advanced contrastive learning approaches in descriptionbased KEs. We are also interested in language models' capacity of modeling composition patterns in KGs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Distribution of entity degrees and performance of Ro-tatE (a typical structure-based method) on WN18RR. Entities are grouped by the logarithm of degrees (right). For each group, we report the number of relevant triples and their average performance on link prediction. Please refer to Sec 4.3 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of LMKE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of contrastive LMKE. The queries and keys are encoded separately before contrastive matching within the batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Average Hits@1 and Hits@10 performance grouped by the logarithm of entity degrees on FB15k-237.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Hits@10 of C-LMKE with BERT-tiny on FB15k-237 with different negative sampling size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 .</head><label>1</label><figDesc>Expensive negative sampling. While negative sampling is vital to KE learning, existing description-based methods are only allowed to have limited negative samples because of the encoding expense of language models. 2. Restrictive description demand. Existing methods generally require descriptions of all entities in the KG, and discard entities with no or short descriptions. Although delicate benchmarks can satisfy this demand, real-world KGs can hardly contain descriptions for all entities.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The time complexity of training and link prediction evalu- ation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, UMLS<ref type="bibr" target="#b0">[Dettmers et al., 2018]</ref> and WN18RR<ref type="bibr" target="#b0">[Dettmers et al., 2018]</ref>, whose statistics are shown inTable 3. FB13 and FB15k-237 are derived from Freebase. WN18RR is derived from WordNet. UMLS is a medical ontology describing relations between medical concepts. FB15k-237 and WN18RR are used for link prediction, where abundant inverse relations are removed in case they can serve as shortcuts. For triple classification, FB13 and UMLS are used. Only the test split of FB13 contains negative samples. In other situations, negative samples are created by randomly replacing head or tail entities, where the ground truth (training triples for the training split and all triples for the test split) would be avoided. Following KG-BERT, the entity descriptions we use are synsets definitions for WN18RR,</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell cols="5">MR MRR Hits@1 Hits@3 Hits@10</cell><cell>MR</cell><cell cols="4">MRR Hits@1 Hits@3 Hits@10</cell></row><row><cell></cell><cell></cell><cell cols="4">Structure-based Knowledge Embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransE [Bordes et al., 2013]?</cell><cell>323</cell><cell>0.279</cell><cell>0.198</cell><cell>0.376</cell><cell>0.441</cell><cell cols="2">2300 0.243</cell><cell>0.043</cell><cell>0.441</cell><cell>0.532</cell></row><row><cell>DistMult [Yang et al., 2014]?</cell><cell>254</cell><cell>0.241</cell><cell>0.155</cell><cell>0.263</cell><cell>0.419</cell><cell cols="2">5110 0.430</cell><cell>0.390</cell><cell>0.440</cell><cell>0.490</cell></row><row><cell>ComplEx [Trouillon et al., 2016]?</cell><cell>339</cell><cell>0.247</cell><cell>0.158</cell><cell>0.275</cell><cell>0.428</cell><cell cols="2">5261 0.440</cell><cell>0.410</cell><cell>0.460</cell><cell>0.510</cell></row><row><cell>RotatE [Sun et al., 2019]?</cell><cell>177</cell><cell>0.338</cell><cell>0.241</cell><cell>0.375</cell><cell>0.533</cell><cell cols="2">3340 0.476</cell><cell>0.428</cell><cell>0.492</cell><cell>0.571</cell></row><row><cell>TuckER [Bala?evi? et al., 2019]</cell><cell>-</cell><cell>0.358</cell><cell>0.266</cell><cell>0.394</cell><cell>0.544</cell><cell>-</cell><cell>0.470</cell><cell>0.443</cell><cell>0.482</cell><cell>0.526</cell></row><row><cell>HAKE [Zhang et al., 2020a]</cell><cell>-</cell><cell>0.346</cell><cell>0.250</cell><cell>0.381</cell><cell>0.542</cell><cell>-</cell><cell>0.497</cell><cell>0.452</cell><cell>0.516</cell><cell>0.582</cell></row><row><cell>CoKE [Wang et al., 2019a]</cell><cell>-</cell><cell>0.364</cell><cell>0.272</cell><cell>0.400</cell><cell>0.549</cell><cell>-</cell><cell>0.484</cell><cell>0.450</cell><cell>0.496</cell><cell>0.553</cell></row><row><cell></cell><cell></cell><cell cols="4">Description-based Knowledge Embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pretrain-KGE TransE [Zhang et al., 2020b] 162</cell><cell>0.332</cell><cell>-</cell><cell>-</cell><cell>0.529</cell><cell cols="2">1747 0.235</cell><cell>-</cell><cell>-</cell><cell>0.557</cell></row><row><cell>KG-BERT [Yao et al., 2019]?</cell><cell>153</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.420</cell><cell>97</cell><cell>0.216</cell><cell>0.041</cell><cell>0.302</cell><cell>0.524</cell></row><row><cell>StAR BERT-base [Wang et al., 2020]</cell><cell>136</cell><cell>0.263</cell><cell>0.171</cell><cell>0.287</cell><cell>0.452</cell><cell>99</cell><cell>0.364</cell><cell>0.222</cell><cell>0.436</cell><cell>0.647</cell></row><row><cell>MEM-KGC BERT-base (w/o EP)</cell><cell>-</cell><cell>0.339</cell><cell>0.249</cell><cell>0.372</cell><cell>0.522</cell><cell>-</cell><cell>0.533</cell><cell>0.473</cell><cell>0.570</cell><cell>0.636</cell></row><row><cell>MEM-KGC BERT-base (w/ EP)</cell><cell>-</cell><cell>0.346</cell><cell>0.253</cell><cell>0.381</cell><cell>0.531</cell><cell>-</cell><cell>0.557</cell><cell>0.475</cell><cell>0.604</cell><cell>0.704</cell></row><row><cell>C-LMKE BERT-tiny</cell><cell>132</cell><cell>0.406</cell><cell>0.319</cell><cell>0.445</cell><cell>0.571</cell><cell>148</cell><cell>0.545</cell><cell>0.467</cell><cell>0.587</cell><cell>0.692</cell></row><row><cell>C-LMKE BERT-base</cell><cell>183</cell><cell>0.404</cell><cell>0.324</cell><cell>0.439</cell><cell>0.556</cell><cell>72</cell><cell>0.598</cell><cell>0.480</cell><cell>0.675</cell><cell>0.806</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results of link prediction on FB15k-237 and WN18RR. ? denotes results from. ? denotes results from<ref type="bibr" target="#b5">[Wang et al., 2020]</ref>. We implement StAR on FB15k-237 with BERT-base as the base model. Other results are taken from their original papers. EP denotes the entity prediction task of MEM-KGC. C-LMKE denotes contrastive LMKE. and descriptions from Wikipedia for FB13, from<ref type="bibr" target="#b5">[Xie et al., 2016]</ref> forFB15k-237, and from [Yao et al., 2019]  for UMLS. The relation descriptions are their names for all datasets.</figDesc><table><row><cell cols="7">Dataset #Entity #Relation #Train #Dev #Test Avg DL</cell></row><row><cell>FB13</cell><cell>75,043</cell><cell>13</cell><cell cols="4">316,232 5,908 23,733 110.7</cell></row><row><cell cols="2">FB15k-237 14,541</cell><cell>237</cell><cell cols="4">272,115 17,535 20,466 141.7</cell></row><row><cell>UMLS</cell><cell>135</cell><cell>46</cell><cell>5,216</cell><cell>652</cell><cell>661</cell><cell>161.7</cell></row><row><cell cols="2">WN18RR 40,943</cell><cell>11</cell><cell cols="3">86,835 3,034 3,134</cell><cell>14.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the datasets. Avg DL means the average length (number of words) of descriptions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Accuracy of triple classification on FB13 and UMLS. Results of existing baselines on FB13 and UMLS are taken from [Yao et al., 2019] and [Yaser Jaradeh et al., 2021] respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nora Kassner, Benno Krojer, and Hinrich Sch?tze. Are pretrained language models symbolic reasoners over knowledge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bala?evi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09590</idno>
		<idno>arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of AAAI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised entity alignment via knowledge graph embedding with awareness of degree difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nickel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<idno>arXiv:1902.10197</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng</editor>
		<meeting><address><addrLine>Tim Rockt?schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Language models as knowledge bases. Sun et al., 2019. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName>
		</author>
		<editor>Th?o Trouillon, Johannes Welbl, Sebastian Riedel,?ric Gaussier, and Guillaume Bouchard</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
	<note>Trouillon et al., 2016</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<idno type="arXiv">arXiv:1908.08962</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ?ukasz Kaiser, and Illia Polosukhin</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<idno type="arXiv">arXiv:1911.02168</idno>
		<idno>arXiv:2004.14781</idno>
	</analytic>
	<monogr>
		<title level="m">Structure-augmented text representation learning for efficient knowledge graph completion</title>
		<editor>Wang et al., 2020] Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, and Yi Chang</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of AAAI</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pretrain-kge: Learning knowledge representation from pretrained language models</title>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<idno>arXiv:2111.11845</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<editor>Jianyu Cai, Yongdong Zhang, and Jie Wang</editor>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Xu Sun, and Bin He</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of AAAI</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
