<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-timescale Trajectory Prediction for Abnormal Human Activity Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Royston</forename><surname>Rodrigues</surname></persName>
							<email>royston.rodrigues@protonmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Bhargava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajbabu</forename><surname>Velmurugan</surname></persName>
							<email>rajbabu@ee.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhasis</forename><surname>Chaudhuri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-timescale Trajectory Prediction for Abnormal Human Activity Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A classical approach to abnormal activity detection is to learn a representation for normal activities from the training data and then use this learned representation to detect abnormal activities while testing. Typically, the methods based on this approach operate at a fixed timescale -either a single timeinstant (e.g. frame-based) or a constant time duration (e.g. videoclip based). But human abnormal activities can take place at different timescales. For example, jumping is a short term anomaly and loitering is a long term anomaly in a surveillance scenario. A single and pre-defined timescale is not enough to capture the wide range of anomalies occurring with different time duration. In this paper, we propose a multi-timescale model to capture the temporal dynamics at different timescales. In particular, the proposed model makes future and past predictions at different timescales for a given input pose trajectory. The model is multi-layered where intermediate layers are responsible to generate predictions corresponding to different timescales. These predictions are combined to detect abnormal activities.</p><p>In addition, we also introduce an abnormal activity dataset for research use that contains 4,83,566 annotated frames. Our experiments show that the proposed model can capture the anomalies of different time duration and outperforms existing methods.</p><p>Data-set will be made available at https://rodrigues-royston. github.io/Multi-timescale_Trajectory_Prediction/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>royston.rodrigues@protonmail.com, neha.iitb@gmail.com, rajbabu@ee.iitb.ac.in, sc@ee.iitb.ac.in Abstract-A classical approach to abnormal activity detection is to learn a representation for normal activities from the training data and then use this learned representation to detect abnormal activities while testing. Typically, the methods based on this approach operate at a fixed timescale -either a single timeinstant (e.g. frame-based) or a constant time duration (e.g. videoclip based). But human abnormal activities can take place at different timescales. For example, jumping is a short term anomaly and loitering is a long term anomaly in a surveillance scenario. A single and pre-defined timescale is not enough to capture the wide range of anomalies occurring with different time duration. In this paper, we propose a multi-timescale model to capture the temporal dynamics at different timescales. In particular, the proposed model makes future and past predictions at different timescales for a given input pose trajectory. The model is multi-layered where intermediate layers are responsible to generate predictions corresponding to different timescales. These predictions are combined to detect abnormal activities. In addition, we also introduce an abnormal activity dataset for research use that contains 4,83,566 annotated frames. Our experiments show that the proposed model can capture the anomalies of different time duration and outperforms existing methods.</p><p>Data-set will be made available at https://rodrigues-royston. github.io/Multi-timescale_Trajectory_Prediction/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Detecting abnormal activities is a challenging problem in computer vision. There is no generic definition available for abnormal events and is usually dependent on the scene under consideration. For example, cycling on a footpath is typically an abnormal activity whereas it becomes normal on a road. To address such a scene context dependency, a typical approach is to consider rare or unseen events in a scene as abnormal. But this may classify the unseen normal activities as abnormal. In general, it may not be possible to know all the normal and abnormal activities during training. We only have access to subsets of normal and abnormal activities. The lack of a generic definition and insufficiency in the data, make it extremely hard for any learning algorithm to understand and capture the nature of abnormal activity.</p><p>More often, the abnormal activity detection problem is posed as an unsupervised learning problem. A common setup of the problem is this -the training data consists of only normal activities and the test data contains normal as well as abnormal activities. A standard approach is to build a model that captures the normality present in the training data. During testing, any deviation from the learned normality indicates the level of abnormality in the test data. Most existing methods formulate it as an outlier detection problem <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. They attempt to fit the features corresponding to normal activities in a hyper-sphere and the distance of a test feature from this hyper-sphere indicates its abnormality.</p><p>One major limitation of the current methods is that they are trained at a fixed timescale -either a single time-step or a constant number of time-steps. This restricts the model to build understanding of training data at that timescale and hence it may not capture anomalies that occur at other timescales. For example, consider the case of loitering -someone wanders at a place for a longer period. The investigation at a smaller time-step may not capture it, because at this timescale it appears as a normal walk. It can be captured only when observed for a sufficient amount of time. Hence, a larger timescale is needed to detect this long-term abnormal activity. Similarly, a larger timescale may not capture the short term anomaly (e.g. jumping) efficiently. In this paper, we propose a multi-timescale framework to address this problem. The framework has two models -one that makes future predictions and the other one that makes past predictions. They take in a pose sequence to predict future and past sequences at multiple timescales. This is achieved by providing supervision at intermediate layers in the models. For example, the first layer corresponds to a timescale of 3 steps in our setup. At this layer, the input sequence is first broken down into smaller sequences of length three. The model makes 3-step future (and past) predictions for each of these smaller sequences. We combine these predictions together to get a 3-step future (and past) predictions for the input sequence. Similarly, a few The plots in the left shows the prediction errors at different timescales. The prediction errors are less at lower timescales (3 and 5) because at these timescales, the model considers it as a normal activity (walking). The errors are higher at larger timescales <ref type="bibr">(13 and 25)</ref>. At these timescales, the model understands that it is an abnormal pattern of walking.</p><p>other layers in the model generate predictions at different timescales. All the prediction errors from past and future at different timescales are combined appropriately to generate an anomaly score at each time instant in the input sequence. Since the model is trained at different timescales, it tends to learn temporal dynamics at various timescales. An illustration of a long term anomaly (e.g. loitering) and prediction errors from our model at different timescales is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Contributions: We make the following major contributions in the paper:</p><p>? We propose a bi-directional (past and future) prediction framework that considers an input pose trajectory and predicts pose trajectories at different timescales. Such a framework allows inspection of activities at different timescales (i.e. , with different time duration). ? We introduce a large dataset that contains a diverse set of abnormal activities. Unlike other datasets, it contains anomalies involving single person to a group of persons. To the best of our knowledge, this is the largest dataset in terms of volume, with 4,83,566 frames. The paper is organised as follows. The next section reviews the related work in the area of abnormal activity detection. Section III details the proposed model. We introduce our dataset in Section IV. We provide the experimental setup, ablation studies, and results in Section V. Finally, the paper concludes in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The problem of human abnormal activity detection has been receiving a lot of interest from computer vision researchers. This is partly because of challenges inherent to the problem and mainly due to its applications. It is interesting to see the evolution of ideas over the years, especially after the introduction of deep learning in this area. In this section, we attempt to summarise this evolution with a few key papers. One of the common and initial approaches was based on reconstruction. Hasan et al. in <ref type="bibr" target="#b4">[5]</ref> used an auto-encoder to learn appearance based representation under reconstruction framework. In <ref type="bibr" target="#b17">[18]</ref>, Xu et al. augmented the appearance features with optical flow to integrate motion information. Tran et al. learnt sparse representations by using convolutional winner-take-all autoencoders <ref type="bibr" target="#b15">[16]</ref>. Luo et al. in <ref type="bibr" target="#b11">[12]</ref> proposed a method based on learning a dictionary for the normal scenes under a sparse coding scheme. To smoothen the predictions over time, they proposed a Temporally-coherent Sparse Coding (TSC) formulation. Ravanbakhsh et al. used GAN to learn the normal representation <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, Liu et al. used GAN with U-net to predict the future frames. Hinami et al. in <ref type="bibr" target="#b5">[6]</ref> proposed a framework that jointly detects and recounts abnormal events. They also learn the basic visual concepts into the abnormal event detection framework. Abati et al. in <ref type="bibr" target="#b0">[1]</ref> captured surprisal component of anomalous samples, by introducing an auto-regressive density estimator to learn latent vector distribution of autoencoders. In <ref type="bibr" target="#b16">[17]</ref>, Ionescu et al. used unmasking to detect anomalies without any training samples. Recently, Romera et al. in <ref type="bibr" target="#b12">[13]</ref> has proposed a joint framework for trajectory prediction and reconstruction for anomaly detection. One major limitation of the existing methods is that they operate at a single timescale. We take a step forward and propose a multi-timescale model to address the limitations of operating with a single timescale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL</head><p>In this section, we present details of the proposed model. To restate, our objective is to develop a framework that is capable of detecting abnormal human activities of different time duration. Keeping this aim in mind, we propose a multi-timescale model that predicts the future trajectories at different timescales. The idea is to develop understanding at different timescales. To further improve the performance, we add an identical model in the framework that interprets the past. We use pose trajectory of human as the input. Besides being compact in nature, the pose trajectory captures the human movements quite well. The top-level illustration of our framework is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. It has two models that To generate predictions at timescale 1, the model first splits the sequence into smaller sub-sequences and then makes future predictions for these sub-sequences. These predictions are combined to get the future prediction for the input sequence at this timescale. To get the past prediction, we reverse the input sequence and pass it to the past prediction model. Finally, all the predictions are combined appropriately to get a final prediction errors for the input sequence that is used to detect abnormal events. make past and future predictions, respectively. At a particular timescale, we combine the predictions from both the models to generate a combined prediction at every time instant. For example, to generate future predictions at timescale 1 (in our setup, timescale 1 represents time duration of 3 steps), the model first splits the sequence into smaller sub-sequences (of length 3) and then makes future predictions (for next 3 steps) for these sub-sequences. These predictions are combined to get the future prediction for the complete input sequence at this timescale. To get the past prediction, we reverse the input sequence and pass it to the past prediction model. Both the models have the same architecture but are trained differently. The past and future predictions at a timescale are combined to get a predicted sequence at that timescale. Finally, all the predictions from different timescales are appropriately combined to get a final prediction error sequence for the input sequence. These prediction error values are compared against a pre-defined threshold. At any time instant t, if the error is more than the threshold, the particular time instant is tagged as abnormal. An illustration of our predicted poses is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. It demonstrates higher prediction errors for abnormal activities and low errors for normal activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem setup</head><p>We use human pose trajectory, represented by a collection of 25 points on the human skeleton over a time period, as the input to the model. Let p i</p><formula xml:id="formula_0">j (t) = [x i j (t), y i j (t)]</formula><p>be the image coordinates of j th point in the pose representation of i th person at time t. At time t, the pose of i th person is represented as</p><formula xml:id="formula_1">X i t = [p i 1 (t), p i 2 (t), . . . , p i 25 (t)] T ? R 50?1 and</formula><p>the corresponding predicted pose is represented byX i t . The model takes pose trajectory of a certain length as input and generates predictions at different timescales under a hierarchical framework. For any time instant at any timescale, the model generates multiple predictions because it runs a sliding window over the input signal. These multiple predictions are combined together by averaging to get a final prediction at a particular time instant. Similar approach is adopted to get the past predictions. Finally, the prediction errors from all the timescales are combined to get an abnormality score. In the next sub-section, we discuss the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model</head><p>The proposed model is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. The input to the model is the pose trajectory of an individual {X t } t=1,2,...,T , where the superscript to identify a person is dropped for better clarity. The pose information at each time instant X t is passed through an encoder E to get the corresponding encoded vector f t of length 1024. These vectors are passed to a series of 1D convolutional filters. Here, a seven layer deep 1D convolutional model is used. We use 1D filters of length 3 for the initial two layers and then we have 1D filters of length 5 for the next five layers. We use 1024 filters at each layer. We train certain intermediate layers to produce predictions at different timescales. This is achieved by providing supervision at these layers. A specific layer corresponds to the k th timescale if its reception field is of length t k and is responsible to generate t k future (and past) steps in the trajectory. To provide supervision, we train a decoder to predict the future sequence. In our setup,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss function</head><p>To compute the total loss of the model during training, we add the losses generated by the intermediate layers where we provide supervision. At such a layer, we have two types of losses -one at a node level, and another at the layer level. The loss at a node level computes the prediction error between the predictions generated by a node and the corresponding ground truth. The loss at a layer level computes the total prediction error generated by the layer for a complete input sequence. Since, there are multiple predictions generated at a particular time instant by different nodes, we use a sliding window approach to calculate the total loss at a layer. In particular, we take the average of all the prediction errors generated at a time instant by different nodes to compute the total prediction error at a particular time instant. The average prediction errors at all the time instants are added to get the layer loss. The total loss at the j th layer with M j number of nodes is given as,</p><formula xml:id="formula_2">L j = Mj i=1 L j 1 (i) + T t=1 L j 2 (t),<label>(1)</label></formula><p>where L j 1 (i) is the loss for i th node and L j 2 (t) is the loss at time t in the j th layer. The first term corresponds to the total node loss and the second term corresponds to the total layer loss at j th layer. Let the i th node in j th layer make predictions for the duration T(i) = [t si , t ei ] and generate prediction error e(t, i) for a particular time instant t ? [t si , t ei ]. The i th node loss is computed as follows:</p><formula xml:id="formula_3">L j 1 (i) = tei t=tsi e(t, i)<label>(2)</label></formula><p>To compute the layer loss, we simply take the average of prediction errors generated by different nodes for a particular time instant. We finally add these average errors for all the time instants to get the layer loss.</p><formula xml:id="formula_4">L j 2 = Mj i=1 e(t, i)1(t ? T(i)) Mj i=1 1(t ? T(i))<label>(3)</label></formula><p>A simple example to illustrate the loss computation at a layer is shown in <ref type="figure">Figure 5</ref>. The total model loss for a model with N ts number of timescales is,</p><formula xml:id="formula_5">Loss = Nts j=1 L j<label>(4)</label></formula><p>We use weighted mean square error (mse) to compute the prediction error.</p><formula xml:id="formula_6">e = 25 k=1 w k (p k ? p k ) 2 ,<label>(5)</label></formula><p>where p k ,p k are the original and predicted k th pose points, respectively. The weight w k is obtained from the confidence of pose estimator <ref type="bibr" target="#b2">[3]</ref> for the k th point as where c k is the confidence score for the k th point given by the pose detector.</p><formula xml:id="formula_7">w k = c k 25 i=1 c i ,<label>(6)</label></formula><p>With the loss function in Eq. (1), we jointly minimise the local loss at a node and the global loss at a layer. Minimising the node loss makes the prediction better at a node level whereas minimising the layer loss drives the nodes to interact with each other to reduce the layer loss. Another advantage of using the layer loss under a sliding window approach during training is that it simulates the testing scenario. During testing, it is common to use a sliding window over a long test sequence to get an input sample of suitable length for the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Anomaly detection</head><p>In this section, we discuss the method for anomaly detection during testing. The trained model predicts pose trajectories for a human at different timescales. The past and future prediction errors produced at these different timescales are combined using a voting mechanism to compute the final prediction error. At any time instant t, the errors are combined as follows:</p><formula xml:id="formula_8">error(t) = j?S L j 2 (t) |S| ,<label>(7)</label></formula><p>where S is a set of timescales that contains predictions for the time instant t and L j 2 (t) is the j th layer loss at time t, as in <ref type="bibr" target="#b2">(3)</ref>. Note that at any time instant, there can be more than one human resulting in multiple error plots -one for each human. In such a case, we take the maximum of prediction errors among all the individuals, at a time instant. That is,</p><formula xml:id="formula_9">error(t) = j?S max{L j 2 (t, p k ), ?k} |S| ,<label>(8)</label></formula><p>where L j 2 (t, p k ) is the j th layer loss at time t for the k th individual. During testing, error(t) is compared against a threshold. If it exceeds the threshold, then the time instant t is tagged as abnormal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CORRIDOR DATASET</head><p>We provide a brief introduction of the proposed Corridor dataset for abnormal human activity. The videos are captured in a college campus. The scene consists of a corridor where the normal activities are usually walking and standing. We enacted various abnormal activities with the help of volunteers. The dataset contains variety of activities and has single person to group level anomalies. The annotations for normal and abnormal are provided at frame level. To the best of our knowledge, this is the largest dataset in terms of volume. A comparison of the proposed dataset with other datasets is given in <ref type="table" target="#tab_1">Table II</ref>. The last column mentions the abnormal <ref type="figure">Fig. 5</ref>: Computation of node and layer losses at a particular timescale. To compute the prediction loss generated by the node n1, we simply add the errors e(1, 1) to e(1, 5) because n1 makes predictions for t1 to t5. We add all the node losses to get the total node loss. To compute the layer loss, we first find the average loss at each time instant that is contributed by multiple nodes i.e. , L2(t1) to L2(t8). For example, to compute L2(t2), we take weighted average of e(1, 2) and e(2, 2). We add these average errors to get the total layer loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR-ShanghaiTech ShanghaiTech HR-Avenue Avenue Corridor</head><p>Conv-AE <ref type="bibr" target="#b4">[5]</ref> 69  activities present in the datasets. Out of 1,81,567 test frames, the number of abnormal frames is 1,08,278. Additionally, we provide the class of each abnormal activity that can be used for classification purposes. <ref type="table" target="#tab_1">Table II</ref> shows sample images from the dataset corresponding to all the activities. To keep the privacy of volunteers intact, we have blurred the faces in the images. We used the algorithm proposed by <ref type="bibr" target="#b13">[14]</ref> to detect the faces.</p><p>In the next section, we discuss our experimentation in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSIONS</head><p>In this section, we discuss our experimental setup and results. We tested our method on the proposed dataset and two public datasets namely, ShanghaiTech Campus dataset <ref type="bibr" target="#b11">[12]</ref> and Avenue <ref type="bibr" target="#b10">[11]</ref>. The ShanghaiTech Campus dataset contains videos from 13 different cameras around the ShanghaiTech University campus. A few examples of human anomalies present in the dataset are running, fighting, loitering, and jumping. Avenue dataset is captured at CUHK campus. It contains anomalies such as throwing a bag, running, walking near the camera, and dancing. The training set has 16 videos and the test set has 21 videos. Since we are interested in human anomaly, similar to <ref type="bibr" target="#b12">[13]</ref>, we test our algorithm primarily on HR-ShanghaiTech and HR-Avenue datasets, proposed by them. In this section, we first discuss our pre-processing to generate the pose trajectories, followed by training and testing schemes. We then compare the performance of our model with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data preparation</head><p>In this section, we discuss our method to obtain the pose trajectories from the videos. We first run a human detector <ref type="bibr" target="#b6">[7]</ref> on all the videos. We obtain bounding box trajectories from the detections using the multi-target tracker proposed by <ref type="bibr" target="#b9">[10]</ref>. Finally, we run a pose-detector <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> on these trajectories to get the pose trajectories. This pose detector also produces confidence values for each of 25 points. We use these values in Eq. (5) to calculate the weighted mse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training scheme</head><p>In this section, we discuss our training paradigm. To generate predictions at different timescales, we provide supervision at pre-chosen layers in our multi-layered model. In our model, we provide supervision to the layers -1, 2, 4, and 7 corresponding to timescales of 3, 5, 13, and 25, respectively. Each   training epoch consists of sub-epochs. In each sub-epoch, we train up to a particular layer corresponding to one of the timescales. In the first sub-epoch, we train only the first layer corresponding to timescale of 3. In the last sub-epoch, we train the complete model up to layer 7 that corresponds to timescale of 25. There are 4 sub-epochs corresponding to each timescale. To train the first sub-epoch, we split the training pose trajectories in to smaller trajectories of length 6 (3 in and 3 out). To train the last sub-epoch, we split the input trajectories in to length of 50 (25 in and 25 out). The loss after an epoch is equal to the loss incurred at the last subepoch. We used Adam as the optimiser to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Testing</head><p>To test an input sequence, we split the input sequence in to smaller sequences of length 6, 10, 26, and 50. We use sequences of length 6 to generate predictions from layer 1 (i.e. , timescale of 3). Similarly, we use sequences of lengths 10, 26, and 50 to produce predictions at layer 2, 4, and 7, respectively. This is done for both future and past prediction models. We combine the prediction errors by voting. Finally at any time instant, if the error value is higher than a threshold, it is considered as abnormal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Evaluation</head><p>We compare our results with <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b0">[1]</ref>, and <ref type="bibr" target="#b12">[13]</ref>. The method proposed by Luo et al. in <ref type="bibr" target="#b11">[12]</ref> is based on learning a dictionary for the normal scenes under a sparse coding scheme. To smoothen the predictions over time, they proposed a Temporally-coherent Sparse Coding (TSC) formulation. Liu et al. <ref type="bibr" target="#b8">[9]</ref> proposed a future frame prediction based method to detect anomaly. They also use optical flow to enforce the temporal constraint along with the spatial closeness. The method proposed by Morais et al. <ref type="bibr" target="#b12">[13]</ref> uses pose trajectory under the joint framework of reconstruction and prediction. To compare with these existing approaches, we also use Frame-AUC as the evaluating criteria. The comparison is given in <ref type="table" target="#tab_1">Table I</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation studies</head><p>In this section, we provide ablation studies. In particular, we discuss the effects of timescales and past prediction model. 1) Effect of multi-timescale framework: In this sub-section, we discuss the effect of various timescales on the performance. In order to compare, we use our trained model and then test the model with incremental combinations of timescales. <ref type="table" target="#tab_1">Table III</ref> compares the Frame AUC when using different timescales. In most of the cases, we see an improvement in the performance as we include more timescales. However, in case of HR-Avenue, we do not see this trend in the last row. This is because the dataset does not have long-term anomaly corresponding to timescale of 25.</p><p>2) Effect of past prediction model: To see the effect of past prediction model, we compare the performances of future prediction model, past prediction model, and combined. In <ref type="table" target="#tab_1">Table III</ref>, we can observe from the rows that adding past prediction model improved the overall performance in most of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this work, we developed a multi-timescale framework to capture abnormal human activities occurring at different timescales. We use this framework to predict pose trajectories in both the directions (past and future) at multiple timescales. These multi-timescale predictions are used to detect abnormal activity. Our experiments shows that the multi-timescale framework outperforms the state-of-the-art models. In addition, we also release a challenging abnormal activity data set for research use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An illustration of how our model captures a long term anomaly. The anomaly in consideration is loitering -the intermediate frames are shown in the right figure with the person involved in red box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Top-level block diagram of the proposed framework. The future prediction model takes the input sequence and generate predictions at different timescales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>An illustration of predicted poses from the proposed model. The first scene has a walking action while the other three have abnormal actions. The green and blue poses represent the actual and predicted poses, respectively. The model generate large prediction errors for the abnormal poses. we provide supervision to the layers corresponding to the timescales -3, 5, 13 and 25. The decoder D k at time scale t k consists of two fully-connected layers, of length 1024 and t k * 50. The encoder E consists of two fully-connected layers of length 1024 to transform frame level pose features of length 50 to 1024. The encoder E operates at an individual frame level. Encoded 1024 dimension pose features f t are stacked to form a time series corresponding to the pose trajectory under consideration. The output length for the decoder depends on the specific timescale. In the next sub-section, we discuss the loss function in detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>The detailed architecture of the proposed model for future prediction. X1-XT is the input pose trajectory. After encoding, the vectors f1-fT are passed to a series of 1D convolutional filters. A few intermediate layers generate predictions at different timescales. To generate predictions, the decoder takes the filtered encoded vector and produces the predictionsX. At these intermediate layers, the node loss and the layer loss are minimised jointly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Samples images from the proposed Corridor dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Performance comparison with the existing techniques.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Corridor (proposed) dataset compared to existing single camera datasets. This dataset is more challenging as can be seen in theTable Ishowing performance of state-of-the-art methods on different datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. Our proposed model outperforms the other</figDesc><table><row><cell>Timescales</cell><cell></cell><cell cols="2">HR-ShanghaiTech</cell><cell></cell><cell cols="2">HR-Avenue</cell></row><row><cell></cell><cell>Future</cell><cell>Past</cell><cell>Future+Past</cell><cell>Future</cell><cell>Past</cell><cell>Future+Past</cell></row><row><cell>3</cell><cell>71.71</cell><cell>70.62</cell><cell>72.05</cell><cell>85.33</cell><cell>83.36</cell><cell>84.99</cell></row><row><cell>3, 5</cell><cell>72.89</cell><cell>71.69</cell><cell>73.39</cell><cell>86.96</cell><cell>84.70</cell><cell>86.82</cell></row><row><cell>3, 5, 13</cell><cell>74.51</cell><cell>73.39</cell><cell>75.65</cell><cell>88.29</cell><cell>86.20</cell><cell>88.43</cell></row><row><cell>3, 5, 13, 25</cell><cell>74.98</cell><cell>74.17</cell><cell>77.04</cell><cell>87.37</cell><cell>85.65</cell><cell>88.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Effect of multiple timescales and past predictions on the overall performance of the model. methods on HR-ShanghaiTech and HR-Avenue datasets. Even though our model doesn't capture the non-human anomalies (e.g. car), it outperforms on ShanghaiTech dataset and provides comparable performance on Avenue dataset. The proposed dataset is more challenging as can be seen in theTable I.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="555" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3619" to="3627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking with deeply learned candidate selection and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning regularity in skeleton trajectories for anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11996" to="12004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSH: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection using a convolutional winner-take-all autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2017. British Machine Vision Association</title>
		<meeting>the British Machine Vision Conference 2017. British Machine Vision Association</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
