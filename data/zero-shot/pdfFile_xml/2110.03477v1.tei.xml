<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InfoSeg: Unsupervised Semantic Image Segmentation with Mutual Information Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-07">7 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Harb</surname></persName>
							<email>robert.harb@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kn?belreiter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">InfoSeg: Unsupervised Semantic Image Segmentation with Mutual Information Maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-07">7 Oct 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Unsupervised Semantic Segmentation ? Representation Learn- ing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel method for unsupervised semantic image segmentation based on mutual information maximization between local and global high-level image features. The core idea of our work is to leverage recent progress in self-supervised image representation learning. Representation learning methods compute a single high-level feature capturing an entire image. In contrast, we compute multiple high-level features, each capturing image segments of one particular semantic class. To this end, we propose a novel two-step learning procedure comprising a segmentation and a mutual information maximization step. In the first step, we segment images based on local and global features. In the second step, we maximize the mutual information between local features and high-level features of their respective class. For training, we provide solely unlabeled images and start from random network initialization. For quantitative and qualitative evaluation, we use established benchmarks, and COCO-Persons, whereby we introduce the latter in this paper as a challenging novel benchmark. InfoSeg significantly outperforms the current state-of-the-art, e.g., we achieve a relative increase of 26% in the Pixel Accuracy metric on the COCO-Stuff dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic image segmentation is the task of assigning a class label to each pixel of an image. Various applications make use of it, including autonomous driving, augmented reality, or medical imaging. As a result, a lot of research was dedicated to semantic segmentation in the past. However, the vast majority of research focused on supervised methods. A major drawback of supervised methods is that they require large labeled training datasets containing images together with pixel-wise class labels. These datasets have to be created manually by humans with great effort. For example, annotating a single image of the Cityscapes <ref type="bibr" target="#b6">[7]</ref> dataset required 90 minutes of human labor on average. This dependence of supervised methods on large human-annotated training datasets limits practical applications. We tackle this problem by introducing a novel approach on semantic image segmentation that does not require any labeled training data.  The major challenge of semantic image segmentation is to identify high-level structures in images. State-of-the-art methods approach this by learning from labeled data. While extensive research exists in segmentation without labeled data, it mainly focuses on non-learning based methods using low-level features such as color or edges <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1]</ref>. In general, low-level features are insufficient for semantic segmentation. They are not homogenous across high-level structures. <ref type="figure" target="#fig_1">Figure 1</ref>(a-b) illustrate this problem. An image depicting a person is segmented based on color. Color changes vastly across image areas, even if they are semantically correlated. Consequently, the resulting segmentation does not capture any high-level structures. Contrarily, <ref type="figure" target="#fig_1">Figure 1(d)</ref> illustrates how InfoSeg maps unlabeled images to segmentations that capture high-level structures. These segmentations often directly capture the semantic classes of labeled datasets.</p><p>The core idea of our method is to leverage image-level representation learning for pixel-level segmentation. Only recently, self-supervised representation learning methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref> showed how to extract high-level features from images without any annotated training data. However, they compute features that capture the entire content of images. Therefore, they are not suitable for segmentation. To enable segmentation, we instead use multiple high-level features, each capturing semantically similar image areas. This allows us to assign pixels to classes based on their attribution to each of these features. <ref type="figure" target="#fig_1">Figure 1</ref>(c-d) illustrate how our approach differs from image-level representation learning. We learn high-level features with a mutual information (MI) maximization approach, inspired by Local Deep InfoMax <ref type="bibr" target="#b13">[14]</ref>. However, unlike Local Deep InfoMax, we follow a novel two-step learning procedure enabling segmentation. At each iteration, we perform a Segmentation and Mutual Information Maximization step. In the first step, we segment images using the current features. In the second step, we update the features based on the segmentation from the first step. This two-step procedure allows us to train InfoSeg using solely unlabeled images and without pre-trained network backbones.</p><p>We motivate the exact structure of InfoSeg by first giving a thorough review of current-state-of-the-art methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>, followed by a discussion of their limitations and how we approach them in InfoSeg. Our qualitative and quantitative evaluation show that InfoSeg significantly outperforms all compared methods.</p><p>For example, we achieve a relative increase of 26% in Pixel-Accuracy (PA) on the COCO-Stuff dataset <ref type="bibr" target="#b3">[4]</ref>. Even though we follow the standard evaluation protocol for quantitative evaluation, we provide a critical discussion of it and uncover problems left undiscussed by recent work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. Furthermore, in addition to established datasets, we introduce COCO-Persons as a novel benchmark. COCO-Persons contains complex scenes requiring high-level interpretation for segmentation. Our experiments show that InfoSeg handles the challenging scenes of COCO-Persons significantly better than compared methods. Finally, we perform an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Self-Supervised Image Representation Learning aims to capture high-level content of images without using any labeled training data. State-of-the-art methods follow a contrastive learning framework <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12]</ref>. In contrastive learning, one computes multiple representations of differently augmented versions of the same input image. Augmentations can include photometric or geometric image transformations. During training, one enforces similarity on representations computed from the same image and dissimilarity on representations of different images. To this end, various objectives exist, such as the normalized cross entropy <ref type="bibr" target="#b25">[26]</ref> or MI <ref type="bibr" target="#b13">[14]</ref>.</p><p>Unsupervised Semantic Image Segmentation. Invariant Information Clustering (IIC) <ref type="bibr" target="#b16">[17]</ref> is a clustering approach also applicable for semantic segmentation. Briefly, IIC uses a MI objective that enforces the same prediction for differently augmented image patches. The authors of IIC proposed to use photometric or geometric image transformations to compute augmentations. For example, one can create augmentations by random color jittering, rotation, or scaling. Ouali et al . <ref type="bibr" target="#b26">[27]</ref> did a follow-up work on IIC. In addition to standard image transformations, they proposed to process image patches through various masked convolutions. We further discuss these two methods and its differences to InfoSeg in Section 3.2. Concurrent to our work, Mirsadeghi et al . proposed InMARS <ref type="bibr" target="#b22">[23]</ref>. InMARS is also related to IIC. However, instead of operating on each pixel individually, InMARS utilizes a superpixel representation. Furthermore, a novel adversarial training scheme is introduced.</p><p>Another recently introduced method that states to perform unsupervised semantic segmentation is SegSort <ref type="bibr" target="#b14">[15]</ref>. However, we note that SegSort still uses supervised learning at multiple stages. First, they initialize parts of their network architecture with pre-trained weights obtained by supervised training of a classifier on the ImageNet <ref type="bibr" target="#b7">[8]</ref> dataset. Second, they use pseudo ground truth masks generated by a HED contour detector <ref type="bibr" target="#b30">[31]</ref>, which is trained supervised using the BSDS500 <ref type="bibr" target="#b0">[1]</ref> dataset. Therefore, we do not consider SegSort as an unsupervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>In this section, we first review how recent work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref> uses MI for unsupervised semantic image segmentation. Then, we discuss limitations of these methods, and how we tackle them in InfoSeg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Semantic Image Segmentation</head><p>State-of-the-art methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref> adapt the MI based image clustering approach of IIC <ref type="bibr" target="#b16">[17]</ref> for segmentation. In the following, we introduce IICs' approach on image clustering and then the proposed modifications for segmentation.</p><p>For clustering, one creates two versions x and x ? of the same image. These versions show the same semantic content, but alter low-level appearance by using random photometric or geometric transformations. Consequently, semantic class predictions y and y ? of the two images x and x ? should be the same. To achieve this, one maximizes the MI between y and y ?</p><formula xml:id="formula_0">max ? I(? ? (x); ? ? (x ? )) = I(y; y ? ),<label>(1)</label></formula><p>where ? is a CNN parametrized by ?. Considering we can express the MI between y and y ? as</p><formula xml:id="formula_1">I(y; y ? ) = H(y) ? H(y|y ? ),<label>(2)</label></formula><p>Equation (1) maximizes the entropy H(y) while minimizing the conditional entropy H(y|y ? ). Minimizing H(y|y ? ) pushes predictions of the two images x and x ? together. Therefore, the network has to compute predictions invariant to the different low-level transformations. This should encourage class predictions to depend on high-level image content instead. While sole minimization of H(y|y ? ) can trivially be done by assigning the same class to all images. Additional maximization of H(y) has a regularization effect against such degenerate solutions.</p><p>Since maximizing H(y) encourages predictions that put equal probability mass on all classes. Consequently, predictions for all images can not collapse to a single class. For segmentation, Ji et al . <ref type="bibr" target="#b16">[17]</ref> proposed to use the previously introduced clustering approach on image patches rather than entire images. Two image versions are pushed through a network that computes dense pixel-wise class predictions. The objective given in Equation <ref type="formula" target="#formula_0">(1)</ref> is now applied on the pixel-wise class predictions. Therefore, each prediction depends on an image patch rather than an entire image. Patches are defined by the receptive field for each output pixel of the network. Additionally, one enforces local spatial invariance by maximizing MI of predictions from adjacent image patches. This approach on unsupervised semantic segmentation was initially proposed by IIC <ref type="bibr" target="#b16">[17]</ref>. Furthermore, Ouali et al . <ref type="bibr" target="#b26">[27]</ref> proposed an extension by generating views using different masked convolutions <ref type="bibr" target="#b24">[25]</ref>. In the following, we discuss three major limitations of these two works, and how we tackle them in InfoSeg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Limitations of current methods</head><p>The first limitation of discussed methods is that they do not incorporate global image context. Global context is essential to capture high-level structures, since they often cover large image areas having diverse local appearance. Therefore observing only small image patches is often not sufficient to identify them. Ideally, each pixel-wise prediction should depend on the entire image. Nevertheless, the discussed approaches make pixel-wise predictions based on image patches. The receptive field of the network ? determines the size of these patches. In general, one could enlarge the receptive field by changing the network architecture. However, adapting IIC from clustering to segmentation is based on restricting each prediction's receptive field from entire images to patches. By making each pixel-wise prediction dependent on the entire image again, one would fall back to clustering. In InfoSeg we capture global context in global high-level features that cover the entire image. We make pixel-wise predictions based on the MI between these global features and local patch-wise features. This allows each pixel-wise prediction to depend on the entire image.</p><p>A second limitation of discussed methods is that they fail to leverage recent advances in image representation learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2]</ref>. These methods are effective at capturing high-level image content, but only at the image-level. Adapting them for pixel-level segmentation is not trivial. Ouali et al . <ref type="bibr" target="#b26">[27]</ref> attempted this with their Autoregressive Representation Learning (ARL) loss, but failed to increase segmentation performance. Despite high-level information is constant across large image areas, ARL computes for each pixel a separate high-level feature. Contrarily, in InfoSeg, we share high-level features over the entire image.</p><p>To still allow pixel-wise segmentation, we compute multiple high-level features. Each high-level feature encodes only image areas depicting one class. We then assign pixels to classes based on their attribution to each of these features.</p><p>Finally, discussed methods jointly learn features and segmentations. They use intermediate feature representations to assign pixels to class labels. At the beginning of training, features depend on random initialization and contain no high-level information. This can lead to classes that latch onto low-level features instead of capturing high-level information. This issue was first discussed for image classification by SCAN <ref type="bibr" target="#b28">[29]</ref>. Instead, we decouple feature learning and segmentation. Therefore, we perform two steps at each iteration. First, we compute features that are explicitly trained to encode high-level information. Then, we use them for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">InfoSeg</head><p>In InfoSeg, we tackle unsupervised semantic image segmentation. We take a set {X (n) ? X } N n=1 of N unlabeled images and assign a label Z = {z 1 , . . . , z K } to every pixel of each image. Importantly, for one particular image, we do not specify which nor how many labels should be assigned. We only provide the total number of labels K in all images. After training, we follow the standard  InfoSeg is designed to tackle the three limitations of state-of-the-art methods discussed in Section 3.2. <ref type="figure" target="#fig_2">Figure 2</ref> shows an overview of InfoSeg. In the following, we first discuss how we leverage recent progress in representation learning for semantic segmentation in Section 4.1. Then we provide further details of our method in Section 4.2 and Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation Learning for Segmentation</head><p>We first review how Local Deep InfoMax <ref type="bibr" target="#b13">[14]</ref> captures high-level information of entire images, and then how InfoSeg adapts this approach to target image segmentation.</p><p>Local Deep InfoMax <ref type="bibr" target="#b13">[14]</ref> learns global high-level features of images by maximizing their average MI with local features. Local features cover image patches, and the global feature covers the entire image. If the global feature has limited capacity, the network cannot simply copy all local features' content into the global feature to maximize MI. Instead, the network has to encode a compact representation that shares information with as many image patches as possible. Hjelm et al . <ref type="bibr" target="#b13">[14]</ref> showed that the resulting global features encode high-level image information. They motivated this by the idea that high-level information is  often constant over an entire image, while low-level information such as pixellevel noise varies. Consequently, the global feature is encouraged to encode the former while disregarding the latter.</p><formula xml:id="formula_2">H 1 H 2 H K ... ... X (a) Architecture overview (b)</formula><p>To enable pixel-wise segmentation, we compute for each image multiple global features instead of a single one. Each global feature only encodes image areas that depict a particular class. This allows us to segment images by assigning pixels to classes based on their attribution to each global feature. During training, we maximize for each global feature MI only with local features covering its respective class. Therefore, we learn high-level features in a similar way as Local Deep InfoMax <ref type="bibr" target="#b13">[14]</ref>, but target segments instead of entire images. This requires us to learn high-level features together with segmentations. To this end, we alternate two steps at each iteration. In the Segmentation Step, we assign local to global features based on their content, i.e., we segment images. In the Mutual Information Maximization step, we maximize the MI between all global features and assigned local features, i.e., we learn the features. We describe both steps in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Segmentation Step</head><p>Given an input image X ? X = R M?N ?C , we compute P -dimensional global H ? R K?P and patch-wise local L ? R U?V ?P features. The k-th global feature H k ? R P encodes a high-level representation for the k-th class and covers the entire image. The local feature L i,j ? R P at the spatial position (i, j) encodes an image patch. Furthermore, the spatial resolution of L is downsampled by a rate of d from the input resolution, i.e. U = M/d and V = N/d. <ref type="figure" target="#fig_3">Figure 3</ref> shows the architecture of our feature computation network. First, the input image is processed by Block A, resulting in a grid of patch-wise image features. To compute the local features L, we further process these patch-wise features by Block C. Adding this additional residual block of pointwise convolutions led to better performance, than using Block A's output directly for the local features. To compute the global features H k , we first process the output of Block A to image-level features using Block B. Then, similarly, as for the local features, we add a residual block of pointwise convolutions using Block C. Finally, each global feature is computed using a separate linear layer using Block D.</p><p>To compute an image segmentation, we use the dot-product of a local and global feature pair L i,j , H k as a class score. A high score indicates that the k-th class is shown at the position (i, j). We elaborate in Section 4.3 how MI maximization increases the dot-product of a local feature and the global feature of its corresponding class. After computing the class scores, we apply a pixelwise scaled softmax to compute a class-probability volume V ? R U?V ?K with elements</p><formula xml:id="formula_3">V i,j,k = exp (? ? L i,j , H k ) k exp ? ? L i,j , Hk ,<label>(3)</label></formula><p>where ? is a hyper-parameter that controls the smoothness of the resulting distribution. Using the probability volume, we compute the low-res segmentation K for every pixel (i, j) with</p><formula xml:id="formula_4">k i,j = arg max k?Z V i,j,k ,<label>(4)</label></formula><p>by taking the class with the largest probability. We can then compute a full-res segmentation Z by upsampling the low-res segmentation K to the input image resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mutual Information Maximization Step</head><p>We first need to assign each local feature to its corresponding class's global feature. We could do this using the segmentation K. However, this disregards class probabilities, instead of utilizing their exact values, e.g. to account for uncertainty. Especially at the beginning of training, segmentations are uncertain and depend on random network initialization. Reinforcing possibly incorrect predictions can lead to degenerate solutions. To alleviate this problem, we do not make hard class assignments using K, but soft assignments using classprobabilities V. Instead of assigning a single global feature, we weight each global feature by its respective class probability. To this end, we define the function S (i,j) ? that computes a soft global feature assignment for the local feature L (i,j) as follows S</p><formula xml:id="formula_5">(i,j) ? (X) = k V i,j,k ? H (k) ? (X),<label>(5)</label></formula><p>where the function H During training, we maximize the MI between the output of S (i,j) ? (X) and the corresponding local feature L (i,j) for all spatial positions (i, j). Hence our objective is given as</p><formula xml:id="formula_6">max ? E X ? ? ? 1 U V i,j I L (i,j) ? (X); S (i,j) ? (X) ? ? ? ,<label>(6)</label></formula><p>where E X denotes the expectation over all training images X and the function L (i,j) ? (X) computes the local features L i,j given an input image X. To evaluate our objective Equation <ref type="formula" target="#formula_6">(6)</ref>, consider that local and global features are high-dimensional continuous random variables. MI computation of such variables is challenging. Contrarily to discrete variables as in the objective of IIC Equation <ref type="formula" target="#formula_0">(1)</ref>, where exact computation is possible. For continuous variables, Belghazi et al . <ref type="bibr" target="#b2">[3]</ref> proposed MI estimation by maximizing lower bounds parametrized by neural networks. They used a bound based on the Donsker &amp; Varadhan (DV) representation of the Kullback-Leibler (KL) divergence. While several other bounds exist <ref type="bibr" target="#b27">[28]</ref>, we use a bound based on the Jensen-Shannon Divergence (JSD). Mainly because Hjelm et al . <ref type="bibr" target="#b13">[14]</ref> showed favorable properties of the JSD bound compared to others in their representation learning setting. This includes increased training stability and better performance with smaller batch sizes. Nevertheless, we also perform experiments using the DV bound in our ablation studies. A JSD based MI estimator I JSD (X; Y ) for two random variables X and Y can be defined as follows <ref type="bibr" target="#b23">[24]</ref> I(X; Y ) ? I JSD (X; Y ) := E p(x,y) [? sp(?T (x, y))] ? E p(x)p(y) [sp(T (x, y))], <ref type="bibr" target="#b6">(7)</ref> where sp(x) = log (1 + e x ) and T is a discriminator mapping sample pairs from X and Y to a real valued score. The first and second expectations are taken over samples from the joint p(x, y) and marginal p(x)p(y) distributions. Consequently, to tighten the bound, the discriminator T needs to discriminate samples from the joint and marginal distributions by assigning high or low scores, respectively.</p><p>To use the JSD estimator Equation <ref type="formula">(7)</ref> in our objective Equation <ref type="formula" target="#formula_6">(6)</ref>, we have to define the discriminator T and a sampling strategy. Following recent work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>, we create joint and marginal samples by combining feature pairs computed from the same image X and two randomly paired images X and X ? , respectively. The discriminator T can be implemented using any arbitrary function that maps feature pairs to a discrimination score, e.g., a neural network. For efficiency, we use the dot-product to compute discrimination scores i.e., T (x, y) := x, y . This requires only a single expensive forward pass through our network to compute the features, while we can then score any arbitrary combination with a cheap dot-product. Omitting the spatial indices (i, j) to avoid notational clutter, this leads to the MI estimator</p><formula xml:id="formula_7">I JSD (L ? (X); S ? (X)) := E P [? sp(? L ? (x), S ? (x) )] ? E P?P [sp( L ? (x); S ? (x ? ) ],<label>(8)</label></formula><p>where P is the empirical distribution of our dataset, x is an image sampled from P and x ? is an image sampled fromP = P. We can now simply insert the estimator of Equation <ref type="formula" target="#formula_7">(8)</ref> into our objective Equation <ref type="bibr" target="#b5">(6)</ref>. Maximizing the resulting objective increases the dot-product of local features with the global feature of their assigned class. Consequently, we use the dot-product as a class score, as described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first introduce our experimental setup and discuss challenges at the quantitative evaluation of unsupervised segmentation. Then we perform an evaluation using established benchmarks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>, and COCO-Persons, a novel dataset introduced in this work. On all datasets, InfoSeg significantly outperforms compared methods. Finally, we perform ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We start training from random network initialization and provide solely unlabeled images. We set P = 1024, ? = 0.8 and use the ADAM optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 10 ?4 and a batch size of 64. Furthermore, the network architecture we use results in a downsampling rate of d = 4, and we set the number of classes K to be equal to the number of classes in each dataset. Note that InfoSeg requires a network with a different structure as Invariant Information Clustering (IIC) and Autoregressive Clustering (AC). For InfoSeg, the final outputs are 1?1 sized global image features. Contrarily, in IIC and AC, the final outputs are pixel-wise class predictions downscaled from the input image resolution. This impedes a comparison with these methods using the exact same architecture. Nevertheless, we provide an experiment in our ablation study where we apply the objective of IIC on the output of our Segmentation Step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Evaluation</head><p>Meaningful quantitative evaluation of unsupervised semantic segmentation is challenging. Recent work used the PA for quantitative evaluation. The PA is defined as the percentage of pixels assigned to the same class as in a given annotation. However, in unsupervised semantic segmentation, one does not specify which classes should be used for segmentation. Instead, many different segmentations can be considered as equally valid. Nevertheless, quantitative evaluation metrics, such as the PA or mean Intersection-Over-Union (mIoU), evaluate all pixel-wise predictions as incorrect that do not exactly match the given annotations. While this has been left undiscussed by previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>, we emphasize this has to be considered when interpreting quantitative metrics of unsupervised methods.</p><p>We can further illustrate problems at quantitative evaluation using the COCO-Stuff <ref type="bibr" target="#b3">[4]</ref> dataset as an example. The dataset contains the class rawmaterial that labels image areas depicting metal, plastic, paper, or cardboard. We argue that this is a very specific class and aggregating these four materials in one class is an arbitrary design choice of the dataset. It is unfeasible to expect an unsupervised method to come up with this specific solution. Nevertheless, recent methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref> reported significant increases over baseline models on the PA. We attribute this to the dataset's vast class imbalance. Besides very specific classes such as rawmaterial, the dataset also contains more generic classes such as water, or plant. These classes are overrepresented and make up more than 50% of all pixels. Therefore, an algorithm can achieve high PA by focusing mainly on these few overrepresented classes. To illustrate this effect, we provide a confusion matrix of our predictions in the supplementary material.</p><p>Despite the discussed problems, we follow prior work and use the PA to evaluate all of our results quantitatively. Following the standard evaluation protocol <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>, we map each of the predicted classes in Z to one of the annotated classes in Z ? before computing the PA. This is necessary because class ordering is unknown without providing labeled data during training. We find the one-to-one mapping between Z and Z ? by solving the linear assignment problem using the Hungarian method <ref type="bibr" target="#b19">[20]</ref>. We compute this mapping once after training and use the same mapping for all images in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Data</head><p>Recent work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref> established the COCO-Stuff <ref type="bibr" target="#b3">[4]</ref> and Potsdam <ref type="bibr" target="#b21">[22]</ref> datasets as benchmarks. COCO-Stuff contains 15 classes and Potsdam 6 classes. Additionally, for both datasets, a reduced 3-class variation exists. We use the same pre-processing as in the compared methods, resulting in 128 ? 128 sized RGB images for COCO-Stuff, and 200?200 sized RGBIR images for Potsdam.</p><p>While unsupervised segmentation of COCO-Stuff and Potsdam is challenging, most classes in these datasets still have a homogeneous low-level appearance. For example, low-level features such as color and texture are often sufficient to segment areas labeled as water in COCO-Stuff or road in Potsdam. To show that InfoSeg can go one step further, we evaluate on an additional dataset where segmentation is more reliant on high-level image features. To this end, we introduce the COCO-Persons dataset, which we will provide publicly. Each image depicts one or multiple persons and is annotated with a person and a non-person class. Face, hair, and clothing of persons vary vastly in color, texture, and shape, and the non-person areas cover a variety of complex indoor and outdoor scenes. The dataset is a subset of the COCO <ref type="bibr" target="#b20">[21]</ref> dataset and contains 15 399 images having 128?128 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We provide quantitative and qualitative results in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="figure">Figure 4</ref>, respectively. To compute results for COCO-Persons, we used publicly available implementations, if available. In our experiments, InfoSeg significantly outperformed all compared methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>. We discuss qualitative results in the following.</p><p>COCO-Persons. In <ref type="figure">Figure 4</ref>(a-c), we show successful segmentation of images with vastly inhomogenous low-level appearance. InfoSeg even captures the two small persons in the background of <ref type="figure">Figure 4</ref>(a). In <ref type="figure">Figure 4(c)</ref>, the motorbike is assigned to the same class as the person. The dataset contains several images where persons are shown together with motorbikes. Therefore, without supervision, it is challenging to disentangle these two semantic concepts. In <ref type="figure">Figure 4(d)</ref>, we show a challenging example yielding a failure case.</p><p>COCO-Stuff. <ref type="figure">Figure 4</ref>(e-f) show examples where our predictions are close to the annotations. <ref type="figure">Figure 4</ref>(g-h) provide reasonable segmentations, even though large portions differ from the annotations. These examples demonstrate challenges at the evaluation of COCO-Stuff due to overly specific classes. Matching the annotations requires precise distinction of similar high-level concepts, which is difficult without supervision. The example in <ref type="figure">Figure 4</ref>(g) shows multiple houses that are assigned to the same class as the stone wall in <ref type="figure">Figure 4(f)</ref>. However, the ground truth of COCO-Stuff assigns the stone wall to a wall class and the houses to a building class. <ref type="figure">Figure 4(h)</ref> shows a market scene containing vegetables labeled as food but predicted as plants. Arguably, vegetables are food and plants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head><p>Measure MI Max.</p><p>Step To examine the influence of individual components, we perform the following ablation studies. First, we evaluate the effectiveness of soft assignments by replacing them with hard assignments. Therefore, we change our objective Equation <ref type="formula" target="#formula_6">(6)</ref> to maximize the MI at each spatial position between the local feature and the global feature of the assigned class according to the segmentation K. Second, we replace the JSD MI estimator with a DV one. Finally, in the last ablation study, we omit our Mutual Information Maximization step and solely perform our Segmentation</p><p>Step. As a replacement for our Mutual Information Maximization step we apply the MI maximization objective of IIC, referred to as IIC-MI. To create the two image versions required by IIC-MI, we use the same transformations as in IIC. <ref type="table">Table 2</ref> shows the results of our ablation studies, whereby we performed all experiments using the COCO-Stuff-3 dataset. We can observe the following: Using soft assignments increases performance over hard assignments. A JSDbased MI estimator performs better than a DV-based, which aligns with the results of Hjelm et al . <ref type="bibr" target="#b13">[14]</ref>. And replacing our Mutual Information Maximization step with the objective of IIC leads to a decline in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel approach for unsupervised semantic image segmentation. Our experiments showed that our method yields semantically meaningful predictions and significantly outperforms related methods. We used the established datasets for evaluation and introduced a novel challenging benchmark COCO-Person. Furthermore, we discussed several problems making the quantitative evaluation of unsupervised semantic segmentation challenging. Finally, we performed ablation studies on our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Input image. The two magnified image patches have vastly different lowlevel appearance despite covering the same semantic object: a person. (b) Color based segmentation fails to capture any high-level structure of the image. (c) Representation learning captures high-level information of the entire image in a single feature. (d) InfoSeg captures semantically similar image areas in separate features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of InfoSeg for K = 2 classes. At each training iteration, we alternate the following two steps. Segmentation Step (solid lines): An input image X is passed through a CNN to compute local patch-wise features L and for each class k a global image-level feature H k . We then score local L with global H features using a dotproduct. The result is passed through a scaled softmax function to compute the class probability volume V. Finally, we obtain a segmentation by assigning each pixel to the class with the largest probability. Mutual Information Maximization Step (dashed lines): The global feature assignment S is computed as a sum of global features, weighted by their respective class probabilities at each spatial position. Finally, we maximize Mutual Information between local features L and their respective feature assignment S. evaluation protocol and map the learned labels of InfoSeg directly to the semantic classes of an annotated dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Feature computation for K classes. (a) Overview of network architecture. (b) Used blocks. Legend: Conv, W?W, C, d: Convolution with filter size W?W , C channels and stride d. Blocks that are used multiple times, each have their own set of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>computes the k-th global feature H k for an image X, and ? denotes the learnable parameters of our network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Pixel-Accuracy of InfoSeg and compared methods.</figDesc><table><row><cell></cell><cell>Image</cell><cell>Annotation</cell><cell>IIC</cell><cell>Ours</cell><cell cols="3">Image</cell><cell cols="2">Annotation</cell><cell>IIC</cell><cell>Ours</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(e)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>COCO-Persons</cell><cell>(b)</cell><cell></cell><cell></cell><cell>COCO-Stuff</cell><cell>(f)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell>(g)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(d)</cell><cell></cell><cell></cell><cell></cell><cell>(h)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Non-Person</cell><cell>Person</cell><cell></cell><cell></cell><cell>Solid</cell><cell cols="2">Structural</cell><cell>Ground</cell><cell>Building</cell><cell>Food</cell><cell>Plant</cell><cell>Sky</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Furniture</cell><cell>Wall</cell><cell></cell></row><row><cell cols="10">Fig. 4. Qualitative comparison. Non-stuff areas in COCO-Stuff are masked in black.</cell></row><row><cell></cell><cell>Method</cell><cell cols="8">COCO-Persons COCO-Stuff COCO-Stuff-3 Potsdam Potsdam-3</cell></row><row><cell></cell><cell cols="2">Random CNN</cell><cell>52.3</cell><cell>19.4</cell><cell></cell><cell cols="2">37.3</cell><cell></cell><cell>28.3</cell><cell>38.2</cell></row><row><cell></cell><cell cols="2">K-Means</cell><cell>54.3</cell><cell>14.1</cell><cell></cell><cell cols="2">52.2</cell><cell></cell><cell>35.3</cell><cell>45.7</cell></row><row><cell></cell><cell cols="2">Doersch  *  [9]</cell><cell>55.6</cell><cell>23.1</cell><cell></cell><cell cols="2">47.5</cell><cell></cell><cell>37.2</cell><cell>49.6</cell></row><row><cell></cell><cell cols="2">Isola  *  [16]</cell><cell>57.5</cell><cell>24.3</cell><cell></cell><cell cols="2">54.0</cell><cell></cell><cell>44.9</cell><cell>63.9</cell></row><row><cell></cell><cell>IIC [17]</cell><cell></cell><cell>57.1</cell><cell>27.7</cell><cell></cell><cell cols="2">72.3</cell><cell></cell><cell>45.4</cell><cell>65.1</cell></row><row><cell></cell><cell>AC [27]</cell><cell></cell><cell>-</cell><cell>30.8</cell><cell></cell><cell cols="2">72.9</cell><cell></cell><cell>49.3</cell><cell>66.5</cell></row><row><cell></cell><cell cols="2">InMARS [23]</cell><cell>-</cell><cell>31.0</cell><cell></cell><cell cols="2">73.1</cell><cell></cell><cell>47.3</cell><cell>70.1</cell></row><row><cell></cell><cell cols="2">InfoSeg (ours)</cell><cell>69.6</cell><cell>38.8</cell><cell></cell><cell cols="2">73.8</cell><cell></cell><cell>57.3</cell><cell>71.6</cell></row></table><note>* Clustering of features from methods that are not specifically designed for image segmentation.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">InfoSeg: Unsupervised Semantic Image Segm. with Mutual Information Max.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partly funded by the Austrian Research Promotion Agency (FFG) under project 874065.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>R&amp;amp;d</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>Darmstadt</surname></persName>
		</author>
		<ptr target="Tech.rep.,www.cityscapes-dataset.net1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10-02" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Use of the stair vision library within the isprs 2d semantic labeling benchmark (vaihingen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised image segmentation by mutual information maximization and adversarial regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Mirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Royat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Conference on Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748.vol.abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoregressive unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020-08-02" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<meeting><address><addrLine>Hingham, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2017-12" />
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
