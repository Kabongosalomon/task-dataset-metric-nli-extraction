<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributed Representation of Subgraphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07">2016. July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bijaya</forename><surname>Adhikari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhang</surname></persName>
							<email>yaozhang@cs.vt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naren</forename><surname>Ramakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Prakash</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bijaya</forename><surname>Adhikari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naren</forename><surname>Ramakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Prakash</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Distributed Representation of Subgraphs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACM Conference</title>
						<meeting>ACM Conference <address><addrLine>Washington, DC, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">9</biblScope>
							<date type="published" when="2016-07">2016. July 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network embeddings have become very popular in learning effective feature representations of networks. Motivated by the recent successes of embeddings in natural language processing, researchers have tried to nd network embeddings in order to exploit machine learning algorithms for mining tasks like node classication and edge prediction. However, most of the work focuses on nding distributed representations of nodes, which are inherently ill-suited to tasks such as community detection which are intuitively dependent on subgraphs.</p><p>Here, we propose Sub2Vec, an unsupervised scalable algorithm to learn feature representations of arbitrary subgraphs. We provide means to characterize similarties between subgraphs and provide theoretical analysis of Sub2Vec and demonstrate that it preserves the so-called local proximity. We also highlight the usability of Sub2Vec by leveraging it for network mining tasks, like community detection. We show that Sub2Vec gets signi cant gains over stateof-the-art methods and node-embedding methods. In particular, Sub2Vec o ers an approach to generate a richer vocabulary of features of subgraphs to support representation and reasoning. ACM Reference format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are a natural abstraction for representing relational data from multiple domains such as social networks, protein-protein interactions networks, the World Wide Web, and so on. Analysis of such networks include classi cation <ref type="bibr" target="#b4">[5]</ref>, link prediction <ref type="bibr" target="#b20">[20]</ref>, detecting communities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, and so on. Many of these tasks can be solved using machine learning algorithms. Unfortunately, since most machine learning algorithms require data to be represented as features, applying them to graphs is challenging due to their high dimensionality and structure. In this context, learning meaningful feature representation of graphs can help to leverage existing machine learning algorithms more widely on graph data.</p><p>Apart from classical dimensionality reduction techniques (see related work), recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b32">31]</ref> have explored various ways of learning feature representation of nodes in networks Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, Washington, DC, USA ? 2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn exploiting relationships to vector representations in NLP (like word2vec <ref type="bibr" target="#b22">[22]</ref>). However, application of such methods are limited to binary and muti-class node classi cation and edge-prediction. It is not clear how one can exploit these methods for other tasks like community detection which are inherently based on subgraphs and node embeddings result in loss of information of the subgraph structure. Embedding of subgraphs or neighborhoods themselves seem to be be er suited for these tasks. Surprisingly, learning feature representation of networks themselves (subgraphs and graphs) has not gained much a ention thus far. In this paper, we address this gap by studying the problem of learning distributed representation of subgraphs. Our contributions are:</p><p>(1) We propose Sub2Vec, a scalable subgraph embedding method to learn features for arbitrary subgraphs that maintains the so-called local proximity. <ref type="bibr" target="#b1">(2)</ref> We also provide theoretical justi cation of network embedding using Sub2Vec, based on language modeling tools. We also propose meaningful ways to measure how similar two subgraphs are to each other. (3) We conduct multiple experiments over large diverse real datasets to show correctness, scalability, and utility of features learnt by Sub2Vec in several tasks. In particular we get upto 4x be er results in tasks such as community detection compared to just node-embeddings. e rest of the paper is organized as follows: we rst formulate and motivate our problem, then present Sub2Vec, discuss experiments, and nally present related work, discussion and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>In this paper, we are interested in embedding subgraphs into a low dimensional continuous vector space. As shown later, the vector representation of subgraphs enables us to apply o -the-shelf machine learning algorithms directly to solve subgraph mining tasks. For example, to group subgraphs together, we can apply clustering algorithms like KMeans directly. <ref type="figure">Figure 1</ref> (a-c) gives an illustration. Given a set of subgraphs ( <ref type="figure">Figure 1 (b)</ref>) of a graph G <ref type="figure">(Figure 1 (a)</ref>), we learn a low-dimensional feature representation of each subgraph <ref type="figure">(Figure 1(d)</ref>). Now we are ready to formulate our Subgraph Embedding problem. We are given a graph G(V , E) where V is the vertex set, and E is the associated edge-set (we assume undirected graphs here, but our framework can be easily extended to directed graphs as well). We de ne i ( i , e i ) as a subgraph of G, where i ? V and e i ? E. For simplicity, we write i ( i , e i ) as i . As input we require a set of subgraphs S = { 1 , 2 , . . . , n }. Our goal is to embed subgraphs in S into d-dimensional feature space R d , where d &lt;&lt; |V |. In addition, we want to ensure the subgraph proximity is well-preserved in such a d-dimensional space. In this paper, we consider to preserve the (a) A network G (b) A set, S, of subgraphs of G (c) embedding learned for each subgraph (d) Intermediate neighborhoods on each subgraph <ref type="figure">Figure 1</ref>: An overview of our Sub2Vec. Our input is a set of subgraphs S drawn from a network G. We obtain d dimensional embedding of subgraphs such that we maximize the likelihood of observing intermediate neighborhoods.</p><p>"local neighborhood" of each subgraph i . e idea is that if two subgraphs share common structure, then their vector representations in R d are close. We call such a measure Local Proximity.</p><formula xml:id="formula_0">I D 1.</formula><p>(Local Proximity). Given two subgraphs i ( i , e 1 ) and j ( j , e j ), the local proximity between i and j is larger if the commonly induced subgraph is larger.</p><p>Intuitively, local proximity measures how many nodes, edges, and paths are shared by two subgraphs. For illustration of the local proximity, let us consider an example. In <ref type="figure">Figure 2</ref>, suppose 1 , 2 , and 3 are subgraphs induced by nodes {a, b, c, e} and {b, c, d, e}, and {d, e, f , j}. Since, the subgraph commonly induced by 1 and 2 is larger than the subgraph commonly induced by 1 and 3 , we say 1 and 2 to be more "locally proximal" to each other than 1 and 3 . Note that the local proximity is not just the Jaccard similarity of nodes in the two subgraphs, as it also takes the connections among the common nodes into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: A toy network</head><p>Having de ned the local proximity of two subgraphs, we focus on learning vector representations of subgraphs such that the local proximity is preserved. Formally, our Subgraph Embedding problem is, P 1. Given a graph G(V , E), d and set of S subgraphs (of G) S = { 1 , 2 , . . . , n }, learn an embedding function f : i ? y i ? R d such that Local Proximity among subgraphs is preserved.</p><p>According to Problem 1, if i and j are closer to each other in terms of the local proximity that k and k then the sim f ( i ), f ( j ) has to be greater than sim (f ( i ), f ( k )), where sim(x, y) is a similarity metric between two real vectors x and in R d . Hence, if we embed the subgraphs in <ref type="figure">Figure 2</ref> from the previous example, then a correct algorithm to solve Problem 1 has to ensure that</p><formula xml:id="formula_1">sim (f ( 1 ), f ( 2 )) &gt; sim f ( i ), f ( j )</formula><p>. We propose an e cient algorithm for Problem 1 based on two di erent optimization objectives in the next section.</p><p>A natural question to ask is that if there are other metrics of subgraph similarity. Indeed, one can think of other measures of proximity, which may result in di erent embeddings. We will discuss this point further in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING FEATURE REPRESENTATIONS</head><p>In this section, we propose two optimization objectives for Problem 1 and propose an unsupervised deep learning technique to optimize the objectives.</p><p>Mikolov et al. proposed the continuous bag of words and skipgram models in <ref type="bibr" target="#b22">[22]</ref>, which have been extensively used in learning continuous feature representation of words. Building on these two models, Le et al. <ref type="bibr" target="#b15">[15]</ref> proposed two models: the Distributed Memory of Paragraph Vector (PV-DM), and the Distributed Bag of Words version of Paragraph Vector (PV-DBOW), which can learn continuous feature representations of paragraphs and documents.</p><p>Our main idea is to pose our feature learning problem as a maximum likelihood problem by extending PV-DM and PV-DBOW to networks. e direct analog is to treat each node as a word, and each subgraph as a paragraph. e edges within a subgraph can be thought as the adjacency relation of two words in a paragraph. PV-DBOW and PV-DM assume that if two paragraphs share similar sequence of words, they are close in the embedded feature space. e local proximity of subgraphs naturally follows the above assumption. Hence, we can leverage deep learning techniques in <ref type="bibr" target="#b15">[15]</ref> for our subgraph embedding problem. PV-DBOW and PV-DM learn a latent representation by maximizing a distribution of word cooccurrences (using either n-gram or skip-gram model). Similarly, in this paper, we maximize a distribution of "node neighborhood". e so-called "node neighborhood" is generated by subgraph-truncated random walks (see details in Section 3.3). We call our models Distributed Bag of Nodes version of Subgraph Vector (Sub2Vec-DBON) and Distributed Memory version of Subgraph Vector (Sub2Vec-DM) respectively.</p><p>Next, we will introduce Sub2Vec-DM, Sub2Vec-DBON rst, then study how to generate "node neighborhood" and give a justi cation from matrix multiplication view. Finally, we summarize our algorithm Sub2Vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sub2Vec-DM</head><p>In the Sub2Vec-DM model, we seek to predict a node u given other nodes in u's neighborhoods and the subgraph u belongs to. Consider the subgraph 1 (a subgraph induced by nodes {a, b, c, e}) in <ref type="figure">Figure  2</ref>. Suppose the sequence of nodes returned by random walks in 1 is [a, b, c], and we consider neighborhood of distance 2, then the model asks to predict node c given subgraph 1 , and its predecessors (a and b), i.e., Pr(c | 1 , {a, b}).</p><p>More precisely, given a G (V , E ) as the union graph of all the subgraphs in S = { 1 , 2 , . . . , n }, where V = i i and E = i e i , consider a function m: V ? R d (m(n) = x). We de ne M as a d ? |V | node vector matrix, where each column is m(n) (the vector representation of nodes n ? V ). Similarly, we de ne function f ( i ) as the embedding function for subgraph i , where f ( i ) is a d-dimensional vector. We denote S as the subgraph matrix,</p><p>where each column is f ( i ) for all subgraphs in S. e matrices M and S are indexed by node and subgraph ids. In Sub2Vec-DM, we use the node and subgraph vectors to predict the next node in the neighborhood N n . We assume N n is given, and will discuss N n later in Section 3.3. Now, given a node n and its neighborhood N n and the subgraph i from which the N n is drawn, the objective of Sub2Vec-DM is to maximize the following:</p><formula xml:id="formula_2">max f i ?S n ? i log(Pr(n|m(N n ), f ( i )),<label>(1)</label></formula><p>where Pr(n|m(N n ), f ( i )) is the probability of predicting node n in i given the vector representations of its neighborhood m(N n ) and the subgraph from which the node and its neighborhood is drawn, f ( i ). Note that for ease of description, we extend the function m from a node to a node set (neighborhood N n ). Pr(n|m(N n ), f ( i )) is de ned using the so max function:</p><formula xml:id="formula_3">Pr(n|m(N n ), f ( i )) = e U n ?h(m(N n ), f ( i )) ?V e U ?h(m(N n ),f ( i ))<label>(2)</label></formula><p>where matrix U is a so max parameter and h(x, y) is average or concatanation of vectors x and y <ref type="bibr" target="#b15">[15]</ref>. In practice, to compute Equation 2, hierarchical so max is used <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sub2Vec-DBON</head><p>In the Sub2Vec-DBON model, we want to predict the nodes in the subgraph given only the subgraph vector f ( i ). For example, consider the same example in Section 3.1: the subgraph 1 in <ref type="figure">Figure 2</ref>, and the node sequence [a, b, c] generated by random walks. Now, in the Sub2Vec-DBON model the goal is to predict the neighborhood {a, b, c} given the subgraph 1 . is model is parallel to the popular skip-gram model. Formally, given a subgraph i , and neighborhood N drawn from i , the objective of Sub2Vec-DBON is the following:</p><formula xml:id="formula_4">max f i ?S N ? i log(Pr(N | f ( i )),<label>(3)</label></formula><p>where Pr(N | f ( i ) is also a so max function, i.e.,</p><formula xml:id="formula_5">Pr(N | f ( i ) = e m(N ).f ( i ) N ?G e m(N ).f ( i ) ,<label>(4)</label></formula><p>Since computing Equation 4 involves summation over all possible neighborhoods, we use negative sampling to optimize it. e negative sampling objective is as follows:</p><formula xml:id="formula_6">L = i ?S c ? i #( i , c) log(? ( (c) ? f ( i ))+k E c N P [log(? (? (c N ) ? f ( i ))]<label>(5)</label></formula><p>where k is a parameter for negative sampling, c is a context generated by random walks, and ? (x) = 1 1+e ?x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Subgraph Truncated Random Walks</head><p>Our problem seeks to preserve the local proximity between subgraph in S. As mentioned in Section 2, intuitively the local proximity measures how many nodes, edges, and paths are shared by two subgraphs. However, quantify local proximity is challenging. A possible way to measure the local proximity between two subgraphs i and j , would be to look at their neighborhoods, and compare every neighborhood in i with every neighborhood in j . However, it is not feasible as we have a large number of neighborhoods. Another approach to measure local proximity is that we can enumerate all possible paths in each subgraphs. However, there are exponential number of paths in each subgraphs. To bypass these challenges, we resort to random walks to implement the local proximity. Given a set of subgraphs S = { 1 , 2 , . . . , n }, we generate neighborhood in each i ? S by xed length subgraph-truncated random walks. Speci cally, for a subgraph i , we choose a node 1 from nodes in i uniformly at random. Next we generate a sequence of nodes 1 , 2 , 3 . . . k to get a random walk of length k, where j is a node chosen from the neighbors of node j?1 uniformly at random. We repeat the process for each subgraph in S. Overlaps in the random walks of i and j serve as a metric for local proximity. e intuition is that if the subgraph commonly induced by i and j is large, then we have more overlaps in their random walks.</p><p>Apart from being tractable in capturing the notion of local proximity between subgraphs, random walks have other advantages. First, the notion of neighborhood in other data types, such as texts, is naturally de ned due to the sequential nature of text data. However, graphs are not sequential, hence it is more challenging to de ne the neighborhoods of subgraphs. Random walks help sequentialize subgraphs. Moreover, random walks generate meaningful sequences, for example, the frequency of nodes in random walk follows power law distribution <ref type="bibr" target="#b25">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Matrix Multiplication based Justi cation of our Model</head><p>Here we demonstrate that optimizing the objective function of SV-DBON with negative sampling preserves the local proximity of subgraphs. Leveraging the idea in <ref type="bibr" target="#b19">[19]</ref>, we can write Equation 5 as a factorization of matrix M, where each element M i j corresponds to subgraph i and context j:</p><formula xml:id="formula_7">M i j = log( #(context j in subgraph i) #(context j in D) ) + log( |D| ? w k ? l ),<label>(6)</label></formula><p>k is a negative sampling parameter, w is a window size of context, and l is a length of a random walk in each subgraph. Note that if subgraph i in D has contexts j that is never observed, then in M,</p><formula xml:id="formula_8">M i j = log(0) = ??. A common practice in NLP is to replace M with M 0 where, M 0 = 0 if #(context j in subgraph i) = 0. Suppose M 0</formula><p>a is the a-th row in matrix M 0 , and M a ? M b is a dot-product. Now, we have the following lemma. L 3.1. Assuming random walks in subgraphs a and b visit every path of size w at least once, then</p><formula xml:id="formula_9">M 0 a ? M 0 b ? x log 2 ( |D|w |S|kl ),<label>(7)</label></formula><p>where S is set of input subgraphs in the data, D is the set of all the subgraph-context pairs observed ,and x is the number of overlapping paths of length w in subgraphs a and b . P . Now, by the de nition of dot product, we have the following:</p><formula xml:id="formula_10">M 0 a ? M 0 b = C j=1 log #(j, a) ? |D| ? w #(j, D) ? k ? l log #( j, b) ? |D| ? w #(j, D) ? k ? l ,<label>(8)</label></formula><p>where #(j, a) is the number of times context j appears in subgraph a . Now, we know that maximum value of #(j, D) is N ? (l ? w + 1) when random walk produces only context j. And the minimum value of #(j, a) is 1, as the random walk visits each path in the subgraph if it exists. Now, summing only over non-zero entries.</p><formula xml:id="formula_11">M 0 a ? M 0 b ? j ?#((j,a)) 0,#((j,b)) 0 log 2 |D| ? w N ? k ? l(l ? w + 1)<label>(9)</label></formula><p>Now using the fact that l ? (l ? w + 1) for any w &lt; l and that there are exactly x non-zero entries in the summation, we get</p><formula xml:id="formula_12">M 0 a ? M 0 b ? x log 2 |D| ? w N ? k ? l 2<label>(10)</label></formula><p>Lemma 3.1 shows that as the number of overlapping paths increases, the lower bound of any M 0 a ? M 0 b (corresponding to subgraphs a and b ) increases as well. Since optimizing Sub2Vec's objective is closely related to the factorization of matrix M 0 , we can expect the embedding of subgraphs with higher overlaps to be closer to each other in the feature space. Hence, Sub2Vec preserves the local proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Algorithm Algorithm 1 Sub2Vec</head><p>Require: Graph G, subgraph set S = { 1 , 2 , . . . , n }, length of the context window w, dimension d 1: walkSet = {} 2: for each i in s do for each randomly sampled Neighborhood N in walk i do <ref type="bibr">4:</ref> Compute L(f ) based in SV-DM or SV-DBON objective <ref type="bibr" target="#b4">5</ref>:</p><formula xml:id="formula_13">f = f ? ? ? ?L(f 6: end for 7: end for</formula><p>In our algorithm, we rst generate the neighborhood in each subgraph by running random walk. We then learn the vector representation of the subgraphs based on the random walks generated on each subgraph. en stochastic gradient descent is used to optimize SV-DBON/ SV-DM objectives. e complete pseudocode is presented in Algorithms 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We brie y describe our set-up next. All experiments are conducted using a 4 Xeon E7-4850 CPU with 512GB 1066Mhz RAM. We set the length of the random walk as 1000 and following literature <ref type="bibr" target="#b10">[11]</ref>, we set dimension of the embedding as 128 unless mentioned otherwise. e code was implemented in Python and we will release it for research purposes. We answer the following questions in our experiments: Q1. Are the embeddings learnt by Sub2Vec useful for community detection? Q2. Are the embeddings learnt by Sub2Vec e ective for link prediction? Q3. How scalable is Sub2Vec for large networks? Q4. Do parameter variations in Sub2Vec lead to over ing? Q5. Are the representations learnt by Sub2Vec meaningful? Datasets. We run Sub2Vec on multiple real world datasets from multiple domains like social-interactions, co-authorship, social networks and so on of varying sizes. See <ref type="table" target="#tab_0">Table 1</ref>.</p><p>(1) WorkPlace is a publicly available social contact network between employees of a company with ve departments <ref type="bibr" target="#b0">1</ref> . Edges indicate that two people were in proximity of each other.</p><p>(2) HighSchool is a social contact network 1 . Nodes are high school students belonging to one of ve di erent sections and edges indicate that two students were in vicinity of each other.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Community Detection</head><p>Setup. Here we show how to leverage Sub2Vec for the well-known community detection problem. A community of nodes in a network is a coherent group of nodes which are roughly densely connected among themselves and sparsely connected with the rest of the network. As nodes in a community are densely connected to each other, we expect neighboring nodes in the same community to have a similar surrounding. We know that Sub2Vec embeds subgraphs while preserving local proximity. erefore, intuitively we can use features generated by Sub2Vec to detect communities. Speci cally, we propose to solve the community detection problem using Sub2Vec by embedding the surrounding neighborhood of each node. First, we extract the neighborhood C of each node ? V from the input graph G(V , E). en we run Sub2Vec on S = {C | ? V } to learn feature representation of f (C ) for all C ? S. We then use a simple clustering algorithm (K-Means) to cluster the feature vectors f (C ) of all ego-nets. Cluster membership of ego-nets determines the community membership of the ego. e complete pseudocode is in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Community Detection using Sub2Vec</head><p>Require: A network G(V , E), Sub2Vec parameters, k number of communities 1: neighborhoodSet = {} 2: for each in V do 3: neighborhoodSet = neighborhoodSet ? neighbordhood of in G. 4: end for 5: vecs = Sub2Vec (neighborhoodSet, w, d) 6: clusters = K-Means(vecs, k) 7: return clusters In Algorithm 3, we de ne neighborhood of each node to be its ego-network for dense networks (HighSchool and WorkPlace) and 2-hop ego-networks for sparse networks. e ego-network of a node is the subgraph induced by the node and its neighbors. Similarly, the 2-hop ego-network of a node is de ned as the subgraph induced by the node, its neighbors, and neighbors' neighbors.</p><p>We compare Sub2Vec with various traditional community detection algorithms and network embedding based methods. Newman [10] is a community detection algorithm based on betweenness. It is a greedy agglomerative hierarchical clustering algorithm. Louvian <ref type="bibr" target="#b5">[6]</ref> is a greedy optimization method. Node2Vec is a network embedding method which learns feature representation of nodes in the network which we then cluster to obtain communities.</p><p>We run Sub2Vec and baselines on the following networks with ground truth communities and compute Precision, Recall, and F-1 score to evaluate all the methods.</p><p>(1) WorkPlace: Each department as a ground truth community.</p><p>(2) HighSchool: Each section as a ground truth community.</p><p>(3) Texas, Cornell, Washington: Each webpage belongs to one of ve classes: course, faculty, student, project, and sta , which serve as ground-truth. (4) PolBlogs: Conservative and liberal blogs as ground-truth communities.</p><p>Results. See <ref type="table" target="#tab_1">Table 2</ref>. Both versions of Sub2Vec signi cantly and consistently outperform all the baselines (upto a factor of 4 times against closest competitor, Node2Vec). We do be er than Node2Vec because intuitively, we learn the feature vector of the neighborhood of each node for the community detection task; while Node2Vec just does random probes of the neighborhood. Precision for Louvian is high in dense networks as it outputs small communities and recall is consistently poor across all datasets for the same reason, while for Newman the performance is not consistent. Performance of Node2Vec is satisfactory in the sparse networks like PolBlogs and Texas, but it is signi cantly worse for dense networks like WorkPlace and HighSchool. On the other hand, performance of Sub2Vec is even more impressive in these networks.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we plot the community structure of the HighSchool dataset. In the HighSchool dataset, we consider ve sections as the ground truth community. In the gure, the color of nodes indicate the community membership. e gure highlights the superiority of Sub2Vec compared to Node2Vec. e communities discovered by Sub2Vec matches the ground truth very closely, while those discovered by Node2Vec appear to be near random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Link Prediction</head><p>Setup. In this section, we focus on the Link Prediction problem. Given a network G(V , E), the link prediction problem asks to predict the likelihood of formation of an edge between two nodes 1 ? V and 2 ? V , such that ( 1 , 2 ) E. It is well known that nodes with common neighbors tend to form future links <ref type="bibr" target="#b20">[20]</ref>. For example, in a social network two individuals who have multiple friends in common have higher chances of eventually forming a friendship. It is evident from the example that likelihood of future edges depends on the similarity of neighborhood around each end-point. Hence we propose exploiting the embeddings of ego-nets of each node obtained from Sub2Vec to predict whether two nodes will form an edge.</p><p>Speci cally, we rst hide a P percentage of edges randomly sampled from the network, while ensuring that the remaining network remains connected. We consider these "hidden" edges as the ground truth. en we extract the ego-network, C , for each node ? V . We then run Sub2Vec on S = {C | ? V } and use the resulting embedding to predict link. Following methodology in literature <ref type="bibr" target="#b32">[31]</ref>, to evaluate our method, we calculate the Mean  Here i is the i th node predicted to have edge with node and 1( , i ) = 1 if ( , 1 ) is in the ground truth, 0 otherwise. en we compute the Average Precision as AP( ) = i Precision@i( )?1( , i )</p><formula xml:id="formula_14">i 1( , i )</formula><p>. Finally, MAP is given as:</p><formula xml:id="formula_15">MAP = ?Q AP( ) |Q |</formula><p>We compare our result with Node2Vec only as it was previously shown to be be er than other baselines <ref type="bibr" target="#b10">[11]</ref>. Results. See <ref type="table" target="#tab_2">Table 3</ref>. Firstly, note that Sub2Vec outperforms Node2Vec as P varies from 10 to 30 in all the datasets. We also notice that Sub2Vec DM performs surprisingly worse than Node2Vec and Sub2Vec DBON on Facebook. e reason for its poor performance in Facebook is that the network is dense with average clustering co-e cient of 0.6 and e ective radius of 4 for 90% of the nodes. Recall that the Sub2Vec DM optimization relies on nding the embedding of the nodes as well, which will not be discriminative for dense networks. In contrast, Sub2Vec DBON learns the features of subgraps directly, without relying on node embeddings, and hence it performs very well on large dense networks including Facebook. Finally we see that Node2Vec consistently improves as P increases, while both versions of Sub2Vec either deteriorate or stagnate. We discuss this more in Section 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Sensitivity</head><p>Here we discuss the parameter sensitivity of Sub2Vec. We show how the F-1 score for community detection task on PolBlogs dataset changes when we change the two parameters of Sub2Vec: (i) length of the random walk and (ii) dimension of the embedding. As shown in <ref type="figure">Figure 4 (a)</ref>, the F-1 score is 0.85 even when we do random walks of length 500. For the higher length, the F-1 score remains constant.</p><p>Similarly, to see how the results of the community detection task changes with the size of the embedding, we run the community detection task on PolBlogs with varying embedding dimension. See <ref type="figure">Figure 4 (b)</ref>. e F-1 score saturates when the dimension of vector is greater than 100.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scalability</head><p>Here we show the scalability of Sub2Vec with respect to the number and the size of subgraphs. We extract connected subgraphs of Youtube dataset of induced by varying percentage of nodes. We then run Sub2Vec on the set of ego-nets in each resulting network. As shown in <ref type="figure" target="#fig_4">Figure 5</ref> (a), Sub2Vec is linear w.r.t number of subgraphs. In <ref type="figure" target="#fig_4">Figure 5</ref> (b), we run Sub2Vec on 1 to 3 hops ego-nets of Astro-PH dataset. We see a signi cant jump in the running time when the hop increases from 2 to 3.</p><p>is is due to the fact that as the hop of ego-net increases, the size of the subgraph increases exponentially due to the low diameter of real world networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Studies</head><p>We perform case-studies on MemeTracker 3 and DBLP to investigate if our embeddings are interpretable. MemeTracker consists of a series of cascades caused by memes spreading on the network of linked web pages. Each meme-cascade induces a subgraph in the underlying network. We rst embed these subgraphs in a continuous vector space by leveraging Sub2Vec. We then cluster these vectors to explore what kind of meme cascade-graphs are grouped together, what characteristics of memes determine their similarity and distance to each other and so on. For this case-study, we pick the top 1000 memes by volume in the data. And we cluster them into 10 clusters using K-Means.</p><p>We nd coherent clusters which are meaningful groupings of memes based on topics. For example we nd cluster of memes related to di erent topics such as entertainment, politics, religion, technology and so on. Visualization of these clusters is presented in <ref type="figure" target="#fig_6">Figure 6</ref>. In the entertainment cluster, we nd memes which are names of popular songs and movies such as "sweet home alabama", "somewhere over the rainbow", "Madagascar 2" and so on. Similarly, we also nd a cluster of religious memes. ese memes 3 snap.stanford.edu are quotes from the Bible. We also nd memes related to politics and religion in the same cluster such as "separation of church and state"'. In politics cluster, we nd popular quotes from the 2008 presidential election season e.g. Barack Obama's popular slogan "yes we can" along with his controversial quotes like "you can put lipstick on a pig" in the cluster. We also nd Sarah Palin's quote like "the chant is drill baby drill". Similarly, we also nd a cluster of technology/video games related memes.</p><p>Interestingly, we nd that all the memes in Spanish language were clustered together.</p><p>is indicates that memes in di erent language travel though separate websites, which matches with the reality as most webpages use one primary language. We also noticed that some of the clusters did not belong to any particular topic. Upon closer examination we found out that these clusters contained memes which were covered by general news website such as msnbc.com, yahoo.com, news.google.com and local news websites such as philly.com from Philadelphia and breakingnews.ie from Ireland.</p><p>For DBLP, we follow the methodology in <ref type="bibr" target="#b13">[14]</ref>, and extract subgraphs of the coauthorship network based on the keywords contained in the title of the papers.</p><p>We include keywords such as 'classi cation', 'clustering', 'xml', and so on. Once we extract the subgraphs, we run Sub2Vec to learn embedding of these subgraphs. We then project the embeddings down to 2-dimensions using t-SNE <ref type="bibr" target="#b21">[21]</ref>. See <ref type="figure" target="#fig_7">Figure 7</ref>. We see some meaningful groupings in the plot. We see that the keyword related to each other such as 'graphs', 'pagerank', 'crawling', and 'clustering' appear together. e classi cation related keywords such as 'boosting', 'svm', and 'classi cation' are grouped together. We also see that 'streams' and 'wavelets' are close to each other. ese meaningful groups of keywords highlight the fact that Sub2Vec results in meaningful embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Network Embedding. e network embedding problem has been well studied. Most of work seeks to generate low dimensional feature representation of nodes. Early work includes Laplacian Eigenmap <ref type="bibr" target="#b3">[4]</ref>, IsoMap <ref type="bibr" target="#b31">[30]</ref>, locally linear embedding <ref type="bibr" target="#b27">[27]</ref>, and spectral techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>. Recently, several deep learning based network embeddings algorithms were proposed to learn feature representations of nodes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b32">31]</ref>. Perozzi et. al <ref type="bibr" target="#b25">[25]</ref> proposed DeepWalk, which extends skip-Gram model <ref type="bibr" target="#b22">[22]</ref> to networks and learns feature representation based on contexts generated by random walks. Grover et. al. proposed a more general method, Node2Vec <ref type="bibr" target="#b10">[11]</ref>, which generalizes random walks to generate various contexts. SDNE <ref type="bibr" target="#b32">[31]</ref> and LINE <ref type="bibr" target="#b29">[29]</ref> learn feature representation   of nodes while preserving rst and second order proximity. However, all of them learn low dimensional feature vector of nodes, while our goal is to embed subgraphs. e most similar network embedding literature includes <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b33">32]</ref>. Risen and Bunke propose to learn vector representations of graphs based on edit distance to a set of pre-de ned prototype graphs <ref type="bibr" target="#b26">[26]</ref>. Yanardag et. al. <ref type="bibr" target="#b33">[32]</ref> and Narayanan et al. <ref type="bibr" target="#b23">[23]</ref> learn vector representation of the subgraphs using the Word2Vec <ref type="bibr" target="#b22">[22]</ref> by generating "corpus" of subgraphs where each subgraph is treated as a word. e above work focuses on some speci c subgraphs like graphlets and rooted subgraphs. None of them embed subgraphs with arbitrary structure. In addition, we interpret subgraphs as paragraphs, and leverage the PV-DBOW and PV-DM model <ref type="bibr" target="#b15">[15]</ref>. Other Subgraph Problems.</p><p>ere has been a lot of work on subgraph related problems. For example, the subgraph discovery problems have been studies extensively. Finding the largest clique is a well-known NP-complete problem <ref type="bibr" target="#b12">[13]</ref>, which is also hard to approximate <ref type="bibr" target="#b11">[12]</ref>. Lee et al. surveyed dense subgraph discovery algorithms for several subgraphs including clique, K-core, K-club, etc <ref type="bibr" target="#b16">[16]</ref>. Akoglu et al. extended the subgraph discovery problem to a ributed graphs <ref type="bibr" target="#b1">[2]</ref>. Perozzi et al. studied the a ributed graph anomaly detection by exploring the neighborhood subgraph of a nodes <ref type="bibr" target="#b24">[24]</ref>. Di erent from the above works, we seek to nd feature representations of subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>We have shown that Sub2Vec gives meaningful interpretable embeddings of arbitrary subgraphs. We have also shown via our experiments that Sub2Vec outperforms traditional algorithms as well as node-level embedding algorithms for extracting communities from networks, especially in challenging dense graphs. Similarly for link prediction, we also showed that embedding neighborhoods is be er for nding correct links.</p><p>So for which tasks will Sub2Vec not be ideal? For link prediction, as previously mentioned in Section 4, the performance of Sub2Vec deteriorates when higher percentages of edges are removed from the network. e results for higher percentages, P = 40 to 60, is presented in <ref type="table" target="#tab_3">Table 4</ref>. e result shows that Node2Vec outperforms Sub2Vec in such cases, despite performing poorly for lower values of P. is happens because, as P increases, the density of the network decreases and results in lesser overlaps in the neighborhoods of nearby nodes. Hence Sub2Vec which preserves the local proximity of subgraphs, does not embed such subgraphs very close to each other, resulting in poorer prediction performance.</p><p>We believe, in such situations, perhaps using other proximity measures between subgraphs is more meaningful to preserve during the embedding process than only local proximity.</p><p>One such way can be using 'positional promixity', where two subgraphs are proximal based on their position in the network. For example, in <ref type="figure">Figure 2</ref>, subgraphs induced by nodes {c, d, e} and { , h, j} are similar to each other as the member nodes in these two subgraphs have similar roles. Nodes e and h both connect to central node f and nodes d and both have degree two. Using just local proximity, these subgraphs are not similar. Positional Proximity: If we are given two subgraphs i ( i , e 1 ) and j ( j , e j ), then the positional proximity between i and j is determined by similarity of position of nodes in i and j .</p><p>Similarly, another way can be using similarity based on structure of subgraphs. For example, in <ref type="figure">Figure 2</ref>, subgraphs induced by nodes Structural Proximity: If we are given two subgraphs i ( i , e 1 ) and j ( j , e j ), then the structural proximity between i and j is determined by the structural properties of i and j . For link prediction in very sparse networks, Positional Proximity might give more useful embeddings than Local Proximity. We leave the task of embedding subgraphs based on Structural and Positional proximities (or using a combination with Local proximity) and leveraging them for graph mining as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have presented Sub2Vec, a scalable feature learning framework for a set of subgraphs such that the local proximity between them are preserved. In contrast most prior work focused on nding nodelevel embeddings. We give a theoretical justi cation and showed that the embeddings generated by Sub2Vec can be leveraged in downstream applications such as community detection and link prediction. We also performed case-studies on two real networks to validate the usefulness of the subgraph features generated by Sub2Vec.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>i ] = walk 5: end for 6: f = StochasticGradientDescent(walkSet, d, w) 7: return f Algorithm 2 Sub2Vec: StochasticGradientDescent(walkSet, d, w) 1: randomly intialize features f 2: for each walk i in walkset do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 ) 4 ) 7 )</head><label>347</label><figDesc>Texas, Cornell, Washington, Wisconsin are networks from the WebKB dataset 2 . ese are networks of webpages and hyperlinks. (PolBlogs is a directed network of hyperlinks between weblogs on US politics, recorded in 2005. (5) Astro-PH and DBLP are coauthorship networks from Arxiv High-energy Physics and DBLP bibliographies respectively, where two authors have an edge if they have co-authored a paper. (6) Facebook [18] is an anonymized social network where nodes are Facebook users and edges indicate that two users are friends. (Youtube is a social network, where edges indicate friendship between two users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of community detection in dense HighSchool network. Communities obtained by clustering ego-nets vectors returned by Sub2Vec matches the ground truth, while the result from Node2Vec appears to be random. Average Precision (MAP). To calculate MAP rst we compute Preci-sion@K, as Precision@k( ) = i &lt;k 1( , i ) k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Walk length (b) Dimension of VectorsFigure 4: F-1 score on PolBlogs for various values of walk length and dimension of embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Scalability w.r.t. number of subgraphs on Youtube and w.r.t size of subgraphs on Astro-PH datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Di erent Clusters of Memes for the MemeTracker dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>2D projection of feature vectors learnt by Sub2Vec of subgraphs of DBLP induced by di erent keywords.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets Information.</figDesc><table><row><cell>Dataset</cell><cell>|V |</cell><cell>|E |</cell><cell>Domain</cell></row><row><cell>WorkPlace [9]</cell><cell>92</cell><cell>757</cell><cell>contact</cell></row><row><cell>Cornell [28]</cell><cell>195</cell><cell>304</cell><cell>web</cell></row><row><cell>HighSchool [8]</cell><cell>182</cell><cell>2221</cell><cell>contact</cell></row><row><cell>Texas [28]</cell><cell>187</cell><cell>328</cell><cell>web</cell></row><row><cell>Washington [28]</cell><cell>230</cell><cell>446</cell><cell>web</cell></row><row><cell>Wisconsin [28]</cell><cell>265</cell><cell>530</cell><cell>web</cell></row><row><cell>PolBlogs [1]</cell><cell>1490</cell><cell>16783</cell><cell>web</cell></row><row><cell>Facebook [18]</cell><cell>4039</cell><cell>88234</cell><cell>social-network</cell></row><row><cell>Astro-PH [17]</cell><cell cols="2">18722 199110</cell><cell>co-author</cell></row><row><cell>DBLP [33]</cell><cell>317k</cell><cell>1.04 M</cell><cell>co-author</cell></row><row><cell>Youtube [33]</cell><cell cols="2">1.13M 2.97M</cell><cell>social</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Sub2Vec easily out-performs all baselines in all datasets. Precision P, Recall R, and F-1 score, of various algorithms for community detection. Winners in F-1 score have been bolded for each dataset.</figDesc><table><row><cell></cell><cell></cell><cell>WorkPlace</cell><cell></cell><cell></cell><cell cols="2">HighSchool</cell><cell></cell><cell>PolBlogs</cell><cell></cell><cell></cell><cell>Texas</cell><cell></cell><cell></cell><cell>Cornell</cell><cell></cell><cell cols="3">Washington</cell><cell></cell><cell>Wisconsin</cell><cell></cell></row><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>F-1</cell><cell>P</cell><cell>R</cell><cell>F-1</cell></row><row><cell>Newman</cell><cell cols="2">0.26 0.27</cell><cell>0.27</cell><cell cols="2">0.23 0.32</cell><cell>0.27</cell><cell cols="2">0.67 0.64</cell><cell>0.66</cell><cell cols="2">0.43 0.15</cell><cell>0.22</cell><cell cols="2">0.38 0.25</cell><cell>0.30</cell><cell cols="2">0.32 0.87</cell><cell>0.47</cell><cell cols="2">0.35 0.13</cell><cell>0.19</cell></row><row><cell>Louvian</cell><cell cols="2">0.57 0.04</cell><cell>0.07</cell><cell cols="2">0.49 0.04</cell><cell>0.08</cell><cell cols="2">0.91 0.83</cell><cell>0.87</cell><cell cols="2">0.54 0.14</cell><cell>0.23</cell><cell cols="2">0.36 0.15</cell><cell>0.22</cell><cell>0.45</cell><cell>0.1</cell><cell>0.16</cell><cell cols="2">0.40 0.12</cell><cell>0.19</cell></row><row><cell>Node2Vec</cell><cell cols="2">0.26 0.21</cell><cell>0.23</cell><cell cols="2">0.21 0.22</cell><cell>0.22</cell><cell cols="2">0.92 0.92</cell><cell>0.92</cell><cell cols="2">0.41 0.63</cell><cell>0.50</cell><cell cols="2">0.30 0.36</cell><cell>0.33</cell><cell cols="2">0.37 0.45</cell><cell>0.40</cell><cell cols="2">0.34 0.24</cell><cell>0.29</cell></row><row><cell>Sub2Vec DM</cell><cell cols="3">0.87 0.69 0.77</cell><cell cols="3">0.95 0.95 0.95</cell><cell cols="3">0.92 0.93 0.93</cell><cell cols="3">0.49 0.57 0.53</cell><cell cols="2">0.34 0.47</cell><cell>0.39</cell><cell cols="3">0.45 0.64 0.53</cell><cell cols="3">0.40 0.42 0.41</cell></row><row><cell>Sub2Vec DBON</cell><cell cols="2">0.86 0.67</cell><cell>0.77</cell><cell cols="2">0.94 0.94</cell><cell>0.94</cell><cell cols="2">0.92 0.92</cell><cell>0.92</cell><cell cols="2">0.44 0.59</cell><cell>0.51</cell><cell cols="3">0.31 0.55 0.40</cell><cell cols="2">0.43 0.66</cell><cell>0.52</cell><cell cols="2">0.35 0.41</cell><cell>0.38</cell></row><row><cell></cell><cell cols="3">(a) Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Result of</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) Result of Sub2Vec</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean Average Precision for the link prediction task. P is the percentage of edge removed from the network and S stands for Sub2Vec. Winners have been bolded for each dataset. Either Sub2Vec DM or Sub2Vec DBON outperform Node2Vec across all the datasets.</figDesc><table><row><cell></cell><cell></cell><cell>WorkPlace</cell><cell></cell><cell></cell><cell>HighSchool</cell><cell></cell><cell></cell><cell>Facebook</cell><cell></cell><cell></cell><cell>Astro-PH</cell><cell></cell></row><row><cell>P</cell><cell cols="12">Node2Vec S DBON S DM Node2Vec S DBON S DM Node2Vec S DBON S DM Node2Vec S DBON S DM</cell></row><row><cell>10</cell><cell>0.25</cell><cell>0.37</cell><cell>0.33</cell><cell>0.39</cell><cell>0.42</cell><cell>0.52</cell><cell>0.50</cell><cell>0.77</cell><cell>0.29</cell><cell>0.12</cell><cell>0.24</cell><cell>0.31</cell></row><row><cell>20</cell><cell>0.36</cell><cell>0.28</cell><cell>0.42</cell><cell>0.41</cell><cell>0.52</cell><cell>0.26</cell><cell>0.68</cell><cell>0.84</cell><cell>0.34</cell><cell>0.21</cell><cell>0.31</cell><cell>0.28</cell></row><row><cell>30</cell><cell>0.39</cell><cell>0.28</cell><cell>0.40</cell><cell>0.50</cell><cell>0.45</cell><cell>0.57</cell><cell>0.72</cell><cell>0.83</cell><cell>0.35</cell><cell>0.26</cell><cell>0.37</cell><cell>0.44</cell></row><row><cell></cell><cell cols="2">(a) No of Subgraphs</cell><cell></cell><cell cols="2">(b) Size of Subgraphs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Mean Average Precision for the link prediction task. P is the percentage of edge removed and S stands for Sub2Vec. Node2Vec S DBON S DM Node2Vec S DBON S DM Node2Vec S DBON S DM Node2Vec S DBON S DM {a, b, c, e} and {h, i, j, k } are similar to each other as both of them are cliques of size four.</figDesc><table><row><cell></cell><cell></cell><cell>WorkPlace</cell><cell></cell><cell></cell><cell>HighSchool</cell><cell></cell><cell></cell><cell>Facebook</cell><cell></cell><cell></cell><cell>Astro-PH</cell><cell></cell></row><row><cell>P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell>0.45</cell><cell>0.32</cell><cell>0.35</cell><cell>0.60</cell><cell>0.47</cell><cell>0.56</cell><cell>0.75</cell><cell>0.78</cell><cell>0.22</cell><cell>0.30</cell><cell>0.39</cell><cell>0.33</cell></row><row><cell>50</cell><cell>0.48</cell><cell>0.31</cell><cell>0.33</cell><cell>0.57</cell><cell>0.42</cell><cell>0.49</cell><cell>0.78</cell><cell>0.75</cell><cell>0.12</cell><cell>0.33</cell><cell>0.26</cell><cell>0.34</cell></row><row><cell>60</cell><cell>0.50</cell><cell>0.33</cell><cell>0.32</cell><cell>0.60</cell><cell>0.40</cell><cell>0.43</cell><cell>0.79</cell><cell>0.53</cell><cell>0.1</cell><cell>0.34</cell><cell>0.29</cell><cell>0.29</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Conference'17, July 2017, Washington, DC, USA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">h p://www.sociopa erns.org/ 2 h p://linqs.cs.umd.edu/projects/projects/lbc/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2005. e political blogosphere and the 2004 US election: divided they blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international workshop on Link discovery</title>
		<meeting>the 3rd international workshop on Link discovery</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PICS: Parameter-free identi cation of cohesive subgroups in large a ributed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Meeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 SIAM international conference on data mining. SIAM</title>
		<meeting>the 2012 SIAM international conference on data mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="439" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Node classi cation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smriti</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Loup</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Lambio E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contact Pa erns among High School Students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Fournet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Barrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">107878</biblScope>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data on face-to-face contacts in an o ce building suggest a low-cost vaccination strategy based on community linkers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Genois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Vestergaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Fournet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Panisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Bonmarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Barrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="326" to="347" />
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
	<note>Issue 03</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences 99</title>
		<meeting>the national academy of sciences 99</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clique is hard to approximate within n1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th Symp. on Found. Comput. Sci</title>
		<meeting>37th Symp. on Found. Comput. Sci</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducibility among combinatorial problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complexity of computer computations</title>
		<imprint>
			<biblScope unit="page" from="85" to="103" />
			<date type="published" when="1972" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evimaria</forename><surname>Eodoros Lappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heikki</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mannila</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding e ectors in social networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey of algorithms for dense subgraph discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Managing and Mining Graph Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="303" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph evolution: Densi cation and shrinking diameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">e link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santhoshkumar</forename><surname>Saminathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08928</idno>
		<title level="m">subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable anomaly ranking of a ributed neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining. SIAM</title>
		<meeting>the 2016 SIAM International Conference on Data Mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph classi cation and clustering based on vector space embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>World Scienti c Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collective Classi cation in Network Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><forename type="middle">Mark</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">De ning and evaluating network communities based on ground-truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
