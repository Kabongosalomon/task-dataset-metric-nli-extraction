<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Chen</surname></persName>
							<email>chenyuxin2019@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Deng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Aeronautical Manufacturing Engineering</orgName>
								<orgName type="institution">Nanchang Hangkong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<email>wmhu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. In GCNs, graph topology dominates feature aggregation and therefore is the key to extracting representative features. In this work, we propose a novel Channel-wise Topology Refinement Graph Convolution (CTR-GC) to dynamically learn different topologies and effectively aggregate joint features in different channels for skeleton-based action recognition. The proposed CTR-GC models channel-wise topologies through learning a shared topology as a generic prior for all channels and refining it with channel-specific correlations for each channel.</p><p>Our refinement method introduces few extra parameters and significantly reduces the difficulty of modeling channel-wise topologies. Furthermore, via reformulating graph convolutions into a unified form, we find that CTR-GC relaxes strict constraints of graph convolutions, leading to stronger representation capability. Combining CTR-GC with temporal modeling modules, we develop a powerful graph convolutional network named CTR-GCN which notably outperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets. 1 * Corresponding author. 1 https://github.com/Uason-Chen/CTR-GCN. arXiv:2107.12213v2 [cs.CV] 23 Aug 2021 are learned independently and the model becomes too heavy when setting channel-wise parameterized topologies, which increases the difficulty of optimization and hinders effective modeling of channel-wise topologies. Moreover, parameterized topologies remain the same for all samples, which is unable to model sample-dependent correlations.</p><p>In this paper, we propose a channel-wise topology refinement graph convolution which models channel-wise topology dynamically and effectively. Instead of learning topologies of different channels independently, CTR-GC learns channel-wise topologies in a refinement way. Specifically, CTR-GC learns a shared topology and channel-specific correlations simultaneously. The shared topology is a parameterized adjacency matrix that serves as topological priors for all channels and provides generic correlations between vertices. The channel-specific correlations are dynamically inferred for each sample and they capture subtle relationships between vertices within each channel. By refining the shared topology with channel-specific correlations, CTR-GC obtains channel-wise topologies (illustrated in <ref type="figure">Figure  1</ref>). Our refinement method avoids modeling the topology of each channel independently and introduces few extra parameters, which significantly reduces the difficulty of modeling channel-wise topologies. Moreover, through reformulating four categories of graph convolutions into a unified form, we verify the proposed CTR-GC essentially relaxes strict constraints of other categories of graph convolutions and improves the representation capability.</p><p>Combining CTR-GC with temporal modeling modules, we construct a powerful graph convolutional network named CTR-GCN for skeleton-based action recognition. Extensive experimental results on NTU RGB+D, NTU RGB+D 120, and NW-UCLA show that (1) our CTR-GC significantly outperforms other graph convolutions proposed for skeleton-based action recognition with comparable parameters and computation cost; (2) Our CTR-GCN exceeds state-of-the-art methods notably on all three datasets.</p><p>Our contributions are summarized as follows:</p><p>? We propose a channel-wise topology refinement graph convolution which dynamically models channel-wise topologies in a refinement approach, leading to flexible and effective correlation modeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition is an important task with various applications ranging from human-robot interaction to video surveillance. In recent years, skeleton-based human action recognition has attracted much attention due to the development of depth sensors and its robustness against complicated backgrounds. Early deep-learning-based methods treat human joints as a set of independent features and organize them into a feature sequence or a pseudo-image, which is fed into RNNs or CNNs to predict action labels. However, these methods overlook inherent correlations between joints, which reveals human body topology and is important information of human skeleton. Yan et al. <ref type="bibr" target="#b31">[32]</ref> firstly modeled correlations between human joints with graphs and apply GCNs along with temporal convolutions to extract motion features. While the manually defined topology they employ is difficult to achieve relationship modeling between unnaturally connected joints and limits representation capability of GCNs. In order to boost power of GCNs, recent approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref> adaptively learn the topology of human skeleton through attention or other mechanisms. They use a topology for all channels, which forces GCNs to aggregate features with the same topology in different channels and thus limits the flexibility of feature extraction. Since different channels represent different types of motion features and correlations between joints under different motion features are not always the same, it's not optimal to use one shared topology. Cheng et al. <ref type="bibr" target="#b2">[3]</ref> set individual parameterized topologies for channel groups. However, the topologies of different groups ? We mathematically unify the form of existing graph convolutions in skeleton-based action recognition and find that CTR-GC relaxes constraints of other graph convolutions, providing more powerful graph modeling capability.</p><p>? The extensive experimental results highlight the benefits of channel-wise topology and the refinement method. The proposed CTR-GCN outperforms state-of-the-art methods significantly on three skeletonbased action recognition benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Convolutional Networks</head><p>Convolutional Neural Networks (CNNs) have achieved remarkable results in processing Euclidean data like images. To process non-Euclidean data like graphs, there is an increasing interest in developing Graph Convolutional Networks (GCNs). GCNs are often categorized as spectral methods and spatial methods. Spectral methods conduct convolution on spectral domain <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref>. However, they depend on the Laplacian eigenbasis which is related to graph structure and thus can only be applied to graphs with same structure. Spatial methods define convolutions directly on the graph <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. One of the challenges of spatial methods is to handle different sized neighborhoods. Among different GCN variants, the GCN proposed by Kipf et al. <ref type="bibr" target="#b10">[11]</ref> is widely adapted to various tasks due to its simplicity. The feature update rule in <ref type="bibr" target="#b10">[11]</ref> consists of two steps: (1) Transform features into high-level representations; and (2) Aggregate features according to graph topology. Our work adopts the same feature update rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">GCN-based Skeleton Action Recognition</head><p>GCNs have been successfully adopted to skeleton-based action recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b26">27]</ref> and most of them follow the feature update rule of <ref type="bibr" target="#b10">[11]</ref>. Due to the importance of topology (namely vertex connection relationship) in GCN, many GCN-based methods focus on topology modeling. According to the difference of topology, GCNbased methods can be categorized as follows: (1) According to whether the topology is dynamically adjusted during inference, GCN-based methods can be classified into static methods and dynamic methods. (2) According to whether the topology is shared across different channels, GCN-based methods can be classified into topology-shared methods and topology-non-shared methods. Static / Dynamic Methods. For static methods, the topologies of GCNs keep fixed during inference. Yan et al. <ref type="bibr" target="#b31">[32]</ref> proposed an ST-GCN which predefines topology according to human body structure and the topology is fixed in both training and testing phase. Liu et al. <ref type="bibr" target="#b19">[20]</ref> and Huang et al. <ref type="bibr" target="#b8">[9]</ref> introduced multi-scale graph topologies to GCNs to enable multi-range joint relationship modeling. For dynamic methods, the topologies of GCNs are dynamically inferred during inference. Li et al. <ref type="bibr" target="#b14">[15]</ref> proposed an Alinks inference module to capture action-specific correlations. Shi et al. <ref type="bibr" target="#b23">[24]</ref> and Zhang et al. <ref type="bibr" target="#b34">[35]</ref> enhanced topology learning with self-attention mechanism, which models correlation between two joints given corresponding features. These methods infer correlations between two joints <ref type="figure">Figure 2</ref>. Framework of the proposed channel-wise topology refinement graph convolution. The channel-wise topology modeling refines the trainable shared topology with inferred channel-specific correlations. The feature transformation aims at transforming input features into high-level representations. Eventually, the output feature is obtained by channel-wise aggregation. with local features. Ye et al. <ref type="bibr" target="#b33">[34]</ref> proposed a Dynamic GCN, where contextual features of all joints are incorporated to learn correlations between any pairs of joints. Compared with static methods, dynamic methods have stronger generalization ability due to dynamic topologies.</p><p>Topology-shared / Topology-non-shared Methods. For topology-shared methods, the static or dynamic topologies are shared in all channels. These methods force GCNs to aggregate features in different channels with the same topology, limiting the upper bound of model performance. Most GCN-based methods follow topology-shared manner, including aforementioned static methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref> and dynamic methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Topology-non-shared methods use different topologies in different channels or channel groups, which naturally overcome limitations of topology-shared methods. Cheng et al. <ref type="bibr" target="#b2">[3]</ref> proposed a DC-GCN which sets individual parameterized topologies for different channel groups. However, the DC-GCN faces difficulty of optimization caused by excessive parameters when setting channel-wise topologies. To our best knowledge, topology-non-shared graph convolutions are rarely explored in the skeleton-based action recognition, and this work is the first to model dynamic channel-wise topologies. Note that our method also belongs to dynamic methods because topologies are dynamically inferred during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first define related notations and formulate conventional graph convolution. Then we elaborate our Channel-wise Topology Refinement Graph Convolution (CTR-GC) and mathematically analyze the representation capability of CTR-GC and other graph convolutions. Finally, we introduce the structure of our CTR-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Notations. A human skeleton is represented as a graph with joints as vertices and bones as edges. The graph is denoted</p><formula xml:id="formula_0">as G = (V, E, X ), where V = {v 1 , v 2 , ......, v N } is the set of N vertices.</formula><p>E is the edge set, which is formulated as an adjacency matrix A ? R N ?N and its element a ij reflects the correlation strength between v i and v j . The neighborhood of v i is represented as N (v i ) = {v j |a ij = 0}. X is the feature set of N vertices, which is represented as a matrix X ? R N ?C and v i 's feature is represented as</p><formula xml:id="formula_1">x i ? R C . Topology-shared Graph Convolution.</formula><p>The normal topology-shared graph convolution utilizes the weight W for feature transformation and aggregate representations of v i 's neighbor vertices through a ij to update its representation z i , which is formulated as</p><formula xml:id="formula_2">z i = vj ?N (vi) a ij x j W<label>(1)</label></formula><p>For static methods, a ij is defined manually or set as trainable parameter. For dynamic methods, a ij is usually generated by the model depending on the input sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Channel-wise Topology Refinement Graph Convolution</head><p>The general framework of our CTR-GC is shown in <ref type="figure">Figure</ref> 2. We first transform input features into high-level features, then dynamically infer channel-wise topologies to capture pairwise correlations between input sample's joints under different types of motion features, and aggregate features in each channel with corresponding topology to get the final output. Specifically, our CTR-GC contains three parts: (1) Feature transformation which is done by transformation function T (?); (2) Channel-wise topology modeling which consists of correlation modeling function M(?) and refinement function R(?); (3) Channel-wise aggregation which is completed by aggregation function A(?). Given the input feature X ? R N ?C , the output Z ? R N ?C of CTR-GC is formulated as</p><formula xml:id="formula_3">Z = A T (X), R(M(X), A) ,<label>(2)</label></formula><p>where A ? R N ?N is the learnable shared topology. Next, we introduce these three parts in detailed. Feature Transformation. As shown in the orange block in <ref type="figure">Figure 2</ref>, feature transformation aims at transforming input features into high-level representations via T (?). We adopt a simple linear transformation here as the topology-shared graph convolution, which is formulated as</p><formula xml:id="formula_4">X = T (X) = XW,<label>(3)</label></formula><p>where X ? R N ?C is the transformed feature and W ? R C?C is the weight matrix. Note that other transformations can also be used, e.g., multi-layer perceptron. Channel-wise Topology Modeling. The channel-wise topology modeling is shown in the blue block in <ref type="figure">Figure</ref> 2. The adjacency matrix is used as shared topology for all channels and is learned through backpropagation. Moreover, we learn channel-specific correlations Q ? R N ?N ?C to model specific relationships between vertices in C channels. Then the channel-wise topologies R ? R N ?N ?C are obtained by refining the shared topology A with Q.</p><p>Specifically, we first employ correlation modeling function M(?) to model channel-wise correlations between vertices. To reduce computation cost, we utilize linear transformations ? and ? to reduce feature dimension before sending input features into M(?). Given a pair of vertices (v i , v j ) and their corresponding features (x i , x j ), we design two simple yet effective correlation modeling functions. The first correlation modeling function M 1 (?) is formulated as</p><formula xml:id="formula_5">M 1 (?(x i ), ?(x j )) = ?(?(x i ) ? ?(x j )),<label>(4)</label></formula><p>where ?(?) is activation function. M 1 (?) essentially calculates distances between ?(x i ) and ?(x j ) along channel dimension and utilizes the nonlinear transformations of these distances as channel-specific topological relationship between v i and v j . The second correlation modeling function M 2 (?) is formulated as</p><formula xml:id="formula_6">M 2 (?(x i ), ?(x j )) = M LP (?(x i )||?(x j )),<label>(5)</label></formula><p>where || is concatenate operation and MLP is multi-layer perceptron. We utilize MLP here due to its powerful fitting capability.</p><p>Based on the correlation modeling function, the channelspecific correlations Q ? R N ?N ?C are obtained by employing linear transformation ? to raise the channel dimension, which is formulated as</p><formula xml:id="formula_7">q ij = ? M ?(x i ), ?(x j ) , i, j ? {1, 2, ? ? ? , N },<label>(6)</label></formula><p>where q ij ? R C is a vector in Q and reflects the channelspecific topological relationship between v i and v j . Note that Q is not forced to be symmetric, i.e., q ij = q ji , which increases the flexibility of correlation modeling.</p><p>Eventually, the channel-wise topologies R ? R N ?N ?C are obtained by refining the shared topology A with channel-specific correlations Q:</p><formula xml:id="formula_8">R = R(Q, A) = A + ? ? Q,<label>(7)</label></formula><p>where ? is a trainable scalar to adjust the intensity of refinement. The addition is conducted in a broadcast way where A is added to each channel of ? ? Q.</p><p>Channel-wise Aggregation. Given the refined channelwise topologies R and high-level features X, CTR-GC aggregates features in a channel-wise manner. Specifically, CTR-GC constructs a channel-graph for each channel with corresponding refined topology R c ? R N ?N and featur? x :,c ? R N ?1 , where R c andx :,c are respectively from c-th channel of R c and X (c ? {1, ? ? ? , C }). Each channelgraph reflects relationships of vertices under a certain type of motion feature. Consequently, feature aggregation is performed on each channel-graph, and the final output Z is obtained by concatenating the output features of all channelgraphs, which is formulated as</p><formula xml:id="formula_9">Z = A( X, R) = [R 1x:,1 ||R 2x:,2 || ? ? ? ||R C x :,C ],<label>(8)</label></formula><p>where || is concatenate operation. During the whole process, the inference of channel-specific correlations Q relies on input samples as shown in Equation <ref type="bibr" target="#b5">6</ref>. Therefore, the proposed CTR-GC is a dynamic graph convolution and it adaptively varies with different input samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis of Graph Convolutions</head><p>We analyze the representation capability of different graph convolutions by reformulating them into a unified form and comparing them with dynamic convolution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref> employed in CNNs.</p><p>We first recall dynamic convolution which enhances vanilla convolution with dynamic weights. In dynamic convolution, each neighbor pixel p j of the center pixel p i has a corresponding weight in the convolution kernel, and the weight can be dynamically adjusted according to different input samples, which makes the dynamic convolution have strong representation ability. The dynamic convolution can be formulated as</p><formula xml:id="formula_10">z k i = pj ?N (pi) x k j W k j ,<label>(9)</label></formula><p>where k indicates the index of input sample. x k j and z k i are the input feature of p j and the output feature of p i of the k-th sample. W k j is the dynamic weight. Due to the irregular structure of the graph, the correspondence between neighbor vertices and weights is difficult to establish. Thus, graph convolutions (GCs) degrade convolution weights into adjacency weights (i.e., topology) and weights shared in the neighborhood. However, sharing weights in the neighborhood limits representation capability of GCs. To analyze the gap of representation ability between different GCs and dynamic convolution, we integrate adjacency weights and weights shared in the neighborhood into a generalized weight matrix E k ij . Namely, we formulate all GCs in the form of</p><formula xml:id="formula_11">z k i = vj ?N (vi) x k j E k ij where E k ij is generalized weight.</formula><p>We classified GCs into four categories as mentioned before. Static Topology-shared GCs. In static topology-shared GCs, the topologies keep fixed for different samples and are shared across all channels, which can be formulated as</p><formula xml:id="formula_12">z k i = vj ?N (vi) a ij x k j W = vj ?N (vi) x k j (a ij W),<label>(10)</label></formula><p>where a ij W is the generalized weight of static topologyshared GC. From Equation 9 and 10, it can be seen that the difference between dynamic convolution and static topology-shared GC lies in their (generalized) weights. Specifically, the weights of dynamic convolution W k j is individual for each j and k, while generalized weights of static topology-shared GC is subject to following constraints:</p><formula xml:id="formula_13">Constraint 1: E k1</formula><p>ij and E k2 ij are forced to be same. Constraint 2: E k ij1 and E k ij2 differ by a scaling factor. Note that k 1 , k 2 are different sample indices and j 1 , j 2 are different neighbor vertex indices. These constraints cause the gap of representation ability between static topology-shared GCs and dynamic convolutions. Note that we concentrate on the neighborhood rooted at v i and do not consider the change of v i for simplicity. Dynamic topology-shared GCs. Compared with static topology-shared GCs, the dynamic ones infer topologies dynamically and thus have better generalization ability. The formulation of dynamic topology-shared GCs is</p><formula xml:id="formula_14">z k i = vj ?N (vi) a k ij x k j W = vj ?N (vi) x k j (a k ij W),<label>(11)</label></formula><p>where a k ij is dynamic topological relationship between v i , v j and depends on input sample. It can be seen that the generalized weights of dynamic topology-shared GCs still suffer from Constraint 2 but relax Constraint 1 into the following constraint:</p><formula xml:id="formula_15">Constraint 3: E k1</formula><p>ij , E k2 ij differ by a scaling factor. Static topology-non-shared GCs. This kind of GCs utilize different topologies for different channels (groups). Here we just analyze static GCs with channel-wise topologies because it is the most generalized form of static topologynon-shared GCs and can degenerate into others, e.g., static group-wise-topology GCs. The specific formulation is</p><formula xml:id="formula_16">z k i = vj ?N (vi) p ij (x k j W)<label>(12)</label></formula><p>= vj ?N (vi) where is element-wise multiplication and p ij ? R C is channel-wise topological relationship between v i , v j . p ijc is the c-th element of p ij . w :,c is the c-th column of W. (We omit the derivation of Equation 12 and 13 for clarity. The details can be found in the supplementary materials.) From Equation <ref type="bibr" target="#b12">13</ref>, we observe that generalized weights of this kind of GCs suffer from Constraint 1 due to static topology but relax Constraint 2 into the following constraint: Constraint 4: Different corresponding columns of E k ij1 and E k ij2 differ by different scaling factors. Dynamic topology-non-shared GCs. The only difference between static topology-non-shared GCs and dynamic topology-non-shared GCs is that dynamic topology-nonshared GCs infers non-shared topologies dynamically, thus dynamic topology-non-shared GCs can be formulated as</p><formula xml:id="formula_17">x k j [p ij1 w :,1 , ? ? ? , p ijC w :,C ] ,<label>(13)</label></formula><formula xml:id="formula_18">z k i = vj ?N (vi) x k j [r k ij1 w :,1 , ? ? ? , r k ijC w :,C ] ,<label>(14)</label></formula><p>where r k ijc is the k-th sample's dynamic topological relationship between v i , v j in the c-th channel. Obviously, generalized weights of dynamic topology-non-shared graph convolution relax both Constraint 1 and 2. Specifically, it relaxes Constraint 2 into Constraint 4 and relaxes Constraint 1 into the following constraint: Constraint 5: Different corresponding columns of E k1 ij and E k2 ij differ by different scaling factors.</p><p>We conclude different categories of graph convolutions and their constraints in <ref type="table" target="#tab_0">Table 1</ref>. It can be seen that dynamic topology-non-shared GC is the least constrained. Our CTR-GC belongs to dynamic topology-non-shared GC and Equation 8 can be reformulated to <ref type="bibr">Equation 14</ref>, indicating that theoretically CTR-GC has stronger representation capability than previous graph convolutions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. The specific reformulation is shown in supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Architecture</head><p>Based on CTR-GC, we construct a powerful graph convolutional network CTR-GCN for skeleton-based action recognition. We set the neighborhood of each joint as the entire human skeleton graph, which is proved to be more effective in this task by previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>. The entire network consists of ten basic blocks, followed by a global average pooling and a softmax classifier to predict action labels. The number of channels for ten blocks are 64-64-64-64-128-128-128-256-256-256. Temporal dimension is halved at the 5-th and 8-th blocks by strided temporal convolution. The basic block of our CTR-GCN is shown in <ref type="figure" target="#fig_1">Figure 3</ref> (a). Each block mainly consists of a spatial modeling module, a temporal modeling module and residual connections.</p><p>Spatial Modeling. In a spatial modeling module, we use three CTR-GCs in parallel to extract correlations between human joints and sum up their results as output. For clarity, an instance of CTR-GC with M 1 (?) is illustrated in <ref type="figure" target="#fig_1">Figure 3 (b)</ref>. Our CTR-GC is designed to extract features of a graph with input feature X ? R N ?C . To adopt CTR-GC to a skeleton graph sequence S ? R T ?N ?C , we pool S along temporal dimension and use pooled features to infer channel-wise topologies. Specifically, CTR-GC first utilizes ? and ? with reduction rate r to extract compact representations. Then temporal pooling is used to aggregate temporal features. After that, CTR-GC conducts pairwise subtraction and activation following Equation 4. The channel dimension of activation is then raised with ? to obtain channel-specific correlations, which are used to refine the shared topology A to obtain channel-wise topologies. Eventually, channel-wise aggregation (implemented by batch matrix multiplication) is conducted in each skeleton graph to obtain the output representation S o .</p><p>Temporal Modeling. To model actions with different duration, we design a multi-scale temporal modeling module following <ref type="bibr" target="#b19">[20]</ref>. The main difference is that we use fewer branches for that too many branches slow down inference speed. As shown in <ref type="figure" target="#fig_1">Figure 3</ref> (a), this module contains four branches, each containing a 1 ? 1 convolution to reduce channel dimension. The first three branches contain two temporal convolutions with different dilations and one Max-Pool respectively following 1 ? 1 convolution. The results of four branches are concatenated to obtain the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>NTU RGB+D. NTU RGB+D <ref type="bibr" target="#b21">[22]</ref> is a large-scale human action recognition dataset containing 56,880 skeleton action sequences. The action samples are performed by 40 volunteers and categorized into 60 classes. Each sample contains an action and is guaranteed to have at most 2 subjects, which is captured by three Microsoft Kinect v2 cameras from different views concurrently. The authors of this dataset recommend two benchmarks: (1) cross-subject (Xsub): training data comes from 20 subjects, and testing data comes from the other 20 subjects. (2) cross-view (X-view): training data comes from camera views 2 and 3, and testing data comes from camera view 1. NTU RGB+D 120. NTU RGB+D 120 <ref type="bibr" target="#b16">[17]</ref> is currently the largest dataset with 3D joints annotations for human action recognition, which extends NTU RGB+D with additional 57,367 skeleton sequences over 60 extra action classes. Totally 113,945 samples over 120 classes are performed by 106 volunteers, captured with three cameras views. This dataset contains 32 setups, each denoting a specific location and background. The authors of this dataset recommend two benchmarks: (1) cross-subject (X-sub): training data comes from 53 subjects, and testing data comes from the other 53 subjects. (2) cross-setup (X-setup): training data comes from samples with even setup IDs, and testing data comes from samples with odd setup IDs. Northwestern-UCLA. Northwestern-UCLA dataset <ref type="bibr" target="#b30">[31]</ref> is captured by three Kinect cameras simultaneously from multiple viewpoints. It contains 1494 video clips covering 10 action categories. Each action is performed by 10 different subjects. We follow the same evaluation protocol in <ref type="bibr" target="#b30">[31]</ref>: training data from the first two cameras, and testing data from the other camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>All experiments are conducted on one RTX 2080 TI GPU with the PyTorch deep learning framework. Our models are trained with SGD with momentum 0.9, weight decay 0.0004. The training epoch is set to 65 and a warmup strategy <ref type="bibr" target="#b7">[8]</ref> is used in the first 5 epochs to make the training procedure more stable. Learning rate is set to 0.1 and decays with a factor 0.1 at epoch 35 and 55. For NTU RGB+D and NTU RGB+D 120, the batch size is 64, each sample is resized to 64 frames, and we adopt the data pre-processing in <ref type="bibr" target="#b34">[35]</ref>. For Northwestern-UCLA, the batch size is 16, and we adopt the data pre-processing in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we analyze the proposed channel-wise topology refinement graph convolution and its configuration on the X-sub benchmark of the NTU RGB+D 120 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Param.</head><p>Acc  Effectiveness of CTR-GC. We employ ST-GCN <ref type="bibr" target="#b31">[32]</ref> as the baseline, which belongs to static topology-shared graph convolution and the topology is untrainable. We further add residual connections in ST-GCN as our basic block and replace its temporal convolution with temporal modeling module described in Section 3.4 for fair comparison.</p><p>The experimental results are shown in <ref type="table">Table 2</ref>. First, we gradually replace GCs with CTR-GCs (shown in <ref type="figure" target="#fig_1">Figure 3 (b)</ref> and r = 8) in the baseline. We observe that accuracies increase steadily and the accuracy is substantially improved when all GCs are replaced by CTR-GCs (CTR-GCN), which validates the effectiveness of CTR-GC.</p><p>Then we validate effects of the shared topology A and the channel-specific correlations Q respectively by removing either of them from CTR-GCN. CTR-GCN w/o Q shares a trainable topology across different channels. We observe that its performance drops 1.2% compared with CTR-GCN, indicating the importance of modeling channel-wise topologies. The performance of CTR-GCN w/o A drops 0.9%, confirming that it's hard to model individual topology for each channel directly and topology refinement provides an effective approach to solve this problem. Configuration Exploration. We explore different configurations of CTR-GC, including the choice of correlation modeling functions M, the reduction rate r of ? and ?, activation function ? of correlation modeling function. As shown in <ref type="table" target="#tab_2">Table 3</ref>, we observe that models under all configurations outperform the baseline, confirming the robustness of CTR-GC. (1) Comparing models A, B and C, we find models with different correlation modeling functions all achieve good performance, which indicates that channel- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Other GCs</head><p>In order to validate the effectiveness of our CTR-GC, we compare performance, parameters and computation cost of CTR-GC against other graph convolutions in <ref type="table" target="#tab_3">Table 4</ref>. Specifically, we keep the backbone of the baseline model and only replace graph convolutions for fair comparison. Note that DC-GC split channels into 16 groups and set a trainable adjacency matrix for each group, while DC-GC* set a trainable adjacency matrix for each channel. From <ref type="table" target="#tab_3">Table 4</ref>, we observe that (1) On the whole, topology-nonshared methods achieve better performance than topologyshared methods, and dynamic methods perform better than static methods, indicating the importance of modeling nonshared topologies and dynamic topologies; (2) Compared with DC-GC, DC-GC* performs worse while has much more parameters, confirming that it's not effective to model channel-wise topologies with parameterized adjacency matrices alone; (3) CTR-GC outperforms DC-GC* by 0.9%, proving that our refinement approach is effective to model channel-wise topologies. Moreover, our CTR-GC introduces little extra parameters and computation cost compared with other graph convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization of Learned Topologies</head><p>We illustrate the shared topology and refined channelwise topologies of an action sample "typing on the keyboard" in <ref type="figure" target="#fig_2">Figure 4</ref>. The values close to 0 indicate weak relationships between joints and vice versa. We observe that (1) the shared topology is different from refined channel-wise </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>NTU-RGB+D 120 X-Sub (%) X-Set (%) ST-LSTM <ref type="bibr" target="#b17">[18]</ref> 55.7 57.9 GCA-LSTM <ref type="bibr" target="#b18">[19]</ref> 61.2 63.3 RotClips+MTCNN <ref type="bibr" target="#b9">[10]</ref> 62.2 61.8 SGN <ref type="bibr" target="#b34">[35]</ref> 79.2 81.5 2s-AGCN <ref type="bibr" target="#b23">[24]</ref> 82.9 84.9 Shift-GCN <ref type="bibr" target="#b3">[4]</ref> 85.9 87.6 DC-GCN+ADG <ref type="bibr" target="#b2">[3]</ref> 86.5 88.1 MS-G3D <ref type="bibr" target="#b19">[20]</ref> 86.9 88.4 PA-ResGCN-B19 <ref type="bibr" target="#b25">[26]</ref> 87.3 88.3 Dynamic GCN <ref type="bibr" target="#b33">[34]</ref> 87.3 88.6 CTR-GCN (Bone Only) 85.7 87.5 CTR-GCN (Joint+Bone) 88.7 90.1 CTR-GCN 88.9 90.6 <ref type="table">Table 5</ref>. Classification accuracy comparison against state-of-theart methods on the NTU RGB+D 120 dataset.</p><p>topologies, indicating that our method can effectively refine the shared topology. (2) the refined channel-wise topologies are different, demonstrating that our method can learn individual topologies depending on specific motion features for different channels. (3) Some correlations are consistently strong in all channels, indicating that these joint pairs are strongly relevant in general, e.g., the correlation between left elbow and left-hand tip (blue square in the green box), and the correlation between left-hand tip and left wrist (red square in the green box). It's reasonable for "typing on the keyboard" where main motion happens on hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison with the State-of-the-Art</head><p>Many state-of-the-art methods employ a multi-stream fusion framework. We adopt same framework as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref> for fair comparison. Specifically, we fuse results of four modalities, i.e., joint, bone, joint motion, and bone motion.</p><p>We compare our models with the state-of-the-art methods on NTU RGB+D 120, NTU RGB+D and NW-UCLA in Tables 5, 6 and 7 respectively. On three datasets, our method outperforms all existing methods under nearly all evaluation benchmarks. On NTU-RGB+D 120, our model with joint-bone fusion achieves state-of-the-art performance, and our CTR-GCN outperforms current state-ofthe-art Dynamic GCN [34] by 1.6% and 2.0% on the two benchmarks respectively. Notably, our method is the first to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>NTU-RGB+D X-Sub (%) X-View (%) Ind-RNN <ref type="bibr" target="#b15">[16]</ref> 81.8 88.0 HCN <ref type="bibr" target="#b13">[14]</ref> 86.5 91.1 ST-GCN <ref type="bibr" target="#b31">[32]</ref> 81.5 88.3 2s-AGCN <ref type="bibr" target="#b23">[24]</ref> 88.5 95.1 SGN <ref type="bibr" target="#b34">[35]</ref> 89.0 94.5 AGC-LSTM <ref type="bibr" target="#b24">[25]</ref> 89.2 95.0 DGNN <ref type="bibr" target="#b22">[23]</ref> 89.9 96.1 Shift-GCN <ref type="bibr" target="#b3">[4]</ref> 90.7 96.5 DC-GCN+ADG <ref type="bibr" target="#b2">[3]</ref> 90.8 96.6 PA-ResGCN-B19 <ref type="bibr" target="#b25">[26]</ref> 90.9 96.0 DDGCN <ref type="bibr" target="#b11">[12]</ref> 91.1 97.1 Dynamic GCN <ref type="bibr" target="#b33">[34]</ref> 91.5 96.0 MS-G3D <ref type="bibr" target="#b19">[20]</ref> 91.5 96.2 CTR-GCN 92.4 96.8 <ref type="table">Table 6</ref>. Classification accuracy comparison against state-of-theart methods on the NTU RGB+D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Northwestern-UCLA Top-1 (%) Lie Group <ref type="bibr" target="#b27">[28]</ref> 74.2 Actionlet ensemble <ref type="bibr" target="#b29">[30]</ref> 76.0 HBRNN-L <ref type="bibr" target="#b5">[6]</ref> 78.5 Ensemble TS-LSTM <ref type="bibr" target="#b12">[13]</ref> 89.2 AGC-LSTM <ref type="bibr" target="#b24">[25]</ref> 93.3 Shift-GCN <ref type="bibr" target="#b3">[4]</ref> 94.6 DC-GCN+ADG <ref type="bibr" target="#b2">[3]</ref> 95.3 CTR-GCN 96.5 <ref type="table">Table 7</ref>. Classification accuracy comparison against state-of-theart methods on the Northwestern-UCLA dataset.</p><p>model channel-wise topologies dynamically which is very effective in skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a novel channel-wise topology refinement graph convolution (CTR-GC) for skeleton-based action recognition. CTR-GC learns channel-wise topologies in a refinement way which shows powerful correlation modeling capability. Both mathematical analysis and experimental results demonstrate that CTR-GC has stronger representation capability than other graph convolutions. On three datasets, the proposed CTR-GCN outperforms stateof-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials for Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition</head><p>This supplemental materials include details about formula derivation, architecture setting, more visualizations and other ablation studies. Specifically, we give the derivation from Equation 12 to 13 and from Equation 8 to 14. Then we show the detailed architecture of CTR-GCN, including input size, output size and specific hyperparameters of each block. Moreover, we visualize shared topologies and channel-specific correlations. At last, we conduct ablation studies on the effect of CTR-GC's number per block, temporal convolution, and analyze the performance of different graph convolutions on hard classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formula Derivation</head><p>We first give the derivation from Equation 12 to 13. The Equation 12 is</p><formula xml:id="formula_19">z k i = vj ?N (vi) p ij (x k j W),<label>(15)</label></formula><p>where z k i ? R 1?C is the output feature of v i and p ij ? R 1?C is the channel-wise relationship between v i and v j .</p><formula xml:id="formula_20">x k j ? R 1?C is the input feature of v j and W ? R C?C is weight matrix. The c-th element of z k i is formulated as z k ic = vj ?N (vi) p ijc (x k j W) c = vj ?N (vi) p ijc (x k j w :,c ) = vj ?N (vi) x k j (p ijc w :,c ),<label>(16)</label></formula><p>where p ijc is the c-th element of p ij . (x k j W) c ? R 1 is the c-th element of x k j W and w :,c ? R C?1 is the c-th column of W. Therefore, z k i can be formulated as</p><formula xml:id="formula_21">z k i = ? ? ? vj ?N (vi) x k j (p ij1 w :,1 ) . . . vj ?N (vi) x k j (p ijC w :,C ) ? ? ? T = vj ?N (vi) x k j ([p ij1 w :,1 , ? ? ? , p ijC w :,C ]),<label>(17)</label></formula><p>which is the same as Equation <ref type="bibr" target="#b12">13</ref>. Then we give the derivation from Equation 8 to 14. We add sample index k in Equation 8, which is formulated as</p><formula xml:id="formula_22">Z k = [R k 1x k :,1 ||R k 2x k :,2 || ? ? ? ||R k C x k :,C ].<label>(18)</label></formula><p>The c-th column of Z k ? R N ?C can be formulated as</p><formula xml:id="formula_23">z k :,c = R k cx k :,c = R k c (X k W) :,c = R k c (X k w :,c ),<label>(19)</label></formula><p>where X k ? R N ?C is the input feature. The i-th element of z k :,c , i.e., the c-th element of v i 's output feature is</p><formula xml:id="formula_24">z k ic = r k i,:,c (X k w :,c ) = vj ?N (vi) r k ijc (x k j w :,c ) = vj ?N (vi) x k j (r k ijc w :,c ),<label>(20)</label></formula><p>where x k j ([r k ij1 w :,1 , ? ? ? , r k ijC w :,C ]).  <ref type="table">Table 8</ref>. Detailed architecture of CTR-GCN. M, T, and N refer to the number of people, the length, and the number of joints of input sequences. "SM" and "TM" indicate the spatial modeling module and temporal modeling module respectively. The two numbers after SM are the input channel and output channel of SM. The three numbers after TM are the input channel, output channel and temporal stride. nc is the number of action classes.</p><formula xml:id="formula_25">r k i,:,c ? R 1?N is the i-th row of R k c ? R N ?N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Architecture</head><p>The detailed architecture of the proposed CTR-GCN is shown in <ref type="table">Table 8</ref>, CTR-GCN contains ten basic blocks and a classification layer which consists of a global average pooling, a fully connected layer and a softmax operation. M refers to the number of people in the sequences, which is set to 2, 2, and 1 for NTU RGB+D, NTU RGB+D 120, and NW-UCLA respectively. In a sequence, M skeleton sequences are processed independently by ten basic blocks and are average pooled by the classification layer to obtain the final score. T and N refer to the length and the number of joints of input skeleton sequences, which are {64, 25}, {64, 25} and {52, 20} for NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA respectively. C is the basic channel number which is set to 64 for CTR-GCN. "SM" and "TM" indicate the spatial modeling module and temporal modeling module respectively. The two numbers after SM are the input channel and output channel of SM. The three numbers after TM are the input channel, output channel and temporal stride. At the Basic Blocks 5 and 8, the strides of convolutions in temporal modeling module (TM) are set to 2 to reduce the temporal dimension by half. n c is the number of action classes, which is 60, 120, 10 for NTU-RGB+D, NTU-RGB+D120, and NW-UCLA respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we visualize the shared topologies and channel-specific correlations of our CTR-GCN. The input sample belongs to "typing on a keyboard". It can be seen that (1) the shared topologies in three layers tend to be coarse and dense, which captures global features for recognizing actions; (2) the channel-specific correlations varies with different channels, indicating that our CTR-GCN models individual joints relationships under different types of motion features; (3) most channel-specific correlations focus on two hands, which capture subtle inter-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number</head><p>Param. Acc (%) 3(CTR-GCN) <ref type="bibr" target="#b0">1</ref>   actions on hands and are helpful for recognizing "typing on a keyboard".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Channel-wise topology refinement. Lines of different colors correspond to topologies in different channels and the thickness of lines indicates the correlation strength between joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(a) The basic block of our CTR-GCN. (b)CTR-GC with correlation modeling function M1(?) or M2(?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>(a) The shared topology. (b) and (c) The refined channelwise topologies of different channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of the shared topologies and channelspecific correlations. The green lines show the natural connections of human skeleton. The intensity of red lines indicates the connection strength of correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of classification accuracy of different graph convolutions on hard action classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Constraints on different categories of graph convolutions and corresponding instances. The number 1-5 correspond to five constraints. Red, Green and Blue respectively indicate the relatively High, Mid and Low constraint strength.</figDesc><table><row><cell>Topology Non-shared Dynamic 1 2 3 4 5 Constraints</cell><cell>Instance</cell></row><row><cell></cell><cell>ST-GC[32]</cell></row><row><cell></cell><cell>AGC [24], Dy-GC[34]</cell></row><row><cell></cell><cell>DC-GC [3]</cell></row><row><cell></cell><cell>CTR-GC (ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of the validation accuracy of CTR-GC with different settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of CTR-GC with other graph convolutions. The first two columns show the categories of graph convolutions.</figDesc><table><row><cell>Topology Non-share Dynamic</cell><cell>Methods</cell><cell cols="2">Param. FLOPs Acc (%)</cell></row><row><cell></cell><cell>ST-GC [32]</cell><cell>1.22M?1.65G</cell><cell>83.4</cell></row><row><cell></cell><cell>AGC [24]</cell><cell>1.55M?2.11G</cell><cell>83.9</cell></row><row><cell></cell><cell cols="2">Dy-GC [34] 1.73M?1.66G</cell><cell>83.9</cell></row><row><cell></cell><cell>DC-GC [3]</cell><cell>1.51M?1.65G</cell><cell>84.2</cell></row><row><cell></cell><cell>DC-GC*[3]</cell><cell>3.37M?1.65G</cell><cell>84.0</cell></row><row><cell></cell><cell>CTR-GC</cell><cell>1.46M?1.97G</cell><cell>84.9</cell></row><row><cell cols="4">wise topology refinement is a generic idea and is compatible</cell></row><row><cell cols="4">with many different correlation modeling functions (M + 1</cell></row><row><cell cols="4">replaces the subtraction in M 1 with addition). (2) Compar-</cell></row><row><cell cols="4">ing models B, D and E, we find models with r = 4, 8 (mod-</cell></row><row><cell cols="4">els B, D) achieve better results and the model with r = 8</cell></row><row><cell cols="4">(model B) performs better slightly with fewer parameters.</cell></row><row><cell cols="4">Model E with r = 16 performs worse because too few chan-</cell></row><row><cell cols="4">nels are used in correlation modeling function, which is not</cell></row><row><cell cols="4">sufficient to effectively model channel-specific correlations.</cell></row><row><cell cols="4">(3) Comparing models B, F and G, Sigmoid and ReLU per-</cell></row><row><cell cols="4">form worse than Tanh and we argue that non-negative out-</cell></row><row><cell cols="4">put values of Sigmoid and ReLU constrains the flexibility</cell></row><row><cell cols="4">of correlation modeling. Considering performance and effi-</cell></row><row><cell cols="3">ciency, we choose model B as our final model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>It can be seen that Equation 21 is the same as Equation<ref type="bibr" target="#b13">14</ref>, i.e., Equation 8 can be reformulated toEquation 14.</figDesc><table><row><cell>Layers</cell><cell>Output Sizes</cell><cell></cell><cell>Hyperparameters</cell><cell></cell></row><row><cell>Basic Block 1</cell><cell>M?T?N</cell><cell></cell><cell>SM: 3, C TM: C, C, 1</cell><cell></cell></row><row><cell>Basic Block 2</cell><cell>M?T?N</cell><cell></cell><cell>SM: C, C TM: C, C, 1</cell><cell></cell></row><row><cell>Basic Block 3</cell><cell>M?T?N</cell><cell></cell><cell>SM: C, C TM: C, C, 1</cell><cell></cell></row><row><cell>Basic Block 4</cell><cell>M?T?N</cell><cell></cell><cell>SM: C, C TM: C, C, 1</cell><cell></cell></row><row><cell>Basic Block 5</cell><cell>M? T 2 ?N</cell><cell></cell><cell>SM: C, 2C TM: 2C, 2C, 2</cell><cell></cell></row><row><cell>Basic Block 6</cell><cell>M? T 2 ?N</cell><cell></cell><cell>SM: 2C, 2C TM: 2C, 2C, 1</cell><cell></cell></row><row><cell>Basic Block 7</cell><cell>M? T 2 ?N</cell><cell></cell><cell>SM: 2C, 2C TM: 2C, 2C, 1</cell><cell></cell></row><row><cell>Basic Block 8</cell><cell>M? T 4 ?N</cell><cell></cell><cell>SM: 2C, 4C TM: 4C, 4C, 2</cell><cell></cell></row><row><cell>Basic Block 9</cell><cell>M? T 4 ?N</cell><cell></cell><cell>SM: 4C, 4C TM: 4C, 4C, 1</cell><cell></cell></row><row><cell>Basic Block 10</cell><cell>M? T 4 ?N</cell><cell></cell><cell>SM: 4C, 4C TM: 4C, 4C, 1</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">? global averge pool</cell><cell>?</cell></row><row><cell>Classification</cell><cell>1?1?1</cell><cell>?</cell><cell>n c -d fc</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>softmax</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 9. Comparisons of model performances with different number of CTR-GCs.</figDesc><table><row><cell></cell><cell>.46M</cell><cell>84.9</cell></row><row><cell>1</cell><cell>0.85M</cell><cell>84.3 ?0.5</cell></row><row><cell>2</cell><cell>1.15M</cell><cell>84.7 ?0.2</cell></row><row><cell>4</cell><cell>1.76M</cell><cell>85.2 ?0.3</cell></row><row><cell>5</cell><cell>2.07M</cell><cell>85.4 ?0.5</cell></row><row><cell>6</cell><cell>2.37M</cell><cell>85.0 ?0.1</cell></row><row><cell cols="2">Temporal Modeling</cell><cell>Acc (%)</cell></row><row><cell cols="2">Temporal Conv(CTR-GCN)</cell><cell>84.9</cell></row><row><cell cols="2">Temporal Pooling</cell><cell>72.8 ?12.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .</head><label>10</label><figDesc>Comparisons of model performances with different number of CTR-GCs.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Effect of CTR-GC's number. In CTR-GCN, we use three CTR-GCs for fair comparison with other methods (e.g., AGCN, MSG3D), which mostly use three or more GCs to increase model capacity. To verify the effectiveness of CTR-GC's number to our method, We test the model with 1 6 CTR-GCs. As shown in <ref type="table">Table 9</ref>, accuracies first increase due to increased model capacity, but drops at 6 CTR-GCs, which may be caused by overfitting. Effect of temporal convolutions. It's a common practice to use (multi-scale) temporal convolutions for temporal modeling in skeleton-based action recognition. To validate the effect of temporal convolutions, we try to use global average pooling for temporal modeling. As shown in <ref type="table">Table 10</ref>, the performance drops from 84.9% to 72.8%, probably because the pooling loses too much temporal information to extract joints' trajectory features effectively. Performance on hard classes. We further analyze the performance of different graph convolutions on hard classes on NTU-RGB+D 120, i.e., "staple book", "count money", "play with phone" and "cut nails", "playing magic cube" and "open bottle". These actions mainly involve subtle in-teractions between fingers, making them difficult to be recognized correctly. As shown in <ref type="figure">Figure 6</ref>, CTR-GC outperforms other graph convolutions on all classes. Especially, CTR-GC exceeds other methods at least by 7.03% and 4.36% on "cut nails" and "open bottle" respectively, showing that, compared with other GCs, our CTR-GC can effectively extract features of subtle interactions and classify them more accurately.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decoupling gcn with dropgraph module for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatio-temporal inception graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Senjian An, Ferdous Sohel, and Farid Boussaid</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ddgcn: A dynamic directed graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Korban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="761" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoungyoon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06055</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">Lisboa</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamila</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Twostream adaptive graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1625" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naifan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="914" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07455</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="1305" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic gcn: Context-enriched topology learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanfan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian graph convolution lstm for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6882" to="6892" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
