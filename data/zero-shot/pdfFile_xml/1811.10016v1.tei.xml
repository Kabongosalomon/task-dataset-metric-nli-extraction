<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dissimilarity Coefficient based Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Arun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pawan Kumar</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">The Alan Turing Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dissimilarity Coefficient based Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of weakly supervised object detection, where the training samples are annotated using only image-level labels that indicate the presence or absence of an object category. In order to model the uncertainty in the location of the objects, we employ a dissimilarity coefficient based probabilistic learning objective. The learning objective minimizes the difference between an annotation agnostic prediction distribution and an annotation aware conditional distribution. The main computational challenge is the complex nature of the conditional distribution, which consists of terms over hundreds or thousands of variables. The complexity of the conditional distribution rules out the possibility of explicitly modeling it. Instead, we exploit the fact that deep learning frameworks rely on stochastic optimization. This allows us to use a state of the art discrete generative model that can provide annotation consistent samples from the conditional distribution. Extensive experiments on PASCAL VOC 2007 and 2012 data sets demonstrate the efficacy of our proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection requires us to localize all the instances of an object category of interest in a given image. In recent years, significant advances in speed and accuracy have been achieved by detection frameworks based on Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Most of the existing methods require a strongly supervised data set, where each image is labeled with the groundtruth bounding boxes of all the object instances. Given the high cost of obtaining such detailed annotations, researchers have recently started exploring the weakly supervised object detection (WSOD) problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. The goal of WSOD is to learn an accurate detector using training samples that are annotated with image-level labels (which indicate the presence of an object category).</p><p>Given the wide availability of image-level labels, WSOD offers a cost-effective and highly scalable learning paradigm. However, this comes at the cost of introducing uncertainty in the location of the object instances during training. For example, consider the task of detecting a car. Given a training image annotated to indicate the presence of a car, we are still faced with the challenge of identifying the bounding box for the car.</p><p>In order to effectively model uncertainty in weakly supervised learning, Kumar et al. <ref type="bibr" target="#b18">[19]</ref> proposed a probabilistic framework that models two distributions: (i) a conditional distribution, which represents the probability of an output conditioned on the given annotation during training; and (ii) a prediction distribution which represents the probability of an output at test time. The parameters of the two distributions are estimated jointly by minimizing the dissimilarity coefficient <ref type="bibr" target="#b23">[24]</ref>, which measures the distance between any two distributions using a task specific loss function.</p><p>The aforementioned dissimilarity coefficient based framework has provided promising results in domains where the conditional distribution is simple to model (that is, consists of terms that depend on a few variables at a time) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>. However, WSOD presents a more challenging scenario due to the complexity of the underlying conditional distribution. Specifically, given the hundreds or even thousands of bounding box proposals for an image, the annotation constraint imposes a term over all of these bounding box proposals such that at least one of them corresponds to the given image-level label. This leads to a challenging scenario where the distribution is not factorizable over the bounding box proposals. While previous works have approximated this uncertainty as a fully factorized distribution for computational efficiency, we argue that such a choice leads to poor accuracy.</p><p>To overcome the difficulty of a complex conditional distribution, we make the key observation that deep learning relies on stochastic optimization. Therefore, we do not need to explicitly model this complex distribution but simply estimate the distribution using samples. This observation opens the door to the use of state-of-the-art deep generative models such as the Discrete DISCO Net <ref type="bibr" target="#b3">[4]</ref>.</p><p>We test the efficacy of our approach on the challenging PASCAL VOC 2007 and 2012 data sets. To generate the weakly supervised data sets, we use the image-level labels, discarding the bounding box annotations. We achieve 53.6% detection AP on PASCAL VOC 2007 and 49.5% detection AP on PASCAL VOC 2012 data set, significantly improving the state-of-the-art by 1.5% on both data sets.</p><p>To summarize, we make the following contributions.</p><p>? Efficiently model the complex non-factorizable, annotation aware conditional distribution using the deep generative model, the Discrete DISCO Net.</p><p>? Empirically show the importance of modeling the uncertainty in the annotations in a single unified probabilistic learning objective, the dissimilarity coefficient.</p><p>? State-of-the art performance for the task of WSOD on challenging PASCAL VOC 2007 and 2012 data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conventional methods often treat WSOD as a Multiple Instance Learning (MIL) problem <ref type="bibr" target="#b8">[9]</ref> by representing each image as a bag of instances (that is, putative bounding boxes) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. The learning procedure alternates between training an object classifier and selecting the most confident positive instances. However, these methods are susceptible to poor initialization. To address this, different strategies have been developed, which aim to improve the initialization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, regularize the model with extra cues <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>, or relax the MIL constraint <ref type="bibr" target="#b36">[37]</ref> to make the objective differentiable. These hard-MIL based methods have demonstrated their effectiveness, specially when CNN features are used to represent object proposals <ref type="bibr" target="#b4">[5]</ref>. However, these models are not end to end trainable and also do not explicitly model the uncertainty.</p><p>A more interesting line of work is to integrate MIL strategy as deep networks such that they are end to end trainable <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. In their work, Bilen et al. <ref type="bibr" target="#b2">[3]</ref> proposed a smoothed version of MIL that softly labels object proposals instead of choosing the highest scoring ones. Building on this soft-MIL based approach, Diba et al. <ref type="bibr" target="#b7">[8]</ref> integrate the MIL strategy with better bounding box proposals into an end-to-end cascaded deep network. Tang et al. <ref type="bibr" target="#b31">[32]</ref> refine the prediction iteratively through multistage instance classifier. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> add curriculum learning using the MIL framework. As we shall see, our formulation brings out the curriculum learning naturally during training. Other end-to-end trainable frameworks for WSOD employ domain adaptation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>, expectationmaximization algorithm <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref> and saliency based methods <ref type="bibr" target="#b19">[20]</ref>. Although these methods are end to end trainable, they not only model a single distribution for two related tasks, but also model the complex distribution with a fully factorized one. This makes these approach sub-optimal as what we truly want is to model a distribution which enforces at least one bounding box proposals corresponding to the image-level label.</p><p>There have been attempts to further improve the performance of the weakly supervised detectors by combining them with the strongly supervised detectors. Typically, the predicted instances from a trained weakly supervised detector are treated as a pseudo-strong label to train a strongly supervised network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. However, there is only a unidirectional connection between the two detectors. In their work, Wang et al. <ref type="bibr" target="#b35">[36]</ref> train a weakly and strongly supervised model jointly, in a collaborative manner. This is similar in spirit to ours in using two distributions. However, they model their weakly supervised detector with a fully factorized distribution. The improvement in results reported by these papers advocates the importance of modeling two separate distributions. In this work, we explicitly define the two distributions employed during training and test time and jointly train them by minimizing the dissimilarity coefficient <ref type="bibr" target="#b23">[24]</ref> based objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>We denote an input image as x ? R (H?W ?3) , where H and W are the height and the width of the image respectively. For the sake of simplifying the subsequent description of our approach, we assume that we have extracted B bounding box proposals from each image. In our experiments, we use Selective Search <ref type="bibr" target="#b33">[34]</ref>. Each bounding box proposal can belong to one of C + 1 categories from the set {0, 1, . . . , C}, where category 0 is background, and categories {1, . . . , C} are object classes.</p><p>We denote an image-level label by a ? {0, 1} C , where a (j) = 1 if image x contains the j-th object. Furthermore, we denote the unknown bounding box labels by y ? {0, . . . , C} B , where y (i) = j if the i-th bounding box is of the j-th category. A weakly supervised data set W = {(x i , a i )|i = 1, . . . , N } contains N pairs of images x i and their corresponding image-level labels a i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Probabilistic Modeling</head><p>Given a weakly supervised data set W, we wish to learn an object detector that can predict the bounding box labels y of a previously unseen image. Due to the uncertainty inherent in this task, we advocate the use of a probabilistic formulation. Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, we define two distributions. The first one is the prediction distribution Pr p (y|x; ? p ), which models the probability of the bounding box labels y given an input image x. Here ? p are the parameters of the distribution. As the name suggest, this distribution is used to make the prediction at test time.</p><p>In addition to the prediction distribution, we also construct a conditional distribution Pr c (y|x, a; ? c ), which models the probability of the bounding box labels y given the input image x and its image-level annotations a. Here ? c are the parameters of the distribution. The conditional distribution contains additional information, namely the presence of foreground objects in each image. Thus, we can expect it to provide better predictions for the bounding box labels y. We will use this property during training in order to learn an accurate prediction distribution using the conditional distribution. The details on the modeling of the two distributions are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Prediction Distribution</head><p>The task of the prediction distribution is to accurately model the probability of the bounding box labels given the input image. Taking inspiration from the supervised models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>, we assume independence between the probability of the output for each bounding box proposal. Therefore, the overall distribution for an image equals the product of the probabilities of each proposal,</p><formula xml:id="formula_0">Pr p (y|x; ? p ) = B i=1</formula><p>Pr p (y (i) |x; ? p ).</p><p>(1)</p><p>We model this distribution using the Fast-RCNN architecture <ref type="bibr" target="#b11">[12]</ref> (see <ref type="figure">Figure 1</ref>(a)). As the prediction distribution is specified by a neural network, we henceforth refer to it as the prediction net. In this setting, the parameters of the distribution ? p are the weights of the prediction net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Conditional Distribution</head><p>Given B bounding box proposals for an image x and the image-level label a, the conditional distribution Pr c (y|x, a; ? c ) models the probability of bounding box labels y under the constraint that they are compatible with the annotation a. Specifically, there exists at least one bounding box i such that y (i) = j, for every positive image-level label a (j) = 1.</p><p>Note that due to the requirement that the bounding box labels y are compatible with the annotation a, the conditional distribution cannot be trivially decomposed over bounding box proposals. This is in stark contrast to the simple prediction net, which uses a fully factorized distribution. If one were to explicitly model the conditional distribution, then one would be required to compute its partition function during training, which would be prohibitively expensive. To alleviate this computational challenge, we make a key observation that in practice we only need access to a representative set of samples from the conditional distribution. This opens the door to the use of the recently proposed Discrete DISCO Net <ref type="bibr" target="#b3">[4]</ref>. In what follows, we briefly describe Discrete DISCO Nets while highlighting their applicability to our framework.</p><p>Discrete DISCO Net: Discrete DISCO Net <ref type="bibr" target="#b3">[4]</ref> is a deep probabilistic framework that implicitly represents a probability distribution over a discrete structured output space. The strength of the framework lies in the fact that it allows us to adapt a pointwise deep network (a network that provides a single pointwise prediction) to a probabilistic one by the introduction of noise.</p><p>In the context of our setting, consider the modified Fast-RCNN network in <ref type="figure">Figure 1</ref>(b) for the conditional distribution. Once again, as we are using a neural network, we will henceforth refer to it as the conditional net. The parameters of the conditional distribution ? c are the weights of the conditional net. The colored filters in the middle of the network represent the noise that is sampled from a uniform distribution. Each value of the noise filter z k results in a different score function 1 G k (y; ? c ) ? R B?C . Note that we generate K different score functions using K different noise samples. These score functions are then used to sample corresponding bounding box labels? k c such that all ground truth labels are present in it. This enables us to generate samples from the underlying distribution encoded by the network parameters. Note that obtaining a single sample is as efficient as a simple forward pass through the network. By placing the filters sufficiently far away from the output layer of the network, we can learn a highly non-linear mapping from the uniform distribution (used to generate the noise filter) to the output distribution (used to generate bounding box labels).</p><p>Inference: For the input pair (x, z k ), the classification branch of the conditional net outputs a score function G k (y; ? c ), which is a B ? C matrix. The (i, j)-th element of the matrix, denoted by G (i,j) k , denotes the score of the bounding box i belonging to the category j. We will now redefine this score function such that it respects the constraints imposed by the annotation a. In other words, for each category j such that a (j) = 1 there must exist at least one bounding box i in y such that y (i) = j. The joint score for all the bounding box labels y is given by,</p><formula xml:id="formula_1">S k (y; ? c ) = B i=1 G k (y (i) ; ? c ) ? H k (y),<label>(2)</label></formula><p>where, Note that in equation <ref type="formula">(4)</ref> the arg max needs to be computed over the entire output space Y. A na?ve brute force algorithm for this would be computationally infeasible. However, by using the structure of the higher order term H k , we can design an efficient yet exact algorithm for equation <ref type="bibr" target="#b3">(4)</ref>. Specifically, we assign each bounding box proposal i to its maximum scoring object class. If all the ground truth annotations a are not present in the generated bounding box labels, then we sample the bounding box which has the highest score corresponding to the foreground label, otherwise we sample all bounding boxes which satisfies the constraint.</p><formula xml:id="formula_2">H k (y) = ? ? ? ? ? 0 if ?j ? {1, . . . , C} s.t. a (j) = 1, ?i ? {1, . . . , B} s.t. y (i) = j, ? otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Objective</head><p>In order to estimate the parameters of the prediction and conditional distribution, ? p and ? c , we define a unified probabilistic learning objective based on the dissimilarity coefficient <ref type="bibr" target="#b23">[24]</ref>. To this end, we require a task specific loss function, which we define next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Task Specific Loss Function</head><p>We define a loss function for object detection that decomposes over the bounding box proposals as follows:</p><formula xml:id="formula_3">?(y 1 , y 2 ) = 1 B B i=1 ?(y (i) 1 , y (i) 2 ).<label>(5)</label></formula><p>Following the standard practice in most modern object detectors <ref type="bibr" target="#b15">[16]</ref>, ?(y</p><formula xml:id="formula_4">(i) 1 , y (i)</formula><p>2 ) is further decomposed as a weighted combination of the classification loss and the localization loss. We use ? to denote the loss ratio ( ratio of the weight of localization loss to the weight of classification loss). We use a simple 0 ? 1 loss as our classification loss ? cls , and smoothL1 <ref type="bibr" target="#b11">[12]</ref> for our localization loss ? loc . Formally, the task specific loss is given by,</p><formula xml:id="formula_5">?(y (i) 1 , y (i) 2 ) = ? cls (y (i) 1 , y (i) 2 ) + ?? loc (y (i) 1 , y (i) 2 ). (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Objective Function</head><p>The task of both the prediction distribution and the conditional distribution is to predict the bounding box labels. Moreover, as the conditional distribution utilizes the extra information in the form of the image-level label, it is expected to provide more accurate predictions for the bounding box labels y. Leveraging on the task similarity between the two distributions, we would like to bring the two distributions close to each other, so that the extra knowledge of the conditional distribution can be transferred to the prediction distribution. Taking inspiration from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, we design a joint learning objective that can minimize the dissimilarity coefficient <ref type="bibr" target="#b23">[24]</ref> between the prediction distribution and conditional distribution. In what follows, we briefly describe the concept of dissimilarity coefficient before applying it to our setting.</p><p>Dissimilarity Coefficient: The dissimilarity coefficient between any two distributions Pr 1 (?) and Pr 2 (?) is determined by measuring their diversities. The diversity of a distribution Pr 1 (?) and a distribution Pr 2 (?) is defined as the expected difference between their samples, where the difference is measured by a task-specific loss function ? (?, ?). Formally, we define the diversity as,</p><formula xml:id="formula_6">DIV ? (Pr 1 , Pr 2 ) =E y1?Pr1(?) [E y2?Pr2(?) [? (y 1 , y 2 )]].<label>(7)</label></formula><p>If the model correctly brings the two distribution close to each other, we could expect the diversity DIV ? (Pr 1 , Pr 2 ) to be small. Using this definition of diversity, the dissimilarity coefficient of Pr 1 and Pr 2 is given by,</p><formula xml:id="formula_7">DISC ? (Pr 1 , Pr 2 ) =DIV ? (Pr 1 , Pr 2 ) ? ?DIV ? (Pr 2 , Pr 2 ) ? (1 ? ?)DIV ? (Pr 1 , Pr 1 ),<label>(8)</label></formula><p>where ? ? [0, 1]. In other words, the dissimilarity coefficient between Pr 1 and Pr 2 is the difference between the diversity of Pr 1 and Pr 2 , and a convex combination of their self-diversities. The self-diversity terms encourages the samples from each of the two distribution to be diverse, thus better representing the uncertainty of the task. In our experiments, we use ? = 0.5, which results in a symmetric dissimilarity coefficient between two distributions.</p><p>Learning Objective for Detection: Given the above definition of dissimilarity coefficient, we can now specify our learning objective for the task specific loss ? tuned for object detection <ref type="formula">(6)</ref> as</p><formula xml:id="formula_8">? * p , ? * c = arg min ?p,?c DISC ? (Pr p (? p ), Pr c (? c )),<label>(9)</label></formula><p>where each of the diversity terms can be derived from equation <ref type="bibr" target="#b6">(7)</ref>. As discussed in Section 3.2, the conditional distribution is difficult to model directly. Therefore, the corresponding diversity terms are computed by stochastic estimators from K samples? k c of the conditional net. Thus, each of the diversity terms can be written as 2 <ref type="bibr" target="#b9">(10)</ref> DIV ? (Pr p , Pr c )</p><formula xml:id="formula_9">= 1 BK B i=1 K k=1 y (i) p Pr p (y (i) p ; ? p )?(y (i) p ,? k,(i) c ), 2 Details in Appendix A (11) DIV ? (Pr c , Pr c ) = 1 K(K ? 1)B K k,k =1 k =k B i=1 ?(? k,(i) c ,? k ,(i) c ), (12) DIV ? (Pr p , Pr p ) = 1 B B i=1 y (i) p y (i) p Pr p (y (i) p ; ? p ) Pr p (y (i) p ; ? p )?(y (i) p , y (i) p ).</formula><p>Here, DIV ? (Pr p , Pr c ) measures the diversity between the prediction net and the conditional net, which is the expected difference between the samples from the two distributions as measured by the task specific loss function ?. Here Pr p is explicitly modeled, hence the expectation of its sample can be computed easily. However, as Pr c is not explicitly modeled, we compute the required expectation by drawing K samples from the distribution. Likewise, DIV ? (Pr c , Pr c ) measures the self diversity of the conditional net. We draw K samples from the distribution to compute the required expectation. Also, the self diversity of the prediction net DIV ? (Pr p , Pr p ) can be exactly computed as Pr p is explicitly modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Optimization</head><p>As we employ deep neural networks to model the two distributions, our objective function <ref type="formula" target="#formula_8">(9)</ref> is ideally suited to be minimized by stochastic gradient descent. While it may be possible to compute the gradients of both the networks simultaneously, in this work we use a simple coordinate descent optimization strategy. In more detail, the optimization proceeds by iteratively fixing the prediction network and learning the conditional network, followed by learning the prediction network for fixed conditional network.</p><p>The main advantage of using the iterative training strategy is that it results in an approach similar to the fully supervised learning of each network. This in turn allows us to readily use the algorithm developed in Fast-RCNN <ref type="bibr" target="#b11">[12]</ref> and Discrete DISCO Net <ref type="bibr" target="#b3">[4]</ref>. The outputs from the fixed network are treated as the pseudo ground truth bounding box labels for the other network. Furthermore, the iterative learning strategy also reduces the memory complexity of learning as only one network is trained at a time. <ref type="figure">Figure 2</ref> provides the visualization of the performance of the two networks over the different iterations of the iterative learning procedure. The estimated bounding box labels from the prediction net and those sampled from the conditional net for two images are depicted. For conditional net, we superimpose five different samples of bounding box labels. If all the samples agree with each other on bounding <ref type="bibr">Figure 2</ref>. Example of predictions of prediction net and conditional net. For prediction net, the visualization is after taking standard non maximal suppression using standard score threshold = 0.7. Column 1 and 3 are output of the prediction network while column 2 and 4 are output from the conditional network. Row 1 represents prediction of the two networks after first iteration and row 2 and 3 represents prediction of the two networks after third and sixth (final) iteration respectively. Each object class is represented by different colored bounding box, where green box represents the car category and red and blue represents the bottle and dog category respectively. box labels, then the bounding boxes will have a high overlap, otherwise they will be scattered across the image. For visualization purposes only, a standard non maximal suppression (NMS) is applied with a score threshold of 0.7 on the output of the prediction net. However, note that the non maximal suppression is not used during training of the prediction net. The two steps of the iterative algorithm are described below in brief. For completeness, the details are provided in appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Optimization over Prediction Distribution</head><p>For a fixed set of parameters ? c of the conditional network, the learning objective of the prediction net corresponds to the following: (13) Note that, due to the use of dissimilarity coefficient, the above objective differs slightly from the one used for Fast-RCNN <ref type="bibr" target="#b11">[12]</ref>. However, importantly, it is still differentiable with respect to ? p . Hence, the prediction net can be directly optimized via stochastic gradient descent.</p><p>In order to visualize the optimization of the prediction net, let us consider <ref type="figure">Figure 2</ref>. The first two columns show the bounding box labels from the prediction and the conditional nets for an image with single foreground object. As the image has a large foreground object with a clean background, both the prediction and the conditional nets have low uncertainty. This represents an easy case where the prediction net already has a high confidence for the bounding box labels in initial iterations, and therefore has little to gain from the conditional net. As expected, we see only a minor improvement in the predicted bounding box labels of the prediction net over the iterations.</p><p>The last two columns show bounding box labels from the prediction and conditional nets for a challenging image. The object dog presents moderate difficulty to our algorithm, where initially the prediction net is highly uncertain while the conditional net has low uncertainty. After few iterations, the information present in the conditional net is successfully transferred over to the prediction net. This is shown in last row of the third column where the prediction net does a reasonable job at estimating the bounding boxes.</p><p>The second object bottle in the image is a difficult exam-ple because of its small scale. We observe high uncertainty in both the networks. In such cases the prediction and the conditional nets will reject the bounding box labels having high diversity. Moreover, the uncertainty in the prediction net also decreases by learning from other easier instances of the object present in the data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Optimization over Conditional Distribution</head><p>For a fixed set of parameters ? p of the prediction network, the learning objective for the conditional network corresponds to the following,</p><formula xml:id="formula_10">? * c = arg min ?c DIV ? (Pr p , Pr c ) ? ?DIV ? (Pr c , Pr c ).<label>(14)</label></formula><p>The above objective function is similar to the one used in <ref type="bibr" target="#b3">[4]</ref> for supervised learning of Discrete DISCO Nets. As our conditional net employs a sampling procedure over the scoring function S k (y; ? c ), objective <ref type="formula" target="#formula_10">(14)</ref> is non-differentiable. However, as observed in <ref type="bibr" target="#b3">[4]</ref>, it is possible to compute an unbiased estimate of the gradients using the direct loss minimization technique <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>. Therefore, the conditional net can be optimized using stochastic gradient descent. We present the technical details of optimization, which are similar to those in <ref type="bibr" target="#b3">[4]</ref>, in appendix B.</p><p>In order to visualize the optimization of the conditional net, let us first consider the easy case in <ref type="figure">Figure 2</ref> (columns 1-2). Similar to the prediction net in the previous subsection, the uncertainty in the conditional net decreases marginally over the iterations, as it already has high confidence for the bounding box labels. For the challenging objects present in the image of the last two columns, we see that the prediction net has high uncertainty. The improvement in the predictions of the conditional net for these two cases are mainly attributed to the information gained by training on other easier examples of the dog and the bottle category present in the data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data set and Evaluation Metrics</head><p>Data set: We evaluate our method on the challenging PASCAL VOC 2007 and 2012 data sets <ref type="bibr" target="#b9">[10]</ref> which have 9, 962 and 22, 531 images respectively for 20 object categories. These two data sets are divided into the train, val and test sets. Here we choose trainval set of 5011 images for VOC 2007 and 11, 540 images for VOC 2012 to train our network. The trainval set is further split into 80% ? 20% to create new training and validation sets. We use a nonstandard training-validation split in order to maximize the number of training images for our networks, while not overfitting our hyper-parameters on the test set. As we focus on weakly supervised detection, only image-level labels are utilized during training.</p><p>Evaluation Metric We use two metrics to evaluate our detection performance. First we evaluate detection using mean Average Precision (mAP) on the PASCAL VOC 2007 and 2012 test sets, following the standard PASCAL VOC protocol <ref type="bibr" target="#b9">[10]</ref>. Second, we compute CorLoc <ref type="bibr" target="#b6">[7]</ref> on the PAS-CAL VOC 2007 and 2012 trainval splits. CorLoc is the fraction of positive training images in which we localize an object of the target category correctly. Following <ref type="bibr" target="#b9">[10]</ref>, a detected bounding box is considered correct if it has at least 0.5 IoU with a ground truth bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Implementation Details</head><p>We use standard Fast-RCNN <ref type="bibr" target="#b11">[12]</ref> to model prediction distribution and a modified Fast-RCNN to model the conditional distribution, as shown in <ref type="figure">Figure 1(a)</ref>. We use the Im-ageNet pre-trained VGG16 Network <ref type="bibr" target="#b26">[27]</ref> as the base CNN architecture for both our prediction and conditional nets.</p><p>The Fast-RCNN architecture is modified by adding a noise filter in its 5 th conv-layer as an extra channel as shown in <ref type="figure">Figure 1(b)</ref>. A 1 ? 1 filter is used to bring the number of channels back to the original dimensions (512 channels). No architectural changes are made for the prediction net. The bounding box proposals required for the Fast-RCNN is obtained from the Selective Search algorithm <ref type="bibr" target="#b33">[34]</ref>. Results based on the Region Proposal Networks are given in appendix C.</p><p>Following the standard practice followed in Fast-RCNN, we train and test our method on a single scale. We also construct an ensemble by taking the ImageNet pre-trained VGG11 and VGG13 along with VGG16 and report its results. For all our experiments we choose K = 5 for the conditional net. That is, we sample 5 bounding boxes corresponding to 5 noise filters, which are themselves sampled from a uniform distribution. For all other hyper-parameters, we use the same configurations as described in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>In this subsection, we will first compare our method with existing state-of-the-art methods for detection and correct localization tasks on VOC 2007 and 2012 data sets. Then through ablation experiments, see how various terms of our dissimilarity coefficient based objective function contribute towards the accuracy gained. We present further ablation studies in appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Comparison with other methods</head><p>We compare our proposed method with other state-of-theart weakly supervised methods. The detection average precision (AP) and correct localization (CorLoc) on the PAS-CAL VOC 2007 and 2012 data sets are shown in <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> respectively. Compared with the other methods, our proposed framework achieves state-of-the-art performance using a single model.   Compared to the state-of-the-art method, if we were to only train and test Zhang et al. <ref type="bibr" target="#b40">[41]</ref> (W2F) using a single scale, where they achieve 49.0% mAP, we get an improvement of 3.9%. Our framework trained on a single scale still outperforms W2F by 0.5% even when they train and test using multiple scales. We approximate the use of multiple scales by ensembling, which gives us a further improvement over the state-of-the-art method by over 1.2%.</p><p>The weakly supervised detector employed in W2F models the annotation constraint using a fully factorized distribution. We argue that our choice of modeling the annotation aware conditional distribution exactly but efficiently, using Discrete DISCO Net, gives us the improved performance. Moreover, unlike W2F, our method combines the weakly supervised and the strongly supervised detectors with a single learning objective instead of training them in a non-endto-end, cascaded fashion. We note that the pseudo-groundtruth excavation (PGE) algorithm proposed in W2F is complementary to our method, and can also be employed over the samples generated from conditional distribution to further improve the accuracy of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Effect of the diversity coefficient terms</head><p>In order to understand the effect of various diversity coefficient terms in our objective <ref type="bibr" target="#b7">(8)</ref>, we remove the self-diversity term in one or both of our probabilistic networks (Pr c and Pr p ). In order to obtain a single sample from our conditional network, we feed a zero noise vector (denoted by P W c ). The prediction network still outputs the probabil- ity of each bounding box belonging to each class. However, by removing the self-diversity term, we encourage it to output a peakier distribution (denoted by P W p ). <ref type="table">Table 4</ref> shows that both the self-diversity terms are important to obtain the maximum accuracy. Relatively speaking, it is more important to include the self-diversity in the conditional network in order to deal with the difficult examples (example, bottle in <ref type="figure">figure 2</ref>). Moreover, this enforces a diverse set of outputs from the conditional network, which helps the prediction network to avoid overfitting the samples during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>We presented a novel framework to train an object detector using a weakly supervised data set. Our framework employs a probabilistic objective based on dissimilarity coefficient to model the uncertainty in the location of objects. We show that explicitly modeling the complex non-factorizable conditional distribution is a necessary modeling choice and present an efficient mechanism based on a discrete generative model, the Discrete DISCO Nets, to do so. Extensive experiments on the benchmark data sets have shown that our framework successfully transfers the information present in the image-level annotations for the task of object detection.</p><p>In future, we would like to investigate the use of active learning, to further benefit our network in terms of the accuracy of the fully supervised annotations. This will help bridge the performance gap between the strongly supervised detectors and detectors trained using low-cost annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Learning Objective</head><p>In this section we provide detailed derivation of the objective function presented in Section 4.2 of the paper.</p><p>Given the loss function ? (equation (6) of main paper), which is tuned for the task of object detection, we compute the diversity terms as given in equation <ref type="formula" target="#formula_6">(7)</ref> of the main paper. Recall that the diversity for any two distributions is the expected loss of the samples drawn from the two distributions. For the prediction distribution Pr p and the conditional distribution Pr c , we derive the diversity between them and their self diversities as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diversity between prediction net and conditional net:</head><p>Following equation <ref type="formula" target="#formula_6">(7)</ref> of the main paper, the diversity between prediction and conditional distribution can be written as,</p><formula xml:id="formula_11">(15) DIV ? (Pr p , Pr c ) = E yp?Prp(y|x;?p) [E yc?Prc(y|x,h;?c) [?(y p , y c )]].</formula><p>The task specific loss function is decomposed over the bounding boxes as given in equation <ref type="formula" target="#formula_3">(5)</ref> of the main paper. We then write the expectation with respect to the conditional distribution (the inner distribution) as expectation over the random variables z with distribution Pr(z) using Law of the Unconscious Statistician (LOTUS).</p><formula xml:id="formula_12">DIV ? (Pr p , Pr c ) = E yp?Prp(y|x;?p) [E z?Pr(z) [ 1 B B i=1 ?(y (i) p ,? k,(i) c )]].<label>(16)</label></formula><p>The expectation over the random variable z with distribution Pr(z) is approximated by taking K samples from</p><formula xml:id="formula_13">Pr(z),<label>(17)</label></formula><formula xml:id="formula_14">DIV ? (Pr p , Pr c ) = E yp?Prp(y|x;?p) [ 1 K K k=1 1 B B i=1 ?(y (i) p ,? k,(i) c )].</formula><p>We finally compute the expectation with respect to the prediction distribution as,</p><p>DIV ? (Pr p , Pr c )</p><formula xml:id="formula_16">= 1 BK B i=1 K k=1 y (i) p Pr p (y (i) p ; ? p )?(y (i) p ,? k,(i) c ).</formula><p>Self diversity for conditional net: As above, using equation (7) of the main paper, we write the self diversity coefficient of the conditional distribution as</p><formula xml:id="formula_17">DIV ? (Pr c , Pr c ) = E yc?Prc(y|x,h;?c) [E y c ?Prc(y|x,h;?c) [?(y c , y c )]].<label>(19)</label></formula><p>We now write the two expectations with respect to the conditional distribution as the expectation over the random variables z and z respectively. The task specific loss function is decomposed over the bounding box as shown in equation (5) of the main paper. Therefore, we re-write the above equation as <ref type="bibr" target="#b19">(20)</ref> DIV ? (Pr c , Pr c )</p><formula xml:id="formula_18">= E z?Pr(z) [E z ?Pr(z) [ 1 B B i=1 ?(? k,(i) c ,? k,(i) c )]].</formula><p>In order to approximate the expectation over the random variables z and z , we use K samples from the distribution Pr(z) as</p><p>DIV ? (Pr c , Pr c )</p><formula xml:id="formula_20">= 1 K K k=1 1 K ? 1 K k =1, k =k 1 B B i=1 ?(? k,(i) c ,? k ,(i) c ).</formula><p>On re-arranging the above equation, we get <ref type="bibr" target="#b21">(22)</ref> DIV ? (Pr c , Pr c )</p><formula xml:id="formula_21">= 1 K(K ? 1)B K k,k =1 K k =k B i=1 ?(? k,(i) c ,? k ,(i) c</formula><p>).</p><p>Self diversity for prediction net: Similar to the above two cases, using equation <ref type="formula" target="#formula_6">(7)</ref> of the main paper, we can write the self diversity of the prediction net as</p><formula xml:id="formula_22">(23) DIV ? (Pr p , Pr p ) = E yp?Prp(y|x;?p) [E y p ?Prp(y|x;?p) [?(y p , y p )]].</formula><p>We then decompose the task specific loss function over the bounding boxes as described in equation (5) of the main paper,</p><formula xml:id="formula_23">DIV ? (Pr p , Pr p ) = E yp?Prp(y|x;?p) [E y 1 ?Prp(y|x;?p) [ 1 B B i=1 ?(y (i) p , y (i) p )]]<label>(24)</label></formula><p>Note that the prediction distribution is a fully factorized distribution, and we can compute its exact expectation. Therefore, we compute the two expectations with respect to the prediction distribution as,</p><formula xml:id="formula_24">E yp?Prp(y |x;?p) [ 1 B B i=1 y (i) p Pr p (y (i) p ; ? p )?(y (i) p , y (i) p )] = 1 B B i=1 y (i) p y (i) p Pr p (y (i) p ; ? 1 ) Pr p (y (i) p ; ? p )?(y (i) p , y (i) p )<label>(25)</label></formula><p>Appendix B. Optimization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Optimization over Prediction Distribution</head><p>As parameters ? c of the conditional distribution are constant, the learning objective of the prediction distribution (equation (13) of the main paper) results in a fully supervised training of the Fast-RCNN network <ref type="bibr" target="#b11">[12]</ref>. Note that the only difference between training of a standard Fast-RCNN architecture and our prediction net is the use of the dissimilarity objective function (equation (13) of the main paper) instead of minimizing the multi-task loss of the Fast-RCNN.</p><p>The prediction net takes as the input an image and the K predictions sampled from the conditional net. Treating these predictions of the conditional net as the pseudo ground truth label, we compute the gradient of our dissimilarity coefficient based loss function. As the objective given in equation <ref type="bibr" target="#b12">(13)</ref> of the main paper is differentiable with respect to parameters ? p , we update the network by employing stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Optimization over Conditional Distribution</head><p>A non-differentiable training procedure: The conditional net is modeled using a Discrete DISCO Net which employs a sampling step from the scoring function S k (y; ? c ). This sampling step makes the objective function nondifferentiable with respect to the parameters ? c , even though the scoring function S k (y; ? c ) itself is differentiable. However, as the prediction network is fixed, the above objective function reduces to the one used in Bouchacourt et al. <ref type="bibr" target="#b3">[4]</ref> for fully supervised training. Therefore, similar to Bouchacourt et al. <ref type="bibr" target="#b3">[4]</ref> we solve this problem by estimating the gradients of our objective function with the help of temperature parameter as, </p><p>where,</p><formula xml:id="formula_26">DIV ? (Pr p , Pr c ) = E yp?Prp(?p) [E z k ?Pr(z) [? ?c S k (? a ; ? c ) ? ? ?c S k (? c ; ? c )]]<label>(27)</label></formula><formula xml:id="formula_27">DIV ? (Pr c , Pr c ) = E z k ?Pr(z) [E z k ?Pr(z) [? ?c S k (? b ; ? c ) ? ? ?c S k (? c ; ? c )]]<label>(28)</label></formula><p>and,?</p><formula xml:id="formula_28">c = arg max y?Y S k (y; ? c ) y c = arg max y?Y S k (y; ? c ) y a = arg max y?Y S k (y; ? c ) ? ?(y p ,? c ) y b = arg max y?Y S k (y; ? c ) ? ?(? c ,? c )<label>(29)</label></formula><p>In our experiments, we fix the temperature parameter as, = +1.</p><p>Intuition for the gradient computation: We now present an intuitive explanation of the computation of gradient, as given in equation <ref type="bibr" target="#b25">(26)</ref>. For an input x and two noise samples z k , z k , the conditional net outputs two scores S k (y; ? c ) and S k (y; ? c ), with the corresponding maximum scoring outputs? c and? c . The model parameters ? c are updated via gradient descent in the negative direction of ? ?c DISC ? (Pr p (? p ), Pr c (? c )).</p><p>? The term DIV ? (Pr p , Pr c ) updates the model parameters towards the maximum scoring prediction? c of the score S k (y; ? c ) while moving away from? a , wher? y a is the sample corresponding to the maximum loss augmented score S k (y; ? c ) ? ?(y p ,? c ) with respect to the fixed prediction distribution samples y p . This encourages the model to move away from the prediction providing high loss with respect to the pseudo ground truth labels.</p><p>? The term ?DIV ? (Pr c , Pr c ) updates the model towards y b and away from the? c . Note the two negative signs giving the update in the positive direction.</p><p>Here y b is the sample corresponding to the maximum loss augmented score S k (y; ? c ) ? ?(? c ,? ) with respect to the other prediction? c , encouraging diversity between? c and? c .</p><p>Training algorithm for conditional net: Pseudo-code for training the conditional network for a single sample from weakly supervised data is presented in algorithm 1 below. In algorithm 1, statements 1 to 3 describe the sampling process and computing the loss augmented prediction. We first sample K different predictions? k c corresponding to each noise vector z k in statement 2. For the sampled prediction? k c we compute the maximum loss augmented score S k (y; ? c ) ? ?(y p ,? c ). This is then used to find the loss augmented prediction? a given in statement 3.</p><p>In order to compute the gradients of the self diversity of conditional distribution, we need to find the maximum loss augmented prediction y b . Here, the loss is computed between a pair of K different predictions of the conditional net that we have already obtained. This is shown by statements 4 to 7 in algorithm 1.</p><p>For the purpose of optimizing the conditional net using gradient descent, we need to find the gradients for the objective function of the conditional net defined in equation <ref type="bibr" target="#b13">(14)</ref> of the main paper. The computation of the unbiased approximate gradients for the individual terms in the objective function is shown in statement 8. We finally optimize the conditional net by the employing gradient descent step and updating the model parameters by descending to the approximated gradients as shown in statement 9 of algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Implementation Details</head><p>In this section, we provide additional implementation details. For the input pair (x, z k ), the classification branch of the conditional net outputs a score function G k (y; ? c ), which is a B ? C matrix. We then sample? k c as described in Section 3.2 of the paper. A non-maximal suppression is applied to further reduce the number of sampled bounding boxes. Corresponding to these samples, we mask the bounding box regression branch of the conditional net such that every bounding box which is not present in the sampled output? k c is multiplied by a 0 row vector. This ensures that only those bounding boxes which are sampled by the conditional net are retained in the regression branch. The approximated gradients of the loss function is then computed and fed explicitly to the non-differentiable output branch to update the parameters of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Ablation Experiments</head><p>In this subsection we discuss the effects of the loss ratio and the thresholding operation on the score function for the detection task on VOC 2007 data set. Effects of the loss ratio: The loss ratio ?, as defined in Section 4.1 of the main paper, is the ratio of the weight of the localization loss to the weight of the classification loss. </p><formula xml:id="formula_29">DIV ? (Pr p , Pr c ) = 1 KB K k=1 B i=1 ? ?c S k (? (i) a ; ? c ) ? ? ?c S k (? (i) c ; ? c )<label>(30)</label></formula><p>DIV ? (Pr c , Pr c )</p><formula xml:id="formula_31">= 2 K(K ? 1)B K k,k =1 k =k B i=1 ? ?c S k (? (i) b ; ? c ) ? ? ?c S k (? (i) c ; ? c )</formula><p>Update model parameters by descending to the approximated gradients:</p><formula xml:id="formula_32">? t+1 c = ? t c ? ?? ?c DISC ? (Pr p (? p ), Pr c (? c ))</formula><p>In other words, with the higher the loss ratio more importance will be given by the objective function to correctly regress the bounding box labels. We choose three different loss ratios ? = {1, 0.33, 3} for evaluation. The result of We empirically observe that assigning more weight to the localization loss helps, indicating that it is important for the networks to tweak the bounding boxes labels generated from the selective search region proposals.</p><p>Effect of thresholding the score function: As seen in Section 3.2 of the main paper, the conditional net generates samples from the score function (4) of the main paper. A low score value indicates that the conditional net is not certain of the bounding box label for an input image. Thresholding the score function would mean that we only sample bounding box labels from the conditional net when it has high certainty over the class distribution. We evaluate the result of the detection task on VOC 2007 test set for the threshold values of {0.1, 0.2, 0.3, 0.4, 0.5}. Without any threshold, our method has a mean average precision of 51.4%. The corresponding mean average precision for the threshold values are {51.7%, 52.2%, 51.5%, 51.0%, 50.6%}. These results indicate that it helps to apply threshold when the network is uncertain over the output classes. This is because we would not like the prediction net to learn from highly uncertain samples. We get the best results for the threshold value of 0.2. However, we also observe that choosing a large value for threshold has no effect on the detection accuracy. In this case, the network is already reasonably certain of the bounding box label, and we would not like to reject such samples.</p><p>Note that for the choice of loss ratio ? = 3 and threshold kept at 0.2, our method achieves the best detection average precision of 52.9% mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Results on VOC 2012</head><p>Here, we compare our proposed method with other stateof-the art weakly supervised methods. Results for the task of detection average precision (AP) and correct localization (CorLoc) are presented in <ref type="table">table 5 and table 6</ref> respectively for PASCAL VOC 2012 data set. Our results are consistent with those observed for VOC 2007 data set and we get an overall increase of 1.7% over previous state-of-the-art method, W2F <ref type="bibr" target="#b40">[41]</ref>. Our network trained and tested on a single scale outperforms W2F <ref type="bibr" target="#b40">[41]</ref>, which is trained and tested on multiple scales. In this subsection we show that our method extends to architectures with region proposal networks (RPN) <ref type="bibr" target="#b25">[26]</ref>, thus eliminating the need for external bounding box proposer like Selective Search <ref type="bibr" target="#b33">[34]</ref>. This enables our framework to perform inference in real-time, while the entire pipeline is trained in an end-to-end fashion including the RPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Results with Region Proposal Networks</head><p>For this, we replace our prediction net with Faster-RCNN <ref type="bibr" target="#b25">[26]</ref> as shown in <ref type="figure" target="#fig_4">Figure 3</ref>. As we wish to use the same set of bounding boxes for both the networks, we share the bounding box proposals generated from RPN as shown in the figure. Furthermore, reusing the computation also makes our training efficient.</p><p>The algorithm proceeds by randomly initializing the RPN and extracting 300 bounding box proposals for each image. These proposals are then fed to the conditional net, which samples the bounding boxes corresponding to the image-level labels for the given image from the proposals. Note that as we introduce noise samples in our conditional net, we get a diverse set of sampled bounding boxes. These bounding boxes are then used to train the conditional net, which also updates the RPN thereby gradually improving the localization of the objects present in the image.</p><p>The results when using bounding box proposals from RPN is presented in <ref type="table" target="#tab_5">Table 7</ref>. We compare the results against those achieved by using Selective Search bounding boxes. Note that, 300 bounding box proposals generated from the randomly initialized RPN has a recall rate of 44.5%?13.2% on PASCAL VOC 2007 data set. However, after several iterations of training, the final recall rate achieved from 300 bounding box proposals from RPN is 94.7%. This is still low when compared to the recall rate achieved by 2000 bounding box proposals from Selective Search method. We argue that due to this difference, we observe a 2% drop in accuracy. This makes a case of using more bounding box proposals for a better recall rate or using better RPN, like the one proposed in <ref type="bibr" target="#b32">[33]</ref>. Finally, our choice of employing Faster-RCNN for the prediction net enables our framework to perform inference in real-time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 3 ) 4 )Figure 1 .</head><label>341</label><figDesc>Given the scoring function in equation<ref type="bibr" target="#b1">(2)</ref>, we compute the k-th sample as? k c = arg max y?Y S k (y; ? c ).(The overall architecture. (a) Prediction Network: a standard Fast-RCNN architecture is used to model the prediction net. For an input image, bounding box proposals are generated from selective search<ref type="bibr" target="#b33">[34]</ref>. Features from each of these proposals are computed by the region of interest (ROI) pooling layers, which are then passed through the classifier and regressor to predict the final bounding box. (b) Conditional Network: a modified Fast-RCNN architecture is used to model the conditional net. For a single input image x and three different noise samples {z1, z2, z3} (represented as red, green and blue matrix), three different bounding boxes {y<ref type="bibr" target="#b0">(1)</ref> , y<ref type="bibr" target="#b1">(2)</ref> , y (3) } are sampled for the given image-level label (bird in this example). Here the noise filter is concatenated as an extra channel to the final convolutional layer. For both the networks, the initial conv-layers are fixed during training. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? * p = arg min ?p DIV ? (Pr p , Pr p )?(1??)DIV ? (Pr p , Pr p ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>?c DISC ? (Pr p (? p ), Pr c (? c )) = ? lim ?0 1 (DIV ? (Pr p , Pr c ) ? ?DIV ? (Pr c , Pr c ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 3 Find4 7 Find8</head><label>137</label><figDesc>Conditional net training algorithm Input : Training input (x, a) ? W, and prediction net output y p Output:? 1 c , . . . ,? K c , sample K predictions from the model 1 for k = 1 . . . K do 2 Sample noise vector z k , generate output? k c : y k c = arg max y?Y S k (y; ? c ) loss augmented prediction? k a w.r.t. output from prediction net y p : y k a = arg max y?Y S k (y; ? c ) ? ?(y p ,? k c ) Compute loss augmented predictions: 5 for k = 1, . . . , K do 6 for k = 1, . . . , K, k = k do loss augmented prediction? k b w.r.t. other conditional net outputs? k c : y?Y S k (y; ? c ) ? ?(? k c ,? ) Compute unbiased approximate gradients for DIV ? (Pr c , Pr c ) and DIV ? (Pr c , Pr c ) as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The overall architecture. (a) Prediction Network: a standard Faster-RCNN architecture is used to model the prediction net. For an input image, the region proposal network (RPN) generates a set of bounding box proposals. Features from each of these proposals are computed by the region of interest (ROI) pooling layers, which are then passed through the classifier and regressor to predict the final bounding box. (b) Conditional Network: a modified Fast-RCNN architecture is used to model the conditional net. For a single input image x and three different noise samples {z1, z2, z3} (represented as red, green and blue matrix), three different bounding boxes {y (1) , y (2) , y (3) } are sampled for the given image-level label (bird in this example). Here the noise filter is concatenated as an extra channel to the final convolutional layer. The bounding box proposals required for the conditional net are acquired from the RPN of the prediction net. For both the networks, the initial conv-layers are fixed during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>66.6 48.3 26.0 15.8 66.5 65.4 53.9 24.7 61.2 46.2 53.5 48.5 66.1 12.1 22.0 49.2 53.2 66.2 59.4 48.3 ML-LocNet [40] 60.8 70.6 47.8 30.2 24.8 64.9 68.4 57.9 11.0 51.3 55.5 48.1 68.7 Detection average precision (%) for different methods on VOC 2007 test set.</figDesc><table><row><cell>Method</cell><cell>aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="5">cat chair cow table dog horse mbike pson plant sheep sofa train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>WSDDN [3]</cell><cell cols="3">46.4 58.3 35.5 25.9 14.0 66.7 53.0 39.2</cell><cell>8.9</cell><cell>41.8 26.6 38.6 44.7</cell><cell>59.0</cell><cell>10.8 17.3</cell><cell>40.7 49.6 56.9 50.8 39.3</cell></row><row><cell>WSCCN [8]</cell><cell cols="5">49.5 60.6 38.6 29.2 16.2 70.8 56.9 42.5 10.9 44.1 29.9 42.2 47.9</cell><cell>64.1</cell><cell>13.8 23.5</cell><cell>45.9 54.1 60.8 54.5 42.8</cell></row><row><cell>k-EM [38]</cell><cell cols="5">59.8 64.6 47.8 28.8 21.4 67.7 70.3 61.2 17.2 51.5 34.0 42.3 48.8</cell><cell>65.9</cell><cell>9.3</cell><cell>21.1</cell><cell>53.6 51.4 54.7 50.7 46.1</cell></row><row><cell>OICR [32]</cell><cell cols="3">65.5 67.2 47.2 21.6 22.1 68.0 68.5 35.9</cell><cell>5.7</cell><cell>63.1 49.5 30.3 64.7</cell><cell>66.1</cell><cell>13.0 25.6</cell><cell>50.0 57.1 60.2 59.0 47.0</cell></row><row><cell>ZLDN [39]</cell><cell cols="3">55.4 68.5 50.1 16.8 20.8 62.7 66.8 56.5</cell><cell>2.1</cell><cell>57.8 47.5 40.1 69.7</cell><cell>68.2</cell><cell>21.6 27.2</cell><cell>53.4 56.1 52.5 58.2 47.6</cell></row><row><cell>CL [36]</cell><cell cols="6">61.2 69.5</cell><cell>28.3 25.2</cell><cell>51.3 56.5 60.0 43.1 49.7</cell></row><row><cell>WS-RPN [33]</cell><cell cols="5">63.0 69.7 40.8 11.6 27.7 70.5 74.1 58.5 10.0 66.7 60.6 34.7 75.7</cell><cell>70.3</cell><cell>25.7 26.5</cell><cell>55.4 56.4 55.5 54.9 50.4</cell></row><row><cell>W2F [41]</cell><cell cols="5">63.5 70.1 50.5 31.9 14.4 72.0 67.8 73.7 23.3 53.4 49.4 65.9 57.2</cell><cell>67.2</cell><cell>27.6 23.8</cell><cell>51.8 58.7 64.0 62.3 52.4</cell></row><row><cell cols="6">Pred Net (VGG) 66.7 69.5 52.8 31.4 24.7 74.5 74.1 67.3 14.6 53.0 46.1 52.9 69.9</cell><cell>70.8</cell><cell>18.5 28.4</cell><cell>54.6 60.7 67.1 60.4 52.9</cell></row><row><cell>Pred Net (Ens)</cell><cell cols="5">67.7 70.4 52.9 31.3 26.1 75.5 73.7 68.6 14.9 54.0 47.3 53.7 70.8</cell><cell>70.2</cell><cell>19.7 29.2</cell><cell>54.9 61.3 67.6 61.2 53.6</cell></row><row><cell>Method</cell><cell>aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="5">cat chair cow table dog horse mbike pson plant sheep sofa train</cell><cell>tv</cell><cell>mean</cell></row><row><cell>WSCCN [8]</cell><cell cols="5">83.9 72.8 64.5 44.1 40.1 65.7 82.5 58.9 33.7 72.5 25.6 53.7 67.4</cell><cell>77.4</cell><cell>26.8 49.1</cell><cell>68.1 27.9 64.5 55.7 56.7</cell></row><row><cell>WSDDN [3]</cell><cell cols="5">68.9 68.7 65.2 42.5 40.6 72.6 75.2 53.7 29.7 68.1 33.5 45.6 65.9</cell><cell>86.1</cell><cell>27.5 44.9</cell><cell>76.0 62.4 66.3 66.8 58.0</cell></row><row><cell>ZLDN [39]</cell><cell cols="5">74.0 77.8 65.2 37.0 46.7 75.8 83.7 58.8 17.5 73.1 49.0 51.3 76.7</cell><cell>87.4</cell><cell>30.6 47.8</cell><cell>75.0 62.5 64.8 68.8 61.2</cell></row><row><cell>OICR [32]</cell><cell cols="5">85.8 82.7 62.8 45.2 43.5 84.8 87.0 46.8 15.7 82.2 51.0 45.6 83.7</cell><cell>91.2</cell><cell>22.2 59.7</cell><cell>75.3 65.1 76.8 78.1 64.3</cell></row><row><cell>CL [36]</cell><cell cols="5">85.8 80.4 73.0 42.6 36.6 79.7 82.8 66.0 34.1 78.1 36.9 68.6 72.4</cell><cell>91.6</cell><cell>22.2 51.3</cell><cell>79.4 63.7 74.5 74.6 64.7</cell></row><row><cell>k-EM [38]</cell><cell cols="5">79.8 77.8 66.7 50.3 57.0 80.1 89.9 71.5 29.9 75.9 30.5 58.9 73.2</cell><cell>90.2</cell><cell>25.4 51.8</cell><cell>80.2 60.3 72.4 78.9 65.0</cell></row><row><cell>WS-RPN [33]</cell><cell cols="5">83.8 82.7 60.7 35.1 53.8 82.7 88.6 67.4 22.0 86.3 68.8 50.9 90.8</cell><cell>93.6</cell><cell>44.0 61.2</cell><cell>82.5 65.9 71.1 76.7 68.4</cell></row><row><cell cols="6">ML-LocNet [40] 81.7 82.9 68.7 44.4 53.9 80.3 88.9 70.5 32.6 74.0 62.7 61.7 81.4</cell><cell>91.6</cell><cell>46.0 60.6</cell><cell>75.2 69.2 78.7 65.8 68.6</cell></row><row><cell>W2F [41]</cell><cell cols="5">85.4 87.5 62.5 54.3 35.5 85.3 86.6 82.3 39.7 82.9 49.4 76.5 74.8</cell><cell>90.0</cell><cell>46.8 53.9</cell><cell>84.5 68.3 79.1 79.9 70.3</cell></row><row><cell cols="6">Pred Net (VGG) 88.6 86.3 71.8 53.4 51.2 87.6 89.0 65.3 33.2 86.6 58.8 65.9 87.7</cell><cell>93.3</cell><cell>30.9 58.9</cell><cell>83.4 67.8 78.7 80.2 70.9</cell></row><row><cell>Pred Net (Ens)</cell><cell cols="5">89.2 86.7 72.2 50.9 51.8 88.3 89.5 65.6 33.6 87.4 59.7 66.4 88.5</cell><cell>94.6</cell><cell>30.4 60.2</cell><cell>83.8 68.9 78.9 81.3 71.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>CorLoc (in %) for different methods on VOC 2007 trainval set.</figDesc><table><row><cell>Method</cell><cell cols="6">WSCCN [8] DSL [17] OICR [32] W2F [41] PredNet(VGG) PredNet(Ens)</cell></row><row><cell>mAP %</cell><cell>37.9</cell><cell>38.3</cell><cell>42.5</cell><cell>47.8</cell><cell>48.4</cell><cell>49.5</cell></row><row><cell>CorLoc %</cell><cell>-</cell><cell>58.8</cell><cell>65.6</cell><cell>69.4</cell><cell>69.5</cell><cell>70.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Results for different methods on VOC 2012. See ap- pendix D for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Detection average precision (%) for different methods on VOC 2012 test set. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike pson plant sheep sofa train tv mAP Jie et al. [17] 60.8 54.2 34.1 14.9 13.1 54.3 53.4 58.6 69.4 55.1 29.8 28.1 55.0 57.1 24.4 17.2 59.1 21.8 26.6 57.8 71.3 1.0 23.1 52.7 37.5 33.5 56.6 42.5 W2F [41] 73.0 69.4 45.8 30.0 28.7 58.8 58.6 56.7 20.5 58.9 10.0 69.5 67.0 CorLoc (in %) for different methods on VOC 2012 trainval set. detection task on VOC 2007 test set are 52.1%, 51.6% and 52.4% mAP respectively.</figDesc><table><row><cell>3.7</cell><cell>53.1</cell><cell>8.3</cell><cell>43.4 49.8</cell><cell>69.2</cell><cell>4.1</cell><cell>17.5</cell><cell>43.8 25.6</cell><cell>55</cell><cell>50.1 38.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Comparison of results when using bounding box proposals from Selective Search and RPN.</figDesc><table><row><cell></cell><cell cols="2">Selective Search</cell><cell>RPN</cell><cell></cell></row><row><cell></cell><cell cols="4">mAP % CorLoc % mAP % CorLoc %</cell></row><row><cell>VOC 2007</cell><cell>52.9</cell><cell>70.9</cell><cell>50.9</cell><cell>69.1</cell></row><row><cell>VOC 2012</cell><cell>49.5</cell><cell>70.2</cell><cell>46.1</cell><cell>67.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The use of score function in this paper should not be confused with the scoring rule theory, which is used to design the learning objective of DISCO Nets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning human poses from actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Task-Oriented Learning of Structured Probability Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct loss minimization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling latent variable uncertainty for loss-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency guided end-to-end learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple instance curriculum learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Diversity and dissimilarity coefficients: a unified approach. Theoretical population biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training deep neural networks via direct loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collaborative learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relaxed multipleinstance svm with application to object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Weaklyand semi-supervised object detection with expectationmaximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08740</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ML-Locnet: Improving object localization with multi-view learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">W2F: A weakly-supervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
