<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
							<email>zilonghuang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent GY-Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youcheng</forename><surname>Ben</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent GY-Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent GY-Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Cheng</surname></persName>
							<email>peicheng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent GY-Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent GY-Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent GY-Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Very recently, Window-based Transformers, which computed self-attention within non-overlapping local windows, demonstrated promising results on image classification, semantic segmentation, and object detection. However, less study has been devoted to the cross-window connection which is the key element to improve the representation ability. In this work, we revisit the spatial shuffle as an efficient way to build connections among windows. As a result, we propose a new vision transformer, named Shuffle Transformer, which is highly efficient and easy to implement by modifying two lines of code. Furthermore, the depth-wise convolution is introduced to complement the spatial shuffle for enhancing neighbor-window connections. The proposed architectures achieve excellent performance on a wide range of visual tasks including image-level classification, object detection, and semantic segmentation. Code will be released for reproduction.</p><p>Recently, Vision Transformers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46]</ref> have been absorbed more and more investigations. Due to the great flexibility in modeling long-range dependencies in vision tasks, introducing less inductive bias, vision transformers have already achieved performances quite competitive with their CNN counterparts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref> and can naturally process multi-modality input data including images, videos, texts, speech signals, and point clouds.</p><p>These vision transformers have quadratic computation complexity to input image size due to computation of self-attention globally. This property makes it difficult to apply vision transformers to dense prediction tasks, such as semantic segmentation, which inherently require high-resolution images. A workaround is the window-based self-attention, where the input is spatially partitioned into non-overlapped windows and the standard self-attention is computed only within each local window. On the one hand, it can significantly reduce the complexity, On the other hand, limiting the range to compute self-attention can make the model more efficient to train <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref>. However, computing self-attention within non-overlapping windows results in a limited receptive field. To build information communication across windows, HaloNet [36] scales the window for collecting information inside and outside the original window. Swin <ref type="bibr" target="#b27">[28]</ref> alleviates this issue by shifting the window partition between consecutive self-attention layers. Despite being effective, HaloNet and Swin both are not good at building long-range cross-window connections for enlarging receptive fields efficiently.</p><p>To build the cross-window connections, especially long-range cross-window connections, we pay attention to window partitioning. Inspired by ShuffleNet <ref type="bibr" target="#b44">[45]</ref>, which comes up with a novel channel shuffle operation to help the information flowing across feature channels, we introduce a spatial shuffle operation into the window-based self-attention module for providing connections among windows and significantly enhance modeling power. Different from channel shuffle, spatial shuffle operation will result in spatial misalignment between features and image content. Thus, we need an Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>inverse process of spatial shuffle operator to align the feature with image content, which is named as spatial alignment operator. In this paper, the window-based transformer block with a spatial shuffle operator and a spatial alignment operator, named as Shuffle Transformer Block, will be used as the basic component to build the Shuffle Transformer.</p><p>Although spatial shuffle is efficient for building long-range cross-window connections, the "grid issue" may arise when image size is far greater than the window size. To enhance the neighbor-window connections, we introduce a depth-wise convolutional layer with a residual connection into the Shuffle Transformer Block. Finally, with the help of successive Shuffle Transformer Blocks, the proposed Shuffle Transformer could make information flow across all windows.</p><p>In summary, our proposed Shuffle Transformer achieves linear computational complexity in the number of input tokens by computing self-attention within non-overlapping local windows. To build rich cross-window connections, we propose the Shuffle Transformer block which integrates the spatial shuffle and neighbor-window connections. The experiments conducted on a number of visual tasks, ranging from image-level classification to pixel-level semantic/instance segmentation and object detection show that both of our proposed architectures perform favorably against other state-of-the-art vision transformers with similar computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision Transformers The transformer was firstly proposed by <ref type="bibr" target="#b36">[37]</ref> for machine translation tasks and has dominated in natural language modeling. Recently, ViT <ref type="bibr" target="#b10">[11]</ref> is the first work to apply a pure transformer to image classification with state-of-the-art performance (e.g. ResNets <ref type="bibr" target="#b16">[17]</ref>, EfficientNet <ref type="bibr" target="#b33">[34]</ref>) on image classification when the data is large enough. It splits each image into a sequence of tokens and then applies multiple standard Transformer layers to model their global relation for classification. Specifically, the standard Transformer layers consist of a Multi-head Self-Attention module (MSA) and a Multiple Layer Perceptron (MLP). Following that, DeiT <ref type="bibr" target="#b34">[35]</ref> introduces token-based distillation to reduce the data necessary for training the transformer. T2T-ViT <ref type="bibr" target="#b42">[43]</ref> structures the image to tokens by recursively aggregating neighboring tokens into one token to reduce tokens length. Transformer-iN-Transformer (TNT) <ref type="bibr" target="#b13">[14]</ref> utilizes both an outer Transformer block that processes the patch embeddings, and an inner Transformer block that models the relation among pixel embeddings, to model both patch-level and pixel-level representation.</p><p>To produce a hierarchical representation that is required by dense prediction tasks such as object detection and segmentation, Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b38">[39]</ref>, is proposed and can output the feature pyramid <ref type="bibr" target="#b25">[26]</ref> as in convolutional neural networks (CNNs).</p><p>To reduce the complexity, The recent Swin Transformer <ref type="bibr" target="#b27">[28]</ref> introduces non-overlapping window partitions and restricts self-attention within each local window, resulting in linear computational complexity in the number of input tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrating Convolution and Vision Transformer</head><p>To enhance local context in Vision Transformers, the Conditional Position encodings Visual Transformer (CPVT) <ref type="bibr" target="#b8">[9]</ref> replaces the predefined positional embedding used in ViT with conditional position encodings (CPE), enabling Transformers to process input images of arbitrary size without interpolation. CvT <ref type="bibr" target="#b39">[40]</ref> and CCT <ref type="bibr" target="#b14">[15]</ref> introducing convolutions into the Vision Transformer architecture to merge the benefits of Transformers with the benefits of CNNs for image recognition tasks. LocalViT <ref type="bibr" target="#b24">[25]</ref> brings a locality mechanism to vision transformers by inserting introducing depth-wise convolutions into the MLP module. We also introduce convolution into the window-based vision transformer for enhancing the neighbor-window connections. Different from the others, we place the depth-wise convolution between the window-based multi-head selfattention and the MLP module.</p><p>Window-based Self-Attention The standard Transformer architecture <ref type="bibr" target="#b36">[37]</ref> and its adaptation for image classification <ref type="bibr" target="#b10">[11]</ref> both conduct global self-attention, where the relationships for each tokenpair are computed. The computation for dense relationships leads to quadratic complexity with respect to the number of tokens, making it unsuitable for the dense prediction tasks, such as semantic segmentation, inherently requires high-resolution images.</p><p>For efficient modeling, there are some works apply local/window constraints to self-attention, proposed by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8]</ref>, reduces the computation cost. These window-based self-attention based methods can be grouped into two categories: sliding-window based meth-  <ref type="figure">Figure 1</ref>: Spatial shuffle with two stacked window-based Transformer block. The MLP is omitted in the visualization because it does not affect the information interaction in the spatial dimension. WMSA stands for window-based multi-head self-attention. a) two stacked window-based Transformer blocks with the same window size. Each output token only relates to the tokens within the window. No cross-talk; b) tokens from different windows are fully related when WMSA2 takes data from different windows after WMSA1; c) an equivalent implementation to b) using spatial shuffle and alignment.</p><p>ods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7]</ref> and non-overlapping window-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19]</ref>. As mentioned in Swin <ref type="bibr" target="#b27">[28]</ref>, compared with non-overlapping window-based methods, sliding window-based selfattention approaches suffer from low latency on general hardware due to different key sets for different query pixels. However, non-overlapping window-based methods compute self-attention in the non-overlapping local window, which lacks cross-window connections. Swin <ref type="bibr" target="#b27">[28]</ref> alleviates this issue by shifting the window partition between consecutive self-attention layers. The proposed Shuffle Transformer also utilizes non-overlapping window-based self-attention and takes spatial shuffle to make information flow across windows.</p><p>Channel Shuffle and Spatial Shuffle Modern efficient convolutional neural networks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref> apply group or depth-wise convolutions to reduce the complexity, which meets the issue of channel sparse connections. To exchange information of channels from different groups, ShuffleNet shuffles the channel to make cross-group information flow for multiple group convolution layers.</p><p>Following that, Spatially Shuffled Convolution <ref type="bibr" target="#b23">[24]</ref> incorporate the random spatial shuffle in the regular convolution. The most related work to ours is Interlaced Sparse Self-Attention (ISSA) <ref type="bibr" target="#b18">[19]</ref>, which apply the interlacing mechanism to decompose the dense affinity matrix within the selfattention mechanism with the product of two sparse affinity matrices. However, the number of windows is fixed in ISSA, which also leads to quadratic complexity with respect to the input size. We fix the window size rather than the number of windows, which results in linear computational complexity. Besides, the motivations are also different. We adapt spatial shuffle for window-based vision transformers to bridge the connection among the non-overlapping windows. ISSA uses two sparse self-attention to mimic the global self-attention for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Shuffle Transformer</head><p>In this section, we start from the standard window-based multi-head self-attention. To build longrange cross-window talks, the spatial shuffle is proposed. The neighbor-window connection module is used to enhance the connections of neighborhood windows. Then, we integrate the spatial shuffle and neighbor-window connection module into the shuffle transformer block for building rich crosswindow connections. Finally, the overall network architecture with its variants is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Window-based Multi-head Self-Attention</head><p>For efficient modeling, there are some works that apply local constraints to self-attention, proposed by <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28]</ref>, reduces the computation cost. Window-based Multi-head Self-Attention (WMSA) Compared with a global MSA module, the WMSA module needs a window partition operation before computing self-attention and a windows-to-image operation after computing self-attention. However, the computation cost of the additional operations is negligible in practical implementation. Except for the window partition, WMSA shares the same structure with the global MSA module.</p><formula xml:id="formula_0">Norm WMSA Norm MLP NWC Norm Shuffle WMSA Norm MLP NWC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Shuffle for Cross-Window Connections</head><p>Although the window-based multi-head self-attention is computation friendly. However, the image is divided into non-overlapping sub-windows. If multiple window-based self-attention modules stack together, there is one side effect: the receptive field is limited within the window, which has an adverse effect on tasks such as segmentation, especially on high-resolution inputs. <ref type="figure">Fig 1(a)</ref> illustrates a situation of two stacked window-based self-attention. It is clear that outputs from a certain window only relate to the inputs within the window. This property blocks information flow among windows and weakens representation.</p><p>To address the issue, a straightforward solution is to allow the second window-based self-attention to obtain input data from different windows (as shown in <ref type="figure">Fig 1(b)</ref>), the tokens in different windows will be related. Inspired by ShuffleNet <ref type="bibr" target="#b44">[45]</ref>, this can be efficiently and elegantly implemented by a spatial shuffle operation <ref type="figure">(Fig 1 (c)</ref>). Without loss of generality, input is assumed as the 1D sequence. Suppose a Window-based Self-Attention with window size M whose input has N tokens; we first reshape the output spatial dimension into (M, N M ), transposing and then flattening it back as the input of the next layer. This kind of operation puts the tokens from distant windows together and helps to build long-range cross-window connections. Different from channel shuffle, spatial shuffle needs the spatial alignment operation to adjust the spatial tokens into the original positions for aligning features and image content spatially. The spatial alignment operation first reshapes the output spatial dimension into ( N M , M ), transposing and then flattening it, which is an inverse process of the spatial shuffle.</p><p>Considering there are always the window partition operation and windows-to-image operation before and after computing self-attention, we could merge the window partition operation with the spatial shuffle operation, windows-to-image operation with the spatial alignment operation, thus the spatial shuffle operation and spatial alignment do not bring extra computation and are easy to implement by modifying two lines of code. Moreover, spatial shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neighbor-Window Connection Enhancement</head><p>Bringing the spatial shuffle into the window-based multi-head self-attention could build cross-window connections, especially long-range cross-window. However, there is an underlying apprehension when processing a high-resolution image. The "grid issue" may arise when image size is far greater than the window size.  Fortunately, there are several approaches to this issue by enhancing the neighbor-window connections. 1) scale the window size <ref type="bibr" target="#b35">[36]</ref>. 2) cooperate with shifted window <ref type="bibr" target="#b27">[28]</ref>. 3) introduce convolution to shuffle transformer block <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>. Considering the efficiency, we insert a depth-wise convolution layer with a residual connection between the WMSA module and the MLP module. The kernel size of the depth-wise convolution layer is the same as the window size. This operator could strengthen the information flow among nearby windows and alleviate the "grid issue". To find a better place for a depth-wise convolution layer, we conduct the ablation studies, as shown in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Shuffle Transformer Block</head><p>The Shuffle Transformer Block consists of the Shuffle Multi-Head Self-Attention module (Shuffle-MHSA), the Neighbor-Window Connection module (NWC), and the MLP module. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a strategy which alternates between WMSA and Shuffle-WMSA in consecutive Shuffle Transformer blocks. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The first window-based transformer block uses regular window partition strategy and the second window-based transformer block uses window-based selfattention with spatial shuffle. Besides, the Neighbor-Window Connection moduel (NWC) is added into each block for enhancing connections among neighborhood windows. Thus the proposed shuffle transformer block could build rich cross-window connections and augments representation. Finally, the consecutive Shuffle Transformer blocks are computed as</p><formula xml:id="formula_1">x l = WMSA(BN(z l?1 )) + z l?1 , y l = NWC(x l ) + x l , z l = MLP(BN(y l )) + y l , x l+1 = Shuffle-WMSA(BN(z l )) + z l , y l+1 = NWC(x l+1 ) + x l+1 , z l+1 = MLP(BN(y l+1 )) + y l+1 .</formula><p>where z l , y l and z l denote the output features of the (Shuffle-)WMSA module, the Neighbor-Window Connection module and the MLP module for block l, respectively; WMSA and Shuffle-WMSA denote window-based multi-head self-attention without/with spatial shuffle, respectively. To better handle 2D input, we adapt the standard transformer block by replacing the Layernorm <ref type="bibr" target="#b0">[1]</ref> with Batchnorm <ref type="bibr" target="#b22">[23]</ref>. Meanwhile, the Linear layer in Shuffle WMSA and MLP is changed to a convolutional layer with kernel size 1 ? 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Architecture and Variants</head><p>An overview of the Shuffle Transformer architecture is presented in <ref type="figure" target="#fig_2">Figure 3</ref>, which illustrates the tiny version (Shuffle-T). The Shuffle Transformer consists of a token embedding layer and several shuffle transformer blocks and token merging layers. In our implementation, we use two stacked convolution layers as token embedding layers. To produce a hierarchical representation, we use a convolutional layer with kernel size 2 ? 2 and stride 2 as the token merging layer to reduce the number of tokens.</p><p>For a fair comparison, we follow the settings of swin <ref type="bibr" target="#b27">[28]</ref>. We build our base model, called Shuffle-B, to have the model size and computation complexity similar to Swin-B/ViTB/DeiT-B. We also  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To showcase the effectiveness of our approach, we conduct comprehensive experiments on three different tasks: ImageNet-1K image classification <ref type="bibr" target="#b9">[10]</ref>, ADE20K semantic segmentation <ref type="bibr" target="#b46">[47]</ref> and COCO instance segmentation <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification on ImageNet-1K</head><p>Settings For image classification, we evaluate the proposed Shuffle Transformer on ImageNet-1K <ref type="bibr" target="#b9">[10]</ref>, which contains 1.28M training images and 50K validation images from 1, 000 classes. We follow <ref type="bibr" target="#b27">[28]</ref> to adopt regular 1K setting and report the top-1 accuracy on a single crop. To be specific, we employ an AdamW <ref type="bibr" target="#b28">[29]</ref> optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1, 024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We also utilize the same augmentation and regularization strategies as shown in <ref type="bibr" target="#b27">[28]</ref> for a fair comparison.  trade-off. Meanwhile, when compared to existing Transformer-based approaches, e.g. Swin <ref type="bibr" target="#b27">[28]</ref>, our Shuffle Transformer stably outperforms the counterparts with similar computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Segmentation on ADE20K</head><p>Settings ADE20K <ref type="bibr" target="#b46">[47]</ref> is a challenging scene parsing dataset for semantic segmentation. There are 150 classes and diverse scenes with 1, 038 image level labels. The dataset is divided into three sets (20, 210 / 2, 000 / 3, 352) for training, validation and testing.</p><p>For fair comparison with the recent state-of-the-art model Swin <ref type="bibr" target="#b27">[28]</ref>, we follow it to adopt Uper-Net <ref type="bibr" target="#b40">[41]</ref> as the base framework for training. The AdamW <ref type="bibr" target="#b28">[29]</ref> optimizer with an initial learning rate of 6 ? 10 ?5 and a weight decay of 0.01 is used. We also utilize the warm-up during the first 1, 500 iterations. All models are trained for 160K iterations with a batch size of 16. Data augmentation contains random horizontal flip, random resizing with a scale range of [0.5, 2.0], and random cropping with a crop size of 512 ? 512. For quantitative evaluation, the mean of class-wise intersection-overunion (mIoU) is used for accuracy comparison, and the number of float-point operations (FLOPs) and frames per second (FPS) are adopted for speed comparison. Results of both single-scale and multi-scale testing are reported with the scaling factor ranging from 0.5 to 1.75.</p><p>Results As shown in <ref type="table" target="#tab_3">Table 2</ref>, we prepare different variants of the Shuffle Transformer for detailed comparison with Swin <ref type="bibr" target="#b27">[28]</ref>. The results show that our three models (Shuffle-T/S/B) consistently achieve better mIoU performance than Swin with comparable inference speed. To be specific, under multi-scale testing, Shuffle-T outperforms Swin-T by 1.4% mIoU, Shuffle-B achieves new stateof-the-art result 50.5% mIoU which outperforms Swin-B by 0.8% mIoU. Shuffle-S also achieves comparable performance to Swin-S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Instance Segmentation on COCO</head><p>Settings Experiments of object detection and instance segmentation are conducted on COCO 2017 dataset <ref type="bibr" target="#b26">[27]</ref>, which contains 118K training, 5K validation and 41K test images. We follow <ref type="bibr" target="#b27">[28]</ref> to evaluate the performance of our method based on Mask R-CNN and Cascade Mask R-CNN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To be specific, we replace the backbones of these detectors with our shuffle transformer blocks. All the models are trained under the same setting as in <ref type="bibr" target="#b27">[28]</ref>: multi-scale training <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> (resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1, 333), AdamW <ref type="bibr" target="#b28">[29]</ref> optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and 3? schedule (36 epochs).  Results As shown in <ref type="table" target="#tab_4">Table 3</ref>, we compare our model to standard ConvNets, e.g. ResNe(X)t <ref type="bibr" target="#b41">[42]</ref>, as well as existing Transformer networks, e.g. Swin <ref type="bibr" target="#b27">[28]</ref>, DeiT <ref type="bibr" target="#b34">[35]</ref> and PVT-Medium <ref type="bibr" target="#b38">[39]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We also perform comprehensive studies on the effectiveness of different design modules in our proposed Shuffle Transformer (Shuffle-T), using ImageNet-1K image classification and UperNet on  ADE20K semantic segmentation. To be specific, we take the following aspects into consideration, i.e., the design of spatial shuffle operation, and the usage of the neighbor-window connection at different positions of the transformer block.</p><p>The effect of the spatial shuffle &amp; neighbor-window connection Here, we discuss the influence of the spatial shuffle operation and the neighbor-window connection. Ablations of the spatial shuffle operation and the neighbor-window connection on the two tasks are reported in <ref type="table" target="#tab_5">Table 4</ref>. Shuffle-T with the spatial shuffle outperforms the counterpart built on a regular window-based multi-head self-attention used at each stage by +1.3% top-1 accuracy on ImageNet-1K, +2.1 mIoU on ADE20K.</p><p>The results indicate the effectiveness of using the spatial shuffle to build connections among windows in the preceding layers. Besides, adding the neighbor-window connection can further bring +1.0 % top-1 accuracy on ImageNet-1K, +2.8 mIoU on ADE20K. The results indicate the effectiveness of using the neighbor-window connection to enhance the connection among neighborhood windows.</p><p>The effect of the way to spatial shuffle Here, we discuss the influence of the way to spatial shuffle. Three kinds of spatial shuffles will be discussed. 1) long-range spatial shuffle is introduced in subsection 3.2. 2) Short-range spatial shuffle reshapes the output spatial dimension into ( N 2M , M, 2), transposing and then flattening it. 3) random spatial shuffle will randomly shuffle the spatial dimension. As shown in <ref type="table" target="#tab_7">Table 5</ref>, the long-range spatial shuffle achieves best performance on both image classification and segmentation tasks, which demonstrates the effectiveness of long-range spatial shuffle. And surprisingly, the random spatial shuffle also can achieve comparable performance.</p><p>The effect of the position to insert the neighbor-window connection We discuss the influence of inserting the neighbor-window connection at different positions of the transformer block. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, there are three positions A, B, and C used to place the neighbor-window connection (NWC), a depth-wise convolution layer with a residual connection. The results indicate that inserting the neighbor-window connection between the shuffle WMSA and the MLP bock achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented a Shuffle Transformer for a number of visual tasks, ranging from image-level classification to pixel-level semantic/instance segmentation and object detection. For efficient modeling, we use the window-based multi-head self-attention which computes self-attention within the non-overlapping windows. To build cross-window connections, we introduce spatial shuffle into window-based multi-head self-attention. Meanwhile, to enhance the neighbor-window connections, we introduce a depth-wise convolutional layer with a residual connection into the Shuffle Transformer Block. Finally, with the help of successive Shuffle Transformer Block, the proposed Shuffle Transformer could make information flow across all windows. Extensive experiments show that both of our proposed architectures perform favorably against other state-of-the-art vision transformers with similar computational complexity. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two successive Shuffle Transformer Block. The WMSA and Shuffle WMSA are windowbased multi-head self attention without/with spatial shuffle, respectively. is proposed to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner. Supposing each window contains M ? M tokens, the computational complexity of a global MSA module and a window-based one on an input of H ? W tokens with dimension C are O(H 2 W 2 C) and O(M 2 HW C), respectively. Thus, the Window-based Multi-head Self-Attention is significantly more efficient when M H and M W and grows linearly with HW if M is fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of a Shuffle Transformer (Shuffle-T).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>introduce</head><label></label><figDesc>Shuffle-T and Shuffle-S, which are similar to Swin-T and Swin-S, respectively. The window size is set to M = 7 by default. The query dimension of each head is d = 32, and the expansion layer of each MLP is ? = 4, for all experiments. The architecture hyper-parameters of these model variants are: Shuffle-T: C = 96, layer numbers = {2, 2, 6, 2}. Shuffle-S: C = 96, layer numbers = {2, 2, 18, 2}. Shuffle-B: C = 128, layer numbers = {2, 2, 18, 2}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Left: Visualization of three different positions to insert the neighbor-window connection. A: before the shuffle WMSA; B: after the residual connection of the shuffle WMSA; C: inside the MLP block. Right: Ablation study on the effect of the neighbor-window connection inserted at different positions, where A, B and C refer to three positions depicted left, and w/o NWC means no neighbor-window connection is inserted. FLOPs is measured on 224 ? 224 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different backbones on ImageNet-1K classification. Throughput is measured with the batch size of 192 on a single V100 GPU. All models are trained and evaluated on 224 ? 224 resolution.</figDesc><table><row><cell>Method</cell><cell cols="4">Params GFLOPs Throughput (Images / s) Top-1 (%)</cell></row><row><cell></cell><cell></cell><cell>ConvNet</cell><cell></cell><cell></cell></row><row><cell>ReGNetY-4G [30]</cell><cell>21M</cell><cell>4.0</cell><cell>1157</cell><cell>80.0</cell></row><row><cell>ReGNetY-8G [30]</cell><cell>39M</cell><cell>8.0</cell><cell>592</cell><cell>81.7</cell></row><row><cell>ReGNetY-16G [30]</cell><cell>84M</cell><cell>16.0</cell><cell>335</cell><cell>82.9</cell></row><row><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell></cell></row><row><cell>DeiT-S/16 [35]</cell><cell>22M</cell><cell>4.6</cell><cell>437</cell><cell>79.9</cell></row><row><cell>CrossViT-S [5]</cell><cell>27M</cell><cell>5.6</cell><cell>-</cell><cell>81.0</cell></row><row><cell>T2T-ViT-14 [43]</cell><cell>22M</cell><cell>5.2</cell><cell>-</cell><cell>81.5</cell></row><row><cell>TNT-S [14]</cell><cell>24M</cell><cell>5.2</cell><cell>-</cell><cell>81.3</cell></row><row><cell>PVT-Small [39]</cell><cell>25M</cell><cell>3.8</cell><cell>820</cell><cell>79.8</cell></row><row><cell>Swin-T [28]</cell><cell>29M</cell><cell>4.5</cell><cell>766</cell><cell>81.3</cell></row><row><cell>Shuffle-T (ours)</cell><cell>29M</cell><cell>4.6</cell><cell>791</cell><cell>82.5</cell></row><row><cell>T2T-ViT-19 [43]</cell><cell>39M</cell><cell>8.9</cell><cell>-</cell><cell>81.9</cell></row><row><cell>PVT-Medium [39]</cell><cell>44M</cell><cell>6.7</cell><cell>526</cell><cell>81.2</cell></row><row><cell>Swin-S [28]</cell><cell>50M</cell><cell>8.7</cell><cell>444</cell><cell>83.0</cell></row><row><cell>Shuffle-S (ours)</cell><cell>50M</cell><cell>8.9</cell><cell>450</cell><cell>83.5</cell></row><row><cell>ViT-B/16 [11]</cell><cell>87M</cell><cell>17.6</cell><cell>86</cell><cell>77.9</cell></row><row><cell>DeiT-B/16 [35]</cell><cell>87M</cell><cell>17.6</cell><cell>292</cell><cell>81.8</cell></row><row><cell>T2T-ViT-24 [43]</cell><cell>64M</cell><cell>14.1</cell><cell>-</cell><cell>82.3</cell></row><row><cell>CrossViT-B [5]</cell><cell>105M</cell><cell>21.2</cell><cell>-</cell><cell>82.2</cell></row><row><cell>TNT-B [14]</cell><cell>66M</cell><cell>14.1</cell><cell>-</cell><cell>82.8</cell></row><row><cell>PVT-Large [39]</cell><cell>61M</cell><cell>9.8</cell><cell>367</cell><cell>81.7</cell></row><row><cell>Swin-B [28]</cell><cell>88M</cell><cell>15.4</cell><cell>275</cell><cell>83.3</cell></row><row><cell>Shuffle-B (ours)</cell><cell>88M</cell><cell>15.6</cell><cell>279</cell><cell>84.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>presents detailed comparisons between our Shuffle Transformer and other backbones used for the training of ImageNet classification, including both ConvNet-based and Transformer- based approaches. All models are trained and evaluated on 224 ? 224 resolution. Compared to the state-of-the-art ConvNets, i.e. RegNet [30], our Shuffle Transformer achieves a better speed-accuracy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of semantic segmentation on the ADE20K validation set. ? indicates that the model is pretrained on ImageNet-22K. FLOPs is measured on 1024 ? 1024 resolution. * indicates the FPS reproduced by us and is measured on 512 ? 512 resolution.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">Params GFLOPs FPS mIoU / MS mIoU (%)</cell></row><row><cell>DANet [12]</cell><cell>ResNet-101</cell><cell>69M</cell><cell>1119</cell><cell>15.2</cell><cell>43.6 / 45.2</cell></row><row><cell>CCNet [20]</cell><cell>ResNet-101</cell><cell>64M</cell><cell>981</cell><cell>15.0</cell><cell>44.3 / 45.7</cell></row><row><cell>DeepLabv3+ [6]</cell><cell>ResNet-101</cell><cell>63M</cell><cell>1021</cell><cell>16.0</cell><cell>45.5 / 46.4</cell></row><row><cell>AlignSeg [22]</cell><cell>ResNet-101</cell><cell>67M</cell><cell>956</cell><cell>20.0</cell><cell>44.7 / 46.0</cell></row><row><cell>OCRNet [44]</cell><cell>ResNet-101</cell><cell>56M</cell><cell>923</cell><cell>19.3</cell><cell>-/ 45.3</cell></row><row><cell>UperNet [41]</cell><cell>ResNet-101</cell><cell>86M</cell><cell>1029</cell><cell>20.1</cell><cell>43.8 / 44.9</cell></row><row><cell>OCRNet [44]</cell><cell>HRNet-w48</cell><cell>71M</cell><cell>664</cell><cell>12.5</cell><cell>-/ 45.7</cell></row><row><cell>DeepLabv3+ [6]</cell><cell>ResNeSt-101</cell><cell>66M</cell><cell>1051</cell><cell>11.9</cell><cell>-/ 46.9</cell></row><row><cell>SETR [46]</cell><cell>ViT-Large  ?</cell><cell>308M</cell><cell>-</cell><cell>-</cell><cell>48.6 / 50.3</cell></row><row><cell>UperNet</cell><cell>Swin-T [28] Shuffle-T (ours)</cell><cell>60M 60M</cell><cell>945 949</cell><cell>28.6  *  30.1  *</cell><cell>44.5 / 45.8 46.6/47.6</cell></row><row><cell>UperNet</cell><cell>Swin-S [28] Shuffle-S (ours)</cell><cell>81M 81M</cell><cell>1038 1044</cell><cell>21.9  *  22.6  *</cell><cell>47.6 / 49.5 48.4/49.6</cell></row><row><cell>UperNet</cell><cell cols="2">Swin-B [28] Shuffle-B (ours) 121M 121M</cell><cell>1188 1196</cell><cell>19.9  *  21.4  *</cell><cell>48.1 / 49.7 49.0/50.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Object detection and instance segmentation performance on the COCO val2017 dataset using the Mask R-CNN and Cascade Mask R-CNN framework. FLOPs is evaluated on 1280 ? 800 resolution.</figDesc><table><row><cell>Backbone</cell><cell cols="2">AP b AP b 50</cell><cell>AP b 75</cell><cell cols="2">AP m AP m 50</cell><cell>AP m 75</cell><cell cols="2">Params GFLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mask R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet50 [17]</cell><cell>41.0</cell><cell>61.7</cell><cell>44.9</cell><cell>37.1</cell><cell>58.4</cell><cell>40.1</cell><cell>44M</cell><cell>260</cell></row><row><cell>PVT-Small [39]</cell><cell>43.0</cell><cell>65.3</cell><cell>46.9</cell><cell>39.9</cell><cell>62.5</cell><cell>42.8</cell><cell>44M</cell><cell>245</cell></row><row><cell>Swin-T [28]</cell><cell>46.0</cell><cell>68.2</cell><cell>50.2</cell><cell>41.6</cell><cell>65.1</cell><cell>44.8</cell><cell>48M</cell><cell>264</cell></row><row><cell>Shuffle-T(ours)</cell><cell cols="6">46.8 68.9 51.5 42.3 66.0 45.6</cell><cell>48M</cell><cell>268</cell></row><row><cell>ResNet101 [17]</cell><cell>42.8</cell><cell>63.2</cell><cell>47.1</cell><cell>38.5</cell><cell>60.1</cell><cell>41.3</cell><cell>63M</cell><cell>336</cell></row><row><cell>PVT-Medium [39]</cell><cell>44.2</cell><cell>66.0</cell><cell>48.2</cell><cell>40.5</cell><cell>63.1</cell><cell>43.5</cell><cell>64M</cell><cell>302</cell></row><row><cell>Swin-S [28]</cell><cell cols="4">48.5 70.2 53.5 43.3</cell><cell>67.3</cell><cell>46.6</cell><cell>69M</cell><cell>354</cell></row><row><cell>Shuffle-S(ours)</cell><cell>48.4</cell><cell>70.1</cell><cell cols="4">53.5 43.3 67.3 46.7</cell><cell>69M</cell><cell>359</cell></row><row><cell></cell><cell></cell><cell cols="4">Cascade Mask R-CNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-S [35]</cell><cell>48.0</cell><cell>67.2</cell><cell>51.7</cell><cell>41.4</cell><cell>64.2</cell><cell>44.3</cell><cell>80M</cell><cell>889</cell></row><row><cell>ResNet50 [17]</cell><cell>46.3</cell><cell>64.3</cell><cell>50.5</cell><cell>40.1</cell><cell>61.7</cell><cell>43.4</cell><cell>82M</cell><cell>739</cell></row><row><cell>Swin-T [28]</cell><cell>50.5</cell><cell>69.3</cell><cell>54.9</cell><cell>43.7</cell><cell>66.6</cell><cell>47.1</cell><cell>86M</cell><cell>745</cell></row><row><cell>Shuffle-T(ours)</cell><cell cols="6">50.8 69.6 55.1 44.1 66.9 48.0</cell><cell>86M</cell><cell>746</cell></row><row><cell cols="2">ResNext101-32 [42] 48.1</cell><cell>66.5</cell><cell>52.4</cell><cell>41.6</cell><cell>63.9</cell><cell>45.2</cell><cell>101M</cell><cell>819</cell></row><row><cell>Swin-S [28]</cell><cell>51.8</cell><cell>70.4</cell><cell>56.3</cell><cell cols="3">44.7 67.9 48.5</cell><cell>107M</cell><cell>838</cell></row><row><cell>Shuffle-S(ours)</cell><cell cols="6">51.9 70.9 56.4 44.9 67.8 48.6</cell><cell>107M</cell><cell>844</cell></row><row><cell cols="2">ResNext101-64 [42] 48.3</cell><cell>66.4</cell><cell>52.3</cell><cell>41.7</cell><cell>64.0</cell><cell>45.1</cell><cell>140M</cell><cell>972</cell></row><row><cell>Swin-B [28]</cell><cell>51.9</cell><cell>70.9</cell><cell>56.5</cell><cell>45.0</cell><cell>68.4</cell><cell>48.7</cell><cell>145M</cell><cell>982</cell></row><row><cell>Shuffle-B(ours)</cell><cell cols="6">52.2 71.3 57.0 45.3 68.5 48.9</cell><cell>145M</cell><cell>989</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the effect of spatial shuffle and the neighbor-window connection on two benchmarks, FLOPs is measured on 224 ? 224 resolution.</figDesc><table><row><cell>Method</cell><cell cols="4">Params GFLOPs ImageNet Top-1 (%) ADE20K mIoU (%)</cell></row><row><cell>w/o shuffle</cell><cell>28M</cell><cell>4.5</cell><cell>80.2</cell><cell>41.7</cell></row><row><cell>w/ shuffle</cell><cell>28M</cell><cell>4.5</cell><cell>81.5</cell><cell>43.8</cell></row><row><cell>w/ shuffle &amp; NWC</cell><cell>29M</cell><cell>4.6</cell><cell>82.5</cell><cell>46.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Compared with Swin Transformer, Shuffle Transformer achieve comparable results on all metrics. For comparison with the Cascade Mask R-CNN framework, Shuffle Transformer could stably outperform the other networks in the aspects of AP b and AP m . The results indicate the effectiveness of the proposed Shuffle Transformer on the tasks of object detection and instance segmentation.</figDesc><table><row><cell>For</cell></row><row><cell>comparison with the Mask R-CNN framework, Shuffle-T and Shuffle-S surpass ConvNets based</cell></row><row><cell>methods and DeiT [35] and PVT-Medium [39] by a large margin with comparable parameters size and</cell></row><row><cell>GFLOPs. Besides,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on different ways to spatial shuffle on two benchmarks.</figDesc><table><row><cell>Method</cell><cell cols="2">ImageNet Top-1 (%) ADE20K mIoU (%)</cell></row><row><cell>long-range spatial shuffle</cell><cell>82.5</cell><cell>46.6</cell></row><row><cell>short-range spatial shuffle</cell><cell>81.9</cell><cell>45.2</cell></row><row><cell>random spatial shuffle</cell><cell>82.0</cell><cell>45.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150,2020.1</idno>
		<title level="m">The long-document transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05704</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12273</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Alignseg: Feature-aligned segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Incorporating horizontal connections in convolution by spatial shuffling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikki</forename><surname>Kishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkgODpVFDr,2020.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12450</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Axialdeeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
