<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<email>iamtingchen@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
							<email>ruixiangz@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
							<email>geoffhinton@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and IMAGENET 64?64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State-of-the-art generative models for discrete data, such as discrete images and text, are based on autoregressive modeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b60">60]</ref>, where the networks, often Transformers <ref type="bibr" target="#b61">[61]</ref>, are trained to predict each token given its preceding ones in a sequential manner or with causal attention masks. One major drawback of such approaches is that they typically require computation and memory that is quadratic to the dimension of data (e.g., sequence length or image size), leading to difficulties in modeling large images or sequences. Another drawback is that, during generation, autoregressive models generate one token at a time so the total number of sequential sampling steps is often the same as the dimension of data, making it slow in generating large images or long sequences.</p><p>In contrast, diffusion models <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref>, or score-based generative models <ref type="bibr" target="#b52">[52]</ref><ref type="bibr" target="#b53">[53]</ref><ref type="bibr" target="#b54">[54]</ref>, can model much higher dimensional data without running into computation and memory issues. During generation, diffusion models iteratively refine samples with a high degree of parallelism, so the total number of sequential sampling steps can be much less than the dimension of data. However, state-of-the-art diffusion models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b46">46]</ref> can only generate continuous data (mainly real valued pixels), and have not yet achieved results competitive competitive with autoregressive models in generating discrete/categorical data, such as generating discrete/categorical images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">26]</ref>.</p><p>In this work, we propose a simple and generic approach for enabling continuous state diffusion models to generate discrete data. The key ingredient in our approach is analog bits: real numbers used to model the bits that represent the discrete data. Analog bits can be directly modeled by continuous state diffusion models, without requiring a discrete state space or re-formulation of the continuous ? Work done as a student researcher at Google.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint.</head><p>arXiv:2208.04202v1 [cs.CV] 8 Aug 2022 diffusion process. At sampling time, the generated analog bits can be decoded into discrete variables by a simple thresholding operation. Our approach, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, is based on the following high-level conjecture. With strong continuous generative models (diffusion models in particular), it should not be too difficult to generate highly concentrated bimodal data where each real-valued analog bit is close to a binary bit. To reduce the prediction loss (such as negative log likelihood), the network has to model structures among analog bits that can actually lead to meaningful discrete variables after thresholding.</p><p>Besides analog bits, we further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals that greatly improve the sample quality. We evaluate the proposed approach on both discrete image generation, and image-conditional text / caption generation. On discrete CIFAR-10 and IMAGENET 64?64, the proposed Bit Diffusion model significantly improves both existing discrete diffusion models but also the best autoregressive model. For example, on categorical CIFAR-10, the best autoregressive model <ref type="bibr" target="#b27">[27]</ref> obtains a FID of 12.75, while our model (with 1 /3 of the model size of the autoregressive model) achieves a much better 6.93. For image captioning on MS-COCO dataset, our model achieves a result competitive with a strong autoregressive captioner based on a Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Preliminaries We start with a short introduction to diffusion models <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b54">54]</ref>. Diffusion models learn a series of state transitions to map noise from a known prior distribution to x 0 from the data distribution. To learn this (reverse) transition from the noise distribution to the data distribution, a forward transition from x 0 to x t is first defined:</p><formula xml:id="formula_0">x t = ?(t) x 0 + 1 ? ?(t) ,<label>(1)</label></formula><p>where ? N (0, I), t ? U(0, T ), and ?(t) is a monotonically decreasing function from 1 to 0. Instead of directly learning a neural net to model the transition from x t to x t?? , one can learn a neural net f (x t , t) to predict x 0 (or ) from x t , and estimate x t?? from x t and estimatedx 0 (or? ).</p><p>This training of f (x t , t) is based on denoising with a 2 regression loss:</p><formula xml:id="formula_1">L x0 = E t?U (0,T ), ?N (0,1) f ( ?(t) x 0 + 1 ? ?(t) , t) ? x 0 2 .<label>(2)</label></formula><p>To generate samples from a learned model, it follows a series of (reverse) state transition x T ? x T ?? ? ? ? ? ? x 0 . This can be achieved by iteratively applying denoising function f on each state x t to estimate x 0 , and then make a transition to x t?? with the estimatedx 0 (using transition rules such as those specified in DDPM <ref type="bibr" target="#b23">[23]</ref> or DDIM <ref type="bibr" target="#b51">[51]</ref>). Note that state transitions in these diffusion models assume a continuous data space and state space. Therefore, one cannot directly apply it to model and generate discrete/categorical data.</p><p>Analog Bits A discrete data variable from an alphabet of size K can be represented using n = log 2 K bits, as {0, 1} n . Due to the discreteness, existing work has to re-formulate continuous diffusion models by adopting a discrete data space and state space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b50">50]</ref>. In contrast, we propose to simply cast the binary bits {0, 1} n into real numbers R n for the continuous diffusion models 1 . We term these real numbers analog bits since they learn to share the same bimodal values as binary bits but are modeled as real numbers. To draw samples, we follow the same procedure as sampling in a continuous diffusion model, except that we apply a quantization operation at the end by simply thresholding the generated analog bits. This yields binary bits which can be then converted into original discrete/categorical variables. Notably, there is no hard constraint to force the model to</p><formula xml:id="formula_2">? ? ? x t+?</formula><p>x t x t?? ? ? ? x 0x 0 (a) Standard reverse diffusion steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ? ?</head><p>x t+?</p><p>x t x t?? ? ? ? x 0x 0 (b) Self-Conditioning on the previous x0 estimate. <ref type="figure">Figure 2</ref>: An illustration of reverse diffusion sampling steps (a) without or (b) with Self-Conditioning.</p><p>x 0 denotes the estimation of data sample by the denoising network f at a sampling step. We propose to condition the network directly on its previously generated/estimated samples. generate exact binary bits, but we expect a strong continuous generative model to generate real numbers that exhibit very clear bimodal concentrations and this is what happens in our experiments.</p><p>For simplicity, we use the same regression loss function (Eq. 2) for modeling analog bits. However, it is possible to use other loss functions such as the cross entropy loss. We also note that the binary encoding mechanism for constructing analog bits is extensible as well (e.g., one-hot encoding instead of base-2 bits). Extensions of loss functions and binary encoding are described in the appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Conditioning</head><p>Conditioning is a useful technique for improving diffusion models <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b39">39]</ref>. However, a typical conditioning variable is either from some external sources, such as class labels <ref type="bibr" target="#b39">[39]</ref> or low-resolution images from another network <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b45">45]</ref>. Here we propose a technique for the model to directly condition on previously generated samples of its own during the iterative sampling process, which can significantly improve the sample quality of diffusion models.</p><p>In a typical diffusion sampling process, the model iteratively predicts x 0 (or ) in order to progress the chain of mapping noise into data. However, as shown in <ref type="figure">Figure 2a</ref>, the previously estimatedx 0 is simply discard when estimating x 0 from a new time step, i.e. the denoising function f (x t , t) does not directly depend on a previously estimatedx 0 . Here we consider a slightly different denoising function of f (x t ,x 0 , t) that also takes previous generated samples as its input, illustrated in <ref type="figure">Figure 2b</ref>. A simple implementation of Self-Conditioning is to concatenate x t with previously estimatedx 0 . Given thatx 0 is from the earlier prediction of the model in the sampling chain, this comes at a negligible extra cost during sampling. In order to train the denoising function f (x t ,x 0 , t), we make some small changes to the training. With some probability (e.g., 50%), we setx 0 = 0 which falls back to modeling without Self-Conditioning. At other times, we first estimatex 0 = f (x t , 0, t) and then use it for Self-Conditioning. Note that we do not backpropagate through the estimatedx 0 so the overall increase of training time is small (e.g., less than 25%).</p><p>Asymmetric Time Intervals Besides Self-Conditioning, we identify another factor, time step t, that can also impact Bit Diffusion models. Time step t is an integral part of both denoising network f (x t , t) as well as the state transitions. During a typical reverse diffusion process, the model takes symmetric time intervals (i.e., ? as in t ? t ? ?) for both the state transition and time reduction itself, resulting in the same/shared t for both arguments of f (x t , t). However, we find that, when taking large reverse steps, using asymmetric time intervals, implemented via a simple manipulation of time scheduling at generation, can lead to improved sampling quality for Bit Diffusion models.</p><p>More specially, with asymmetric time intervals during the sampling process, we have f (x t , t ), where t = t + ? and ? is a small non-negative time difference parameter. Note that training remains unchanged, and the same/shared t is used for both arguments of the f (x t , t). <ref type="figure">Figure 3</ref> illustrates the effect with a trained Bit Diffusion model, where it is asked to take two reversing steps from a stat? <ref type="figure">Figure 3</ref>: When taking a large reverse step from x t=0.6 to x t=0.1 in Bit Diffusion with maximum time T = 1.0, we see that asymmetric time intervals with a positive time difference ? improve the denoising quality of x t=0.1 (by reducing the number of noisy pixels).</p><formula xml:id="formula_3">x t=0.1 |x t=0.6 f (x t=0.1 , t =0.1) f (x t=0.1 , t =0.3) f (x t=0.1 , t =0.5) f (x t=0.1 , t =0.7)</formula><p>x t constructed using the forward diffusion, and it shows that asymmetric time intervals reduce the number of noisy pixels (after thresholding and converting back to discrete variables).</p><p>Putting it together Algorithm 1 and 2 summarize the training and sampling algorithms for the proposed Bit Diffusion model with Analog Bits, Self-Conditioning, and Asymmetric Time Intervals (via the td parameter). The proposed changes to the existing diffusion models are highlighted in blue. Note that unlike standard diffusion models <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b50">50]</ref>, we use a continuous time parameterization between 0 and 1 instead of a fixed discrete time for maximal flexibility but they perform similarly. More details of the algorithm (including some important functions) can be found in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We experiment with two different discrete data generation tasks, namely discrete/categorical image generation, and image captioning (image-conditional text generation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup and Implementation Details</head><p>Datasets We use CIFAR-10 <ref type="bibr" target="#b32">[32]</ref> and IMAGENET 64?64 <ref type="bibr" target="#b13">[14]</ref> 2 for image generation experiments. We adopt widely used FID <ref type="bibr" target="#b20">[20]</ref> as the main evaluation metric, and it is computed between 50K generated samples and the whole training set. For image captioning, following <ref type="bibr" target="#b10">[11]</ref>, we use MS-COCO 2017 captioning dataset <ref type="bibr" target="#b35">[35]</ref>.</p><p>Binary encoding Each pixel consists of 3 sub-pixels (RGB channels), and each sub-pixel is an integer in [0, 256) representing the intensity. Standard continuous generative models cast RGB channels as real numbers and normalize them in [?1, 1]. For discrete image generation, we consider three discrete encoding for sub-pixels, namely UINT8, GRAY CODE, and UINT8 (RAND). In UINT8, we use 8-bit binary codes converted from the corresponding sub-pixel integer in [0, 256). In GRAY CODE, we assign 8-bit binary codes uniquely to each sub-pixel integer such that two adjacent integers only differ by 1 bit. And in UINT8 (RAND), we assign 8-bit binary codes to every sub-pixel integer by randomly shuffling the integer-to-bits mapping in UINT8. The binary codes in UINT8 and GRAY CODE are loosely correlated with its original sub-pixel intensities, while UINT8 (RAND) has no correlation so each sub-pixel is a categorical variable. The details of the binary codes and their correlations with sub-pixel intensity can be found in the appendix C. We shift and scale the binary bits from 0, 1 to ?1, 1 for the analog bits.</p><p>For image captioning, we follow <ref type="bibr" target="#b10">[11]</ref>, and use sentencepiece <ref type="bibr" target="#b33">[33]</ref> with a vocabulary of size 32K to tokenize the captions. After tokenization, we encode each token into 15 analog bits using the binary codes converted from the corresponding integer. We set the maximum number of tokens to 64 so the total sequence length is 960 bits. Since we directly model bits, it is also possible to directly work with their byte representations without a tokenizer, but we leave this for future work.</p><p>Architecture We use the U-Net architecture <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b43">43]</ref> for image generation. For CIFAR-10, we use a single channel dimension of 256, 3 stages and 3 residual blocks <ref type="bibr" target="#b19">[19]</ref> per stage, with a total of 51M parameters. We only use dropout <ref type="bibr" target="#b55">[55]</ref> of 0.3 for continuous diffusion models on CIFAR-10. For IMAGENET 64?64, following <ref type="bibr" target="#b39">[39]</ref>, we use a base channel dimension of 192, multiplied by 1,2,3,4 in 4 stages and 3 residual blocks per stage, which account for a total of 240M parameters <ref type="bibr" target="#b2">3</ref> . For UINT8 (RAND) encoding, we find the following "softmax factorization" architectural tweak on the final output layer can lead to a better performance. Instead of using a linear output layer to predict analog bits directly, we first predict a probability distribution over 256 classes per sub-pixel (with each class corresponds to one of the 256 different 8-bit codes), and then map class distribution into analog bits by taking weighted average over all 256 different 8-bit codes.</p><p>For image captioning, we follow the architecture used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, with a pre-trained image encoder using the object detection task, for both autoregressive baseline as well as the proposed method. Both decoders are randomly initialized 6-layer Transformer <ref type="bibr" target="#b61">[61]</ref> decoder with 512 dimension per layer. For the autoregressive decoder, the token attention matrix is offset by the causal masks, but it is non-masked all-to-all attention for our Bit Diffusion.</p><p>Other settings We train our models with the Adam optimizer <ref type="bibr" target="#b28">[28]</ref>. For CIFAR-10, we train the model for 1.5M steps with a constant learning rate of 0.0001 and batch size of 128. For IMAGENET 64?64, we train the model for 500K steps with a constant learning rate of 0.0002 4 and batch size of 1024. For Bit Diffusion, we use Self-Conditioning by default, unless otherwise specified. We use an exponential moving average of the weights during training with a decay factor of 0.9999. For our best image generation results, we sweep over a few sampling hyper-parameters, such as sampler (DDIM vs DDPM), sampling steps in {100, 250, 400, 1000}, and time difference in {0., 0.01, 0.1, 0.2, 0.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discrete Image Generation</head><p>We compare our model against state-of-the-art generative models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b27">27]</ref> on generating discrete CIFAR-10 images in <ref type="table" target="#tab_0">Table 1</ref>. Our model achieves better results compared to both existing discrete diffusion models and the best autoregressive model. When compared to continuous diffusion models (i.e., DDPM), our Bit Diffusion models on UINT8 and GRAY CODE can achieve similar performance.  <ref type="figure">Figure 4</ref>: Class-conditional generations on continuous v.s. discrete ImageNet 64?64. Each row represents random samples conditioned on a class, and the classes are adopted from <ref type="bibr" target="#b39">[39]</ref>, namely, 9: ostrich, 11: goldfinch, 130: flamingo, 141: redshank, 154: pekinese, 157: papillon, 97: drake and 28: spotted salamander. More samples from random classes are shown in <ref type="figure" target="#fig_0">Figure 11</ref>.</p><p>Discrete generation of IMAGENET 64?64 is significantly harder than CIFAR-10, and we have not found other competing methods that report FIDs, so we only compare the proposed method against DDPM on continuous pixels. Results are shown in <ref type="table" target="#tab_1">Table 2</ref>. We find that the diffusion model on continuous pixels has the best FID while the diffusion model on UINT8 (RAND), i.e., categorical data, has the worst FID, indicating the increase of hardness when removing intensity/order information in sub-pixels. Note that, in these experiments, there is no extra model capacity to compensate for the loss of intensity/order information since the model sizes are the same. <ref type="figure">Figure 4</ref> shows generated images of different diffusion models on continuous and discrete IMAGENET 64?64. Despite the differences in FIDs, visually these samples look similar.</p><p>6     Ablation of Self-Conditioning <ref type="figure" target="#fig_5">Figure 5</ref> shows the effectiveness of the Self-Conditioning technique in both Bit Diffusion and continuous diffusion models. Note that the experiments are performed in three settings, namely CIFAR-10 with UINT8, CIFAR-10 with UINT8 (RAND), and IMAGENET 64?64 with continuous pixels, where the only difference for pairs in each setting is whether the Self-Conditioning is used. For CIFAR-10, we find that Self-Conditioning greatly improves the performance across different binary encodings. We also notice that for Bit Diffusion, predicting x 0 is much more effective than predicting . For IMAGENET 64?64, we find that the proposed Self-Conditioning also leads to improved FIDs for continuous diffusion (i.e., DDPM). Therefore, we conclude that Self-Conditioning by itself is a generic technique that can benefit diffusion models on both continuous and discrete data. <ref type="figure" target="#fig_6">Figure 6</ref> shows the FID on generated IMAGENET 64?64 samples as we vary the time difference parameter during the sampling process. We find that as the number of steps increases (from 100 to 400), the optimal time difference shrinks to 0. For 100 steps, a non-zero time difference leads to a significant improvement of FID. We also note that for Bit Diffusion on UINT8 (RAND), using 400 sampling steps actually leads to a drastically worse sample quality than using 100 steps. This is related to how the Self-Conditioning is applied and we present alternative Self-Conditioning sampling strategies in the Appendix F, some of which lead to improved FIDs at a cost of longer sampling time. <ref type="figure" target="#fig_8">Figure 7</ref> visualizes the distribution of generated analog bits from 64 generated images on IMAGENET 64?64. Although there is no hard constraint on the analog bits being binary / bimodal, the generated ones are highly concentrated on two modes, which makes the thresholding / quantization easy and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation of asymmetric time intervals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concentration of generated analog bits</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Captioning</head><p>We compare our Bit Diffusion model with an autoregressive Transformer baseline <ref type="bibr" target="#b10">[11]</ref>. As mentioned, both models have similar architectures, with an object detection pretrained <ref type="bibr" target="#b9">[10]</ref> image encoder, and  a randomly initialized Transformer <ref type="bibr" target="#b61">[61]</ref> decoder. <ref type="table" target="#tab_2">Table 3</ref> presents the main comparison. Overall, our model achieves similar performance as the autoregressive model. We find that generally it only needs about 10 steps for the model to achieve good results, despite that there are a total of maximum 960 bits for caption that the model has to model. We find that the asymmetric time intervals play an important role in the final performance of our model, as demonstrated in <ref type="table" target="#tab_3">Table 4</ref>, especially when sampling steps are fewer.   <ref type="table" target="#tab_4">Table 5</ref> provides some generated samples of our model when different inference steps are used. The model makes mistakes when the sampling steps are too few, and the mistakes may not always be interpretable due to that the model directly predicts the bits behind the tokenized word pieces and a small difference in bits can lead to total different words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steps=1: A group of diets in for foraa</head><p>Steps=2: A group of elephants in in a\ufffd.</p><p>Steps=5: A group of elephants standing in a Irish.</p><p>Steps=10: A group of elephants standing by a fence.</p><p>Steps=1: A says man fora You a\u0000.</p><p>Steps=2: A says man a skateboard' aa.</p><p>Steps=5: A man on a skateboard on a skateboard.</p><p>Steps=10: A man is doing a trick on a skateboard. <ref type="bibr" target="#b7">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Autoregressive models for discrete data Autoregressive models have demonstrated state-of-the-art results when it comes to generating discrete data. In particular, text generation, or language modeling, is dominated by autoregressive approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b56">56]</ref>. Autoregressive models are also applied to discrete/categorical image generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b60">60]</ref>, where they work well on small image resolutions. However, the computation cost and memory requirement increase drastically (typically in a quadratic relation) as the size of sequence or the image resolution increase, so it becomes very challenging to scale these approaches to data with large dimensions.</p><p>Diffusion models for discrete data State-of-the-art diffusion models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b46">46]</ref> cannot generate discrete or categorical data. Existing extensions of these continuous diffusion models to discrete data are based on both discrete data space and state space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b50">50]</ref>. Compared to discrete state space, continuous state space is more flexible and potentially more efficient. Our approach is also compatible with both discrete and continuous time, and does not require re-formulation of existing continuous models, thus it is simpler and can potentially be plugged into a broader family of generative models.</p><p>Another line of discrete diffusion models is based on the embedding of discrete data <ref type="bibr" target="#b34">[34]</ref>. One can also consider our binary encoding with analog bits as a simple fixed encoder, and the decoding / quantization of bimodal analog bits is easy and robust via a simple thresholding operation. In contrast, the quantization of real numbers in generated continuous embedding vectors may contain multiple modes per dimension, leading to potential difficulty in thresholding/quantization.</p><p>Normalizing Flows for discrete data Normalizing Flows <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b42">42]</ref> are a powerful family of generative models for high-dimensional continuous distributions based on some invertible mapping. However, straightforward application of flow-based models on categorical data is limited due to the inherent challenges on discrete support. Discrete flows <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b59">59]</ref> introduce invertible transformations of random variables in discrete space without the need of computing the log-determinant of Jacobian. Other works <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b57">57]</ref> introduce various embedding methods for transforming discrete data into continuous space with disjoint support, which can be interpreted as a variational inference problem <ref type="bibr" target="#b58">[58]</ref> with different dequantization distribution families. Several works <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b64">64]</ref> also explore normalizing flows on discrete data under the Variational Autoencoders <ref type="bibr" target="#b29">[29]</ref> framework by enriching the prior. Compared to our diffusion-based approach, these models suffer from strict invertible restrictions on network architecture, thus limiting their capacity.</p><p>Other generative models for discrete data Other generative models, such as Varational Autoencoders (VAE) <ref type="bibr" target="#b29">[29]</ref>, Generateive Adversarial Networks (GAN) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b62">62]</ref> have also been applied to generate discrete data. These methods have not yet achieved the level of performance as autoregressive models on tasks such as discrete image generation or text generation, in terms of sample quality or data likelihood. Potentially, the proposed analog bits can also be applied to these continuous generative models, by having the networks directly model and generate analog bits, but it is not explored in this work.</p><p>Other related work The proposed Self-Conditioning technique shares some similarities with selfmodulation in GANs <ref type="bibr" target="#b6">[7]</ref> (where the earlier latent state can directly modulate the later latent states) and step-unrolled denoising autoencoders <ref type="bibr" target="#b48">[48]</ref> (where an inference step is incorporated for denoising).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce a simple and generic technique that enables continuous state diffusion models to generate discrete data. The main idea is to encode discrete or categorical data into bits and then model these bits as real numbers that we call analog bits. We also propose two simple techniques, namely Self-Conditioning (i.e., condition the diffusion models directly on their previously generated samples) and Asymmetric Time Intervals, that lead to improved sample quality. We demonstrate that our approach leads to state-of-the-art results in discrete / categorical image generation, beating the best autoregressive model. In an image-conditional text generation task on MS-COCO dataset, we also achieve competitive results compared to autoregressive models. One limitation of our approach, similar to other existing diffusion models, is that they still require a significant number of inference steps for generating good (image) samples. However, we expect that future improvements from diffusion models for continuous data can also transfer to discrete data using analog bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9</head><p>A More Details of Algorithm 1 and 2 Algorithm 3 and 4 provide more detailed implementations of functions in Algorithm 1 and 2.</p><p>Algorithm 3 Binary encoding and decoding algorithms (in Tensorflow). An alternative binary encoding to the base-2 encoding of the discrete data used in the main paper, is the one-hot encoding, where a discrete variable is represented as a vector whose length is the same as the vocabulary size K, with a single slot being 1 and the rest being 0. The resulting one-hot vector can be similarly treated as analog bits and modeled by continuous state diffusion models. To obtain discrete variables corresponding to the generated analog bits, we use an arg max operation over all candidate categories, instead of the thresholding operation in base-2 analog bits. Note that the one-hot encoding requires K bits, which is less efficient compared to base-2 encoding that only requires log 2 K bits, especially for large K. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Sigmoid Cross Entropy Loss</head><p>As we use 2 loss by default for its simplicity and compatibility with continuous diffusion models. The proposed Bit Diffusion models can work with other loss functions too. Since the analog bits are bimodal, we can use the following sigmoid cross entropy loss:</p><formula xml:id="formula_4">L x0,xt,t = log ?(x 0 f (x t , t)),</formula><p>where we assume x 0 ? {?1, 1} n , and ? is a sigmoid function. During the sampling process, we use 2?(f (x t , t)) ? 1 as the output of denoising network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Softmax Cross Entropy Loss</head><p>For one-hot analog bits, one could also add a softmax activation function for the output of denosing network f , and use the following softmax cross entropy loss:</p><formula xml:id="formula_5">L x0,xt,t = x 0 log softmax(f (x t , t)),</formula><p>where we assume x 0 ? {0, 1} n which is the one-hot representation. <ref type="table" target="#tab_5">Table 6</ref> presents FIDs of Bit Diffusion models with different types of analog bits and loss functions on unconditional CIFAR-10. Note that it is possible some of these results can be improved by more tuning of hyper-parameters or tweaks of the network, but we do not focus on them in this work. In the main paper, we describe three different types of binary encodings of pixels. Here we provide additional detail on how we generate UINT8 (RAND): we first apply a random permutation to 256 sub-pixel values, and then assign the binary binary bits of permuted integers to the non-permuted integers. For example, assume 0 is mapped to 228 after the permutation, the analog bits of 0 would be the binary bits of 228. The random permutation is generated by numpy.random.seed <ref type="bibr" target="#b42">(42)</ref>; numpy.random.shuffle(numpy.arange(256)).     <ref type="figure" target="#fig_13">Figure 8</ref> show the correlation between Hamming distance of three different binary encodings we use and the (absolute) difference of sub-pixel intensity. This is done by taking every pair of subpixel integers (in [0, 256)), compute their absolute difference, as well as the Hamming distance between the corresponding binary bits. We find that both UINT8 and GRAY CODE exhibit partial correlation between the two quantities (with different correlation patterns), meaning that these codes partially contain the order information about the original sub-pixel intensity. However, UINT8 (RAND) exhibits no correlation between hamming distance and sub-pixel intensity, indicating the order information is fully removed, thus can be considered as categorical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Preliminary Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D A Toy Example on Continuous Modeling of Discrete Variables</head><p>An intuitive toy example of how a continuous generative model can generate binary data is given in <ref type="figure" target="#fig_15">Figure 9</ref>, where a mapping from prior distribution at x T to data distribution at x 0 is shown. With a deterministic sampler (such as DDIM), it is straight-forward how they can represent any Bernoulli distribution by dividing the prior into two regions of probability densities corresponding to the Bernoulli distribution. For stochastic samplers, they can achieve a similar effect but the mapping from noise to data is stochastic. For an arbitrary discrete variable, represented as m-dimensional Bernoulli distribution, the mapping from continuous noise distribution to the target Bernoulli distribution also exists but it is more complicated (and difficult to visualize).</p><formula xml:id="formula_6">x T ?1 1 0.5 0.5</formula><p>x 0 (a) Mapping from N (0, 1) to Bernoulli distribution with P (x0 = 1) = 0.5.   F On Sampling Strategies with Self-Conditioning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Method</head><p>In this section, we present extensions to the default sampling strategy with Self-Conditioning in Algorithm 2. The default sampling strategy utilizes data estimate from the previous step as the conditional input to the denoising network for producing data estimate at the current step. While this is both simple and effective, we observe that, for UINT8 (RAND) encoding of pixels, as the number of sampling steps increases (with both DDIM or DDPM samplers), the generated samples tend to be over-smoothed. We propose the following two extensions of the default sampling strategy to mitigate the issues and provide improvements when using larger sampling steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Conditioning based on Momentum Estimate</head><p>The first extension to the default sampling strategy is to adopt an exponential moving average over the previous data estimate to provide a more reliable conditioning input, similar to a momentum optimizer. The detailed procedure is shown in algorithm 5, where the differences from the default sampling strategy are highlighted in blue. Note that the default sampling strategy can also be considered as a special case of this generalized form in that the momentum is set to zero.</p><p>Self-Conditioning based on Self-Guidance One potential issue with the default sampling strategy is the slight discrepancy of the Self-Conditioning signal during training and inference/sampling. Specifically, during training, the Self-Conditioning signal is the data estimate from the same time step, while, during sampling, it is from the past time step(s). Therefore, here we propose an approach that also use the same step data estimate for self-conditioning, which comes at the cost of extra forward pass over the denoising network at sampling time. Specifically, we conduct two forward passes of denoising network per sampling step, one with zero data estimate and the other with current data step estimate, and then we use a weighted combination, similar to <ref type="bibr" target="#b22">[22]</ref>, of both prediction to form the final prediction at the current step. The detailed procedure is given in algorithm 6 with differences to the default sampling strategy highlighted.  <ref type="table" target="#tab_7">Table 7</ref> reports the best FID scores across various sampling strategies discussed here (as well as samplers, sampling steps, time difference in asymmetric time intervals).  <ref type="figure" target="#fig_0">Figure 12</ref> shows FIDs on conditional IMAGENET 64?64 with UINT8 encoding, using Momentum Estimate with different sampling steps. We find that the momentum on the data estimate is only helpful when sampling steps are larger. <ref type="figure" target="#fig_0">Figure 13</ref> shows FIDs on conditional IMAGENET 64?64 with UINT8 encoding, using Self-Guidance with different sampling steps. We find that a guidance weight between 3.0 and 5.0 is generally preferable and robust to other hyper-parameters (such as sampler choice, sampling steps, and time difference).     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Samples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Bit Diffusion: modeling discrete data using continuous diffusion models with analog bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 Algorithm 2</head><label>12</label><figDesc>Bit Diffusion training algorithm. def train_loss(x): # Binary encoding: discrete data to analog bits. x_bits = int2bit(x).astype(float) x_bits = (x_bits * 2 -1) * scale # Corrupt data. t = uniform(0, 1) eps = normal(mean=0, std=1) x_crpt = sqrt(gamma(t)) * x_bits + sqrt(1 -gamma(t)) * eps # Compute self-cond estimate. x_pred = zeros_like(x_crpt) if self_cond and uniform(0, 1) &gt; 0.5: x_pred = net(cat([x_crpt, x_pred], -1), t) x_pred = stop_gradient(x_pred) # Predict and compute loss. x_pred = net(cat([x_crpt, x_pred], -1), t) loss = (x_pred -x_bits)**2 return loss.mean() Bit Diffusion sampling algorithm. def generate(steps, td=0): x_t = normal(mean=0, std=1) x_pred = zeros_like(x_t) for step in range(steps): # Get time for current and next states. t_now = 1 -step / steps t_next = max(1 -(step + 1 + td) / steps, 0) # Predict x_0. if not self_cond: x_pred = zeros_like(x_t) x_pred = net(cat([x_t, x_pred], -1), t_now) # Estimate x at t_next. x_t = ddim_or_ddpm_step( x_t, x_pred, t_now, t_next) # Binary decoding: analog bits to discrete data. x_int = bit2int(x_pred &gt; 0) return x_int</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Self-conditioning is a generic technique that not only greatly improves Bit Diffusion but also leads to improved results for continuous diffusion models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Effect of time difference in class-conditional IMAGENET 64?64. Optimal time difference shrinks to zero as the number of sampling steps increases. For 100 sampling steps, non-zero time difference leads to improved FIDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Histogram distribution (50 bins) of the analog bits from 64 randomly generated IMAGENET 64?64 samples x 0 , with 100 DDIM steps. Most of the generated analog bits are very concentrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 ? 1 ? 1 ? 1 ? 1 ? 1 ?</head><label>111111</label><figDesc>import tensorflow as tf def int2bit(x, n=8): # Convert integers into the corresponding binary bits. x = tf.bitwise.right_shift(tf.expand_dims(x, -1), tf.range(n)) x = tf.math.mod(x, 2) return x def bit2int(x): # Convert binary bits into the corresponding integers. x = tf.cast(x, tf.int32) n = x.shape[-1] x = tf.math.reduce_sum(x * (2 ** tf.range(n)), -1) return x Algorithm 4 xt estimation with DDIM / DDPM updating rules. def gamma(t, ns=0.0002, ds=0.00025): # A scheduling function based on cosine function. return numpy.cos(((t + ns) / (1 + ds)) * numpy.pi / 2)**2 def ddim_step(x_t, x_pred, t_now, t_next): # Estimate x at t_next with DDIM updating rule. ?now = gamma(t_now) ?next = gamma(t_next) x_pred = clip(x_pred, -scale, scale) eps = ?now * (x_t -? ?now * x_pred) x_next = ? ?next * x_pred + ? ?next * eps return x_next def ddpm_step(x_t, x_pred, t_now, t_next): # Estimate x at t_next with DDPM updating rule. ?now = gamma(t_now) ?now = gamma(t_now) / gamma(t_next) ?now = sqrt(1 -?now) z = normal(mean=0, std=1) x_pred = clip(x_pred, -scale, scale) eps = ?now * (x_t -? ?now * x_pred) x_next = ?now * (x_t -1??now ? 1??now * eps) + ?now * z return x_next B Alternative Binary Encoding and Loss Functions B.1 Analog Bits based on One-Hot Encoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Correlation between (absolute) difference in subpixel intensity and the Hamming distance of the corresponding binary bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Mapping from N (0, 1) to Bernoulli distribution with P (x0 = 1) = 0.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>A toy example on continuous modeling of discrete variables. E Extra Random Samples on CIFAR-10 and IMAGENET 64?64Figure 10shows random samples (non cherry-picked) from unconditional diffusion models on CIFAR-10 with continuous pixels and analog bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 Figure 10 :Figure 11 :</head><label>111011</label><figDesc>shows random samples (non cherry-picked) from class-conditional diffusion models on IMAGENET 64?64 with continuous pixels and analog bits.16 (a) DDPM on continuous pixels (FID=3.14) (b) Bit Diffusion on UINT8 (FID=3.71) (c) Bit Diffusion on GRAY CODE (FID=3.88) (d) Bit Diff. on UINT8 (RAND) (FID=6.93) Random samples from unconditional models trained on CIFAR-10. (a) is for continuous image generation, (b), (c), and (d) are for discrete/categorial image generation. (c) Bit Diffusion on GRAY CODE (FID=5.14) (d) Bit Diffusion on UINT8 (RAND) (FID=8.76) Random samples from class-conditional models trained on IMAGENET 64?64. (a) is for continuous image generation, (b), (c), and (d) are for discrete/categorial image generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 and</head><label>14</label><figDesc>15 provide generated samples from different sampling strategies with 100 and 1000 DDIM sampling steps, respectively. DDPM, number of sampling steps &lt; 500. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 DDPM, number of sampling steps &gt; 500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>FID on conditional IMAGENET 64?64 with UINT8 (RAND) encoding using selfcondition sampling based on momentum estimate. The statistics of FID scores in each group are aggregated over the number of sampling steps in {100, 200, 400, 600, 800, 1000}, time difference in {0.0, 0.2, 0.4, 0.6, 0.8}. DDPM, number of sampling steps &gt; 500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 :</head><label>13</label><figDesc>FID on conditional IMAGENET 64?64 with UINT8 (RAND) encoding using self-condition sampling based on self-guidance. The statistics of FID scores in each group are aggregated over the number of sampling steps in {100, 200, 400, 600, 800, 1000}, time difference in {0.0, 0.1}. 21 (a) W/o Self-Conditioning, FID=90.7. (b) W/ previous estimate (momentum=0), FID=12.3. (c) W/ previous estimate (momentum=0.5), FID=35.5. (d) W/ prev. estimate (momentum=0.9), FID=132.6. (e) W/ same-step estimate (w = 3.0), FID=15.6. (f) W/ same-step estimate (w = 5.0), FID=9.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Random samples of Bit Diffusion with UINT8 (RAND) on categorical IMAGENET 64?64 using various Self-Conditioning sampling strategies. Different plots share the same set of x T . Sampling with 100 steps of DDIM without asymmetric time intervals. 22 (a) W/o Self-Conditioning, FID=73.60. (b) W/ previous estimate (momentum=0), FID=58.6. (c) W/ previous estimate (momentum=0.5), FID=46.0. (d) W/ previous estimate (momentum=0.9), FID=11.7. (e) W/ same-step estimate (w = 3.0), FID=11.8. (f) W/ same-step estimate (w = 5.0), FID=9.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 15 :</head><label>15</label><figDesc>Random samples of Bit Diffusion with UINT8 (RAND) on categorical IMAGENET 64?64 using various Self-Conditioning sampling strategies. Different plots share the same set of x T . Sampling with 1000 steps of DDIM without asymmetric time intervals. 23</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of FIDs on unconditional and class-conditional CIFAR-10. Note that both UINT8 and GRAY CODE are only partial/weakly ordinal (see Appendix C). Our Bit Diffusion achieves state-of-the-art FIDs in generating discrete images, beating the best autoregressive model.</figDesc><table><row><cell>Method</cell><cell>State space</cell><cell cols="2">FID (Unconditional) (Conditional) FID</cell></row><row><cell>On continuous pixels (as reference):</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DDPM [23]</cell><cell>Continuous</cell><cell>3.17</cell><cell>-</cell></row><row><cell>DDPM (our reproduction)</cell><cell>Continuous</cell><cell>3.14</cell><cell>2.95</cell></row><row><cell>On discrete (partial) ordinal pixels:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D3PM Gauss+Logistic [1]</cell><cell>Discrete</cell><cell>7.34</cell><cell>-</cell></row><row><cell>? LDR-10 [4]</cell><cell>Discrete</cell><cell>3.74</cell><cell>-</cell></row><row><cell>Bit Diffusion on UINT8</cell><cell>Continuous</cell><cell>3.48</cell><cell>2.72</cell></row><row><cell>Bit Diffusion on GRAY CODE</cell><cell>Continuous</cell><cell>3.86</cell><cell>2.94</cell></row><row><cell>On categorical pixels:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D3PM uniform [1]</cell><cell>Discrete</cell><cell>51.27</cell><cell>-</cell></row><row><cell>D3PM absorbing [1]</cell><cell>Discrete</cell><cell>30.97</cell><cell>-</cell></row><row><cell>Autoregressive Transformer [27]</cell><cell>Discrete</cell><cell>12.75</cell><cell>-</cell></row><row><cell>Bit Diffusion on UINT8 (RAND)</cell><cell>Continuous</cell><cell>6.93</cell><cell>6.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of FIDs on class-conditional IMAGENET 64?64. The corresponding samples can be found in Figure 4 and 11.</figDesc><table><row><cell>DDPM (our repo.)</cell><cell>Bit Diffusion</cell><cell>Bit Diffusion</cell><cell>Bit Diffusion</cell></row><row><cell>on continuous pixels</cell><cell>on UINT8</cell><cell cols="2">on GRAY CODE on UINT8 (RAND)</cell></row><row><cell>3.43</cell><cell>4.84</cell><cell>5.14</cell><cell>8.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Image captioning results on MS-COCO dataset with a randomly initialized text decoder.</figDesc><table><row><cell>Method</cell><cell cols="3">BLEU-4 CIDEr ROUGE-L</cell></row><row><cell>Autoregressive Transformer</cell><cell>33.9</cell><cell>1.18</cell><cell>0.57</cell></row><row><cell>Bit Diffusion (5 steps)</cell><cell>31.5</cell><cell>1.00</cell><cell>0.55</cell></row><row><cell>Bit Diffusion (10 steps)</cell><cell>34.5</cell><cell>1.13</cell><cell>0.57</cell></row><row><cell>Bit Diffusion (20 steps)</cell><cell>34.7</cell><cell>1.15</cell><cell>0.58</cell></row><row><cell>Bit Diffusion (40 steps)</cell><cell>34.4</cell><cell>1.15</cell><cell>0.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Asymmetric time intervals significantly improves the performance of Bit Diffusion. 31.6 31.5 31.5 31.5 31.6 10 steps 17.6 26.3 30.7 32.6 33.4 34.0 34.3 34.5 34.6 20 steps 20.0 27.9 30.6 32.0 32.3 33.9 34.4 34.7 34.5 40 steps 20.7 27.5 30.7 32.2 32.9 33.2 33.8 34.4 34.4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Time difference</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>1.0</cell><cell>2.0</cell><cell>3.0</cell><cell>4.0</cell><cell>5.0</cell><cell>6.0</cell><cell>7.0</cell><cell>8.0</cell></row><row><cell>5 steps</cell><cell cols="4">17.1 27.8 30.8 31.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Generated image captions under different number of sampling steps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>FIDs of Bit Diffusion models with different types of analog bits and loss functions on unconditional CIFAR-10.</figDesc><table><row><cell></cell><cell cols="3">2 loss Logistic loss Softmax loss</cell></row><row><cell>ONE HOT</cell><cell>46.32</cell><cell>26.82</cell><cell>29.49</cell></row><row><cell>UINT8</cell><cell>3.48</cell><cell>3.53</cell><cell>-</cell></row><row><cell>GRAY CODE</cell><cell>3.86</cell><cell>3.71</cell><cell>-</cell></row><row><cell>UINT8 (RAND)</cell><cell>6.93</cell><cell>49.29</cell><cell>-</cell></row><row><cell cols="4">C On Binary Encoding of Pixels: UINT8, GRAY CODE, UINT8 (RAND)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Algorithm 5</head><label>5</label><figDesc>Sampling with Self-Conditioning based on Momentum Estimate. Sampling with Self-Conditioning based on Self-Guidance.</figDesc><table><row><cell>Algorithm 6 def generate(steps, td=0, guide_w=3.0):</cell></row><row><cell>x_t = normal(mean=0, std=1)</cell></row><row><cell>x_pred = zeros_like(x_t)</cell></row><row><cell>for step in range(steps):</cell></row><row><cell># Get time for current and next states.</cell></row><row><cell>t_now = 1 -step / steps</cell></row><row><cell>t_next = max(1 -(step + 1 + td) / steps, 0)</cell></row><row><cell># Predict x_0 wo/ self-cond.</cell></row><row><cell>x_pred_uncond = net(cat([x_t, zeros_like(x_t)], -1), t_now)</cell></row><row><cell># Predict x_0 w/ self-cond.</cell></row><row><cell>x_pred_selfcond = net(cat([x_t, x_pred_uncond], -1), t_now)</cell></row><row><cell># Apply self-guidance.</cell></row><row><cell>x_pred = guide_w * x_pred_selfcond + (1.0 -guide_w) * x_pred_uncond</cell></row><row><cell># Estimate x at t_next.</cell></row><row><cell>x_t = ddim_or_ddpm_step(x_t, x_pred, t_now, t_next)</cell></row><row><cell># Binary decoding: analog bits to discrete data.</cell></row><row><cell>x_int = bit2int(x_pred &gt; 0)</cell></row><row><cell>return x_int</cell></row><row><cell>F.2 Experiments</cell></row><row><cell>def generate(steps, td=0, momentum=0.):</cell></row><row><cell>x_t = normal(mean=0, std=1)</cell></row><row><cell>x_pred = zeros_like(x_t)</cell></row><row><cell>x_accu = zeros_like(x_t)</cell></row><row><cell>for step in range(steps):</cell></row><row><cell># Get time for current and next states.</cell></row><row><cell>t_now = 1 -step / steps</cell></row><row><cell>t_next = max(1 -(step + 1 + td) / steps, 0)</cell></row><row><cell># Predict x_0 (with self-cond).</cell></row><row><cell>x_accu = momentum * x_accu + (1 -momentum) * x_pred</cell></row><row><cell>x_pred = net(cat([x_t, x_accu], -1), t_now)</cell></row><row><cell># Estimate x at t_next.</cell></row><row><cell>x_t = ddim_or_ddpm_step(x_t, x_pred, t_now, t_next)</cell></row><row><cell># Binary decoding: analog bits to discrete data.</cell></row><row><cell>x_int = bit2int(x_pred &gt; 0)</cell></row><row><cell>return x_int</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Best FIDs of Bit Diffusion models with different Self-Conditioning sampling strategies on conditional IMAGENET 64?64.</figDesc><table><row><cell></cell><cell>UINT8</cell><cell>GRAY CODE</cell><cell>UINT8 (RAND)</cell></row><row><cell>Default sampling (momentum= 0)</cell><cell>4.84</cell><cell>5.14</cell><cell>8.76</cell></row><row><cell>Momentum Estimate</cell><cell>4.85</cell><cell>5.14</cell><cell>8.51</cell></row><row><cell>Self-Guidance</cell><cell>5.15</cell><cell>5.65</cell><cell>7.87</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">After casting as real numbers, one may also transform them by shifting and scaling from 0, 1 to ?b, b.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">39]</ref>, we center crop and area downsample images to 64?64.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our model is about 30M parameters smaller than that used in<ref type="bibr" target="#b39">[39]</ref> as we drop the middle blocks for convenience, which may have a minor effect on performance.<ref type="bibr" target="#b3">4</ref> For UINT8 (RAND) encoding, we use learning rate of 0.0001 instead.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Although, one can reduce the vocabulary size K by using sub-tokenization (e.g., subword<ref type="bibr" target="#b49">[49]</ref>), or learned discrete codes<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) DDPM on continuous pixels (FID=3.43) (b) Bit Diffusion on UINT8. (FID=4.84)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Priyank Jaini, Kevin Swersky for providing helpful feedback to our draft. Our implementation is partially based on the Pix2Seq codebase, and we thank Lala Li, Saurabh Saxena, for their contributions to the Pix2Seq codebase.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17981" to="17993" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 2, 5, 9</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1877" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A continuous time framework for discrete denoising models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">De</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14987</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 5, 9</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07983</idno>
		<title level="m">Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01365</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning k-way d-dimensional discrete codes for compact embedding representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differentiable product quantization for end-to-end embedding compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pix2seq: A language modeling framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10852</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 5, 7</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A unified sequence interface for vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07669</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 4, 5, 7</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew M</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07736</idno>
		<title level="m">Maskgan: better text generation via filling in the</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08431</idno>
		<title level="m">Boundary-seeking generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 2, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="47" to="48" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 3, 9</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integer discrete flows and lossless compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Argmax flows and multinomial diffusion: Learning categorical distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyank</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12454" to="12465" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 2, 9</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5006" to="5019" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 2, 5, 9</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14217</idno>
		<title level="m">Diffusion-lm improves controllable text generation</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Lindt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11625</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Discrete denoising flows. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Categorical normalizing flows via continuous transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR, 2021.</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
	<note>Cited on page 3, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Photorealistic text-toimage diffusion models with deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; S Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
	</analytic>
	<monogr>
		<title level="j">Burcu Karagol Ayan</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Seyed Kamyar Seyed Ghasemipour. Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06749</idno>
		<title level="m">Stepunrolled denoising autoencoders for text generation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 2, 4, 9</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cited on page 1, 2)</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12438" to="12448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Cited on page 1, 2)</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to dequantise with truncated flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<title level="m">A?ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Discrete flows: Invertible generative models of discrete data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyon</forename><surname>Vafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>Cited on page 1, 9</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Cited on page 1, 5, 8</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Perceptual generative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paull</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Latent normalizing flows for discrete sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
