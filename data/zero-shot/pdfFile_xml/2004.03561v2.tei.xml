<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Li</surname></persName>
							<email>changmao.li@emory.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Emory University Atlanta</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
							<email>jinho.choi@emory.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Emory University Atlanta</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token-and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. Then, multitask learning between the utterance prediction and the token span prediction is applied to finetune for span-based question answering (QA). Our approach is evaluated on the FRIENDSQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based contextualized embedding approaches such as BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>, XLM (CONNEAU and Lample, 2019), XLNet , RoBERTa <ref type="bibr" target="#b8">(Liu et al., 2019)</ref>, and AlBERT <ref type="bibr" target="#b7">(Lan et al., 2019)</ref> have re-established the state-of-the-art for practically all question answering (QA) tasks on not only general domain datasets such as SQUAD <ref type="bibr" target="#b12">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b11">(Rajpurkar et al., , 2018</ref>, MS MARCO <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref>, TRIVIAQA <ref type="bibr" target="#b5">(Joshi et al., 2017)</ref>, NEWSQA <ref type="bibr" target="#b17">(Trischler et al., 2017)</ref>, or NARRATIVEQA <ref type="bibr" target="#b6">(Koisk et al., 2018)</ref>, but also multiturn question datasets such as SQA <ref type="bibr" target="#b4">(Iyyer et al., 2017)</ref>, QUAC <ref type="bibr" target="#b0">(Choi et al., 2018)</ref>, COQA <ref type="bibr" target="#b13">(Reddy et al., 2019)</ref>, or CQA <ref type="bibr" target="#b15">(Talmor and Berant, 2018)</ref>. However, for span-based QA where the evidence documents are in the form of multiparty dialogue, the performance is still poor even with the latest transformer models <ref type="bibr" target="#b14">(Sun et al., 2019;</ref><ref type="bibr" target="#b19">Yang and Choi, 2019)</ref> due to the challenges in representing utterances composed by heterogeneous speakers.</p><p>Several limitations can be expected for language models trained on general domains to process dialogue. First, most of these models are pre-trained on formal writing, which is notably different from colloquial writing in dialogue; thus, fine-tuning for the end tasks is often not sufficient enough to build robust dialogue models. Second, unlike sentences in a wiki or news article written by one author with a coherent topic, utterances in a dialogue are from multiple speakers who may talk about different topics in distinct manners such that they should not be represented by simply concatenating, but rather as sub-documents interconnected to one another.</p><p>This paper presents a novel approach to the latest transformers that learns hierarchical embeddings for tokens and utterances for a better understanding in dialogue contexts. While fine-tuning for span-based QA, every utterance as well as the question are separated encoded and multi-head attentions and additional transformers are built on the token and utterance embeddings respectively to provide a more comprehensive view of the dialogue to the QA model. As a result, our model achieves a new state-of-the-art result on a span-based QA task where the evidence documents are multiparty dialogue. The contributions of this paper are: 1 ? New pre-training tasks are introduced to improve the quality of both token-level and utterance-level embeddings generated by the transformers, that better suit to handle dialogue contexts ( ?2.1).</p><p>? A new multi-task learning approach is proposed to fine-tune the language model for span-based QA that takes full advantage of the hierarchical embeddings created from the pre-training ( ?2.2).</p><p>? Our approach significantly outperforms the previous state-of-the-art models using BERT and RoBERTa on a span-based QA task using dialogues as evidence documents ( ?3).  </p><formula xml:id="formula_0">? [CLS1] s 1 w 11 w 1n [CLSm] s? m w? m1 w? mn ? ? ? ? [CLSi] ? s? i w? i1 ? w? in ? ? TL2 TL1 Softmax o ? t c 1 t c m ? t c i ? e c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transformers for Learning Dialogue</head><p>This section introduces a novel approach for pretraining (Section 2.1) and fine-tuning (Section 2.2) transformers to effectively learn dialogue contexts. Our approach has been evaluated with two kinds of transformers, BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b8">(Liu et al., 2019)</ref>, and shown significant improvement to a question answering task (QA) on multiparty dialogue (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-training Language Models</head><p>Pre-training involves 3 tasks in sequence, the tokenlevel masked language modeling (MLM; ?2.1.1), the utterance-level MLM ( ?2.1.2), and the utterance order prediction ( ?2.1.3), where the trained weights from each task are transferred to the next task. Note that the weights of publicly available transformer encoders are adapted to train the tokenlevel MLM, which allows our QA model to handle languages in both dialogues, used as evidence documents, and questions written in formal writing. Transformers from BERT and RoBERTa are trained with static and dynamic MLM respectively, as described by <ref type="bibr" target="#b2">Devlin et al. (2019)</ref>; <ref type="bibr" target="#b8">Liu et al. (2019)</ref>. . . , U m } be a dialogue where U i = {s i , w i1 , . . . , w in } is the i'th utterance in D, s i is the speaker of U i , and w ij is the j'th token in U i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Token-level Masked LM</head><p>All speakers and tokens in D are appended in order with the special token CLS, representing the entire dialogue, which creates the input string sequence</p><formula xml:id="formula_1">I = {CLS}?U 1 ?. . .?U n . For every w ij ? I, let I ? ij = (I \ {w ij }) ? {? ij },</formula><p>where ? ij is the masked token substituted in place of w ij . I ? ij is then fed into the transformer encoder (TE), which generates a sequence of embeddings</p><formula xml:id="formula_2">{e c } ? E 1 ? . . . ? E m where E i = {e s i , e w i1 , .</formula><p>., e w in } is the embedding list for U i , and (e c , e s i , e w ij , e ? ij ) are the embeddings of (CLS, s i , w ij , ? ij ) respectively. Finally, e ? ij is fed into a softmax layer that generates the output vector o ? ij ? R |V | to predict ? ij , where V is the set of all vocabularies in the dataset. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Utterance-level Masked LM</head><p>The token-level MLM (t-MLM) learns attentions among all tokens in D regardless of the utterance boundaries, allowing the model to compare every token to a broad context; however, it fails to catch unique aspects about individual utterances that can be important in dialogue. To learn an embedding for each utterance, the utterance-level MLM model is trained <ref type="figure" target="#fig_1">(Figure 1(b)</ref>). Utterance embeddings can be used independently and/or in sequence to match contexts in the question and the dialogue beyond the token-level, showing an advantage in finding utterances with the correct answer spans ( ?2.2.1).  <ref type="figure">Figure 2</ref>: The overview of our fine-tuning model exploiting multi-task learning (Section 2.2).</p><formula xml:id="formula_3">e | t ? ? d E ? ? n?d o u ? ? m+1 o l|r ? ? n+1 ? [CLS1] s 1 w 11 w 1n [CLSm] s m w m1 w mn ? ? [CLSq] q 1 q 2 q n ? ? ? MHA t c q t c 1 t c m ? Softmax E q 1 E q 2 E q m ? o r 1 o r 2 o r m ? o ? 1 o ? 2 o ? m ? o u</formula><p>For every utterance U i , the masked input sequence</p><formula xml:id="formula_4">I ? ij = {CLS i } ? {(U i \ {w ij }) ? ? ij } is generated.</formula><p>Note that CLS i now represents U i instead of D and I ? ij is much shorter than the one used for t-MLM. I ? ij is fed into TE, already trained by t-MLM, and the embedding sequence</p><formula xml:id="formula_5">E i = {e c i , e s i , e w i1 , .</formula><p>., e w in } is generated. Finally, e c i , instead of e ? ij , is fed into a softmax layer that generates o ? ij to predict ? ij . The intuition behind the utterance-level MLM is that once e c i learns enough contents to accurately predict any token in U i , it consists of most essential features about the utterance; thus, e c i can be used as the embedding of U i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Utterance Order Prediction</head><p>The embedding e c i from the utterance-level MLM (u-MLM) learns contents within U i , but not across other utterances. In dialogue, it is often the case that a context is completed by multiple utterances; thus, learning attentions among the utterances is necessary. To create embeddings that contain crossutterance features, the utterance order prediction model is trained <ref type="figure" target="#fig_1">(Figure 1(c)</ref>). Let D = D 1 ? D 2 where D 1 and D 2 comprise the first and the second halves of the utterances in D, respectively. Also, let D = D 1 ? D 2 where D 2 contains the same set of utterances as D 2 although the ordering may be different. The task is whether or not D preserves the same order of utterances as D.</p><p>For each U i ? D , the input I i = {CLS i }?U i is created and fed into TE, already trained by u-MLM, to create the embeddings</p><formula xml:id="formula_6">E i = {e c i , e s i , e w i1 , .., e w in }. The sequence E c = {e c 1 , . .</formula><p>. , e c n } is fed into two transformer layers, TL1 and TL2, that generate the new utterance embedding list T c = {t c 1 , . . . , t c n }. Finally, T c is fed into a softmax layer that generates o ? ? R 2 to predict whether or not D is in order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-tuning for QA on Dialogue</head><p>Fine-tuning exploits multi-task learning between the utterance ID prediction ( ?2.2.1) and the token span prediction ( ?2.2.2), which allows the model to train both the utterance-and token-level attentions. The transformer encoder (TE) trained by the utterance order prediction (UOP) is used for both tasks.</p><formula xml:id="formula_7">Given the question Q = {q 1 , . . . , q n } (q i is the i'th token in Q) and the dialogue D = {U 1 , . . . , U m }, Q and all U * are fed into TE that generates E q = {e c q , e q 1 , .., e q n } and E i = {e c i , e s i , e w i1 , .</formula><p>., e w in } for Q and every U i , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Utterance ID Prediction</head><p>The utterance embedding list E c = {e c q , e c 1 , .., e c n } is fed into TL1 and TL2 from UOP that generate T c = {t c q , t c 1 , .., t c n }. T c is then fed into a softmax layer that generates o u ? R m+1 to predict the ID of the utterance containing the answer span if exists; otherwise, the 0'th label is predicted, implying that the answer span for Q does not exist in D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Token Span Prediction</head><p>For <ref type="bibr" target="#b18">Vaswani et al., 2017</ref>) then generates the attended embedding sequences, T a 1 , . . . , T a m , where T a i = {t s i , t w i1 , .., t w in }. Finally, each T a i is fed into two softmax layers, SL and SR, that generate o i ? R n+1 and o r i ? R n+1 to predict the leftmost and the rightmost tokens in U i respectively, that yield the answer span for Q. It is possible that the answer spans are predicted in multiple utterances, in which case, the span from the utterance that has the highest score for the utterance ID prediction is selected, which is more efficient than the typical dynamic programming approach.</p><formula xml:id="formula_8">every E i , the pair (E q , E i ) is fed into the multi- head attention layer, MHA, where E q = E q \ {e c q } and E i = E i \ {e c i }. MHA (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus</head><p>Despite of all great work in QA, only two datasets are publicly available for machine comprehension that take dialogues as evidence documents. One is DREAM comprising dialogues for language exams with multiple-choice questions <ref type="bibr" target="#b14">(Sun et al., 2019)</ref>. The other is FRIENDSQA containing transcripts from the TV show Friends with annotation for spanbased question answering <ref type="bibr" target="#b19">(Yang and Choi, 2019)</ref>. Since DREAM is for a reading comprehension task that does not need to find the answer contents from the evidence documents, it is not suitable for our approach; thus, FRIENDSQA is chosen.</p><p>Each scene is treated as an independent dialogue in FRIENDSQA. <ref type="bibr" target="#b19">Yang and Choi (2019)</ref> randomly split the corpus to generate training, development, and evaluation sets such that scenes from the same episode can be distributed across those three sets, causing inflated accuracy scores. Thus, we re-split them by episodes to prevent such inflation. For finetuning ( ?2.2), episodes from the first four seasons are used as described in <ref type="table">Table 1</ref>. For pre-training ( ?2.1), all transcripts from Seasons 5-10 are used as an additional training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models</head><p>The weights from the BERT base and RoBERTa base models <ref type="bibr" target="#b2">(Devlin et al., 2019;</ref><ref type="bibr" target="#b8">Liu et al., 2019)</ref> are transferred to all models in our experiments. Four baseline models, BERT, BERT pre , RoBERTa, and RoBERTa pre , are built, where all models are finetuned on the datasets in <ref type="table">Table 1</ref> and the *pre models are pre-trained on the same datasets with the additional training set from Seasons 5-10 ( ?3.1). The baseline models are compared to BERT our and RoBERTA our that are trained by our approach. 3 model is developed three times and their average score as well as the standard deviation are reported. The performance of RoBERTa * is generally higher than BERT * although RoBERTa base is pre-trained with larger datasets including CC-NEWS <ref type="bibr" target="#b9">(Nagel, 2016)</ref>, OPENWEBTEXT <ref type="bibr" target="#b3">(Gokaslan and Cohen, 2019)</ref>, and STORIES (Trinh and Le, 2018) than BERT base such that results from those two types of transformers cannot be directly compared. RoBERTa 52.6(?0.7) 68.2(?0.3) 80.9(?0.8) RoBERTa pre 52.6(?0.7) 68.6(?0.6) 81.7(?0.7) RoBERTa our 53.5(?0.7) 69.6(?0.8) 82.7(?0.5) The *pre models show marginal improvement over their base models, implying that pre-training the language models on FRIENDSQA with the original transformers does not make much impact on this QA task. The models using our approach perform noticeably better than the baseline models, showing 3.8% and 1.4% improvements on SM from BERT and RoBERTa, respectively.    <ref type="table">Table 4</ref> shows the results from ablation studies to analyze the impacts of the individual approaches. BERT pre and RoBERTa pre are the same as in Table 2, that are the transformer models pre-trained by the token-level masked LM ( ?2.1.1) and fine-tuned by the token span prediction ( ?2.2.2). BERT uid and RoBERTa uid are the models that are pre-trained by the token-level masked LM and jointly fine-tuned by the token span prediction as well as the utterance ID prediction (UID: ?2.2.1). Given these two types of transformer models, the utterance-level masked LM (ULM: ?2.1.2) and the utterance order prediction (UOP: ?2.1.3) are separately evaluated. RoBERTa pre 52.6(?0.7) 68.6(?0.6) 81.7(?0.7) ?ULM 52.9(?0.8) 68.7(?1.1) 81.7(?0.6) ?ULM?UOP 52.5(?0.8) 68.8(?0.5) 81.9(?0.7) RoBERTa uid 52.8(?0.9) 68.7(?0.8) 81.9(?0.5) ?ULM 53.2(?0.6) 69.2(?0.7) 82.4(?0.5) ?ULM?UOP 53.5(?0.7) 69.6(?0.8) 82.7(?0.5) <ref type="table">Table 4</ref>: Results for the ablation studies. Note that the *uid ?ULM?UOP models are equivalent to the *our models in <ref type="table" target="#tab_3">Table 2</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies</head><p>These two dialogue-specific LM approaches, ULM and UOP, give very marginal improvement over the baseline models, that is rather surprising. However, they show good improvement when combined with UID, implying that pre-training language models may not be enough to enhance the performance by itself but can be effective when it is coupled with an appropriate fine-tuning approach. Since both ULM and UOP are designed to improve the quality of utterance embeddings, it is expected to improve the accuracy for UID as well. The improvement on UM is indeed encouraging, giving 2% and 1% boosts to BERT pre and RoBERTa pre , respectively and consequently improving the other two metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Error Analysis</head><p>As shown in <ref type="table" target="#tab_7">Table 3</ref>, the major errors are from the three types of questions, who, how, and why; thus, we select 100 dialogues associated with those question types that our best model, RoBERTa our , incorrectly predicts the answer spans for. Specific examples are provided in <ref type="table" target="#tab_3">Tables 12, 13</ref> and 14 ( ?A.3). Following , errors are grouped into 6 categories, entity resolution, paraphrase and partial match, cross-utterance reasoning, question bias, noise in annotation, and miscellaneous. <ref type="table" target="#tab_10">Table 5</ref> shows the errors types and their ratios with respect to the question types. Two main error types are entity resolution and cross-utterance reasoning. The entity resolution error happens when many of the same entities are mentioned in multiple utterances. This error also occurs when the QA system is asked about a specific person, but predicts wrong people where there are so many people appearing in multiple utterances. The cross-utterance reasoning error often happens with the why and how questions where the model relies on pattern matching mostly and predicts the next utterance span of the matched pattern.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper introduces a novel transformer approach that effectively interprets hierarchical contexts in multiparty dialogue by learning utterance embeddings. Two language modeling approaches are proposed, utterance-level masked LM and utterance order prediction. Coupled with the joint inference between token span prediction and utterance ID prediction, these two language models significantly outperform two of the state-of-the-art transformer approaches, BERT and RoBERTa, on a span-based QA task called FriendsQA . We will evaluate our approach on other machine comprehension tasks using dialogues as evidence documents to further verify the generalizability of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Experimental Setup</head><p>The BERT base model and the RoBERTa BASE model use the same configuration. The two models both have 12 hidden transformer layers and 12 attention heads. The hidden size of the model is 768 and the intermediate size in the transformer layers is 3,072. The activation function in the transformer layers is gelu.</p><p>Pre-training The batch size of 32 sequences is used for pre-training. Adam with the learning rate of 5 ? 10 ?5 , ? 1 = 0.9, ? 2 = 0.999, the L2 weight decay of 0.01, the learning rate warm up over the first 10% steps, and the linear decay of the learning rate are used. A dropout probability of 0.1 is applied to all layers. The cross-entropy is used for the training loss of each task. For the masked language modeling tasks, the model is trained until the perplexity stops decreasing on the development set. For the other pre-training tasks, the model is trained until both the loss and the accuracy stop decreasing on the development set.</p><p>Fine-tuning For fine-tuning, the batch size and the optimization approach are the same as the pretraining. The dropout probability is always kept at 0.1. The training loss is the sum of the crossentropy of two fine-tuning tasks as in ?2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Question Types Analysis</head><p>Tables in this section show the results with respect to the question types using all models (Section 3.2) in the order of performance.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Error Examples</head><p>Each table in this section gives an error example from the excerpt. The gold answers are indicated by the solid underlines whereas the predicted answers are indicated by the :::: wavy underlines.</p><p>Q Why is Joey planning a big party? J Oh, ::::: we're :::::: having :: a ::: big ::::: party ::::::::: tomorrow :::::: night. Later! R Whoa! Hey-hey, you planning on inviting us? J Nooo, later. P Hey!! Get your ass back here, Tribbiani!! R Hormones! M What Phoebe meant to say was umm, how come you're having a party and we're not invited? J Oh, it's Ross' bachelor party. M Sooo?  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Utterance order prediction ( ?2.1.3) Figure 1: The overview of our models for the three pre-training tasks (Section 2.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>(a) illustrates the token-level MLM model. Let D = {U 1 , .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table /><note>results achieved by all the models. Following Yang and Choi (2019), exact matching (EM), span matching (SM), and utterance match- ing (UM) are used as the evaluation metrics. Each3 Detailed experimental setup are provided in Appendices.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Accuracies (? standard deviations) achieved by the BERT and RoBERTa models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Results from the RoBERTa our model by different question types.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc>shows the results achieved by RoBERTa our w.r.t. question types. UM drops significantly for Why that often spans out to longer sequences and also requires deeper inferences to answer correctly than the others. Compared to the baseline models, our models show more well-around performance regardless the question types. 4</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Error types and their ratio with respect to the three most challenging question types.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Results from RoBERTa by question types.</figDesc><table><row><cell>Type</cell><cell>Dist.</cell><cell>EM</cell><cell>SM</cell><cell>UM</cell></row><row><cell cols="5">Where 18.16 67.1(?1.2) 78.9(?0.6) 89.0(?1.1)</cell></row><row><cell cols="5">When 13.57 62.3(?0.7) 76.3(?1.3) 88.7(?0.9)</cell></row><row><cell cols="5">What 18.48 55.1(?0.8) 73.1(?0.8) 86.7(?0.8)</cell></row><row><cell>Who</cell><cell cols="4">18.82 56.2(?1.4) 64.0(?1.7) 77.1(?1.3)</cell></row><row><cell>How</cell><cell cols="4">15.32 41.2(?1.1) 61.2(?1.5) 79.8(?0.7)</cell></row><row><cell>Why</cell><cell cols="4">15.65 32.4(?0.7) 57.4(?0.8) 69.1(?1.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Results from RoBERTa pre by question types.</figDesc><table><row><cell>Type</cell><cell>Dist.</cell><cell>EM</cell><cell>SM</cell><cell>UM</cell></row><row><cell cols="5">Where 18.16 66.1(?0.5) 79.9(?0.7) 89.8(?0.7)</cell></row><row><cell cols="5">When 13.57 63.3(?1.3) 76.4(?0.6) 88.9(?1.2)</cell></row><row><cell cols="5">What 18.48 56.4(?1.7) 74.0(?0.5) 87.7(?2.1)</cell></row><row><cell>Who</cell><cell cols="4">18.82 55.9(?0.8) 66.0(?1.7) 79.9(?1.1)</cell></row><row><cell>How</cell><cell cols="4">15.32 43.2(?2.3) 63.2(?2.5) 79.4(?0.7)</cell></row><row><cell>Why</cell><cell cols="4">15.65 33.3(?2.0) 57.3(?0.8) 69.8(?1.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Results from RoBERTa our by question types.</figDesc><table><row><cell>Type</cell><cell>Dist.</cell><cell>EM</cell><cell>SM</cell><cell>UM</cell></row><row><cell cols="5">Where 18.16 57.3(?0.5) 70.2(?1.3) 79.4(?0.9)</cell></row><row><cell cols="5">When 13.57 56.1(?1.1) 69.7(?1.6) 78.6(?1.7)</cell></row><row><cell cols="5">What 18.48 45.0(?1.4) 64.4(?0.7) 77.0(?1.0)</cell></row><row><cell>Who</cell><cell cols="4">18.82 46.9(?1.1) 56.2(?1.4) 67.6(?1.4)</cell></row><row><cell>How</cell><cell cols="4">15.32 29.3(?0.8) 48.4(?1.2) 60.9(?0.7)</cell></row><row><cell>Why</cell><cell cols="4">15.65 23.4(?1.6) 46.1(?0.9) 56.4(?1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Results from BERT by question types.</figDesc><table><row><cell>Type</cell><cell>Dist.</cell><cell>EM</cell><cell>SM</cell><cell>UM</cell></row><row><cell cols="5">Where 18.16 62.8(?1.8) 72.3(?0.8) 82.1(?0.7)</cell></row><row><cell cols="5">When 13.57 60.7(?1.5) 70.7(?1.8) 80.4(?1.1)</cell></row><row><cell cols="5">What 18.48 43.2(?1.3) 64.3(?1.7) 75.6(?1.8)</cell></row><row><cell>Who</cell><cell cols="4">18.82 47.8(?1.1) 56.9(?1.9) 69.7(?0.7)</cell></row><row><cell>How</cell><cell cols="4">15.32 33.2(?1.3) 48.3(?0.6) 59.8(?1.1)</cell></row><row><cell>Why</cell><cell cols="4">15.65 22.9(?1.6) 46.6(?0.7) 54.9(?0.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Results from BERT pre by question types.</figDesc><table><row><cell>Type</cell><cell>Dist.</cell><cell>EM</cell><cell>SM</cell><cell>UM</cell></row><row><cell cols="5">Where 18.16 63.3(?1.2) 72.9(?1.7) 77.0(?1.2)</cell></row><row><cell cols="5">When 13.57 48.4(?1.9) 66.5(?0.8) 79.5(?1.5)</cell></row><row><cell cols="5">What 18.48 52.1(?0.7) 69.2(?1.1) 81.3(?0.7)</cell></row><row><cell>Who</cell><cell cols="4">18.82 51.3(?1.1) 61.9(?0.9) 67.5(?0.9)</cell></row><row><cell>How</cell><cell cols="4">15.32 30.9(?0.9) 52.1(?0.7) 65.4(?1.1)</cell></row><row><cell>Why</cell><cell cols="4">15.65 29.2(?1.6) 53.2(?1.3) 65.7(?0.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Results from BERT our by question types.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>An error example for the why question (Q). J: Joey, R: Rachel, P: Pheobe, M: Monica. Ben... to the rescue! R Ben, you ready? All right, gimme your foot. Ok, on three, Ben. One, two, three. Ok, That's it, Ben. -(Ross and Susan lift Phoebe up into the vent.) S What do you see? P Well, Susan, I see what appears to be a dark vent. Wait. Yes, it is in fact a dark vent. -( : A ::::::: janitor opens the closet door from the outside.)</figDesc><table><row><cell>Q</cell><cell>Who opened the vent?</cell></row><row><cell cols="2">R Ok, got the vent open.</cell></row><row><cell cols="2">P Hi, I'm Ben. I'm hospital worker Ben.</cell></row><row><cell>It's</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>An error example for the who question (Q). P: Pheobe, R: Ross, S: Susan.</figDesc><table /><note>Q How does Joey try to convince the girl to hang out with him?</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All our resources including the source codes and the dataset with the experiment split are available at https://github.com/emorynlp/friendsqa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">n: the maximum number of words in every utterance, m: the maximum number of utterances in every dialogue.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Question type results for all models are in Appendices.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of the AWS Machine Learning Research Awards (MLRA). Any contents in this material are those of the authors and do not necessarily reflect the views of them.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quac: Question answering in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1241</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL&apos;19</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL&apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">OpenWeb-Text Corpus</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The narrativeqa reading comprehension challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Koisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gbor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00023</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">317328</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nagel</surname></persName>
		</author>
		<title level="m">News Dataset Available</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Know what you dont know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00266</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">249266</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1059</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Simple Method for Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-2623</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FriendsQA: Open-domain question answering on TV show transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
