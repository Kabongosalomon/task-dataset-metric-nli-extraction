<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>j.wang@cs.ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is nontrivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning. Recently, recurrent neural networks (RNNs) with long shortterm memory (LSTM) cells <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997)</ref> have shown excellent performance ranging from natural language generation to handwriting generation <ref type="bibr" target="#b19">(Wen et al. 2015;</ref><ref type="bibr" target="#b7">Graves 2013)</ref>. The most common approach to training an RNN is to maximize the log predictive likelihood of each true token in the training sequence given the previous observed tokens <ref type="bibr" target="#b13">(Salakhutdinov 2009</ref>). However, as argued in , the maximum likelihood approaches suffer from so-called exposure bias in the inference stage: the model generates a sequence iteratively and predicts next token conditioned on its previously predicted ones that may be never observed in the training data. Such a discrepancy between training and inference can incur accumulatively along with the sequence and will become prominent as the length of sequence increases. To address this problem,  proposed a training strategy called scheduled sampling (SS), where the generative model is partially fed with its own synthetic data as prefix (observed tokens) rather than the true data when deciding the next token in the training stage. Nevertheless, <ref type="bibr">(Husz?r 2015)</ref> showed that SS is an inconsistent training strategy and fails to address the problem fundamentally. Another possible solution of the training/inference discrepancy problem is to build the loss function on the entire generated sequence instead of each transition. For instance, in the application of machine translation, a task specific sequence score/loss, bilingual evaluation understudy (BLEU) <ref type="bibr" target="#b12">(Papineni et al. 2002)</ref>, can be adopted to guide the sequence generation. However, in many other practical applications, such as poem generation <ref type="bibr" target="#b22">(Zhang and Lapata 2014)</ref> and chatbot (Hingston 2009), a task specific loss may not be directly available to score a generated sequence accurately.</p><p>General adversarial net (GAN) proposed by (Goodfellow and others 2014) is a promising framework for alleviating the above problem. Specifically, in GAN a discriminative net D learns to distinguish whether a given data instance is real or not, and a generative net G learns to confuse D by generating high quality data. This approach has been successful and been mostly applied in computer vision tasks of generating samples of natural images <ref type="bibr" target="#b4">(Denton et al. 2015)</ref>.</p><p>Unfortunately, applying GAN to generating sequences has two problems. Firstly, GAN is designed for generating real-valued, continuous data but has difficulties in directly generating sequences of discrete tokens, such as texts <ref type="bibr">(Husz?r 2015)</ref>. The reason is that in GANs, the generator starts with random sampling first and then a determistic transform, govermented by the model parameters. As such, the gradient of the loss from D w.r.t. the outputs by G is used to guide the generative model G (paramters) to slightly change the generated value to make it more realistic. If the generated data is based on discrete tokens, the "slight change" guidance from the discriminative net makes little sense because there is probably no corresponding token for such slight change in the limited dictionary space <ref type="bibr" target="#b7">(Goodfellow 2016)</ref>. Secondly, GAN can only give the score/loss for an entire sequence when it has been generated; for a partially generated sequence, it is non-trivial to balance how good as it is now and the future score as the entire sequence.</p><p>In this paper, to address the above two issues, we follow <ref type="bibr" target="#b0">(Bachman and Precup 2015;</ref><ref type="bibr" target="#b1">Bahdanau et al. 2016</ref>) and consider the sequence generation procedure as a sequential decision making process. The generative model is treated as an agent of reinforcement learning (RL); the state is the generated tokens so far and the action is the next token to be generated. Unlike the work in <ref type="bibr" target="#b1">(Bahdanau et al. 2016</ref>) that requires a task-specific sequence score, such as BLEU in machine translation, to give the reward, we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model. To solve the problem that the gradient cannot pass back to the generative model when the output is discrete, we regard the generative model as a stochastic parametrized policy. In our policy gradient, we employ Monte Carlo (MC) search to approximate the state-action value. We directly train the policy (generative model) via policy gradient <ref type="bibr" target="#b17">(Sutton et al. 1999)</ref>, which naturally avoids the differentiation difficulty for discrete data in a conventional GAN.</p><p>Extensive experiments based on synthetic and real data are conducted to investigate the efficacy and properties of the proposed SeqGAN. In our synthetic data environment, SeqGAN significantly outperforms the maximum likelihood methods, scheduled sampling and PG-BLEU. In three realworld tasks, i.e. poem generation, speech language generation and music generation, SeqGAN significantly outperforms the compared baselines in various metrics including human expert judgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Deep generative models have recently drawn significant attention, and the ability of learning over large (unlabeled) data endows them with more potential and vitality <ref type="bibr" target="#b13">(Salakhutdinov 2009;</ref>. <ref type="bibr" target="#b9">(Hinton, Osindero, and Teh 2006)</ref> first proposed to use the contrastive divergence algorithm to efficiently training deep belief nets (DBN).  proposed denoising autoencoder (DAE) that learns the data distribution in a supervised learning fashion. Both DBN and DAE learn a low dimensional representation (encoding) for each data instance and generate it from a decoding network. Recently, variational autoencoder (VAE) that combines deep learning with statistical inference intended to represent a data instance in a latent hidden space (Kingma and Welling 2014), while still utilizing (deep) neural networks for non-linear mapping. The inference is done via variational methods. All these generative models are trained by maximizing (the lower bound of) training data likelihood, which, as mentioned by (Goodfellow and others 2014), suffers from the difficulty of approximating intractable probabilistic computations. <ref type="bibr">(Goodfellow and others 2014)</ref> proposed an alternative training methodology to generative models, i.e. GANs, where the training procedure is a minimax game between a generative model and a discriminative model. This framework bypasses the difficulty of maximum likelihood learning and has gained striking successes in natural image generation <ref type="bibr" target="#b4">(Denton et al. 2015)</ref>. However, little progress has been made in applying GANs to sequence discrete data generation problems, e.g. natural language generation <ref type="bibr">(Husz?r 2015)</ref>. This is due to the generator network in GAN is designed to be able to adjust the output continuously, which does not work on discrete data generation <ref type="bibr" target="#b7">(Goodfellow 2016)</ref>.</p><p>On the other hand, a lot of efforts have been made to generate structured sequences. Recurrent neural networks can be trained to produce sequences of tokens in many applications such as machine translation <ref type="bibr" target="#b16">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b1">Bahdanau, Cho, and Bengio 2014)</ref>. The most popular way of training RNNs is to maximize the likelihood of each token in the training data whereas  pointed out that the discrepancy between training and generating makes the maximum likelihood estimation suboptimal and proposed scheduled sampling strategy (SS). Later (Husz?r 2015) theorized that the objective function underneath SS is improper and explained the reason why GANs tend to generate natural-looking samples in theory. Consequently, the GANs have great potential but are not practically feasible to discrete probabilistic models currently.</p><p>As pointed out by <ref type="bibr" target="#b0">(Bachman and Precup 2015)</ref>, the sequence data generation can be formulated as a sequential decision making process, which can be potentially be solved by reinforcement learning techniques. Modeling the sequence generator as a policy of picking the next token, policy gradient methods <ref type="bibr" target="#b17">(Sutton et al. 1999)</ref> can be adopted to optimize the generator once there is an (implicit) reward function to guide the policy. For most practical sequence generation tasks, e.g. machine translation <ref type="bibr" target="#b16">(Sutskever, Vinyals, and Le 2014)</ref>, the reward signal is meaningful only for the entire sequence, for instance in the game of Go <ref type="bibr" target="#b14">(Silver et al. 2016)</ref>, the reward signal is only set at the end of the game. In those cases, state-action evaluation methods such as Monte Carlo (tree) search have been adopted <ref type="bibr" target="#b3">(Browne et al. 2012)</ref>. By contract, our proposed SeqGAN extends GANs with the RL-based generator to solve the sequence generation problem, where a reward signal is provided by the discriminator at the end of each episode via Monte Carlo approach, and the generator picks the action and learns the policy using estimated overall rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Generative Adversarial Nets</head><p>The sequence generation problem is denoted as follows. Given a dataset of real-world structured sequences, train a ?-parameterized generative model G ? to produce a sequence Y 1:T = (y 1 , . . . , y t , . . . , y T ), y t ? Y, where Y is the vocabulary of candidate tokens. We interpret this problem based on reinforcement learning. In timestep t, the state s is the current produced tokens (y 1 , . . . , y t?1 ) and the action a is the next token y t to select. Thus the policy model G ? (y t |Y 1:t?1 ) is stochastic, whereas the state transition is deterministic after an action has been chosen, i.e. ? a s,s = 1 for the next state s = Y 1:t if the current state s = Y 1:t?1 and the action a = y t ; for other next states s , ? a s,s = 0. Additionally, we also train a ?-parameterized discriminative model D ? (Goodfellow and others 2014) to provide a guidance for improving generator G ? . D ? (Y 1:T ) is a probability indicating how likely a sequence Y 1:T is from real sequence data or not. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the dis- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SeqGAN via Policy Gradient</head><p>Following <ref type="bibr" target="#b17">(Sutton et al. 1999)</ref>, when there is no intermediate reward, the objective of the generator model (policy) G ? (y t |Y 1:t?1 ) is to generate a sequence from the start state s 0 to maximize its expected end reward:</p><formula xml:id="formula_0">J(?) = E[RT |s0, ?] = y 1 ?Y G ? (y1|s0) ? Q G ? D ? (s0, y1),<label>(1)</label></formula><p>where R T is the reward for a complete sequence. Note that the reward is from the discriminator D ? , which we will discuss later. Q G ? D ? (s, a) is the action-value function of a sequence, i.e. the expected accumulative reward starting from state s, taking action a, and then following policy G ? . The rational of the objective function for a sequence is that starting from a given initial state, the goal of the generator is to generate a sequence which would make the discriminator consider it is real.</p><p>The next question is how to estimate the action-value function. In this paper, we use the REINFORCE algorithm <ref type="bibr" target="#b20">(Williams 1992</ref>) and consider the estimated probability of being real by the discriminator D ? (Y n 1:T ) as the reward. Formally, we have:</p><formula xml:id="formula_1">Q G ? D ? (a = yT , s = Y1:T ?1) = D ? (Y1:T ).</formula><p>(2) However, the discriminator only provides a reward value for a finished sequence. Since we actually care about the longterm reward, at every timestep, we should not only consider the fitness of previous tokens (prefix) but also the resulted future outcome. This is similar to playing the games such as Go or Chess where players sometimes would give up the immediate interests for the long-term victory <ref type="bibr" target="#b14">(Silver et al. 2016)</ref>. Thus, to evaluate the action-value for an intermediate state, we apply Monte Carlo search with a roll-out policy G ? to sample the unknown last T ? t tokens. We represent an N -time Monte Carlo search as</p><formula xml:id="formula_2">Y 1 1:T , . . . , Y N 1:T = MC G ? (Y1:t; N ),<label>(3)</label></formula><p>where Y n 1:t = (y 1 , . . . , y t ) and Y n t+1:T is sampled based on the roll-out policy G ? and the current state. In our experiment, G ? is set the same as the generator, but one can use a simplified version if the speed is the priority <ref type="bibr" target="#b14">(Silver et al. 2016)</ref>. To reduce the variance and get more accurate assessment of the action value, we run the roll-out policy starting from current state till the end of the sequence for N times to get a batch of output samples. Thus, we have:</p><formula xml:id="formula_3">Q G ? D ? (s = Y1:t?1, a = yt) = (4) 1 N N n=1 D ? (Y n 1:T ), Y n 1:T ? MC G ? (Y1:t; N ) for t &lt; T D ? (Y1:t) for t = T ,</formula><p>where, we see that when no intermediate reward, the function is iteratively defined as the next-state value starting from state s = Y 1:t and rolling out to the end. A benefit of using the discriminator D ? as a reward function is that it can be dynamically updated to further improve the generative model iteratively. Once we have a set of more realistic generated sequences, we shall re-train the discriminator model as follows:</p><formula xml:id="formula_4">min ? ?EY ?p data [log D ? (Y )] ? EY ?G ? [log(1 ? D ? (Y ))]. (5)</formula><p>Each time when a new discriminator model has been obtained, we are ready to update the generator. The proposed policy based method relies upon optimizing a parametrized policy to directly maximize the long-term reward. Following <ref type="bibr" target="#b17">(Sutton et al. 1999)</ref>, the gradient of the objective function J(?) w.r.t. the generator's parameters ? can be derived as</p><formula xml:id="formula_5">? ? J(?) = T t=1 E Y1:t?1?G? yt?Y ? ? G ? (y t |Y 1:t?1 ) ? Q G? D? (Y 1:t?1 , y t ) .<label>(6)</label></formula><p>The above form is due to the deterministic state transition and zero intermediate rewards. The detailed derivation is provided in the appendix. Using likelihood ratios <ref type="bibr" target="#b5">(Glynn 1990;</ref><ref type="bibr" target="#b17">Sutton et al. 1999)</ref>, we build an unbiased estimation for Eq. (6) (on one episode):</p><formula xml:id="formula_6">??J(?) T t=1 yt?Y ??G?(yt|Y1:t?1) ? Q G ? D ? (Y1:t?1, yt) (7) = T t=1 yt?Y G?(yt|Y1:t?1)?? log G?(yt|Y1:t?1) ? Q G ? D ? (Y1:t?1, yt) = T t=1 E yt?G ? (yt|Y 1:t?1 ) [?? log G?(yt|Y1:t?1) ? Q G ? D ? (Y1:t?1, yt)],</formula><p>where Y 1:t?1 is the observed intermediate state sampled from G ? . Since the expectation E[?] can be approximated by sampling methods, we then update the generator's parameters as:</p><formula xml:id="formula_7">? ? ? + ? h ? ? J(?),<label>(8)</label></formula><p>where ? h ? R + denotes the corresponding learning rate at h-th step. Also the advanced gradient algorithms such as Adam and RMSprop can be adopted here.</p><p>In summary, Algorithm 1 shows full details of the proposed SeqGAN. At the beginning of the training, we use the maximum likelihood estimation (MLE) to pre-train G ? on Algorithm 1 Sequence Generative Adversarial Nets Require: generator policy G ? ; roll-out policy G ? ; discriminator D ? ; a sequence dataset S = {X1:T } 1: Initialize G ? , D ? with random weights ?, ?. 2: Pre-train G ? using MLE on S 3: ? ? ? 4: Generate negative samples using G ? for training D ? 5: Pre-train D ? via minimizing the cross entropy 6: repeat 7:</p><p>for g-steps do 8:</p><p>Generate a sequence Y1:T = (y1, . . . , yT ) ? G ? 9:</p><p>for t in 1 : T do 10:</p><p>Compute Q(a = yt; s = Y1:t?1) by Eq. <ref type="formula" target="#formula_0">(4)  11</ref>:</p><p>end for 12:</p><p>Update generator parameters via policy gradient Eq. <ref type="formula" target="#formula_0">(8)  13</ref>: end for 14:</p><p>for d-steps do 15:</p><p>Use current G ? to generate negative examples and combine with given positive examples S 16:</p><p>Train discriminator D ? for k epochs by Eq. <ref type="formula" target="#formula_0">(5)  17</ref>:</p><p>end for 18:</p><p>? ? ? 19: until SeqGAN converges training set S. We found the supervised signal from the pretrained discriminator is informative to help adjust the generator efficiently.</p><p>After the pre-training, the generator and discriminator are trained alternatively. As the generator gets progressed via training on g-steps updates, the discriminator needs to be retrained periodically to keeps a good pace with the generator. When training the discriminator, positive examples are from the given dataset S, whereas negative examples are generated from our generator. In order to keep the balance, the number of negative examples we generate for each d-step is the same as the positive examples. And to reduce the variability of the estimation, we use different sets of negative samples combined with positive ones, which is similar to bootstrapping <ref type="bibr" target="#b13">(Quinlan 1996)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Generative Model for Sequences</head><p>We use recurrent neural networks (RNNs) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997</ref>) as the generative model. An RNN maps the input embedding representations x 1 , . . . , x T of the sequence x 1 , . . . , x T into a sequence of hidden states h 1 , . . . , h T by using the update function g recursively.</p><formula xml:id="formula_8">ht = g(ht?1, xt)<label>(9)</label></formula><p>Moreover, a softmax output layer z maps the hidden states into the output token distribution</p><formula xml:id="formula_9">p(yt|x1, . . . , xt) = z(ht) = softmax(c + V ht),<label>(10)</label></formula><p>where the parameters are a bias vector c and a weight matrix V . To deal with the common vanishing and exploding gradient problem <ref type="bibr" target="#b7">(Goodfellow, Bengio, and Courville 2016)</ref> of the backpropagation through time, we leverage the Long Short-Term Memory (LSTM) cells <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997)</ref> to implement the update function g in Eq. (9). It is worth noticing that most of the RNN variants, such as the gated recurrent unit (GRU) ) and soft attention mechanism <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2014)</ref>, can be used as a generator in SeqGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Discriminative Model for Sequences</head><p>Deep discriminative models such as deep neural network (DNN) <ref type="bibr" target="#b18">(Vesel? et al. 2013)</ref>, convolutional neural network (CNN) <ref type="bibr" target="#b11">(Kim 2014</ref>) and recurrent convolutional neural network (RCNN) <ref type="bibr">(Lai et al. 2015)</ref> have shown a high performance in complicated sequence classification tasks. In this paper, we choose the CNN as our discriminator as CNN has recently been shown of great effectiveness in text (token sequence) classification <ref type="bibr" target="#b23">(Zhang and LeCun 2015)</ref>. Most discriminative models can only perform classification well for an entire sequence rather than the unfinished one. In this paper, we also focus on the situation where the discriminator predicts the probability that a finished sequence is real. <ref type="bibr">1</ref> We first represent an input sequence x 1 , . . . , x T as:</p><formula xml:id="formula_10">E1:T = x1 ? x2 ? . . . ? xT ,<label>(11)</label></formula><p>where x t ? R k is the k-dimensional token embedding and ? is the concatenation operator to build the matrix E 1:T ? R T ?k . Then a kernel w ? R l?k applies a convolutional operation to a window size of l words to produce a new feature map:</p><formula xml:id="formula_11">ci = ?(w ? E i:i+l?1 + b),<label>(12)</label></formula><p>where ? operator is the summation of elementwise production, b is a bias term and ? is a non-linear function. We can use various numbers of kernels with different window sizes to extract different features. Finally we apply a max-over-time pooling operation over the feature maps c = max {c 1 , . . . , c T ?l+1 }.</p><p>To enhance the performance, we also add the highway architecture <ref type="bibr" target="#b15">(Srivastava, Greff, and Schmidhuber 2015)</ref> based on the pooled feature maps. Finally, a fully connected layer with sigmoid activation is used to output the probability that the input sequence is real. The optimization target is to minimize the cross entropy between the ground truth label and the predicted probability as formulated in Eq. (5).</p><p>Detailed implementations of the generative and discriminative models are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Data Experiments</head><p>To test the efficacy and add our understanding of SeqGAN, we conduct a simulated test with synthetic data 2 . To simulate the real-world structured sequences, we consider a language model to capture the dependency of the tokens. We use a randomly initialized LSTM as the true model, aka, the oracle, to generate the real data distribution p(x t |x 1 , . . . , x t?1 ) for the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metric</head><p>The benefit of having such oracle is that firstly, it provides the training dataset and secondly evaluates the exact performance of the generative models, which will not be possible with real data. We know that MLE is trying to minimize the cross-entropy between the true data distribution p and our approximation q, i.e. ?E x?p log q(x). However, the most accurate way of evaluating generative models is that we draw some samples from it and let human observers review them based on their prior knowledge. We assume that the human observer has learned an accurate model of the natural distribution p human (x). Then in order to increase the chance of passing Turing Test, we actually need to minimize the exact opposite average negative log-likelihood ?E x?q log p human (x) (Husz?r 2015), with the role of p and q exchanged. In our synthetic data experiments, we can consider the oracle to be the human observer for real-world problems, thus a perfect evaluation metric should be</p><formula xml:id="formula_12">NLLoracle = ?EY 1:T ?G ? T t=1 log Goracle(yt|Y1:t?1) ,<label>(13)</label></formula><p>where G ? and G oracle denote our generative model and the oracle respectively. At the test stage, we use G ? to generate 100,000 sequence samples and calculate NLL oracle for each sample by G oracle and their average score. Also significance tests are performed to compare the statistical properties of the generation performance between the baselines and SeqGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Setting</head><p>To set up the synthetic data experiments, we first initialize the parameters of an LSTM network following the normal distribution N (0, 1) as the oracle describing the real data distribution G oracle (x t |x 1 , . . . , x t?1 ). Then we use it to generate 10,000 sequences of length 20 as the training set S for the generative models.</p><p>In SeqGAN algorithm, the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from S with the label 1. For different tasks, one should design specific structure for the convolutional layer and in our synthetic data experiments, the kernel size is from 1 to T and the number of each kernel size is between 100 to 200 3 . Dropout <ref type="bibr" target="#b14">(Srivastava et al. 2014</ref>) and L2 regularization are used to avoid over-fitting.</p><p>Four generative models are compared with SeqGAN. The first model is a random token generation. The second one is the MLE trained LSTM G ? . The third one is scheduled sampling . The fourth one is the Policy Gradient with BLEU (PG-BLEU). In the scheduled sampling, the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM, towards a less guided scheme which mostly feeds the LSTM with its generated tokens. A curriculum rate ? is used to control the probability of replacing the true tokens with the generated ones. To get a good and stable performance, we decrease ? by 0.002 for every training epoch. In the PG-BLEU algorithm, we use BLEU, a metric measuring the similarity between a generated sequence and references (training data), to score the finished samples from Monte Carlo search.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The NLL oracle performance of generating sequences from the compared policies is provided in <ref type="table" target="#tab_0">Table 1</ref>. Since the evaluation metric is fundamentally instructive, we can see the impact of SeqGAN, which outperforms other baselines significantly. A significance T-test on the NLL oracle score distribution of the generated sequences from the compared models is also performed, which demonstrates the significant improvement of SeqGAN over all compared models. The learning curves shown in <ref type="figure" target="#fig_3">Figure 4</ref> illustrate the superiority of SeqGAN explicitly. After about 150 training epochs, both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high NLL oracle score, whereas SeqGAN can improve the limit of the generator with the same structure as the baselines significantly. This indicates the prospect of applying adversarial training strategies to discrete sequence generative models to breakthrough the limitations of MLE. Additionally, SeqGAN outperforms PG-BLEU, which means the discriminative signal in GAN is more general and effective than a predefined score (e.g. BLEU) to guide the generative policy to capture the underlying distribution of the sequence data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In our synthetic data experiments, we find that the stability of SeqGAN depends on the training strategy. More specifically, the g-steps, d-steps and k parameters in Algorithm 1 have a large effect on the convergence and performance of SeqGAN. <ref type="figure" target="#fig_2">Figure 3</ref> shows the effect of these parameters. In <ref type="figure" target="#fig_2">Figure 3(a)</ref>, the g-steps is much larger than the d-steps and epoch number k, which means we train the generator for many times until we update the discriminator. This strategy leads to a fast convergence but as the generator improves quickly, the discriminator cannot get fully trained and thus will provide a misleading signal gradually. In <ref type="figure" target="#fig_2">Figure 3(b)</ref>, with more discriminator training epochs, the unstable training process is alleviated. In <ref type="figure" target="#fig_2">Figure 3(c)</ref>, we train the generator for only one epoch and then before the discriminator gets  fooled, we update it immediately based on the more realistic negative examples. In such a case, SeqGAN learns stably. The d-steps in all three training strategies described above is set to 1, which means we only generate one set of negative examples with the same number as the given dataset, and then train the discriminator on it for various k epochs. But actually we can utilize the potentially unlimited number of negative examples to improve the discriminator. This trick can be considered as a type of bootstrapping, where we combine the fixed positive examples with different negative examples to obtain multiple training sets. <ref type="figure" target="#fig_2">Figure 3(d)</ref> shows this technique can improve the overall performance with good stability, since the discriminator is shown more negative examples and each time the positive examples are emphasized, which will lead to a more comprehensive guidance for training generator. This is in line with the theorem in <ref type="bibr">(Goodfellow and others 2014)</ref>. When analyzing the convergence of generative adversarial nets, an important assumption is that the discriminator is allowed to reach its optimum given G. Only if the discriminator is capable of differentiating real data from unnatural data consistently, the supervised signal from it can be meaningful and the whole adversarial training process can be stable and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-world Scenarios</head><p>To complement the previous experiments, we also test Se-qGAN on several real-world tasks, i.e. poem composition, speech language generation and music generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Generation</head><p>For text generation scenarios, we apply the proposed Seq-GAN to generate Chinese poems and Barack Obama political speeches. In the poem composition task, we use a corpus 4 of 16,394 Chinese quatrains, each containing four lines   of twenty characters in total. To focus on a fully automatic solution and stay general, we did not use any prior knowledge of special structure rules in Chinese poems such as specific phonological rules. In the Obama political speech generation task, we use a corpus 5 , which is a collection of 11,092 paragraphs from Obama's political speeches. We use BLEU score as an evaluation metric to measure the similarity degree between the generated texts and the human-created texts. BLEU is originally designed to automatically judge the machine translation quality <ref type="bibr" target="#b12">(Papineni et al. 2002)</ref>. The key point is to compare the similarity between the results created by machine and the references provided by human. Specifically, for poem evaluation, we set n-gram to be 2 (BLEU-2) since most words (dependency) in classical Chinese poems consist of one or two characters <ref type="bibr" target="#b21">(Yi, Li, and Sun 2016)</ref> and for the similar reason, we use BLEU-3 and BLEU-4 to evaluate Obama speech generation performance. In our work, we use the whole test set as the references instead of trying to find some references for the following line given the previous line <ref type="bibr" target="#b8">(He, Zhou, and Jiang 2012)</ref>. The reason is in generation tasks we only provide some positive examples and then let the model catch the patterns of them and generate new ones. In addition to BLEU, we also choose poem generation as a case for human judgement since a poem is a creative text construction and human evaluation is ideal. Specifically, we mix the 20 real poems and 20 each generated from SeqGAN and MLE. Then 70 experts on Chinese poems are invited to judge whether each of the 60 poem is created by human or machines. Once regarded to be real, it gets +1 score, otherwise 0. Finally, the average score for each algorithm is calculated.</p><p>The experiment results are shown in <ref type="table" target="#tab_2">Tables 2 and 3</ref>, from which we can see the significant advantage of SeqGAN over the MLE in text generation. Particularly, for poem composition, SeqGAN performs comparably to real human data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Music Generation</head><p>For music composition, we use Nottingham 6 dataset as our training data, which is a collection of 695 music of folk tunes in midi file format. We study the solo track of each music. In our work, we use 88 numbers to represent 88 pitches, which correspond to the 88 keys on the piano. With the pitch sampling for every 0.4s 7 , we transform the midi files into sequences of numbers from 1 to 88 with the length 32.</p><p>To model the fitness of the discrete piano key patterns, BLEU is used as the evaluation metric. To model the fitness of the continuous pitch data patterns, the mean squared error (MSE) <ref type="bibr" target="#b12">(Manaris et al. 2007</ref>) is used for evaluation.</p><p>From <ref type="table" target="#tab_4">Table 4</ref>, we see that SeqGAN outperforms the MLE significantly in both metrics in the music generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a sequence generation method, SeqGAN, to effectively train generative adversarial nets for structured sequences generation via policy gradient. To our best knowledge, this is the first work extending GANs to generate sequences of discrete tokens. In our synthetic data experiments, we used an oracle evaluation mechanism to explicitly illustrate the superiority of SeqGAN over strong baselines. For three real-world scenarios, i.e., poems, speech language and music generation, SeqGAN showed excellent performance on generating the creative sequences. We also performed a set of experiments to investigate the robustness and stability of training SeqGAN. For future work, we plan to build Monte Carlo tree search and value network <ref type="bibr" target="#b14">(Silver et al. 2016)</ref> to improve action decision making for large scale data and in the case of longer-term planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Eq. (6)</head><p>For readability, we provide the detailed derivation of Eq. (6) here by following <ref type="bibr" target="#b17">(Sutton et al. 1999)</ref>.</p><p>As mentioned in SEQUENCE GENERATIVE ADVERSARIAL NETS section, the state transition is deterministic after an action has been chosen, i.e. ? a s,s = 1 for the next state s = Y 1:t if the current state s = Y 1:t?1 and the action a = y t ; for other next states s , ? a s,s = 0. In addition, the intermediate reward R a s is 0. We re-write the action value and state value as follows:</p><formula xml:id="formula_13">Q G ? (s = Y 1:t?1 , a = y t ) = R a s + s ?S ? a ss V G ? (s ) = V G ? (Y 1:t ) (14) V G ? (s = Y 1:t?1 ) = yt?Y G ? (y t |Y 1:t?1 ) ? Q G ? (Y 1:t?1 , y t )<label>(15)</label></formula><p>For the start state s 0 , the value is calculated as</p><formula xml:id="formula_14">V G ? (s 0 ) = E[R T |s 0 , ?] (16) = y1?Y G ? (y 1 |s 0 ) ? Q G ? (s 0 , y 1 ),</formula><p>which is the objective function J(?) to maximize in Eq.</p><p>(1) of the paper. Then we can obtain the gradient of the objective function, defined in Eq. (1), w.r.t. the generator's parameters ?:</p><formula xml:id="formula_15">? ? J(?) = ? ? V G ? (s0) = ? ? [ y 1 ?Y G ? (y1|s0) ? Q G ? (s0, y1)] = y 1 ?Y [? ? G ? (y1|s0) ? Q G ? (s0, y1) + G ? (y1|s0) ? ? ? Q G ? (s0, y1)] = y 1 ?Y [? ? G ? (y1|s0) ? Q G ? (s0, y1) + G ? (y1|s0) ? ? ? V G ? (Y1:1)] = y 1 ?Y ? ? G ? (y1|s0) ? Q G ? (s0, y1) + y 1 ?Y G ? (y1|s0)? ? [ y 2 ?Y G ? (y2|Y1:1)Q G ? (Y1:1, y2)] = y 1 ?Y ? ? G ? (y1|s0) ? Q G ? (s0, y1) + y 1 ?Y G ? (y1|s0) y 2 ?Y [? ? G ? (y2|Y1:1) ? Q G ? (Y1:1, y2) + G ? (y2|Y1:1)? ? Q G ? (Y1:1, y2)] = y 1 ?Y ? ? G ? (y1|s0) ? Q G ? (s0, y1) + Y 1:1 P (Y1:1|s0; G ? ) y 2 ?Y ? ? G ? (y2|Y1:1) ? Q G ? (Y1:1, y2) + Y 1:2 P (Y1:2|s0; G ? )? ? V G ? (Y1:2) = T t=1 Y 1:t?1 P (Y1:t?1|s0; G ? ) y t ?Y ? ? G ? (yt|Y1:t?1) ? Q G ? (Y1:t?1, yt) = T t=1 EY 1:t?1 ?G ? [ y t ?Y ? ? G ? (yt|Y1:t?1) ? Q G ? (Y1:t?1, yt)],<label>(17)</label></formula><p>which is the result in Eq. (6) of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Implementations</head><p>In this section, we present a full version of the discussed generative model and discriminative model in our paper submission.</p><p>The Generative Model for Sequences We use recurrent neural networks (RNNs) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997)</ref> as the generative model. An RNN maps the input embedding representations x 1 , . . . , x T of the sequence x 1 , . . . , x T into a sequence of hidden states h 1 , . . . , h T by using the update function g recursively.</p><formula xml:id="formula_16">h t = g(h t?1 , x t )<label>(18)</label></formula><p>Moreover, a softmax output layer z maps the hidden states into the output token distribution</p><formula xml:id="formula_17">p(y t |x 1 , . . . , x t ) = z(h t ) = softmax(c + V h t ),<label>(19)</label></formula><p>where the parameters are a bias vector c and a weight matrix V . The vanishing and exploding gradient problem in backpropagation through time (BPTT) issues a challenge of learning longterm dependencies to recurrent neural network <ref type="bibr" target="#b7">(Goodfellow, Bengio, and Courville 2016)</ref>. To address such problems, gated RNNs have been designed based on the basic idea of creating paths through time that have derivatives that neither vanish nor explode. Among various gated RNNs, we choose the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997)</ref> to be our generative networks with the update equations:</p><formula xml:id="formula_18">f t = ?(W f ? [h t?1 , x t ] + b f ), i t = ?(W i ? [h t?1 , x t ] + b i ), o t = ?(W o ? [h t?1 , x t ] + b o ), s t = f t s t?1 + i t tanh(W s ? [h t?1 , x t ] + b s ), h t = o t tanh(s t ),<label>(20)</label></formula><p>where [h, x] is the vector concatenation and is the elementwise product.</p><p>For simplicity, we use the standard LSTM as the generator, while it is worth noticing that most of the RNN variants, such as the gated recurrent unit (GRU) ) and soft attention mechanism <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2014)</ref>, can be used as a generator in SeqGAN.</p><p>The standard way of training an RNN G ? is the maximum likelihood estimation (MLE), which involves minimizing the negative log-likelihood ? T t=1 log G ? (y t = x t | {x 1 , . . . , x t?1 }) for a generated sequence (y 1 , . . . , y T ) given input (x 1 , . . . , x T ). However, when applying MLE to generative models, there is a discrepancy between training and generating <ref type="bibr">Husz?r 2015)</ref>, which motivates our work.</p><p>The Discriminative Model for Sequences Deep discriminative models such as deep neural network (DNN) <ref type="bibr" target="#b18">(Vesel? et al. 2013)</ref>, convolutional neural network (CNN) <ref type="bibr" target="#b11">(Kim 2014</ref>) and recurrent convolutional neural network (RCNN) <ref type="bibr">(Lai et al. 2015)</ref> have shown a high performance in complicated sequence classification tasks. In this paper, we choose the CNN as our discriminator as CNN has recently been shown of great effectiveness in text (token sequence) classification <ref type="bibr" target="#b23">(Zhang and LeCun 2015)</ref>.</p><p>As far as we know, except for some specific tasks, most discriminative models can only perform classification well for a whole sequence rather than the unfinished one. In case of some specific tasks, one may design a classifier to provide intermediate reward signal to enhance the performance of our framework. But to make it more general, we focus on the situation where discriminator can only provide final reward, i.e., the probability that a finished sequence was real.</p><p>We first represent an input sequence x 1 , . . . , x T as:</p><formula xml:id="formula_19">E 1:T = x 1 ? x 2 ? . . . ? x T ,<label>(21)</label></formula><p>where x t ? R k is the k-dimensional token embedding and ? is the vertical concatenation operator to build the matrix E 1:T ? R T ?k . Then a kernel w ? R l?k applies a convolutional operation to a window size of l words to produce a new feature map:</p><formula xml:id="formula_20">c i = ?(w ? E i:i+l?1 + b),<label>(22)</label></formula><p>where ? operator is the summation of elementwise production, b is a bias term and ? is a non-linear function. We can use various numbers of kernels with different window sizes to extract different features. Specifically, a kernel w with window size l applied to the concatenated embeddings of input sequence will produce a feature map</p><formula xml:id="formula_21">c = [c 1 , . . . , c T ?l+1 ].<label>(23)</label></formula><p>Finally we apply a max-over-time pooling operation over the feature mapc = max {c} and pass all pooled features from different kernels to a fully connected softmax layer to get the probability that a given sequence is real.</p><p>We perform an empirical experiment to choose the kernel window sizes and numbers as shown in <ref type="table" target="#tab_5">Table 5</ref>. For different tasks, one should design specific structures for the discriminator.</p><p>To enhance the performance, we also add the highway architecture <ref type="bibr" target="#b15">(Srivastava, Greff, and Schmidhuber 2015)</ref> before the final fully connected layer: </p><formula xml:id="formula_22">? = ?(W T ?c + b T ), C = ? ? H(c, W H ) + (1 ? ? ) ?c,<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20</head><p>(1, 100), <ref type="bibr">(2,</ref><ref type="bibr">200)</ref>, <ref type="bibr">(3,</ref><ref type="bibr">200)</ref>,(4, 200),(5, 200) (6, 100), <ref type="bibr" target="#b5">(7,</ref><ref type="bibr">100)</ref>,(8, 100),(9, 100),(10, 100) (15, 160),(20, 160)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>32</head><p>(1, 100), <ref type="bibr">(2,</ref><ref type="bibr">200)</ref>, <ref type="bibr">(3,</ref><ref type="bibr">200)</ref>,(4, 200),(5, 200) (6, 100), <ref type="bibr" target="#b5">(7,</ref><ref type="bibr">100)</ref>,(8, 100),(9, 100),(10, 100) (16, 160),(24, 160), <ref type="bibr">(32,</ref><ref type="bibr">160)</ref> where W T , b T and W H are highway layer weights, H denotes an affine transform followed by a non-linear activation function such as a rectified linear unit (ReLU) and ? is the "transform gate" with the same dimensionality as H(c, W H ) andc. Finally, we apply a sigmoid transformation to get the probability that a given sequence is real:</p><formula xml:id="formula_23">y = ?(W o ?C + b o )<label>(25)</label></formula><p>where W o and b o is the output layer weight and bias. When optimizing discriminative models, supervised training is applied to minimize the cross entropy, which is widely used as the objective function for classification and prediction tasks:</p><formula xml:id="formula_24">L(y,?) = ?y log? ? (1 ? y) log(1 ??),<label>(26)</label></formula><p>where y is the ground truth label of the input sequence and? is the predicted probability from the discriminative models. In the DISCUSSION subsection of SYNTHETIC DATA EXPERIMENTS section of our paper, we discussed the ablation study of three hyperparameters of SeqGAN, i.e., g-steps, d-steps and k epoch number. Here we provide another ablation study which is instructive for the better training of SeqGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Ablation Study</head><p>As described in our paper, we start the adversarial training process after the convergence of MLE supervised pre-training. Here we further conduct experiments to investigate the performance of SeqGAN when the supervised pre-training is insufficient.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, if we pre-train the generative model with conventional MLE methods for only 20 epochs, which is far from convergence, then the adversarial training process improves the generator quite slowly and unstably. The reason is that in SeqGAN, the discriminative model provides reward guidance when training the generator and if the generator acts almost randomly, the discriminator will identify the generated sequence to be unreal with high confidence and almost every action the generator takes receives a low (unified) reward, which does not guide the generator towards a good improvement direction, resulting in an ineffective training procedure. This indicates that in order to apply adversarial training strategies to sequence generative models, a sufficient pre-training is necessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The illustration of SeqGAN. Left: D is trained over the real data and the generated data by G. Right: G is trained by policy gradient where the final reward signal is provided by D and is passed back to the intermediate action value via Monte Carlo search.criminative model D ? is trained by providing positive examples from the real sequence data and negative examples from the synthetic sequences generated from the generative model G ? . At the same time, the generative model G ? is updated by employing a policy gradient and MC search on the basis of the expected end reward received from the discriminative model D ? . The reward is estimated by the likelihood that it would fool the discriminative model D ? . The specific formulation is given in the next subsection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Negative log-likelihood convergence w.r.t. the training epochs. The vertical dashed line represents the end of pre-training for SeqGAN, SS and PG-BLEU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Negative log-likelihood convergence performance of SeqGAN with different training strategies. The vertical dashed line represents the beginning of adversarial training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Negative log-likelihood performance with different pre-training epochs before the adversarial training. The vertical dashed lines represent the start of adversarial training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Sequence generation performance comparison. The p-value is between SeqGAN and the baseline from T-test.</figDesc><table><row><cell cols="4">Algorithm Random</cell><cell>MLE</cell><cell>SS</cell><cell cols="2">PG-BLEU SeqGAN</cell></row><row><cell>NLL</cell><cell></cell><cell>10.310</cell><cell></cell><cell>9.038</cell><cell>8.985</cell><cell>8.946</cell><cell>8.736</cell></row><row><cell cols="2">p-value</cell><cell cols="2">&lt; 10 ?6</cell><cell cols="2">&lt; 10 ?6 &lt; 10 ?6</cell><cell>&lt; 10 ?6</cell></row><row><cell>NLL by oracle</cell><cell>9.0 9.2 9.4 9.6 9.8 10.0</cell><cell></cell><cell></cell><cell cols="2">Learning curve</cell><cell cols="2">SeqGAN MLE Schedule Sampling PGBLEU</cell></row><row><cell></cell><cell>8.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>8.6</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150 Epochs</cell><cell>200</cell><cell>250</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Chinese poem generation performance comparison.</figDesc><table><row><cell cols="5">Algorithm Human score p-value BLEU-2 p-value</cell></row><row><cell>MLE SeqGAN</cell><cell>0.4165 0.5356</cell><cell>0.0034</cell><cell>0.6670 0.7389</cell><cell>&lt; 10 ?6</cell></row><row><cell>Real data</cell><cell>0.6011</cell><cell></cell><cell>0.746</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Obama political speech generation performance.</figDesc><table><row><cell cols="4">Algorithm BLEU-3 p-value BLEU-4</cell><cell>p-value</cell></row><row><cell>MLE SeqGAN</cell><cell>0.519 0.556</cell><cell>&lt; 10 ?6</cell><cell>0.416 0.427</cell><cell>0.00014</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Music generation performance comparison.</figDesc><table><row><cell cols="3">Algorithm BLEU-4 p-value</cell><cell>MSE</cell><cell>p-value</cell></row><row><cell>MLE SeqGAN</cell><cell>0.9210 0.9406</cell><cell cols="2">&lt; 10 ?6 22.38 0.00034 20.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Convolutional layer structures.</figDesc><table><row><cell>Sequence length</cell><cell>(window size, kernel numbers)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our work, the generated sequence has a fixed length T , but note that CNN is also capable of the variable-length sequence discrimination with the max-over-time pooling technique<ref type="bibr" target="#b11">(Kim 2014</ref>). 2 Experiment code: https://github.com/LantaoYu/SeqGAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Implementation details are in the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://homepages.inf.ed.ac.uk/mlap/Data/EMNLP14/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/samim23/obama-rnn 6 http://www.iro.umontreal.ca/?lisa/deep/data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank Tianxing He for many helpful discussions and comments on the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In Section 1, we present the step-by-step derivation of Eq. (6) in the paper. In Section 2, the detailed realization of the generative model and the discriminative model is discussed, including the model parameter settings. In Section 3, an interesting ablation study is provided, which is a supplementary to the discussions of the synthetic data experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data generation as sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3249" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalized denoising auto-encoders as generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<idno>arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of monte carlo tree search methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCIAIG</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
		<idno>Denton et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
	<note>Deep generative image models using a laplacian pyramid of adversarial networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Likelihood ratio gradient estimation for stochastic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glynn</surname></persName>
		</author>
		<ptr target="http://deeplearning.net/tutorial/rnnrbm.html" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Courville ; Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<ptr target="http://goo.gl/Wg9DR7" />
	</analytic>
	<monogr>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Generative adversarial networks for text</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating chinese classical poems with statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Jiang ; He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="169" to="186" />
		</imprint>
	</monogr>
	<note>A turing test for computer game bots</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osindero</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Teh ; Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05101</idno>
	</analytic>
	<monogr>
		<title level="m">Husz?r 2015] Husz?r, F. 2015. How (not) to train your generative model: Scheduled sampling, likelihood, adversary</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<editor>ICLR. [Lai et al. 2015] Lai,</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A corpus-based hybrid approach to music analysis and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NCAI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>725-730</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Bagging, boosting, and c4. 5. Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>JMLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greff</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">;</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence-discriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vesel?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2345" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01745</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01537</idno>
		<title level="m">Generating chinese classical poems with RNN encoder-decoder</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
